(function(){const H=document.createElement("link").relList;if(H&&H.supports&&H.supports("modulepreload"))return;for(const C of document.querySelectorAll('link[rel="modulepreload"]'))h(C);new MutationObserver(C=>{for(const E of C)if(E.type==="childList")for(const P of E.addedNodes)P.tagName==="LINK"&&P.rel==="modulepreload"&&h(P)}).observe(document,{childList:!0,subtree:!0});function M(C){const E={};return C.integrity&&(E.integrity=C.integrity),C.referrerPolicy&&(E.referrerPolicy=C.referrerPolicy),C.crossOrigin==="use-credentials"?E.credentials="include":C.crossOrigin==="anonymous"?E.credentials="omit":E.credentials="same-origin",E}function h(C){if(C.ep)return;C.ep=!0;const E=M(C);fetch(C.href,E)}})();function Lh(v){return v&&v.__esModule&&Object.prototype.hasOwnProperty.call(v,"default")?v.default:v}var ol={exports:{}},xi={};/**
 * @license React
 * react-jsx-runtime.production.js
 *
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var mh;function Jf(){if(mh)return xi;mh=1;var v=Symbol.for("react.transitional.element"),H=Symbol.for("react.fragment");function M(h,C,E){var P=null;if(E!==void 0&&(P=""+E),C.key!==void 0&&(P=""+C.key),"key"in C){E={};for(var K in C)K!=="key"&&(E[K]=C[K])}else E=C;return C=E.ref,{$$typeof:v,type:h,key:P,ref:C!==void 0?C:null,props:E}}return xi.Fragment=H,xi.jsx=M,xi.jsxs=M,xi}var gh;function $f(){return gh||(gh=1,ol.exports=Jf()),ol.exports}var A=$f(),rl={exports:{}},O={};/**
 * @license React
 * react.production.js
 *
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var yh;function em(){if(yh)return O;yh=1;var v=Symbol.for("react.transitional.element"),H=Symbol.for("react.portal"),M=Symbol.for("react.fragment"),h=Symbol.for("react.strict_mode"),C=Symbol.for("react.profiler"),E=Symbol.for("react.consumer"),P=Symbol.for("react.context"),K=Symbol.for("react.forward_ref"),L=Symbol.for("react.suspense"),y=Symbol.for("react.memo"),_=Symbol.for("react.lazy"),I=Symbol.for("react.activity"),ie=Symbol.iterator;function Pe(u){return u===null||typeof u!="object"?null:(u=ie&&u[ie]||u["@@iterator"],typeof u=="function"?u:null)}var we={isMounted:function(){return!1},enqueueForceUpdate:function(){},enqueueReplaceState:function(){},enqueueSetState:function(){}},re=Object.assign,De={};function je(u,k,q){this.props=u,this.context=k,this.refs=De,this.updater=q||we}je.prototype.isReactComponent={},je.prototype.setState=function(u,k){if(typeof u!="object"&&typeof u!="function"&&u!=null)throw Error("takes an object of state variables to update or a function which returns an object of state variables.");this.updater.enqueueSetState(this,u,k,"setState")},je.prototype.forceUpdate=function(u){this.updater.enqueueForceUpdate(this,u,"forceUpdate")};function St(){}St.prototype=je.prototype;function B(u,k,q){this.props=u,this.context=k,this.refs=De,this.updater=q||we}var pe=B.prototype=new St;pe.constructor=B,re(pe,je.prototype),pe.isPureReactComponent=!0;var le=Array.isArray;function Ee(){}var Y={H:null,A:null,T:null,S:null},Ne=Object.prototype.hasOwnProperty;function Qe(u,k,q){var U=q.ref;return{$$typeof:v,type:u,key:k,ref:U!==void 0?U:null,props:q}}function _a(u,k){return Qe(u.type,k,u.props)}function qt(u){return typeof u=="object"&&u!==null&&u.$$typeof===v}function Ye(u){var k={"=":"=0",":":"=2"};return"$"+u.replace(/[=:]/g,function(q){return k[q]})}var Ta=/\/+/g;function Dt(u,k){return typeof u=="object"&&u!==null&&u.key!=null?Ye(""+u.key):k.toString(36)}function xt(u){switch(u.status){case"fulfilled":return u.value;case"rejected":throw u.reason;default:switch(typeof u.status=="string"?u.then(Ee,Ee):(u.status="pending",u.then(function(k){u.status==="pending"&&(u.status="fulfilled",u.value=k)},function(k){u.status==="pending"&&(u.status="rejected",u.reason=k)})),u.status){case"fulfilled":return u.value;case"rejected":throw u.reason}}throw u}function w(u,k,q,U,j){var Z=typeof u;(Z==="undefined"||Z==="boolean")&&(u=null);var ne=!1;if(u===null)ne=!0;else switch(Z){case"bigint":case"string":case"number":ne=!0;break;case"object":switch(u.$$typeof){case v:case H:ne=!0;break;case _:return ne=u._init,w(ne(u._payload),k,q,U,j)}}if(ne)return j=j(u),ne=U===""?"."+Dt(u,0):U,le(j)?(q="",ne!=null&&(q=ne.replace(Ta,"$&/")+"/"),w(j,k,q,"",function(Rn){return Rn})):j!=null&&(qt(j)&&(j=_a(j,q+(j.key==null||u&&u.key===j.key?"":(""+j.key).replace(Ta,"$&/")+"/")+ne)),k.push(j)),1;ne=0;var Ge=U===""?".":U+":";if(le(u))for(var ke=0;ke<u.length;ke++)U=u[ke],Z=Ge+Dt(U,ke),ne+=w(U,k,q,Z,j);else if(ke=Pe(u),typeof ke=="function")for(u=ke.call(u),ke=0;!(U=u.next()).done;)U=U.value,Z=Ge+Dt(U,ke++),ne+=w(U,k,q,Z,j);else if(Z==="object"){if(typeof u.then=="function")return w(xt(u),k,q,U,j);throw k=String(u),Error("Objects are not valid as a React child (found: "+(k==="[object Object]"?"object with keys {"+Object.keys(u).join(", ")+"}":k)+"). If you meant to render a collection of children, use an array instead.")}return ne}function S(u,k,q){if(u==null)return u;var U=[],j=0;return w(u,U,"","",function(Z){return k.call(q,Z,j++)}),U}function W(u){if(u._status===-1){var k=u._result;k=k(),k.then(function(q){(u._status===0||u._status===-1)&&(u._status=1,u._result=q)},function(q){(u._status===0||u._status===-1)&&(u._status=2,u._result=q)}),u._status===-1&&(u._status=0,u._result=k)}if(u._status===1)return u._result.default;throw u._result}var ce=typeof reportError=="function"?reportError:function(u){if(typeof window=="object"&&typeof window.ErrorEvent=="function"){var k=new window.ErrorEvent("error",{bubbles:!0,cancelable:!0,message:typeof u=="object"&&u!==null&&typeof u.message=="string"?String(u.message):String(u),error:u});if(!window.dispatchEvent(k))return}else if(typeof process=="object"&&typeof process.emit=="function"){process.emit("uncaughtException",u);return}console.error(u)},fe={map:S,forEach:function(u,k,q){S(u,function(){k.apply(this,arguments)},q)},count:function(u){var k=0;return S(u,function(){k++}),k},toArray:function(u){return S(u,function(k){return k})||[]},only:function(u){if(!qt(u))throw Error("React.Children.only expected to receive a single React element child.");return u}};return O.Activity=I,O.Children=fe,O.Component=je,O.Fragment=M,O.Profiler=C,O.PureComponent=B,O.StrictMode=h,O.Suspense=L,O.__CLIENT_INTERNALS_DO_NOT_USE_OR_WARN_USERS_THEY_CANNOT_UPGRADE=Y,O.__COMPILER_RUNTIME={__proto__:null,c:function(u){return Y.H.useMemoCache(u)}},O.cache=function(u){return function(){return u.apply(null,arguments)}},O.cacheSignal=function(){return null},O.cloneElement=function(u,k,q){if(u==null)throw Error("The argument must be a React element, but you passed "+u+".");var U=re({},u.props),j=u.key;if(k!=null)for(Z in k.key!==void 0&&(j=""+k.key),k)!Ne.call(k,Z)||Z==="key"||Z==="__self"||Z==="__source"||Z==="ref"&&k.ref===void 0||(U[Z]=k[Z]);var Z=arguments.length-2;if(Z===1)U.children=q;else if(1<Z){for(var ne=Array(Z),Ge=0;Ge<Z;Ge++)ne[Ge]=arguments[Ge+2];U.children=ne}return Qe(u.type,j,U)},O.createContext=function(u){return u={$$typeof:P,_currentValue:u,_currentValue2:u,_threadCount:0,Provider:null,Consumer:null},u.Provider=u,u.Consumer={$$typeof:E,_context:u},u},O.createElement=function(u,k,q){var U,j={},Z=null;if(k!=null)for(U in k.key!==void 0&&(Z=""+k.key),k)Ne.call(k,U)&&U!=="key"&&U!=="__self"&&U!=="__source"&&(j[U]=k[U]);var ne=arguments.length-2;if(ne===1)j.children=q;else if(1<ne){for(var Ge=Array(ne),ke=0;ke<ne;ke++)Ge[ke]=arguments[ke+2];j.children=Ge}if(u&&u.defaultProps)for(U in ne=u.defaultProps,ne)j[U]===void 0&&(j[U]=ne[U]);return Qe(u,Z,j)},O.createRef=function(){return{current:null}},O.forwardRef=function(u){return{$$typeof:K,render:u}},O.isValidElement=qt,O.lazy=function(u){return{$$typeof:_,_payload:{_status:-1,_result:u},_init:W}},O.memo=function(u,k){return{$$typeof:y,type:u,compare:k===void 0?null:k}},O.startTransition=function(u){var k=Y.T,q={};Y.T=q;try{var U=u(),j=Y.S;j!==null&&j(q,U),typeof U=="object"&&U!==null&&typeof U.then=="function"&&U.then(Ee,ce)}catch(Z){ce(Z)}finally{k!==null&&q.types!==null&&(k.types=q.types),Y.T=k}},O.unstable_useCacheRefresh=function(){return Y.H.useCacheRefresh()},O.use=function(u){return Y.H.use(u)},O.useActionState=function(u,k,q){return Y.H.useActionState(u,k,q)},O.useCallback=function(u,k){return Y.H.useCallback(u,k)},O.useContext=function(u){return Y.H.useContext(u)},O.useDebugValue=function(){},O.useDeferredValue=function(u,k){return Y.H.useDeferredValue(u,k)},O.useEffect=function(u,k){return Y.H.useEffect(u,k)},O.useEffectEvent=function(u){return Y.H.useEffectEvent(u)},O.useId=function(){return Y.H.useId()},O.useImperativeHandle=function(u,k,q){return Y.H.useImperativeHandle(u,k,q)},O.useInsertionEffect=function(u,k){return Y.H.useInsertionEffect(u,k)},O.useLayoutEffect=function(u,k){return Y.H.useLayoutEffect(u,k)},O.useMemo=function(u,k){return Y.H.useMemo(u,k)},O.useOptimistic=function(u,k){return Y.H.useOptimistic(u,k)},O.useReducer=function(u,k,q){return Y.H.useReducer(u,k,q)},O.useRef=function(u){return Y.H.useRef(u)},O.useState=function(u){return Y.H.useState(u)},O.useSyncExternalStore=function(u,k,q){return Y.H.useSyncExternalStore(u,k,q)},O.useTransition=function(){return Y.H.useTransition()},O.version="19.2.4",O}var bh;function hl(){return bh||(bh=1,rl.exports=em()),rl.exports}var be=hl();const tm=Lh(be);var ll={exports:{}},Ti={},cl={exports:{}},ul={};/**
 * @license React
 * scheduler.production.js
 *
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var vh;function am(){return vh||(vh=1,(function(v){function H(w,S){var W=w.length;w.push(S);e:for(;0<W;){var ce=W-1>>>1,fe=w[ce];if(0<C(fe,S))w[ce]=S,w[W]=fe,W=ce;else break e}}function M(w){return w.length===0?null:w[0]}function h(w){if(w.length===0)return null;var S=w[0],W=w.pop();if(W!==S){w[0]=W;e:for(var ce=0,fe=w.length,u=fe>>>1;ce<u;){var k=2*(ce+1)-1,q=w[k],U=k+1,j=w[U];if(0>C(q,W))U<fe&&0>C(j,q)?(w[ce]=j,w[U]=W,ce=U):(w[ce]=q,w[k]=W,ce=k);else if(U<fe&&0>C(j,W))w[ce]=j,w[U]=W,ce=U;else break e}}return S}function C(w,S){var W=w.sortIndex-S.sortIndex;return W!==0?W:w.id-S.id}if(v.unstable_now=void 0,typeof performance=="object"&&typeof performance.now=="function"){var E=performance;v.unstable_now=function(){return E.now()}}else{var P=Date,K=P.now();v.unstable_now=function(){return P.now()-K}}var L=[],y=[],_=1,I=null,ie=3,Pe=!1,we=!1,re=!1,De=!1,je=typeof setTimeout=="function"?setTimeout:null,St=typeof clearTimeout=="function"?clearTimeout:null,B=typeof setImmediate<"u"?setImmediate:null;function pe(w){for(var S=M(y);S!==null;){if(S.callback===null)h(y);else if(S.startTime<=w)h(y),S.sortIndex=S.expirationTime,H(L,S);else break;S=M(y)}}function le(w){if(re=!1,pe(w),!we)if(M(L)!==null)we=!0,Ee||(Ee=!0,Ye());else{var S=M(y);S!==null&&xt(le,S.startTime-w)}}var Ee=!1,Y=-1,Ne=5,Qe=-1;function _a(){return De?!0:!(v.unstable_now()-Qe<Ne)}function qt(){if(De=!1,Ee){var w=v.unstable_now();Qe=w;var S=!0;try{e:{we=!1,re&&(re=!1,St(Y),Y=-1),Pe=!0;var W=ie;try{t:{for(pe(w),I=M(L);I!==null&&!(I.expirationTime>w&&_a());){var ce=I.callback;if(typeof ce=="function"){I.callback=null,ie=I.priorityLevel;var fe=ce(I.expirationTime<=w);if(w=v.unstable_now(),typeof fe=="function"){I.callback=fe,pe(w),S=!0;break t}I===M(L)&&h(L),pe(w)}else h(L);I=M(L)}if(I!==null)S=!0;else{var u=M(y);u!==null&&xt(le,u.startTime-w),S=!1}}break e}finally{I=null,ie=W,Pe=!1}S=void 0}}finally{S?Ye():Ee=!1}}}var Ye;if(typeof B=="function")Ye=function(){B(qt)};else if(typeof MessageChannel<"u"){var Ta=new MessageChannel,Dt=Ta.port2;Ta.port1.onmessage=qt,Ye=function(){Dt.postMessage(null)}}else Ye=function(){je(qt,0)};function xt(w,S){Y=je(function(){w(v.unstable_now())},S)}v.unstable_IdlePriority=5,v.unstable_ImmediatePriority=1,v.unstable_LowPriority=4,v.unstable_NormalPriority=3,v.unstable_Profiling=null,v.unstable_UserBlockingPriority=2,v.unstable_cancelCallback=function(w){w.callback=null},v.unstable_forceFrameRate=function(w){0>w||125<w?console.error("forceFrameRate takes a positive int between 0 and 125, forcing frame rates higher than 125 fps is not supported"):Ne=0<w?Math.floor(1e3/w):5},v.unstable_getCurrentPriorityLevel=function(){return ie},v.unstable_next=function(w){switch(ie){case 1:case 2:case 3:var S=3;break;default:S=ie}var W=ie;ie=S;try{return w()}finally{ie=W}},v.unstable_requestPaint=function(){De=!0},v.unstable_runWithPriority=function(w,S){switch(w){case 1:case 2:case 3:case 4:case 5:break;default:w=3}var W=ie;ie=w;try{return S()}finally{ie=W}},v.unstable_scheduleCallback=function(w,S,W){var ce=v.unstable_now();switch(typeof W=="object"&&W!==null?(W=W.delay,W=typeof W=="number"&&0<W?ce+W:ce):W=ce,w){case 1:var fe=-1;break;case 2:fe=250;break;case 5:fe=1073741823;break;case 4:fe=1e4;break;default:fe=5e3}return fe=W+fe,w={id:_++,callback:S,priorityLevel:w,startTime:W,expirationTime:fe,sortIndex:-1},W>ce?(w.sortIndex=W,H(y,w),M(L)===null&&w===M(y)&&(re?(St(Y),Y=-1):re=!0,xt(le,W-ce))):(w.sortIndex=fe,H(L,w),we||Pe||(we=!0,Ee||(Ee=!0,Ye()))),w},v.unstable_shouldYield=_a,v.unstable_wrapCallback=function(w){var S=ie;return function(){var W=ie;ie=S;try{return w.apply(this,arguments)}finally{ie=W}}}})(ul)),ul}var wh;function nm(){return wh||(wh=1,cl.exports=am()),cl.exports}var dl={exports:{}},_e={};/**
 * @license React
 * react-dom.production.js
 *
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var xh;function im(){if(xh)return _e;xh=1;var v=hl();function H(L){var y="https://react.dev/errors/"+L;if(1<arguments.length){y+="?args[]="+encodeURIComponent(arguments[1]);for(var _=2;_<arguments.length;_++)y+="&args[]="+encodeURIComponent(arguments[_])}return"Minified React error #"+L+"; visit "+y+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}function M(){}var h={d:{f:M,r:function(){throw Error(H(522))},D:M,C:M,L:M,m:M,X:M,S:M,M},p:0,findDOMNode:null},C=Symbol.for("react.portal");function E(L,y,_){var I=3<arguments.length&&arguments[3]!==void 0?arguments[3]:null;return{$$typeof:C,key:I==null?null:""+I,children:L,containerInfo:y,implementation:_}}var P=v.__CLIENT_INTERNALS_DO_NOT_USE_OR_WARN_USERS_THEY_CANNOT_UPGRADE;function K(L,y){if(L==="font")return"";if(typeof y=="string")return y==="use-credentials"?y:""}return _e.__DOM_INTERNALS_DO_NOT_USE_OR_WARN_USERS_THEY_CANNOT_UPGRADE=h,_e.createPortal=function(L,y){var _=2<arguments.length&&arguments[2]!==void 0?arguments[2]:null;if(!y||y.nodeType!==1&&y.nodeType!==9&&y.nodeType!==11)throw Error(H(299));return E(L,y,null,_)},_e.flushSync=function(L){var y=P.T,_=h.p;try{if(P.T=null,h.p=2,L)return L()}finally{P.T=y,h.p=_,h.d.f()}},_e.preconnect=function(L,y){typeof L=="string"&&(y?(y=y.crossOrigin,y=typeof y=="string"?y==="use-credentials"?y:"":void 0):y=null,h.d.C(L,y))},_e.prefetchDNS=function(L){typeof L=="string"&&h.d.D(L)},_e.preinit=function(L,y){if(typeof L=="string"&&y&&typeof y.as=="string"){var _=y.as,I=K(_,y.crossOrigin),ie=typeof y.integrity=="string"?y.integrity:void 0,Pe=typeof y.fetchPriority=="string"?y.fetchPriority:void 0;_==="style"?h.d.S(L,typeof y.precedence=="string"?y.precedence:void 0,{crossOrigin:I,integrity:ie,fetchPriority:Pe}):_==="script"&&h.d.X(L,{crossOrigin:I,integrity:ie,fetchPriority:Pe,nonce:typeof y.nonce=="string"?y.nonce:void 0})}},_e.preinitModule=function(L,y){if(typeof L=="string")if(typeof y=="object"&&y!==null){if(y.as==null||y.as==="script"){var _=K(y.as,y.crossOrigin);h.d.M(L,{crossOrigin:_,integrity:typeof y.integrity=="string"?y.integrity:void 0,nonce:typeof y.nonce=="string"?y.nonce:void 0})}}else y==null&&h.d.M(L)},_e.preload=function(L,y){if(typeof L=="string"&&typeof y=="object"&&y!==null&&typeof y.as=="string"){var _=y.as,I=K(_,y.crossOrigin);h.d.L(L,_,{crossOrigin:I,integrity:typeof y.integrity=="string"?y.integrity:void 0,nonce:typeof y.nonce=="string"?y.nonce:void 0,type:typeof y.type=="string"?y.type:void 0,fetchPriority:typeof y.fetchPriority=="string"?y.fetchPriority:void 0,referrerPolicy:typeof y.referrerPolicy=="string"?y.referrerPolicy:void 0,imageSrcSet:typeof y.imageSrcSet=="string"?y.imageSrcSet:void 0,imageSizes:typeof y.imageSizes=="string"?y.imageSizes:void 0,media:typeof y.media=="string"?y.media:void 0})}},_e.preloadModule=function(L,y){if(typeof L=="string")if(y){var _=K(y.as,y.crossOrigin);h.d.m(L,{as:typeof y.as=="string"&&y.as!=="script"?y.as:void 0,crossOrigin:_,integrity:typeof y.integrity=="string"?y.integrity:void 0})}else h.d.m(L)},_e.requestFormReset=function(L){h.d.r(L)},_e.unstable_batchedUpdates=function(L,y){return L(y)},_e.useFormState=function(L,y,_){return P.H.useFormState(L,y,_)},_e.useFormStatus=function(){return P.H.useHostTransitionStatus()},_e.version="19.2.4",_e}var Th;function sm(){if(Th)return dl.exports;Th=1;function v(){if(!(typeof __REACT_DEVTOOLS_GLOBAL_HOOK__>"u"||typeof __REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE!="function"))try{__REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE(v)}catch(H){console.error(H)}}return v(),dl.exports=im(),dl.exports}/**
 * @license React
 * react-dom-client.production.js
 *
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var kh;function om(){if(kh)return Ti;kh=1;var v=nm(),H=hl(),M=sm();function h(e){var t="https://react.dev/errors/"+e;if(1<arguments.length){t+="?args[]="+encodeURIComponent(arguments[1]);for(var a=2;a<arguments.length;a++)t+="&args[]="+encodeURIComponent(arguments[a])}return"Minified React error #"+e+"; visit "+t+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}function C(e){return!(!e||e.nodeType!==1&&e.nodeType!==9&&e.nodeType!==11)}function E(e){var t=e,a=e;if(e.alternate)for(;t.return;)t=t.return;else{e=t;do t=e,(t.flags&4098)!==0&&(a=t.return),e=t.return;while(e)}return t.tag===3?a:null}function P(e){if(e.tag===13){var t=e.memoizedState;if(t===null&&(e=e.alternate,e!==null&&(t=e.memoizedState)),t!==null)return t.dehydrated}return null}function K(e){if(e.tag===31){var t=e.memoizedState;if(t===null&&(e=e.alternate,e!==null&&(t=e.memoizedState)),t!==null)return t.dehydrated}return null}function L(e){if(E(e)!==e)throw Error(h(188))}function y(e){var t=e.alternate;if(!t){if(t=E(e),t===null)throw Error(h(188));return t!==e?null:e}for(var a=e,n=t;;){var i=a.return;if(i===null)break;var s=i.alternate;if(s===null){if(n=i.return,n!==null){a=n;continue}break}if(i.child===s.child){for(s=i.child;s;){if(s===a)return L(i),e;if(s===n)return L(i),t;s=s.sibling}throw Error(h(188))}if(a.return!==n.return)a=i,n=s;else{for(var o=!1,r=i.child;r;){if(r===a){o=!0,a=i,n=s;break}if(r===n){o=!0,n=i,a=s;break}r=r.sibling}if(!o){for(r=s.child;r;){if(r===a){o=!0,a=s,n=i;break}if(r===n){o=!0,n=s,a=i;break}r=r.sibling}if(!o)throw Error(h(189))}}if(a.alternate!==n)throw Error(h(190))}if(a.tag!==3)throw Error(h(188));return a.stateNode.current===a?e:t}function _(e){var t=e.tag;if(t===5||t===26||t===27||t===6)return e;for(e=e.child;e!==null;){if(t=_(e),t!==null)return t;e=e.sibling}return null}var I=Object.assign,ie=Symbol.for("react.element"),Pe=Symbol.for("react.transitional.element"),we=Symbol.for("react.portal"),re=Symbol.for("react.fragment"),De=Symbol.for("react.strict_mode"),je=Symbol.for("react.profiler"),St=Symbol.for("react.consumer"),B=Symbol.for("react.context"),pe=Symbol.for("react.forward_ref"),le=Symbol.for("react.suspense"),Ee=Symbol.for("react.suspense_list"),Y=Symbol.for("react.memo"),Ne=Symbol.for("react.lazy"),Qe=Symbol.for("react.activity"),_a=Symbol.for("react.memo_cache_sentinel"),qt=Symbol.iterator;function Ye(e){return e===null||typeof e!="object"?null:(e=qt&&e[qt]||e["@@iterator"],typeof e=="function"?e:null)}var Ta=Symbol.for("react.client.reference");function Dt(e){if(e==null)return null;if(typeof e=="function")return e.$$typeof===Ta?null:e.displayName||e.name||null;if(typeof e=="string")return e;switch(e){case re:return"Fragment";case je:return"Profiler";case De:return"StrictMode";case le:return"Suspense";case Ee:return"SuspenseList";case Qe:return"Activity"}if(typeof e=="object")switch(e.$$typeof){case we:return"Portal";case B:return e.displayName||"Context";case St:return(e._context.displayName||"Context")+".Consumer";case pe:var t=e.render;return e=e.displayName,e||(e=t.displayName||t.name||"",e=e!==""?"ForwardRef("+e+")":"ForwardRef"),e;case Y:return t=e.displayName||null,t!==null?t:Dt(e.type)||"Memo";case Ne:t=e._payload,e=e._init;try{return Dt(e(t))}catch{}}return null}var xt=Array.isArray,w=H.__CLIENT_INTERNALS_DO_NOT_USE_OR_WARN_USERS_THEY_CANNOT_UPGRADE,S=M.__DOM_INTERNALS_DO_NOT_USE_OR_WARN_USERS_THEY_CANNOT_UPGRADE,W={pending:!1,data:null,method:null,action:null},ce=[],fe=-1;function u(e){return{current:e}}function k(e){0>fe||(e.current=ce[fe],ce[fe]=null,fe--)}function q(e,t){fe++,ce[fe]=e.current,e.current=t}var U=u(null),j=u(null),Z=u(null),ne=u(null);function Ge(e,t){switch(q(Z,t),q(j,e),q(U,null),t.nodeType){case 9:case 11:e=(e=t.documentElement)&&(e=e.namespaceURI)?Hd(e):0;break;default:if(e=t.tagName,t=t.namespaceURI)t=Hd(t),e=Wd(t,e);else switch(e){case"svg":e=1;break;case"math":e=2;break;default:e=0}}k(U),q(U,e)}function ke(){k(U),k(j),k(Z)}function Rn(e){e.memoizedState!==null&&q(ne,e);var t=U.current,a=Wd(t,e.type);t!==a&&(q(j,e),q(U,a))}function ki(e){j.current===e&&(k(U),k(j)),ne.current===e&&(k(ne),yi._currentValue=W)}var Os,fl;function ka(e){if(Os===void 0)try{throw Error()}catch(a){var t=a.stack.trim().match(/\n( *(at )?)/);Os=t&&t[1]||"",fl=-1<a.stack.indexOf(`
    at`)?" (<anonymous>)":-1<a.stack.indexOf("@")?"@unknown:0:0":""}return`
`+Os+e+fl}var _s=!1;function js(e,t){if(!e||_s)return"";_s=!0;var a=Error.prepareStackTrace;Error.prepareStackTrace=void 0;try{var n={DetermineComponentFrameRoot:function(){try{if(t){var T=function(){throw Error()};if(Object.defineProperty(T.prototype,"props",{set:function(){throw Error()}}),typeof Reflect=="object"&&Reflect.construct){try{Reflect.construct(T,[])}catch(g){var m=g}Reflect.construct(e,[],T)}else{try{T.call()}catch(g){m=g}e.call(T.prototype)}}else{try{throw Error()}catch(g){m=g}(T=e())&&typeof T.catch=="function"&&T.catch(function(){})}}catch(g){if(g&&m&&typeof g.stack=="string")return[g.stack,m.stack]}return[null,null]}};n.DetermineComponentFrameRoot.displayName="DetermineComponentFrameRoot";var i=Object.getOwnPropertyDescriptor(n.DetermineComponentFrameRoot,"name");i&&i.configurable&&Object.defineProperty(n.DetermineComponentFrameRoot,"name",{value:"DetermineComponentFrameRoot"});var s=n.DetermineComponentFrameRoot(),o=s[0],r=s[1];if(o&&r){var l=o.split(`
`),f=r.split(`
`);for(i=n=0;n<l.length&&!l[n].includes("DetermineComponentFrameRoot");)n++;for(;i<f.length&&!f[i].includes("DetermineComponentFrameRoot");)i++;if(n===l.length||i===f.length)for(n=l.length-1,i=f.length-1;1<=n&&0<=i&&l[n]!==f[i];)i--;for(;1<=n&&0<=i;n--,i--)if(l[n]!==f[i]){if(n!==1||i!==1)do if(n--,i--,0>i||l[n]!==f[i]){var b=`
`+l[n].replace(" at new "," at ");return e.displayName&&b.includes("<anonymous>")&&(b=b.replace("<anonymous>",e.displayName)),b}while(1<=n&&0<=i);break}}}finally{_s=!1,Error.prepareStackTrace=a}return(a=e?e.displayName||e.name:"")?ka(a):""}function Rh(e,t){switch(e.tag){case 26:case 27:case 5:return ka(e.type);case 16:return ka("Lazy");case 13:return e.child!==t&&t!==null?ka("Suspense Fallback"):ka("Suspense");case 19:return ka("SuspenseList");case 0:case 15:return js(e.type,!1);case 11:return js(e.type.render,!1);case 1:return js(e.type,!0);case 31:return ka("Activity");default:return""}}function ml(e){try{var t="",a=null;do t+=Rh(e,a),a=e,e=e.return;while(e);return t}catch(n){return`
Error generating stack: `+n.message+`
`+n.stack}}var Qs=Object.prototype.hasOwnProperty,Gs=v.unstable_scheduleCallback,Ks=v.unstable_cancelCallback,Ch=v.unstable_shouldYield,Uh=v.unstable_requestPaint,tt=v.unstable_now,Dh=v.unstable_getCurrentPriorityLevel,gl=v.unstable_ImmediatePriority,yl=v.unstable_UserBlockingPriority,Ai=v.unstable_NormalPriority,Ih=v.unstable_LowPriority,bl=v.unstable_IdlePriority,zh=v.log,Ph=v.unstable_setDisableYieldValue,Cn=null,at=null;function Vt(e){if(typeof zh=="function"&&Ph(e),at&&typeof at.setStrictMode=="function")try{at.setStrictMode(Cn,e)}catch{}}var nt=Math.clz32?Math.clz32:Bh,Eh=Math.log,Nh=Math.LN2;function Bh(e){return e>>>=0,e===0?32:31-(Eh(e)/Nh|0)|0}var Si=256,qi=262144,Li=4194304;function Aa(e){var t=e&42;if(t!==0)return t;switch(e&-e){case 1:return 1;case 2:return 2;case 4:return 4;case 8:return 8;case 16:return 16;case 32:return 32;case 64:return 64;case 128:return 128;case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:return e&261888;case 262144:case 524288:case 1048576:case 2097152:return e&3932160;case 4194304:case 8388608:case 16777216:case 33554432:return e&62914560;case 67108864:return 67108864;case 134217728:return 134217728;case 268435456:return 268435456;case 536870912:return 536870912;case 1073741824:return 0;default:return e}}function Ri(e,t,a){var n=e.pendingLanes;if(n===0)return 0;var i=0,s=e.suspendedLanes,o=e.pingedLanes;e=e.warmLanes;var r=n&134217727;return r!==0?(n=r&~s,n!==0?i=Aa(n):(o&=r,o!==0?i=Aa(o):a||(a=r&~e,a!==0&&(i=Aa(a))))):(r=n&~s,r!==0?i=Aa(r):o!==0?i=Aa(o):a||(a=n&~e,a!==0&&(i=Aa(a)))),i===0?0:t!==0&&t!==i&&(t&s)===0&&(s=i&-i,a=t&-t,s>=a||s===32&&(a&4194048)!==0)?t:i}function Un(e,t){return(e.pendingLanes&~(e.suspendedLanes&~e.pingedLanes)&t)===0}function Hh(e,t){switch(e){case 1:case 2:case 4:case 8:case 64:return t+250;case 16:case 32:case 128:case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:return t+5e3;case 4194304:case 8388608:case 16777216:case 33554432:return-1;case 67108864:case 134217728:case 268435456:case 536870912:case 1073741824:return-1;default:return-1}}function vl(){var e=Li;return Li<<=1,(Li&62914560)===0&&(Li=4194304),e}function Ys(e){for(var t=[],a=0;31>a;a++)t.push(e);return t}function Dn(e,t){e.pendingLanes|=t,t!==268435456&&(e.suspendedLanes=0,e.pingedLanes=0,e.warmLanes=0)}function Wh(e,t,a,n,i,s){var o=e.pendingLanes;e.pendingLanes=a,e.suspendedLanes=0,e.pingedLanes=0,e.warmLanes=0,e.expiredLanes&=a,e.entangledLanes&=a,e.errorRecoveryDisabledLanes&=a,e.shellSuspendCounter=0;var r=e.entanglements,l=e.expirationTimes,f=e.hiddenUpdates;for(a=o&~a;0<a;){var b=31-nt(a),T=1<<b;r[b]=0,l[b]=-1;var m=f[b];if(m!==null)for(f[b]=null,b=0;b<m.length;b++){var g=m[b];g!==null&&(g.lane&=-536870913)}a&=~T}n!==0&&wl(e,n,0),s!==0&&i===0&&e.tag!==0&&(e.suspendedLanes|=s&~(o&~t))}function wl(e,t,a){e.pendingLanes|=t,e.suspendedLanes&=~t;var n=31-nt(t);e.entangledLanes|=t,e.entanglements[n]=e.entanglements[n]|1073741824|a&261930}function xl(e,t){var a=e.entangledLanes|=t;for(e=e.entanglements;a;){var n=31-nt(a),i=1<<n;i&t|e[n]&t&&(e[n]|=t),a&=~i}}function Tl(e,t){var a=t&-t;return a=(a&42)!==0?1:Zs(a),(a&(e.suspendedLanes|t))!==0?0:a}function Zs(e){switch(e){case 2:e=1;break;case 8:e=4;break;case 32:e=16;break;case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:case 4194304:case 8388608:case 16777216:case 33554432:e=128;break;case 268435456:e=134217728;break;default:e=0}return e}function Xs(e){return e&=-e,2<e?8<e?(e&134217727)!==0?32:268435456:8:2}function kl(){var e=S.p;return e!==0?e:(e=window.event,e===void 0?32:lh(e.type))}function Al(e,t){var a=S.p;try{return S.p=e,t()}finally{S.p=a}}var Ft=Math.random().toString(36).slice(2),Be="__reactFiber$"+Ft,Ze="__reactProps$"+Ft,ja="__reactContainer$"+Ft,Vs="__reactEvents$"+Ft,Mh="__reactListeners$"+Ft,Oh="__reactHandles$"+Ft,Sl="__reactResources$"+Ft,In="__reactMarker$"+Ft;function Fs(e){delete e[Be],delete e[Ze],delete e[Vs],delete e[Mh],delete e[Oh]}function Qa(e){var t=e[Be];if(t)return t;for(var a=e.parentNode;a;){if(t=a[ja]||a[Be]){if(a=t.alternate,t.child!==null||a!==null&&a.child!==null)for(e=Kd(e);e!==null;){if(a=e[Be])return a;e=Kd(e)}return t}e=a,a=e.parentNode}return null}function Ga(e){if(e=e[Be]||e[ja]){var t=e.tag;if(t===5||t===6||t===13||t===31||t===26||t===27||t===3)return e}return null}function zn(e){var t=e.tag;if(t===5||t===26||t===27||t===6)return e.stateNode;throw Error(h(33))}function Ka(e){var t=e[Sl];return t||(t=e[Sl]={hoistableStyles:new Map,hoistableScripts:new Map}),t}function Ie(e){e[In]=!0}var ql=new Set,Ll={};function Sa(e,t){Ya(e,t),Ya(e+"Capture",t)}function Ya(e,t){for(Ll[e]=t,e=0;e<t.length;e++)ql.add(t[e])}var _h=RegExp("^[:A-Z_a-z\\u00C0-\\u00D6\\u00D8-\\u00F6\\u00F8-\\u02FF\\u0370-\\u037D\\u037F-\\u1FFF\\u200C-\\u200D\\u2070-\\u218F\\u2C00-\\u2FEF\\u3001-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFFD][:A-Z_a-z\\u00C0-\\u00D6\\u00D8-\\u00F6\\u00F8-\\u02FF\\u0370-\\u037D\\u037F-\\u1FFF\\u200C-\\u200D\\u2070-\\u218F\\u2C00-\\u2FEF\\u3001-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFFD\\-.0-9\\u00B7\\u0300-\\u036F\\u203F-\\u2040]*$"),Rl={},Cl={};function jh(e){return Qs.call(Cl,e)?!0:Qs.call(Rl,e)?!1:_h.test(e)?Cl[e]=!0:(Rl[e]=!0,!1)}function Ci(e,t,a){if(jh(t))if(a===null)e.removeAttribute(t);else{switch(typeof a){case"undefined":case"function":case"symbol":e.removeAttribute(t);return;case"boolean":var n=t.toLowerCase().slice(0,5);if(n!=="data-"&&n!=="aria-"){e.removeAttribute(t);return}}e.setAttribute(t,""+a)}}function Ui(e,t,a){if(a===null)e.removeAttribute(t);else{switch(typeof a){case"undefined":case"function":case"symbol":case"boolean":e.removeAttribute(t);return}e.setAttribute(t,""+a)}}function It(e,t,a,n){if(n===null)e.removeAttribute(a);else{switch(typeof n){case"undefined":case"function":case"symbol":case"boolean":e.removeAttribute(a);return}e.setAttributeNS(t,a,""+n)}}function dt(e){switch(typeof e){case"bigint":case"boolean":case"number":case"string":case"undefined":return e;case"object":return e;default:return""}}function Ul(e){var t=e.type;return(e=e.nodeName)&&e.toLowerCase()==="input"&&(t==="checkbox"||t==="radio")}function Qh(e,t,a){var n=Object.getOwnPropertyDescriptor(e.constructor.prototype,t);if(!e.hasOwnProperty(t)&&typeof n<"u"&&typeof n.get=="function"&&typeof n.set=="function"){var i=n.get,s=n.set;return Object.defineProperty(e,t,{configurable:!0,get:function(){return i.call(this)},set:function(o){a=""+o,s.call(this,o)}}),Object.defineProperty(e,t,{enumerable:n.enumerable}),{getValue:function(){return a},setValue:function(o){a=""+o},stopTracking:function(){e._valueTracker=null,delete e[t]}}}}function Js(e){if(!e._valueTracker){var t=Ul(e)?"checked":"value";e._valueTracker=Qh(e,t,""+e[t])}}function Dl(e){if(!e)return!1;var t=e._valueTracker;if(!t)return!0;var a=t.getValue(),n="";return e&&(n=Ul(e)?e.checked?"true":"false":e.value),e=n,e!==a?(t.setValue(e),!0):!1}function Di(e){if(e=e||(typeof document<"u"?document:void 0),typeof e>"u")return null;try{return e.activeElement||e.body}catch{return e.body}}var Gh=/[\n"\\]/g;function ht(e){return e.replace(Gh,function(t){return"\\"+t.charCodeAt(0).toString(16)+" "})}function $s(e,t,a,n,i,s,o,r){e.name="",o!=null&&typeof o!="function"&&typeof o!="symbol"&&typeof o!="boolean"?e.type=o:e.removeAttribute("type"),t!=null?o==="number"?(t===0&&e.value===""||e.value!=t)&&(e.value=""+dt(t)):e.value!==""+dt(t)&&(e.value=""+dt(t)):o!=="submit"&&o!=="reset"||e.removeAttribute("value"),t!=null?eo(e,o,dt(t)):a!=null?eo(e,o,dt(a)):n!=null&&e.removeAttribute("value"),i==null&&s!=null&&(e.defaultChecked=!!s),i!=null&&(e.checked=i&&typeof i!="function"&&typeof i!="symbol"),r!=null&&typeof r!="function"&&typeof r!="symbol"&&typeof r!="boolean"?e.name=""+dt(r):e.removeAttribute("name")}function Il(e,t,a,n,i,s,o,r){if(s!=null&&typeof s!="function"&&typeof s!="symbol"&&typeof s!="boolean"&&(e.type=s),t!=null||a!=null){if(!(s!=="submit"&&s!=="reset"||t!=null)){Js(e);return}a=a!=null?""+dt(a):"",t=t!=null?""+dt(t):a,r||t===e.value||(e.value=t),e.defaultValue=t}n=n??i,n=typeof n!="function"&&typeof n!="symbol"&&!!n,e.checked=r?e.checked:!!n,e.defaultChecked=!!n,o!=null&&typeof o!="function"&&typeof o!="symbol"&&typeof o!="boolean"&&(e.name=o),Js(e)}function eo(e,t,a){t==="number"&&Di(e.ownerDocument)===e||e.defaultValue===""+a||(e.defaultValue=""+a)}function Za(e,t,a,n){if(e=e.options,t){t={};for(var i=0;i<a.length;i++)t["$"+a[i]]=!0;for(a=0;a<e.length;a++)i=t.hasOwnProperty("$"+e[a].value),e[a].selected!==i&&(e[a].selected=i),i&&n&&(e[a].defaultSelected=!0)}else{for(a=""+dt(a),t=null,i=0;i<e.length;i++){if(e[i].value===a){e[i].selected=!0,n&&(e[i].defaultSelected=!0);return}t!==null||e[i].disabled||(t=e[i])}t!==null&&(t.selected=!0)}}function zl(e,t,a){if(t!=null&&(t=""+dt(t),t!==e.value&&(e.value=t),a==null)){e.defaultValue!==t&&(e.defaultValue=t);return}e.defaultValue=a!=null?""+dt(a):""}function Pl(e,t,a,n){if(t==null){if(n!=null){if(a!=null)throw Error(h(92));if(xt(n)){if(1<n.length)throw Error(h(93));n=n[0]}a=n}a==null&&(a=""),t=a}a=dt(t),e.defaultValue=a,n=e.textContent,n===a&&n!==""&&n!==null&&(e.value=n),Js(e)}function Xa(e,t){if(t){var a=e.firstChild;if(a&&a===e.lastChild&&a.nodeType===3){a.nodeValue=t;return}}e.textContent=t}var Kh=new Set("animationIterationCount aspectRatio borderImageOutset borderImageSlice borderImageWidth boxFlex boxFlexGroup boxOrdinalGroup columnCount columns flex flexGrow flexPositive flexShrink flexNegative flexOrder gridArea gridRow gridRowEnd gridRowSpan gridRowStart gridColumn gridColumnEnd gridColumnSpan gridColumnStart fontWeight lineClamp lineHeight opacity order orphans scale tabSize widows zIndex zoom fillOpacity floodOpacity stopOpacity strokeDasharray strokeDashoffset strokeMiterlimit strokeOpacity strokeWidth MozAnimationIterationCount MozBoxFlex MozBoxFlexGroup MozLineClamp msAnimationIterationCount msFlex msZoom msFlexGrow msFlexNegative msFlexOrder msFlexPositive msFlexShrink msGridColumn msGridColumnSpan msGridRow msGridRowSpan WebkitAnimationIterationCount WebkitBoxFlex WebKitBoxFlexGroup WebkitBoxOrdinalGroup WebkitColumnCount WebkitColumns WebkitFlex WebkitFlexGrow WebkitFlexPositive WebkitFlexShrink WebkitLineClamp".split(" "));function El(e,t,a){var n=t.indexOf("--")===0;a==null||typeof a=="boolean"||a===""?n?e.setProperty(t,""):t==="float"?e.cssFloat="":e[t]="":n?e.setProperty(t,a):typeof a!="number"||a===0||Kh.has(t)?t==="float"?e.cssFloat=a:e[t]=(""+a).trim():e[t]=a+"px"}function Nl(e,t,a){if(t!=null&&typeof t!="object")throw Error(h(62));if(e=e.style,a!=null){for(var n in a)!a.hasOwnProperty(n)||t!=null&&t.hasOwnProperty(n)||(n.indexOf("--")===0?e.setProperty(n,""):n==="float"?e.cssFloat="":e[n]="");for(var i in t)n=t[i],t.hasOwnProperty(i)&&a[i]!==n&&El(e,i,n)}else for(var s in t)t.hasOwnProperty(s)&&El(e,s,t[s])}function to(e){if(e.indexOf("-")===-1)return!1;switch(e){case"annotation-xml":case"color-profile":case"font-face":case"font-face-src":case"font-face-uri":case"font-face-format":case"font-face-name":case"missing-glyph":return!1;default:return!0}}var Yh=new Map([["acceptCharset","accept-charset"],["htmlFor","for"],["httpEquiv","http-equiv"],["crossOrigin","crossorigin"],["accentHeight","accent-height"],["alignmentBaseline","alignment-baseline"],["arabicForm","arabic-form"],["baselineShift","baseline-shift"],["capHeight","cap-height"],["clipPath","clip-path"],["clipRule","clip-rule"],["colorInterpolation","color-interpolation"],["colorInterpolationFilters","color-interpolation-filters"],["colorProfile","color-profile"],["colorRendering","color-rendering"],["dominantBaseline","dominant-baseline"],["enableBackground","enable-background"],["fillOpacity","fill-opacity"],["fillRule","fill-rule"],["floodColor","flood-color"],["floodOpacity","flood-opacity"],["fontFamily","font-family"],["fontSize","font-size"],["fontSizeAdjust","font-size-adjust"],["fontStretch","font-stretch"],["fontStyle","font-style"],["fontVariant","font-variant"],["fontWeight","font-weight"],["glyphName","glyph-name"],["glyphOrientationHorizontal","glyph-orientation-horizontal"],["glyphOrientationVertical","glyph-orientation-vertical"],["horizAdvX","horiz-adv-x"],["horizOriginX","horiz-origin-x"],["imageRendering","image-rendering"],["letterSpacing","letter-spacing"],["lightingColor","lighting-color"],["markerEnd","marker-end"],["markerMid","marker-mid"],["markerStart","marker-start"],["overlinePosition","overline-position"],["overlineThickness","overline-thickness"],["paintOrder","paint-order"],["panose-1","panose-1"],["pointerEvents","pointer-events"],["renderingIntent","rendering-intent"],["shapeRendering","shape-rendering"],["stopColor","stop-color"],["stopOpacity","stop-opacity"],["strikethroughPosition","strikethrough-position"],["strikethroughThickness","strikethrough-thickness"],["strokeDasharray","stroke-dasharray"],["strokeDashoffset","stroke-dashoffset"],["strokeLinecap","stroke-linecap"],["strokeLinejoin","stroke-linejoin"],["strokeMiterlimit","stroke-miterlimit"],["strokeOpacity","stroke-opacity"],["strokeWidth","stroke-width"],["textAnchor","text-anchor"],["textDecoration","text-decoration"],["textRendering","text-rendering"],["transformOrigin","transform-origin"],["underlinePosition","underline-position"],["underlineThickness","underline-thickness"],["unicodeBidi","unicode-bidi"],["unicodeRange","unicode-range"],["unitsPerEm","units-per-em"],["vAlphabetic","v-alphabetic"],["vHanging","v-hanging"],["vIdeographic","v-ideographic"],["vMathematical","v-mathematical"],["vectorEffect","vector-effect"],["vertAdvY","vert-adv-y"],["vertOriginX","vert-origin-x"],["vertOriginY","vert-origin-y"],["wordSpacing","word-spacing"],["writingMode","writing-mode"],["xmlnsXlink","xmlns:xlink"],["xHeight","x-height"]]),Zh=/^[\u0000-\u001F ]*j[\r\n\t]*a[\r\n\t]*v[\r\n\t]*a[\r\n\t]*s[\r\n\t]*c[\r\n\t]*r[\r\n\t]*i[\r\n\t]*p[\r\n\t]*t[\r\n\t]*:/i;function Ii(e){return Zh.test(""+e)?"javascript:throw new Error('React has blocked a javascript: URL as a security precaution.')":e}function zt(){}var ao=null;function no(e){return e=e.target||e.srcElement||window,e.correspondingUseElement&&(e=e.correspondingUseElement),e.nodeType===3?e.parentNode:e}var Va=null,Fa=null;function Bl(e){var t=Ga(e);if(t&&(e=t.stateNode)){var a=e[Ze]||null;e:switch(e=t.stateNode,t.type){case"input":if($s(e,a.value,a.defaultValue,a.defaultValue,a.checked,a.defaultChecked,a.type,a.name),t=a.name,a.type==="radio"&&t!=null){for(a=e;a.parentNode;)a=a.parentNode;for(a=a.querySelectorAll('input[name="'+ht(""+t)+'"][type="radio"]'),t=0;t<a.length;t++){var n=a[t];if(n!==e&&n.form===e.form){var i=n[Ze]||null;if(!i)throw Error(h(90));$s(n,i.value,i.defaultValue,i.defaultValue,i.checked,i.defaultChecked,i.type,i.name)}}for(t=0;t<a.length;t++)n=a[t],n.form===e.form&&Dl(n)}break e;case"textarea":zl(e,a.value,a.defaultValue);break e;case"select":t=a.value,t!=null&&Za(e,!!a.multiple,t,!1)}}}var io=!1;function Hl(e,t,a){if(io)return e(t,a);io=!0;try{var n=e(t);return n}finally{if(io=!1,(Va!==null||Fa!==null)&&(vs(),Va&&(t=Va,e=Fa,Fa=Va=null,Bl(t),e)))for(t=0;t<e.length;t++)Bl(e[t])}}function Pn(e,t){var a=e.stateNode;if(a===null)return null;var n=a[Ze]||null;if(n===null)return null;a=n[t];e:switch(t){case"onClick":case"onClickCapture":case"onDoubleClick":case"onDoubleClickCapture":case"onMouseDown":case"onMouseDownCapture":case"onMouseMove":case"onMouseMoveCapture":case"onMouseUp":case"onMouseUpCapture":case"onMouseEnter":(n=!n.disabled)||(e=e.type,n=!(e==="button"||e==="input"||e==="select"||e==="textarea")),e=!n;break e;default:e=!1}if(e)return null;if(a&&typeof a!="function")throw Error(h(231,t,typeof a));return a}var Pt=!(typeof window>"u"||typeof window.document>"u"||typeof window.document.createElement>"u"),so=!1;if(Pt)try{var En={};Object.defineProperty(En,"passive",{get:function(){so=!0}}),window.addEventListener("test",En,En),window.removeEventListener("test",En,En)}catch{so=!1}var Jt=null,oo=null,zi=null;function Wl(){if(zi)return zi;var e,t=oo,a=t.length,n,i="value"in Jt?Jt.value:Jt.textContent,s=i.length;for(e=0;e<a&&t[e]===i[e];e++);var o=a-e;for(n=1;n<=o&&t[a-n]===i[s-n];n++);return zi=i.slice(e,1<n?1-n:void 0)}function Pi(e){var t=e.keyCode;return"charCode"in e?(e=e.charCode,e===0&&t===13&&(e=13)):e=t,e===10&&(e=13),32<=e||e===13?e:0}function Ei(){return!0}function Ml(){return!1}function Xe(e){function t(a,n,i,s,o){this._reactName=a,this._targetInst=i,this.type=n,this.nativeEvent=s,this.target=o,this.currentTarget=null;for(var r in e)e.hasOwnProperty(r)&&(a=e[r],this[r]=a?a(s):s[r]);return this.isDefaultPrevented=(s.defaultPrevented!=null?s.defaultPrevented:s.returnValue===!1)?Ei:Ml,this.isPropagationStopped=Ml,this}return I(t.prototype,{preventDefault:function(){this.defaultPrevented=!0;var a=this.nativeEvent;a&&(a.preventDefault?a.preventDefault():typeof a.returnValue!="unknown"&&(a.returnValue=!1),this.isDefaultPrevented=Ei)},stopPropagation:function(){var a=this.nativeEvent;a&&(a.stopPropagation?a.stopPropagation():typeof a.cancelBubble!="unknown"&&(a.cancelBubble=!0),this.isPropagationStopped=Ei)},persist:function(){},isPersistent:Ei}),t}var qa={eventPhase:0,bubbles:0,cancelable:0,timeStamp:function(e){return e.timeStamp||Date.now()},defaultPrevented:0,isTrusted:0},Ni=Xe(qa),Nn=I({},qa,{view:0,detail:0}),Xh=Xe(Nn),ro,lo,Bn,Bi=I({},Nn,{screenX:0,screenY:0,clientX:0,clientY:0,pageX:0,pageY:0,ctrlKey:0,shiftKey:0,altKey:0,metaKey:0,getModifierState:uo,button:0,buttons:0,relatedTarget:function(e){return e.relatedTarget===void 0?e.fromElement===e.srcElement?e.toElement:e.fromElement:e.relatedTarget},movementX:function(e){return"movementX"in e?e.movementX:(e!==Bn&&(Bn&&e.type==="mousemove"?(ro=e.screenX-Bn.screenX,lo=e.screenY-Bn.screenY):lo=ro=0,Bn=e),ro)},movementY:function(e){return"movementY"in e?e.movementY:lo}}),Ol=Xe(Bi),Vh=I({},Bi,{dataTransfer:0}),Fh=Xe(Vh),Jh=I({},Nn,{relatedTarget:0}),co=Xe(Jh),$h=I({},qa,{animationName:0,elapsedTime:0,pseudoElement:0}),ep=Xe($h),tp=I({},qa,{clipboardData:function(e){return"clipboardData"in e?e.clipboardData:window.clipboardData}}),ap=Xe(tp),np=I({},qa,{data:0}),_l=Xe(np),ip={Esc:"Escape",Spacebar:" ",Left:"ArrowLeft",Up:"ArrowUp",Right:"ArrowRight",Down:"ArrowDown",Del:"Delete",Win:"OS",Menu:"ContextMenu",Apps:"ContextMenu",Scroll:"ScrollLock",MozPrintableKey:"Unidentified"},sp={8:"Backspace",9:"Tab",12:"Clear",13:"Enter",16:"Shift",17:"Control",18:"Alt",19:"Pause",20:"CapsLock",27:"Escape",32:" ",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"ArrowLeft",38:"ArrowUp",39:"ArrowRight",40:"ArrowDown",45:"Insert",46:"Delete",112:"F1",113:"F2",114:"F3",115:"F4",116:"F5",117:"F6",118:"F7",119:"F8",120:"F9",121:"F10",122:"F11",123:"F12",144:"NumLock",145:"ScrollLock",224:"Meta"},op={Alt:"altKey",Control:"ctrlKey",Meta:"metaKey",Shift:"shiftKey"};function rp(e){var t=this.nativeEvent;return t.getModifierState?t.getModifierState(e):(e=op[e])?!!t[e]:!1}function uo(){return rp}var lp=I({},Nn,{key:function(e){if(e.key){var t=ip[e.key]||e.key;if(t!=="Unidentified")return t}return e.type==="keypress"?(e=Pi(e),e===13?"Enter":String.fromCharCode(e)):e.type==="keydown"||e.type==="keyup"?sp[e.keyCode]||"Unidentified":""},code:0,location:0,ctrlKey:0,shiftKey:0,altKey:0,metaKey:0,repeat:0,locale:0,getModifierState:uo,charCode:function(e){return e.type==="keypress"?Pi(e):0},keyCode:function(e){return e.type==="keydown"||e.type==="keyup"?e.keyCode:0},which:function(e){return e.type==="keypress"?Pi(e):e.type==="keydown"||e.type==="keyup"?e.keyCode:0}}),cp=Xe(lp),up=I({},Bi,{pointerId:0,width:0,height:0,pressure:0,tangentialPressure:0,tiltX:0,tiltY:0,twist:0,pointerType:0,isPrimary:0}),jl=Xe(up),dp=I({},Nn,{touches:0,targetTouches:0,changedTouches:0,altKey:0,metaKey:0,ctrlKey:0,shiftKey:0,getModifierState:uo}),hp=Xe(dp),pp=I({},qa,{propertyName:0,elapsedTime:0,pseudoElement:0}),fp=Xe(pp),mp=I({},Bi,{deltaX:function(e){return"deltaX"in e?e.deltaX:"wheelDeltaX"in e?-e.wheelDeltaX:0},deltaY:function(e){return"deltaY"in e?e.deltaY:"wheelDeltaY"in e?-e.wheelDeltaY:"wheelDelta"in e?-e.wheelDelta:0},deltaZ:0,deltaMode:0}),gp=Xe(mp),yp=I({},qa,{newState:0,oldState:0}),bp=Xe(yp),vp=[9,13,27,32],ho=Pt&&"CompositionEvent"in window,Hn=null;Pt&&"documentMode"in document&&(Hn=document.documentMode);var wp=Pt&&"TextEvent"in window&&!Hn,Ql=Pt&&(!ho||Hn&&8<Hn&&11>=Hn),Gl=" ",Kl=!1;function Yl(e,t){switch(e){case"keyup":return vp.indexOf(t.keyCode)!==-1;case"keydown":return t.keyCode!==229;case"keypress":case"mousedown":case"focusout":return!0;default:return!1}}function Zl(e){return e=e.detail,typeof e=="object"&&"data"in e?e.data:null}var Ja=!1;function xp(e,t){switch(e){case"compositionend":return Zl(t);case"keypress":return t.which!==32?null:(Kl=!0,Gl);case"textInput":return e=t.data,e===Gl&&Kl?null:e;default:return null}}function Tp(e,t){if(Ja)return e==="compositionend"||!ho&&Yl(e,t)?(e=Wl(),zi=oo=Jt=null,Ja=!1,e):null;switch(e){case"paste":return null;case"keypress":if(!(t.ctrlKey||t.altKey||t.metaKey)||t.ctrlKey&&t.altKey){if(t.char&&1<t.char.length)return t.char;if(t.which)return String.fromCharCode(t.which)}return null;case"compositionend":return Ql&&t.locale!=="ko"?null:t.data;default:return null}}var kp={color:!0,date:!0,datetime:!0,"datetime-local":!0,email:!0,month:!0,number:!0,password:!0,range:!0,search:!0,tel:!0,text:!0,time:!0,url:!0,week:!0};function Xl(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return t==="input"?!!kp[e.type]:t==="textarea"}function Vl(e,t,a,n){Va?Fa?Fa.push(n):Fa=[n]:Va=n,t=qs(t,"onChange"),0<t.length&&(a=new Ni("onChange","change",null,a,n),e.push({event:a,listeners:t}))}var Wn=null,Mn=null;function Ap(e){Id(e,0)}function Hi(e){var t=zn(e);if(Dl(t))return e}function Fl(e,t){if(e==="change")return t}var Jl=!1;if(Pt){var po;if(Pt){var fo="oninput"in document;if(!fo){var $l=document.createElement("div");$l.setAttribute("oninput","return;"),fo=typeof $l.oninput=="function"}po=fo}else po=!1;Jl=po&&(!document.documentMode||9<document.documentMode)}function ec(){Wn&&(Wn.detachEvent("onpropertychange",tc),Mn=Wn=null)}function tc(e){if(e.propertyName==="value"&&Hi(Mn)){var t=[];Vl(t,Mn,e,no(e)),Hl(Ap,t)}}function Sp(e,t,a){e==="focusin"?(ec(),Wn=t,Mn=a,Wn.attachEvent("onpropertychange",tc)):e==="focusout"&&ec()}function qp(e){if(e==="selectionchange"||e==="keyup"||e==="keydown")return Hi(Mn)}function Lp(e,t){if(e==="click")return Hi(t)}function Rp(e,t){if(e==="input"||e==="change")return Hi(t)}function Cp(e,t){return e===t&&(e!==0||1/e===1/t)||e!==e&&t!==t}var it=typeof Object.is=="function"?Object.is:Cp;function On(e,t){if(it(e,t))return!0;if(typeof e!="object"||e===null||typeof t!="object"||t===null)return!1;var a=Object.keys(e),n=Object.keys(t);if(a.length!==n.length)return!1;for(n=0;n<a.length;n++){var i=a[n];if(!Qs.call(t,i)||!it(e[i],t[i]))return!1}return!0}function ac(e){for(;e&&e.firstChild;)e=e.firstChild;return e}function nc(e,t){var a=ac(e);e=0;for(var n;a;){if(a.nodeType===3){if(n=e+a.textContent.length,e<=t&&n>=t)return{node:a,offset:t-e};e=n}e:{for(;a;){if(a.nextSibling){a=a.nextSibling;break e}a=a.parentNode}a=void 0}a=ac(a)}}function ic(e,t){return e&&t?e===t?!0:e&&e.nodeType===3?!1:t&&t.nodeType===3?ic(e,t.parentNode):"contains"in e?e.contains(t):e.compareDocumentPosition?!!(e.compareDocumentPosition(t)&16):!1:!1}function sc(e){e=e!=null&&e.ownerDocument!=null&&e.ownerDocument.defaultView!=null?e.ownerDocument.defaultView:window;for(var t=Di(e.document);t instanceof e.HTMLIFrameElement;){try{var a=typeof t.contentWindow.location.href=="string"}catch{a=!1}if(a)e=t.contentWindow;else break;t=Di(e.document)}return t}function mo(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return t&&(t==="input"&&(e.type==="text"||e.type==="search"||e.type==="tel"||e.type==="url"||e.type==="password")||t==="textarea"||e.contentEditable==="true")}var Up=Pt&&"documentMode"in document&&11>=document.documentMode,$a=null,go=null,_n=null,yo=!1;function oc(e,t,a){var n=a.window===a?a.document:a.nodeType===9?a:a.ownerDocument;yo||$a==null||$a!==Di(n)||(n=$a,"selectionStart"in n&&mo(n)?n={start:n.selectionStart,end:n.selectionEnd}:(n=(n.ownerDocument&&n.ownerDocument.defaultView||window).getSelection(),n={anchorNode:n.anchorNode,anchorOffset:n.anchorOffset,focusNode:n.focusNode,focusOffset:n.focusOffset}),_n&&On(_n,n)||(_n=n,n=qs(go,"onSelect"),0<n.length&&(t=new Ni("onSelect","select",null,t,a),e.push({event:t,listeners:n}),t.target=$a)))}function La(e,t){var a={};return a[e.toLowerCase()]=t.toLowerCase(),a["Webkit"+e]="webkit"+t,a["Moz"+e]="moz"+t,a}var en={animationend:La("Animation","AnimationEnd"),animationiteration:La("Animation","AnimationIteration"),animationstart:La("Animation","AnimationStart"),transitionrun:La("Transition","TransitionRun"),transitionstart:La("Transition","TransitionStart"),transitioncancel:La("Transition","TransitionCancel"),transitionend:La("Transition","TransitionEnd")},bo={},rc={};Pt&&(rc=document.createElement("div").style,"AnimationEvent"in window||(delete en.animationend.animation,delete en.animationiteration.animation,delete en.animationstart.animation),"TransitionEvent"in window||delete en.transitionend.transition);function Ra(e){if(bo[e])return bo[e];if(!en[e])return e;var t=en[e],a;for(a in t)if(t.hasOwnProperty(a)&&a in rc)return bo[e]=t[a];return e}var lc=Ra("animationend"),cc=Ra("animationiteration"),uc=Ra("animationstart"),Dp=Ra("transitionrun"),Ip=Ra("transitionstart"),zp=Ra("transitioncancel"),dc=Ra("transitionend"),hc=new Map,vo="abort auxClick beforeToggle cancel canPlay canPlayThrough click close contextMenu copy cut drag dragEnd dragEnter dragExit dragLeave dragOver dragStart drop durationChange emptied encrypted ended error gotPointerCapture input invalid keyDown keyPress keyUp load loadedData loadedMetadata loadStart lostPointerCapture mouseDown mouseMove mouseOut mouseOver mouseUp paste pause play playing pointerCancel pointerDown pointerMove pointerOut pointerOver pointerUp progress rateChange reset resize seeked seeking stalled submit suspend timeUpdate touchCancel touchEnd touchStart volumeChange scroll toggle touchMove waiting wheel".split(" ");vo.push("scrollEnd");function Tt(e,t){hc.set(e,t),Sa(t,[e])}var Wi=typeof reportError=="function"?reportError:function(e){if(typeof window=="object"&&typeof window.ErrorEvent=="function"){var t=new window.ErrorEvent("error",{bubbles:!0,cancelable:!0,message:typeof e=="object"&&e!==null&&typeof e.message=="string"?String(e.message):String(e),error:e});if(!window.dispatchEvent(t))return}else if(typeof process=="object"&&typeof process.emit=="function"){process.emit("uncaughtException",e);return}console.error(e)},pt=[],tn=0,wo=0;function Mi(){for(var e=tn,t=wo=tn=0;t<e;){var a=pt[t];pt[t++]=null;var n=pt[t];pt[t++]=null;var i=pt[t];pt[t++]=null;var s=pt[t];if(pt[t++]=null,n!==null&&i!==null){var o=n.pending;o===null?i.next=i:(i.next=o.next,o.next=i),n.pending=i}s!==0&&pc(a,i,s)}}function Oi(e,t,a,n){pt[tn++]=e,pt[tn++]=t,pt[tn++]=a,pt[tn++]=n,wo|=n,e.lanes|=n,e=e.alternate,e!==null&&(e.lanes|=n)}function xo(e,t,a,n){return Oi(e,t,a,n),_i(e)}function Ca(e,t){return Oi(e,null,null,t),_i(e)}function pc(e,t,a){e.lanes|=a;var n=e.alternate;n!==null&&(n.lanes|=a);for(var i=!1,s=e.return;s!==null;)s.childLanes|=a,n=s.alternate,n!==null&&(n.childLanes|=a),s.tag===22&&(e=s.stateNode,e===null||e._visibility&1||(i=!0)),e=s,s=s.return;return e.tag===3?(s=e.stateNode,i&&t!==null&&(i=31-nt(a),e=s.hiddenUpdates,n=e[i],n===null?e[i]=[t]:n.push(t),t.lane=a|536870912),s):null}function _i(e){if(50<ui)throw ui=0,Ur=null,Error(h(185));for(var t=e.return;t!==null;)e=t,t=e.return;return e.tag===3?e.stateNode:null}var an={};function Pp(e,t,a,n){this.tag=e,this.key=a,this.sibling=this.child=this.return=this.stateNode=this.type=this.elementType=null,this.index=0,this.refCleanup=this.ref=null,this.pendingProps=t,this.dependencies=this.memoizedState=this.updateQueue=this.memoizedProps=null,this.mode=n,this.subtreeFlags=this.flags=0,this.deletions=null,this.childLanes=this.lanes=0,this.alternate=null}function st(e,t,a,n){return new Pp(e,t,a,n)}function To(e){return e=e.prototype,!(!e||!e.isReactComponent)}function Et(e,t){var a=e.alternate;return a===null?(a=st(e.tag,t,e.key,e.mode),a.elementType=e.elementType,a.type=e.type,a.stateNode=e.stateNode,a.alternate=e,e.alternate=a):(a.pendingProps=t,a.type=e.type,a.flags=0,a.subtreeFlags=0,a.deletions=null),a.flags=e.flags&65011712,a.childLanes=e.childLanes,a.lanes=e.lanes,a.child=e.child,a.memoizedProps=e.memoizedProps,a.memoizedState=e.memoizedState,a.updateQueue=e.updateQueue,t=e.dependencies,a.dependencies=t===null?null:{lanes:t.lanes,firstContext:t.firstContext},a.sibling=e.sibling,a.index=e.index,a.ref=e.ref,a.refCleanup=e.refCleanup,a}function fc(e,t){e.flags&=65011714;var a=e.alternate;return a===null?(e.childLanes=0,e.lanes=t,e.child=null,e.subtreeFlags=0,e.memoizedProps=null,e.memoizedState=null,e.updateQueue=null,e.dependencies=null,e.stateNode=null):(e.childLanes=a.childLanes,e.lanes=a.lanes,e.child=a.child,e.subtreeFlags=0,e.deletions=null,e.memoizedProps=a.memoizedProps,e.memoizedState=a.memoizedState,e.updateQueue=a.updateQueue,e.type=a.type,t=a.dependencies,e.dependencies=t===null?null:{lanes:t.lanes,firstContext:t.firstContext}),e}function ji(e,t,a,n,i,s){var o=0;if(n=e,typeof e=="function")To(e)&&(o=1);else if(typeof e=="string")o=Mf(e,a,U.current)?26:e==="html"||e==="head"||e==="body"?27:5;else e:switch(e){case Qe:return e=st(31,a,t,i),e.elementType=Qe,e.lanes=s,e;case re:return Ua(a.children,i,s,t);case De:o=8,i|=24;break;case je:return e=st(12,a,t,i|2),e.elementType=je,e.lanes=s,e;case le:return e=st(13,a,t,i),e.elementType=le,e.lanes=s,e;case Ee:return e=st(19,a,t,i),e.elementType=Ee,e.lanes=s,e;default:if(typeof e=="object"&&e!==null)switch(e.$$typeof){case B:o=10;break e;case St:o=9;break e;case pe:o=11;break e;case Y:o=14;break e;case Ne:o=16,n=null;break e}o=29,a=Error(h(130,e===null?"null":typeof e,"")),n=null}return t=st(o,a,t,i),t.elementType=e,t.type=n,t.lanes=s,t}function Ua(e,t,a,n){return e=st(7,e,n,t),e.lanes=a,e}function ko(e,t,a){return e=st(6,e,null,t),e.lanes=a,e}function mc(e){var t=st(18,null,null,0);return t.stateNode=e,t}function Ao(e,t,a){return t=st(4,e.children!==null?e.children:[],e.key,t),t.lanes=a,t.stateNode={containerInfo:e.containerInfo,pendingChildren:null,implementation:e.implementation},t}var gc=new WeakMap;function ft(e,t){if(typeof e=="object"&&e!==null){var a=gc.get(e);return a!==void 0?a:(t={value:e,source:t,stack:ml(t)},gc.set(e,t),t)}return{value:e,source:t,stack:ml(t)}}var nn=[],sn=0,Qi=null,jn=0,mt=[],gt=0,$t=null,Lt=1,Rt="";function Nt(e,t){nn[sn++]=jn,nn[sn++]=Qi,Qi=e,jn=t}function yc(e,t,a){mt[gt++]=Lt,mt[gt++]=Rt,mt[gt++]=$t,$t=e;var n=Lt;e=Rt;var i=32-nt(n)-1;n&=~(1<<i),a+=1;var s=32-nt(t)+i;if(30<s){var o=i-i%5;s=(n&(1<<o)-1).toString(32),n>>=o,i-=o,Lt=1<<32-nt(t)+i|a<<i|n,Rt=s+e}else Lt=1<<s|a<<i|n,Rt=e}function So(e){e.return!==null&&(Nt(e,1),yc(e,1,0))}function qo(e){for(;e===Qi;)Qi=nn[--sn],nn[sn]=null,jn=nn[--sn],nn[sn]=null;for(;e===$t;)$t=mt[--gt],mt[gt]=null,Rt=mt[--gt],mt[gt]=null,Lt=mt[--gt],mt[gt]=null}function bc(e,t){mt[gt++]=Lt,mt[gt++]=Rt,mt[gt++]=$t,Lt=t.id,Rt=t.overflow,$t=e}var He=null,ge=null,$=!1,ea=null,yt=!1,Lo=Error(h(519));function ta(e){var t=Error(h(418,1<arguments.length&&arguments[1]!==void 0&&arguments[1]?"text":"HTML",""));throw Qn(ft(t,e)),Lo}function vc(e){var t=e.stateNode,a=e.type,n=e.memoizedProps;switch(t[Be]=e,t[Ze]=n,a){case"dialog":V("cancel",t),V("close",t);break;case"iframe":case"object":case"embed":V("load",t);break;case"video":case"audio":for(a=0;a<hi.length;a++)V(hi[a],t);break;case"source":V("error",t);break;case"img":case"image":case"link":V("error",t),V("load",t);break;case"details":V("toggle",t);break;case"input":V("invalid",t),Il(t,n.value,n.defaultValue,n.checked,n.defaultChecked,n.type,n.name,!0);break;case"select":V("invalid",t);break;case"textarea":V("invalid",t),Pl(t,n.value,n.defaultValue,n.children)}a=n.children,typeof a!="string"&&typeof a!="number"&&typeof a!="bigint"||t.textContent===""+a||n.suppressHydrationWarning===!0||Nd(t.textContent,a)?(n.popover!=null&&(V("beforetoggle",t),V("toggle",t)),n.onScroll!=null&&V("scroll",t),n.onScrollEnd!=null&&V("scrollend",t),n.onClick!=null&&(t.onclick=zt),t=!0):t=!1,t||ta(e,!0)}function wc(e){for(He=e.return;He;)switch(He.tag){case 5:case 31:case 13:yt=!1;return;case 27:case 3:yt=!0;return;default:He=He.return}}function on(e){if(e!==He)return!1;if(!$)return wc(e),$=!0,!1;var t=e.tag,a;if((a=t!==3&&t!==27)&&((a=t===5)&&(a=e.type,a=!(a!=="form"&&a!=="button")||Gr(e.type,e.memoizedProps)),a=!a),a&&ge&&ta(e),wc(e),t===13){if(e=e.memoizedState,e=e!==null?e.dehydrated:null,!e)throw Error(h(317));ge=Gd(e)}else if(t===31){if(e=e.memoizedState,e=e!==null?e.dehydrated:null,!e)throw Error(h(317));ge=Gd(e)}else t===27?(t=ge,ma(e.type)?(e=Vr,Vr=null,ge=e):ge=t):ge=He?vt(e.stateNode.nextSibling):null;return!0}function Da(){ge=He=null,$=!1}function Ro(){var e=ea;return e!==null&&($e===null?$e=e:$e.push.apply($e,e),ea=null),e}function Qn(e){ea===null?ea=[e]:ea.push(e)}var Co=u(null),Ia=null,Bt=null;function aa(e,t,a){q(Co,t._currentValue),t._currentValue=a}function Ht(e){e._currentValue=Co.current,k(Co)}function Uo(e,t,a){for(;e!==null;){var n=e.alternate;if((e.childLanes&t)!==t?(e.childLanes|=t,n!==null&&(n.childLanes|=t)):n!==null&&(n.childLanes&t)!==t&&(n.childLanes|=t),e===a)break;e=e.return}}function Do(e,t,a,n){var i=e.child;for(i!==null&&(i.return=e);i!==null;){var s=i.dependencies;if(s!==null){var o=i.child;s=s.firstContext;e:for(;s!==null;){var r=s;s=i;for(var l=0;l<t.length;l++)if(r.context===t[l]){s.lanes|=a,r=s.alternate,r!==null&&(r.lanes|=a),Uo(s.return,a,e),n||(o=null);break e}s=r.next}}else if(i.tag===18){if(o=i.return,o===null)throw Error(h(341));o.lanes|=a,s=o.alternate,s!==null&&(s.lanes|=a),Uo(o,a,e),o=null}else o=i.child;if(o!==null)o.return=i;else for(o=i;o!==null;){if(o===e){o=null;break}if(i=o.sibling,i!==null){i.return=o.return,o=i;break}o=o.return}i=o}}function rn(e,t,a,n){e=null;for(var i=t,s=!1;i!==null;){if(!s){if((i.flags&524288)!==0)s=!0;else if((i.flags&262144)!==0)break}if(i.tag===10){var o=i.alternate;if(o===null)throw Error(h(387));if(o=o.memoizedProps,o!==null){var r=i.type;it(i.pendingProps.value,o.value)||(e!==null?e.push(r):e=[r])}}else if(i===ne.current){if(o=i.alternate,o===null)throw Error(h(387));o.memoizedState.memoizedState!==i.memoizedState.memoizedState&&(e!==null?e.push(yi):e=[yi])}i=i.return}e!==null&&Do(t,e,a,n),t.flags|=262144}function Gi(e){for(e=e.firstContext;e!==null;){if(!it(e.context._currentValue,e.memoizedValue))return!0;e=e.next}return!1}function za(e){Ia=e,Bt=null,e=e.dependencies,e!==null&&(e.firstContext=null)}function We(e){return xc(Ia,e)}function Ki(e,t){return Ia===null&&za(e),xc(e,t)}function xc(e,t){var a=t._currentValue;if(t={context:t,memoizedValue:a,next:null},Bt===null){if(e===null)throw Error(h(308));Bt=t,e.dependencies={lanes:0,firstContext:t},e.flags|=524288}else Bt=Bt.next=t;return a}var Ep=typeof AbortController<"u"?AbortController:function(){var e=[],t=this.signal={aborted:!1,addEventListener:function(a,n){e.push(n)}};this.abort=function(){t.aborted=!0,e.forEach(function(a){return a()})}},Np=v.unstable_scheduleCallback,Bp=v.unstable_NormalPriority,qe={$$typeof:B,Consumer:null,Provider:null,_currentValue:null,_currentValue2:null,_threadCount:0};function Io(){return{controller:new Ep,data:new Map,refCount:0}}function Gn(e){e.refCount--,e.refCount===0&&Np(Bp,function(){e.controller.abort()})}var Kn=null,zo=0,ln=0,cn=null;function Hp(e,t){if(Kn===null){var a=Kn=[];zo=0,ln=Nr(),cn={status:"pending",value:void 0,then:function(n){a.push(n)}}}return zo++,t.then(Tc,Tc),t}function Tc(){if(--zo===0&&Kn!==null){cn!==null&&(cn.status="fulfilled");var e=Kn;Kn=null,ln=0,cn=null;for(var t=0;t<e.length;t++)(0,e[t])()}}function Wp(e,t){var a=[],n={status:"pending",value:null,reason:null,then:function(i){a.push(i)}};return e.then(function(){n.status="fulfilled",n.value=t;for(var i=0;i<a.length;i++)(0,a[i])(t)},function(i){for(n.status="rejected",n.reason=i,i=0;i<a.length;i++)(0,a[i])(void 0)}),n}var kc=w.S;w.S=function(e,t){od=tt(),typeof t=="object"&&t!==null&&typeof t.then=="function"&&Hp(e,t),kc!==null&&kc(e,t)};var Pa=u(null);function Po(){var e=Pa.current;return e!==null?e:me.pooledCache}function Yi(e,t){t===null?q(Pa,Pa.current):q(Pa,t.pool)}function Ac(){var e=Po();return e===null?null:{parent:qe._currentValue,pool:e}}var un=Error(h(460)),Eo=Error(h(474)),Zi=Error(h(542)),Xi={then:function(){}};function Sc(e){return e=e.status,e==="fulfilled"||e==="rejected"}function qc(e,t,a){switch(a=e[a],a===void 0?e.push(t):a!==t&&(t.then(zt,zt),t=a),t.status){case"fulfilled":return t.value;case"rejected":throw e=t.reason,Rc(e),e;default:if(typeof t.status=="string")t.then(zt,zt);else{if(e=me,e!==null&&100<e.shellSuspendCounter)throw Error(h(482));e=t,e.status="pending",e.then(function(n){if(t.status==="pending"){var i=t;i.status="fulfilled",i.value=n}},function(n){if(t.status==="pending"){var i=t;i.status="rejected",i.reason=n}})}switch(t.status){case"fulfilled":return t.value;case"rejected":throw e=t.reason,Rc(e),e}throw Na=t,un}}function Ea(e){try{var t=e._init;return t(e._payload)}catch(a){throw a!==null&&typeof a=="object"&&typeof a.then=="function"?(Na=a,un):a}}var Na=null;function Lc(){if(Na===null)throw Error(h(459));var e=Na;return Na=null,e}function Rc(e){if(e===un||e===Zi)throw Error(h(483))}var dn=null,Yn=0;function Vi(e){var t=Yn;return Yn+=1,dn===null&&(dn=[]),qc(dn,e,t)}function Zn(e,t){t=t.props.ref,e.ref=t!==void 0?t:null}function Fi(e,t){throw t.$$typeof===ie?Error(h(525)):(e=Object.prototype.toString.call(t),Error(h(31,e==="[object Object]"?"object with keys {"+Object.keys(t).join(", ")+"}":e)))}function Cc(e){function t(d,c){if(e){var p=d.deletions;p===null?(d.deletions=[c],d.flags|=16):p.push(c)}}function a(d,c){if(!e)return null;for(;c!==null;)t(d,c),c=c.sibling;return null}function n(d){for(var c=new Map;d!==null;)d.key!==null?c.set(d.key,d):c.set(d.index,d),d=d.sibling;return c}function i(d,c){return d=Et(d,c),d.index=0,d.sibling=null,d}function s(d,c,p){return d.index=p,e?(p=d.alternate,p!==null?(p=p.index,p<c?(d.flags|=67108866,c):p):(d.flags|=67108866,c)):(d.flags|=1048576,c)}function o(d){return e&&d.alternate===null&&(d.flags|=67108866),d}function r(d,c,p,x){return c===null||c.tag!==6?(c=ko(p,d.mode,x),c.return=d,c):(c=i(c,p),c.return=d,c)}function l(d,c,p,x){var z=p.type;return z===re?b(d,c,p.props.children,x,p.key):c!==null&&(c.elementType===z||typeof z=="object"&&z!==null&&z.$$typeof===Ne&&Ea(z)===c.type)?(c=i(c,p.props),Zn(c,p),c.return=d,c):(c=ji(p.type,p.key,p.props,null,d.mode,x),Zn(c,p),c.return=d,c)}function f(d,c,p,x){return c===null||c.tag!==4||c.stateNode.containerInfo!==p.containerInfo||c.stateNode.implementation!==p.implementation?(c=Ao(p,d.mode,x),c.return=d,c):(c=i(c,p.children||[]),c.return=d,c)}function b(d,c,p,x,z){return c===null||c.tag!==7?(c=Ua(p,d.mode,x,z),c.return=d,c):(c=i(c,p),c.return=d,c)}function T(d,c,p){if(typeof c=="string"&&c!==""||typeof c=="number"||typeof c=="bigint")return c=ko(""+c,d.mode,p),c.return=d,c;if(typeof c=="object"&&c!==null){switch(c.$$typeof){case Pe:return p=ji(c.type,c.key,c.props,null,d.mode,p),Zn(p,c),p.return=d,p;case we:return c=Ao(c,d.mode,p),c.return=d,c;case Ne:return c=Ea(c),T(d,c,p)}if(xt(c)||Ye(c))return c=Ua(c,d.mode,p,null),c.return=d,c;if(typeof c.then=="function")return T(d,Vi(c),p);if(c.$$typeof===B)return T(d,Ki(d,c),p);Fi(d,c)}return null}function m(d,c,p,x){var z=c!==null?c.key:null;if(typeof p=="string"&&p!==""||typeof p=="number"||typeof p=="bigint")return z!==null?null:r(d,c,""+p,x);if(typeof p=="object"&&p!==null){switch(p.$$typeof){case Pe:return p.key===z?l(d,c,p,x):null;case we:return p.key===z?f(d,c,p,x):null;case Ne:return p=Ea(p),m(d,c,p,x)}if(xt(p)||Ye(p))return z!==null?null:b(d,c,p,x,null);if(typeof p.then=="function")return m(d,c,Vi(p),x);if(p.$$typeof===B)return m(d,c,Ki(d,p),x);Fi(d,p)}return null}function g(d,c,p,x,z){if(typeof x=="string"&&x!==""||typeof x=="number"||typeof x=="bigint")return d=d.get(p)||null,r(c,d,""+x,z);if(typeof x=="object"&&x!==null){switch(x.$$typeof){case Pe:return d=d.get(x.key===null?p:x.key)||null,l(c,d,x,z);case we:return d=d.get(x.key===null?p:x.key)||null,f(c,d,x,z);case Ne:return x=Ea(x),g(d,c,p,x,z)}if(xt(x)||Ye(x))return d=d.get(p)||null,b(c,d,x,z,null);if(typeof x.then=="function")return g(d,c,p,Vi(x),z);if(x.$$typeof===B)return g(d,c,p,Ki(c,x),z);Fi(c,x)}return null}function R(d,c,p,x){for(var z=null,ee=null,D=c,G=c=0,J=null;D!==null&&G<p.length;G++){D.index>G?(J=D,D=null):J=D.sibling;var te=m(d,D,p[G],x);if(te===null){D===null&&(D=J);break}e&&D&&te.alternate===null&&t(d,D),c=s(te,c,G),ee===null?z=te:ee.sibling=te,ee=te,D=J}if(G===p.length)return a(d,D),$&&Nt(d,G),z;if(D===null){for(;G<p.length;G++)D=T(d,p[G],x),D!==null&&(c=s(D,c,G),ee===null?z=D:ee.sibling=D,ee=D);return $&&Nt(d,G),z}for(D=n(D);G<p.length;G++)J=g(D,d,G,p[G],x),J!==null&&(e&&J.alternate!==null&&D.delete(J.key===null?G:J.key),c=s(J,c,G),ee===null?z=J:ee.sibling=J,ee=J);return e&&D.forEach(function(wa){return t(d,wa)}),$&&Nt(d,G),z}function N(d,c,p,x){if(p==null)throw Error(h(151));for(var z=null,ee=null,D=c,G=c=0,J=null,te=p.next();D!==null&&!te.done;G++,te=p.next()){D.index>G?(J=D,D=null):J=D.sibling;var wa=m(d,D,te.value,x);if(wa===null){D===null&&(D=J);break}e&&D&&wa.alternate===null&&t(d,D),c=s(wa,c,G),ee===null?z=wa:ee.sibling=wa,ee=wa,D=J}if(te.done)return a(d,D),$&&Nt(d,G),z;if(D===null){for(;!te.done;G++,te=p.next())te=T(d,te.value,x),te!==null&&(c=s(te,c,G),ee===null?z=te:ee.sibling=te,ee=te);return $&&Nt(d,G),z}for(D=n(D);!te.done;G++,te=p.next())te=g(D,d,G,te.value,x),te!==null&&(e&&te.alternate!==null&&D.delete(te.key===null?G:te.key),c=s(te,c,G),ee===null?z=te:ee.sibling=te,ee=te);return e&&D.forEach(function(Ff){return t(d,Ff)}),$&&Nt(d,G),z}function he(d,c,p,x){if(typeof p=="object"&&p!==null&&p.type===re&&p.key===null&&(p=p.props.children),typeof p=="object"&&p!==null){switch(p.$$typeof){case Pe:e:{for(var z=p.key;c!==null;){if(c.key===z){if(z=p.type,z===re){if(c.tag===7){a(d,c.sibling),x=i(c,p.props.children),x.return=d,d=x;break e}}else if(c.elementType===z||typeof z=="object"&&z!==null&&z.$$typeof===Ne&&Ea(z)===c.type){a(d,c.sibling),x=i(c,p.props),Zn(x,p),x.return=d,d=x;break e}a(d,c);break}else t(d,c);c=c.sibling}p.type===re?(x=Ua(p.props.children,d.mode,x,p.key),x.return=d,d=x):(x=ji(p.type,p.key,p.props,null,d.mode,x),Zn(x,p),x.return=d,d=x)}return o(d);case we:e:{for(z=p.key;c!==null;){if(c.key===z)if(c.tag===4&&c.stateNode.containerInfo===p.containerInfo&&c.stateNode.implementation===p.implementation){a(d,c.sibling),x=i(c,p.children||[]),x.return=d,d=x;break e}else{a(d,c);break}else t(d,c);c=c.sibling}x=Ao(p,d.mode,x),x.return=d,d=x}return o(d);case Ne:return p=Ea(p),he(d,c,p,x)}if(xt(p))return R(d,c,p,x);if(Ye(p)){if(z=Ye(p),typeof z!="function")throw Error(h(150));return p=z.call(p),N(d,c,p,x)}if(typeof p.then=="function")return he(d,c,Vi(p),x);if(p.$$typeof===B)return he(d,c,Ki(d,p),x);Fi(d,p)}return typeof p=="string"&&p!==""||typeof p=="number"||typeof p=="bigint"?(p=""+p,c!==null&&c.tag===6?(a(d,c.sibling),x=i(c,p),x.return=d,d=x):(a(d,c),x=ko(p,d.mode,x),x.return=d,d=x),o(d)):a(d,c)}return function(d,c,p,x){try{Yn=0;var z=he(d,c,p,x);return dn=null,z}catch(D){if(D===un||D===Zi)throw D;var ee=st(29,D,null,d.mode);return ee.lanes=x,ee.return=d,ee}finally{}}}var Ba=Cc(!0),Uc=Cc(!1),na=!1;function No(e){e.updateQueue={baseState:e.memoizedState,firstBaseUpdate:null,lastBaseUpdate:null,shared:{pending:null,lanes:0,hiddenCallbacks:null},callbacks:null}}function Bo(e,t){e=e.updateQueue,t.updateQueue===e&&(t.updateQueue={baseState:e.baseState,firstBaseUpdate:e.firstBaseUpdate,lastBaseUpdate:e.lastBaseUpdate,shared:e.shared,callbacks:null})}function ia(e){return{lane:e,tag:0,payload:null,callback:null,next:null}}function sa(e,t,a){var n=e.updateQueue;if(n===null)return null;if(n=n.shared,(ae&2)!==0){var i=n.pending;return i===null?t.next=t:(t.next=i.next,i.next=t),n.pending=t,t=_i(e),pc(e,null,a),t}return Oi(e,n,t,a),_i(e)}function Xn(e,t,a){if(t=t.updateQueue,t!==null&&(t=t.shared,(a&4194048)!==0)){var n=t.lanes;n&=e.pendingLanes,a|=n,t.lanes=a,xl(e,a)}}function Ho(e,t){var a=e.updateQueue,n=e.alternate;if(n!==null&&(n=n.updateQueue,a===n)){var i=null,s=null;if(a=a.firstBaseUpdate,a!==null){do{var o={lane:a.lane,tag:a.tag,payload:a.payload,callback:null,next:null};s===null?i=s=o:s=s.next=o,a=a.next}while(a!==null);s===null?i=s=t:s=s.next=t}else i=s=t;a={baseState:n.baseState,firstBaseUpdate:i,lastBaseUpdate:s,shared:n.shared,callbacks:n.callbacks},e.updateQueue=a;return}e=a.lastBaseUpdate,e===null?a.firstBaseUpdate=t:e.next=t,a.lastBaseUpdate=t}var Wo=!1;function Vn(){if(Wo){var e=cn;if(e!==null)throw e}}function Fn(e,t,a,n){Wo=!1;var i=e.updateQueue;na=!1;var s=i.firstBaseUpdate,o=i.lastBaseUpdate,r=i.shared.pending;if(r!==null){i.shared.pending=null;var l=r,f=l.next;l.next=null,o===null?s=f:o.next=f,o=l;var b=e.alternate;b!==null&&(b=b.updateQueue,r=b.lastBaseUpdate,r!==o&&(r===null?b.firstBaseUpdate=f:r.next=f,b.lastBaseUpdate=l))}if(s!==null){var T=i.baseState;o=0,b=f=l=null,r=s;do{var m=r.lane&-536870913,g=m!==r.lane;if(g?(F&m)===m:(n&m)===m){m!==0&&m===ln&&(Wo=!0),b!==null&&(b=b.next={lane:0,tag:r.tag,payload:r.payload,callback:null,next:null});e:{var R=e,N=r;m=t;var he=a;switch(N.tag){case 1:if(R=N.payload,typeof R=="function"){T=R.call(he,T,m);break e}T=R;break e;case 3:R.flags=R.flags&-65537|128;case 0:if(R=N.payload,m=typeof R=="function"?R.call(he,T,m):R,m==null)break e;T=I({},T,m);break e;case 2:na=!0}}m=r.callback,m!==null&&(e.flags|=64,g&&(e.flags|=8192),g=i.callbacks,g===null?i.callbacks=[m]:g.push(m))}else g={lane:m,tag:r.tag,payload:r.payload,callback:r.callback,next:null},b===null?(f=b=g,l=T):b=b.next=g,o|=m;if(r=r.next,r===null){if(r=i.shared.pending,r===null)break;g=r,r=g.next,g.next=null,i.lastBaseUpdate=g,i.shared.pending=null}}while(!0);b===null&&(l=T),i.baseState=l,i.firstBaseUpdate=f,i.lastBaseUpdate=b,s===null&&(i.shared.lanes=0),ua|=o,e.lanes=o,e.memoizedState=T}}function Dc(e,t){if(typeof e!="function")throw Error(h(191,e));e.call(t)}function Ic(e,t){var a=e.callbacks;if(a!==null)for(e.callbacks=null,e=0;e<a.length;e++)Dc(a[e],t)}var hn=u(null),Ji=u(0);function zc(e,t){e=Yt,q(Ji,e),q(hn,t),Yt=e|t.baseLanes}function Mo(){q(Ji,Yt),q(hn,hn.current)}function Oo(){Yt=Ji.current,k(hn),k(Ji)}var ot=u(null),bt=null;function oa(e){var t=e.alternate;q(Ae,Ae.current&1),q(ot,e),bt===null&&(t===null||hn.current!==null||t.memoizedState!==null)&&(bt=e)}function _o(e){q(Ae,Ae.current),q(ot,e),bt===null&&(bt=e)}function Pc(e){e.tag===22?(q(Ae,Ae.current),q(ot,e),bt===null&&(bt=e)):ra()}function ra(){q(Ae,Ae.current),q(ot,ot.current)}function rt(e){k(ot),bt===e&&(bt=null),k(Ae)}var Ae=u(0);function $i(e){for(var t=e;t!==null;){if(t.tag===13){var a=t.memoizedState;if(a!==null&&(a=a.dehydrated,a===null||Zr(a)||Xr(a)))return t}else if(t.tag===19&&(t.memoizedProps.revealOrder==="forwards"||t.memoizedProps.revealOrder==="backwards"||t.memoizedProps.revealOrder==="unstable_legacy-backwards"||t.memoizedProps.revealOrder==="together")){if((t.flags&128)!==0)return t}else if(t.child!==null){t.child.return=t,t=t.child;continue}if(t===e)break;for(;t.sibling===null;){if(t.return===null||t.return===e)return null;t=t.return}t.sibling.return=t.return,t=t.sibling}return null}var Wt=0,Q=null,ue=null,Le=null,es=!1,pn=!1,Ha=!1,ts=0,Jn=0,fn=null,Mp=0;function xe(){throw Error(h(321))}function jo(e,t){if(t===null)return!1;for(var a=0;a<t.length&&a<e.length;a++)if(!it(e[a],t[a]))return!1;return!0}function Qo(e,t,a,n,i,s){return Wt=s,Q=t,t.memoizedState=null,t.updateQueue=null,t.lanes=0,w.H=e===null||e.memoizedState===null?yu:sr,Ha=!1,s=a(n,i),Ha=!1,pn&&(s=Nc(t,a,n,i)),Ec(e),s}function Ec(e){w.H=ti;var t=ue!==null&&ue.next!==null;if(Wt=0,Le=ue=Q=null,es=!1,Jn=0,fn=null,t)throw Error(h(300));e===null||Re||(e=e.dependencies,e!==null&&Gi(e)&&(Re=!0))}function Nc(e,t,a,n){Q=e;var i=0;do{if(pn&&(fn=null),Jn=0,pn=!1,25<=i)throw Error(h(301));if(i+=1,Le=ue=null,e.updateQueue!=null){var s=e.updateQueue;s.lastEffect=null,s.events=null,s.stores=null,s.memoCache!=null&&(s.memoCache.index=0)}w.H=bu,s=t(a,n)}while(pn);return s}function Op(){var e=w.H,t=e.useState()[0];return t=typeof t.then=="function"?$n(t):t,e=e.useState()[0],(ue!==null?ue.memoizedState:null)!==e&&(Q.flags|=1024),t}function Go(){var e=ts!==0;return ts=0,e}function Ko(e,t,a){t.updateQueue=e.updateQueue,t.flags&=-2053,e.lanes&=~a}function Yo(e){if(es){for(e=e.memoizedState;e!==null;){var t=e.queue;t!==null&&(t.pending=null),e=e.next}es=!1}Wt=0,Le=ue=Q=null,pn=!1,Jn=ts=0,fn=null}function Ke(){var e={memoizedState:null,baseState:null,baseQueue:null,queue:null,next:null};return Le===null?Q.memoizedState=Le=e:Le=Le.next=e,Le}function Se(){if(ue===null){var e=Q.alternate;e=e!==null?e.memoizedState:null}else e=ue.next;var t=Le===null?Q.memoizedState:Le.next;if(t!==null)Le=t,ue=e;else{if(e===null)throw Q.alternate===null?Error(h(467)):Error(h(310));ue=e,e={memoizedState:ue.memoizedState,baseState:ue.baseState,baseQueue:ue.baseQueue,queue:ue.queue,next:null},Le===null?Q.memoizedState=Le=e:Le=Le.next=e}return Le}function as(){return{lastEffect:null,events:null,stores:null,memoCache:null}}function $n(e){var t=Jn;return Jn+=1,fn===null&&(fn=[]),e=qc(fn,e,t),t=Q,(Le===null?t.memoizedState:Le.next)===null&&(t=t.alternate,w.H=t===null||t.memoizedState===null?yu:sr),e}function ns(e){if(e!==null&&typeof e=="object"){if(typeof e.then=="function")return $n(e);if(e.$$typeof===B)return We(e)}throw Error(h(438,String(e)))}function Zo(e){var t=null,a=Q.updateQueue;if(a!==null&&(t=a.memoCache),t==null){var n=Q.alternate;n!==null&&(n=n.updateQueue,n!==null&&(n=n.memoCache,n!=null&&(t={data:n.data.map(function(i){return i.slice()}),index:0})))}if(t==null&&(t={data:[],index:0}),a===null&&(a=as(),Q.updateQueue=a),a.memoCache=t,a=t.data[t.index],a===void 0)for(a=t.data[t.index]=Array(e),n=0;n<e;n++)a[n]=_a;return t.index++,a}function Mt(e,t){return typeof t=="function"?t(e):t}function is(e){var t=Se();return Xo(t,ue,e)}function Xo(e,t,a){var n=e.queue;if(n===null)throw Error(h(311));n.lastRenderedReducer=a;var i=e.baseQueue,s=n.pending;if(s!==null){if(i!==null){var o=i.next;i.next=s.next,s.next=o}t.baseQueue=i=s,n.pending=null}if(s=e.baseState,i===null)e.memoizedState=s;else{t=i.next;var r=o=null,l=null,f=t,b=!1;do{var T=f.lane&-536870913;if(T!==f.lane?(F&T)===T:(Wt&T)===T){var m=f.revertLane;if(m===0)l!==null&&(l=l.next={lane:0,revertLane:0,gesture:null,action:f.action,hasEagerState:f.hasEagerState,eagerState:f.eagerState,next:null}),T===ln&&(b=!0);else if((Wt&m)===m){f=f.next,m===ln&&(b=!0);continue}else T={lane:0,revertLane:f.revertLane,gesture:null,action:f.action,hasEagerState:f.hasEagerState,eagerState:f.eagerState,next:null},l===null?(r=l=T,o=s):l=l.next=T,Q.lanes|=m,ua|=m;T=f.action,Ha&&a(s,T),s=f.hasEagerState?f.eagerState:a(s,T)}else m={lane:T,revertLane:f.revertLane,gesture:f.gesture,action:f.action,hasEagerState:f.hasEagerState,eagerState:f.eagerState,next:null},l===null?(r=l=m,o=s):l=l.next=m,Q.lanes|=T,ua|=T;f=f.next}while(f!==null&&f!==t);if(l===null?o=s:l.next=r,!it(s,e.memoizedState)&&(Re=!0,b&&(a=cn,a!==null)))throw a;e.memoizedState=s,e.baseState=o,e.baseQueue=l,n.lastRenderedState=s}return i===null&&(n.lanes=0),[e.memoizedState,n.dispatch]}function Vo(e){var t=Se(),a=t.queue;if(a===null)throw Error(h(311));a.lastRenderedReducer=e;var n=a.dispatch,i=a.pending,s=t.memoizedState;if(i!==null){a.pending=null;var o=i=i.next;do s=e(s,o.action),o=o.next;while(o!==i);it(s,t.memoizedState)||(Re=!0),t.memoizedState=s,t.baseQueue===null&&(t.baseState=s),a.lastRenderedState=s}return[s,n]}function Bc(e,t,a){var n=Q,i=Se(),s=$;if(s){if(a===void 0)throw Error(h(407));a=a()}else a=t();var o=!it((ue||i).memoizedState,a);if(o&&(i.memoizedState=a,Re=!0),i=i.queue,$o(Mc.bind(null,n,i,e),[e]),i.getSnapshot!==t||o||Le!==null&&Le.memoizedState.tag&1){if(n.flags|=2048,mn(9,{destroy:void 0},Wc.bind(null,n,i,a,t),null),me===null)throw Error(h(349));s||(Wt&127)!==0||Hc(n,t,a)}return a}function Hc(e,t,a){e.flags|=16384,e={getSnapshot:t,value:a},t=Q.updateQueue,t===null?(t=as(),Q.updateQueue=t,t.stores=[e]):(a=t.stores,a===null?t.stores=[e]:a.push(e))}function Wc(e,t,a,n){t.value=a,t.getSnapshot=n,Oc(t)&&_c(e)}function Mc(e,t,a){return a(function(){Oc(t)&&_c(e)})}function Oc(e){var t=e.getSnapshot;e=e.value;try{var a=t();return!it(e,a)}catch{return!0}}function _c(e){var t=Ca(e,2);t!==null&&et(t,e,2)}function Fo(e){var t=Ke();if(typeof e=="function"){var a=e;if(e=a(),Ha){Vt(!0);try{a()}finally{Vt(!1)}}}return t.memoizedState=t.baseState=e,t.queue={pending:null,lanes:0,dispatch:null,lastRenderedReducer:Mt,lastRenderedState:e},t}function jc(e,t,a,n){return e.baseState=a,Xo(e,ue,typeof n=="function"?n:Mt)}function _p(e,t,a,n,i){if(rs(e))throw Error(h(485));if(e=t.action,e!==null){var s={payload:i,action:e,next:null,isTransition:!0,status:"pending",value:null,reason:null,listeners:[],then:function(o){s.listeners.push(o)}};w.T!==null?a(!0):s.isTransition=!1,n(s),a=t.pending,a===null?(s.next=t.pending=s,Qc(t,s)):(s.next=a.next,t.pending=a.next=s)}}function Qc(e,t){var a=t.action,n=t.payload,i=e.state;if(t.isTransition){var s=w.T,o={};w.T=o;try{var r=a(i,n),l=w.S;l!==null&&l(o,r),Gc(e,t,r)}catch(f){Jo(e,t,f)}finally{s!==null&&o.types!==null&&(s.types=o.types),w.T=s}}else try{s=a(i,n),Gc(e,t,s)}catch(f){Jo(e,t,f)}}function Gc(e,t,a){a!==null&&typeof a=="object"&&typeof a.then=="function"?a.then(function(n){Kc(e,t,n)},function(n){return Jo(e,t,n)}):Kc(e,t,a)}function Kc(e,t,a){t.status="fulfilled",t.value=a,Yc(t),e.state=a,t=e.pending,t!==null&&(a=t.next,a===t?e.pending=null:(a=a.next,t.next=a,Qc(e,a)))}function Jo(e,t,a){var n=e.pending;if(e.pending=null,n!==null){n=n.next;do t.status="rejected",t.reason=a,Yc(t),t=t.next;while(t!==n)}e.action=null}function Yc(e){e=e.listeners;for(var t=0;t<e.length;t++)(0,e[t])()}function Zc(e,t){return t}function Xc(e,t){if($){var a=me.formState;if(a!==null){e:{var n=Q;if($){if(ge){t:{for(var i=ge,s=yt;i.nodeType!==8;){if(!s){i=null;break t}if(i=vt(i.nextSibling),i===null){i=null;break t}}s=i.data,i=s==="F!"||s==="F"?i:null}if(i){ge=vt(i.nextSibling),n=i.data==="F!";break e}}ta(n)}n=!1}n&&(t=a[0])}}return a=Ke(),a.memoizedState=a.baseState=t,n={pending:null,lanes:0,dispatch:null,lastRenderedReducer:Zc,lastRenderedState:t},a.queue=n,a=fu.bind(null,Q,n),n.dispatch=a,n=Fo(!1),s=ir.bind(null,Q,!1,n.queue),n=Ke(),i={state:t,dispatch:null,action:e,pending:null},n.queue=i,a=_p.bind(null,Q,i,s,a),i.dispatch=a,n.memoizedState=e,[t,a,!1]}function Vc(e){var t=Se();return Fc(t,ue,e)}function Fc(e,t,a){if(t=Xo(e,t,Zc)[0],e=is(Mt)[0],typeof t=="object"&&t!==null&&typeof t.then=="function")try{var n=$n(t)}catch(o){throw o===un?Zi:o}else n=t;t=Se();var i=t.queue,s=i.dispatch;return a!==t.memoizedState&&(Q.flags|=2048,mn(9,{destroy:void 0},jp.bind(null,i,a),null)),[n,s,e]}function jp(e,t){e.action=t}function Jc(e){var t=Se(),a=ue;if(a!==null)return Fc(t,a,e);Se(),t=t.memoizedState,a=Se();var n=a.queue.dispatch;return a.memoizedState=e,[t,n,!1]}function mn(e,t,a,n){return e={tag:e,create:a,deps:n,inst:t,next:null},t=Q.updateQueue,t===null&&(t=as(),Q.updateQueue=t),a=t.lastEffect,a===null?t.lastEffect=e.next=e:(n=a.next,a.next=e,e.next=n,t.lastEffect=e),e}function $c(){return Se().memoizedState}function ss(e,t,a,n){var i=Ke();Q.flags|=e,i.memoizedState=mn(1|t,{destroy:void 0},a,n===void 0?null:n)}function os(e,t,a,n){var i=Se();n=n===void 0?null:n;var s=i.memoizedState.inst;ue!==null&&n!==null&&jo(n,ue.memoizedState.deps)?i.memoizedState=mn(t,s,a,n):(Q.flags|=e,i.memoizedState=mn(1|t,s,a,n))}function eu(e,t){ss(8390656,8,e,t)}function $o(e,t){os(2048,8,e,t)}function Qp(e){Q.flags|=4;var t=Q.updateQueue;if(t===null)t=as(),Q.updateQueue=t,t.events=[e];else{var a=t.events;a===null?t.events=[e]:a.push(e)}}function tu(e){var t=Se().memoizedState;return Qp({ref:t,nextImpl:e}),function(){if((ae&2)!==0)throw Error(h(440));return t.impl.apply(void 0,arguments)}}function au(e,t){return os(4,2,e,t)}function nu(e,t){return os(4,4,e,t)}function iu(e,t){if(typeof t=="function"){e=e();var a=t(e);return function(){typeof a=="function"?a():t(null)}}if(t!=null)return e=e(),t.current=e,function(){t.current=null}}function su(e,t,a){a=a!=null?a.concat([e]):null,os(4,4,iu.bind(null,t,e),a)}function er(){}function ou(e,t){var a=Se();t=t===void 0?null:t;var n=a.memoizedState;return t!==null&&jo(t,n[1])?n[0]:(a.memoizedState=[e,t],e)}function ru(e,t){var a=Se();t=t===void 0?null:t;var n=a.memoizedState;if(t!==null&&jo(t,n[1]))return n[0];if(n=e(),Ha){Vt(!0);try{e()}finally{Vt(!1)}}return a.memoizedState=[n,t],n}function tr(e,t,a){return a===void 0||(Wt&1073741824)!==0&&(F&261930)===0?e.memoizedState=t:(e.memoizedState=a,e=ld(),Q.lanes|=e,ua|=e,a)}function lu(e,t,a,n){return it(a,t)?a:hn.current!==null?(e=tr(e,a,n),it(e,t)||(Re=!0),e):(Wt&42)===0||(Wt&1073741824)!==0&&(F&261930)===0?(Re=!0,e.memoizedState=a):(e=ld(),Q.lanes|=e,ua|=e,t)}function cu(e,t,a,n,i){var s=S.p;S.p=s!==0&&8>s?s:8;var o=w.T,r={};w.T=r,ir(e,!1,t,a);try{var l=i(),f=w.S;if(f!==null&&f(r,l),l!==null&&typeof l=="object"&&typeof l.then=="function"){var b=Wp(l,n);ei(e,t,b,ut(e))}else ei(e,t,n,ut(e))}catch(T){ei(e,t,{then:function(){},status:"rejected",reason:T},ut())}finally{S.p=s,o!==null&&r.types!==null&&(o.types=r.types),w.T=o}}function Gp(){}function ar(e,t,a,n){if(e.tag!==5)throw Error(h(476));var i=uu(e).queue;cu(e,i,t,W,a===null?Gp:function(){return du(e),a(n)})}function uu(e){var t=e.memoizedState;if(t!==null)return t;t={memoizedState:W,baseState:W,baseQueue:null,queue:{pending:null,lanes:0,dispatch:null,lastRenderedReducer:Mt,lastRenderedState:W},next:null};var a={};return t.next={memoizedState:a,baseState:a,baseQueue:null,queue:{pending:null,lanes:0,dispatch:null,lastRenderedReducer:Mt,lastRenderedState:a},next:null},e.memoizedState=t,e=e.alternate,e!==null&&(e.memoizedState=t),t}function du(e){var t=uu(e);t.next===null&&(t=e.alternate.memoizedState),ei(e,t.next.queue,{},ut())}function nr(){return We(yi)}function hu(){return Se().memoizedState}function pu(){return Se().memoizedState}function Kp(e){for(var t=e.return;t!==null;){switch(t.tag){case 24:case 3:var a=ut();e=ia(a);var n=sa(t,e,a);n!==null&&(et(n,t,a),Xn(n,t,a)),t={cache:Io()},e.payload=t;return}t=t.return}}function Yp(e,t,a){var n=ut();a={lane:n,revertLane:0,gesture:null,action:a,hasEagerState:!1,eagerState:null,next:null},rs(e)?mu(t,a):(a=xo(e,t,a,n),a!==null&&(et(a,e,n),gu(a,t,n)))}function fu(e,t,a){var n=ut();ei(e,t,a,n)}function ei(e,t,a,n){var i={lane:n,revertLane:0,gesture:null,action:a,hasEagerState:!1,eagerState:null,next:null};if(rs(e))mu(t,i);else{var s=e.alternate;if(e.lanes===0&&(s===null||s.lanes===0)&&(s=t.lastRenderedReducer,s!==null))try{var o=t.lastRenderedState,r=s(o,a);if(i.hasEagerState=!0,i.eagerState=r,it(r,o))return Oi(e,t,i,0),me===null&&Mi(),!1}catch{}finally{}if(a=xo(e,t,i,n),a!==null)return et(a,e,n),gu(a,t,n),!0}return!1}function ir(e,t,a,n){if(n={lane:2,revertLane:Nr(),gesture:null,action:n,hasEagerState:!1,eagerState:null,next:null},rs(e)){if(t)throw Error(h(479))}else t=xo(e,a,n,2),t!==null&&et(t,e,2)}function rs(e){var t=e.alternate;return e===Q||t!==null&&t===Q}function mu(e,t){pn=es=!0;var a=e.pending;a===null?t.next=t:(t.next=a.next,a.next=t),e.pending=t}function gu(e,t,a){if((a&4194048)!==0){var n=t.lanes;n&=e.pendingLanes,a|=n,t.lanes=a,xl(e,a)}}var ti={readContext:We,use:ns,useCallback:xe,useContext:xe,useEffect:xe,useImperativeHandle:xe,useLayoutEffect:xe,useInsertionEffect:xe,useMemo:xe,useReducer:xe,useRef:xe,useState:xe,useDebugValue:xe,useDeferredValue:xe,useTransition:xe,useSyncExternalStore:xe,useId:xe,useHostTransitionStatus:xe,useFormState:xe,useActionState:xe,useOptimistic:xe,useMemoCache:xe,useCacheRefresh:xe};ti.useEffectEvent=xe;var yu={readContext:We,use:ns,useCallback:function(e,t){return Ke().memoizedState=[e,t===void 0?null:t],e},useContext:We,useEffect:eu,useImperativeHandle:function(e,t,a){a=a!=null?a.concat([e]):null,ss(4194308,4,iu.bind(null,t,e),a)},useLayoutEffect:function(e,t){return ss(4194308,4,e,t)},useInsertionEffect:function(e,t){ss(4,2,e,t)},useMemo:function(e,t){var a=Ke();t=t===void 0?null:t;var n=e();if(Ha){Vt(!0);try{e()}finally{Vt(!1)}}return a.memoizedState=[n,t],n},useReducer:function(e,t,a){var n=Ke();if(a!==void 0){var i=a(t);if(Ha){Vt(!0);try{a(t)}finally{Vt(!1)}}}else i=t;return n.memoizedState=n.baseState=i,e={pending:null,lanes:0,dispatch:null,lastRenderedReducer:e,lastRenderedState:i},n.queue=e,e=e.dispatch=Yp.bind(null,Q,e),[n.memoizedState,e]},useRef:function(e){var t=Ke();return e={current:e},t.memoizedState=e},useState:function(e){e=Fo(e);var t=e.queue,a=fu.bind(null,Q,t);return t.dispatch=a,[e.memoizedState,a]},useDebugValue:er,useDeferredValue:function(e,t){var a=Ke();return tr(a,e,t)},useTransition:function(){var e=Fo(!1);return e=cu.bind(null,Q,e.queue,!0,!1),Ke().memoizedState=e,[!1,e]},useSyncExternalStore:function(e,t,a){var n=Q,i=Ke();if($){if(a===void 0)throw Error(h(407));a=a()}else{if(a=t(),me===null)throw Error(h(349));(F&127)!==0||Hc(n,t,a)}i.memoizedState=a;var s={value:a,getSnapshot:t};return i.queue=s,eu(Mc.bind(null,n,s,e),[e]),n.flags|=2048,mn(9,{destroy:void 0},Wc.bind(null,n,s,a,t),null),a},useId:function(){var e=Ke(),t=me.identifierPrefix;if($){var a=Rt,n=Lt;a=(n&~(1<<32-nt(n)-1)).toString(32)+a,t="_"+t+"R_"+a,a=ts++,0<a&&(t+="H"+a.toString(32)),t+="_"}else a=Mp++,t="_"+t+"r_"+a.toString(32)+"_";return e.memoizedState=t},useHostTransitionStatus:nr,useFormState:Xc,useActionState:Xc,useOptimistic:function(e){var t=Ke();t.memoizedState=t.baseState=e;var a={pending:null,lanes:0,dispatch:null,lastRenderedReducer:null,lastRenderedState:null};return t.queue=a,t=ir.bind(null,Q,!0,a),a.dispatch=t,[e,t]},useMemoCache:Zo,useCacheRefresh:function(){return Ke().memoizedState=Kp.bind(null,Q)},useEffectEvent:function(e){var t=Ke(),a={impl:e};return t.memoizedState=a,function(){if((ae&2)!==0)throw Error(h(440));return a.impl.apply(void 0,arguments)}}},sr={readContext:We,use:ns,useCallback:ou,useContext:We,useEffect:$o,useImperativeHandle:su,useInsertionEffect:au,useLayoutEffect:nu,useMemo:ru,useReducer:is,useRef:$c,useState:function(){return is(Mt)},useDebugValue:er,useDeferredValue:function(e,t){var a=Se();return lu(a,ue.memoizedState,e,t)},useTransition:function(){var e=is(Mt)[0],t=Se().memoizedState;return[typeof e=="boolean"?e:$n(e),t]},useSyncExternalStore:Bc,useId:hu,useHostTransitionStatus:nr,useFormState:Vc,useActionState:Vc,useOptimistic:function(e,t){var a=Se();return jc(a,ue,e,t)},useMemoCache:Zo,useCacheRefresh:pu};sr.useEffectEvent=tu;var bu={readContext:We,use:ns,useCallback:ou,useContext:We,useEffect:$o,useImperativeHandle:su,useInsertionEffect:au,useLayoutEffect:nu,useMemo:ru,useReducer:Vo,useRef:$c,useState:function(){return Vo(Mt)},useDebugValue:er,useDeferredValue:function(e,t){var a=Se();return ue===null?tr(a,e,t):lu(a,ue.memoizedState,e,t)},useTransition:function(){var e=Vo(Mt)[0],t=Se().memoizedState;return[typeof e=="boolean"?e:$n(e),t]},useSyncExternalStore:Bc,useId:hu,useHostTransitionStatus:nr,useFormState:Jc,useActionState:Jc,useOptimistic:function(e,t){var a=Se();return ue!==null?jc(a,ue,e,t):(a.baseState=e,[e,a.queue.dispatch])},useMemoCache:Zo,useCacheRefresh:pu};bu.useEffectEvent=tu;function or(e,t,a,n){t=e.memoizedState,a=a(n,t),a=a==null?t:I({},t,a),e.memoizedState=a,e.lanes===0&&(e.updateQueue.baseState=a)}var rr={enqueueSetState:function(e,t,a){e=e._reactInternals;var n=ut(),i=ia(n);i.payload=t,a!=null&&(i.callback=a),t=sa(e,i,n),t!==null&&(et(t,e,n),Xn(t,e,n))},enqueueReplaceState:function(e,t,a){e=e._reactInternals;var n=ut(),i=ia(n);i.tag=1,i.payload=t,a!=null&&(i.callback=a),t=sa(e,i,n),t!==null&&(et(t,e,n),Xn(t,e,n))},enqueueForceUpdate:function(e,t){e=e._reactInternals;var a=ut(),n=ia(a);n.tag=2,t!=null&&(n.callback=t),t=sa(e,n,a),t!==null&&(et(t,e,a),Xn(t,e,a))}};function vu(e,t,a,n,i,s,o){return e=e.stateNode,typeof e.shouldComponentUpdate=="function"?e.shouldComponentUpdate(n,s,o):t.prototype&&t.prototype.isPureReactComponent?!On(a,n)||!On(i,s):!0}function wu(e,t,a,n){e=t.state,typeof t.componentWillReceiveProps=="function"&&t.componentWillReceiveProps(a,n),typeof t.UNSAFE_componentWillReceiveProps=="function"&&t.UNSAFE_componentWillReceiveProps(a,n),t.state!==e&&rr.enqueueReplaceState(t,t.state,null)}function Wa(e,t){var a=t;if("ref"in t){a={};for(var n in t)n!=="ref"&&(a[n]=t[n])}if(e=e.defaultProps){a===t&&(a=I({},a));for(var i in e)a[i]===void 0&&(a[i]=e[i])}return a}function xu(e){Wi(e)}function Tu(e){console.error(e)}function ku(e){Wi(e)}function ls(e,t){try{var a=e.onUncaughtError;a(t.value,{componentStack:t.stack})}catch(n){setTimeout(function(){throw n})}}function Au(e,t,a){try{var n=e.onCaughtError;n(a.value,{componentStack:a.stack,errorBoundary:t.tag===1?t.stateNode:null})}catch(i){setTimeout(function(){throw i})}}function lr(e,t,a){return a=ia(a),a.tag=3,a.payload={element:null},a.callback=function(){ls(e,t)},a}function Su(e){return e=ia(e),e.tag=3,e}function qu(e,t,a,n){var i=a.type.getDerivedStateFromError;if(typeof i=="function"){var s=n.value;e.payload=function(){return i(s)},e.callback=function(){Au(t,a,n)}}var o=a.stateNode;o!==null&&typeof o.componentDidCatch=="function"&&(e.callback=function(){Au(t,a,n),typeof i!="function"&&(da===null?da=new Set([this]):da.add(this));var r=n.stack;this.componentDidCatch(n.value,{componentStack:r!==null?r:""})})}function Zp(e,t,a,n,i){if(a.flags|=32768,n!==null&&typeof n=="object"&&typeof n.then=="function"){if(t=a.alternate,t!==null&&rn(t,a,i,!0),a=ot.current,a!==null){switch(a.tag){case 31:case 13:return bt===null?ws():a.alternate===null&&Te===0&&(Te=3),a.flags&=-257,a.flags|=65536,a.lanes=i,n===Xi?a.flags|=16384:(t=a.updateQueue,t===null?a.updateQueue=new Set([n]):t.add(n),zr(e,n,i)),!1;case 22:return a.flags|=65536,n===Xi?a.flags|=16384:(t=a.updateQueue,t===null?(t={transitions:null,markerInstances:null,retryQueue:new Set([n])},a.updateQueue=t):(a=t.retryQueue,a===null?t.retryQueue=new Set([n]):a.add(n)),zr(e,n,i)),!1}throw Error(h(435,a.tag))}return zr(e,n,i),ws(),!1}if($)return t=ot.current,t!==null?((t.flags&65536)===0&&(t.flags|=256),t.flags|=65536,t.lanes=i,n!==Lo&&(e=Error(h(422),{cause:n}),Qn(ft(e,a)))):(n!==Lo&&(t=Error(h(423),{cause:n}),Qn(ft(t,a))),e=e.current.alternate,e.flags|=65536,i&=-i,e.lanes|=i,n=ft(n,a),i=lr(e.stateNode,n,i),Ho(e,i),Te!==4&&(Te=2)),!1;var s=Error(h(520),{cause:n});if(s=ft(s,a),ci===null?ci=[s]:ci.push(s),Te!==4&&(Te=2),t===null)return!0;n=ft(n,a),a=t;do{switch(a.tag){case 3:return a.flags|=65536,e=i&-i,a.lanes|=e,e=lr(a.stateNode,n,e),Ho(a,e),!1;case 1:if(t=a.type,s=a.stateNode,(a.flags&128)===0&&(typeof t.getDerivedStateFromError=="function"||s!==null&&typeof s.componentDidCatch=="function"&&(da===null||!da.has(s))))return a.flags|=65536,i&=-i,a.lanes|=i,i=Su(i),qu(i,e,a,n),Ho(a,i),!1}a=a.return}while(a!==null);return!1}var cr=Error(h(461)),Re=!1;function Me(e,t,a,n){t.child=e===null?Uc(t,null,a,n):Ba(t,e.child,a,n)}function Lu(e,t,a,n,i){a=a.render;var s=t.ref;if("ref"in n){var o={};for(var r in n)r!=="ref"&&(o[r]=n[r])}else o=n;return za(t),n=Qo(e,t,a,o,s,i),r=Go(),e!==null&&!Re?(Ko(e,t,i),Ot(e,t,i)):($&&r&&So(t),t.flags|=1,Me(e,t,n,i),t.child)}function Ru(e,t,a,n,i){if(e===null){var s=a.type;return typeof s=="function"&&!To(s)&&s.defaultProps===void 0&&a.compare===null?(t.tag=15,t.type=s,Cu(e,t,s,n,i)):(e=ji(a.type,null,n,t,t.mode,i),e.ref=t.ref,e.return=t,t.child=e)}if(s=e.child,!yr(e,i)){var o=s.memoizedProps;if(a=a.compare,a=a!==null?a:On,a(o,n)&&e.ref===t.ref)return Ot(e,t,i)}return t.flags|=1,e=Et(s,n),e.ref=t.ref,e.return=t,t.child=e}function Cu(e,t,a,n,i){if(e!==null){var s=e.memoizedProps;if(On(s,n)&&e.ref===t.ref)if(Re=!1,t.pendingProps=n=s,yr(e,i))(e.flags&131072)!==0&&(Re=!0);else return t.lanes=e.lanes,Ot(e,t,i)}return ur(e,t,a,n,i)}function Uu(e,t,a,n){var i=n.children,s=e!==null?e.memoizedState:null;if(e===null&&t.stateNode===null&&(t.stateNode={_visibility:1,_pendingMarkers:null,_retryCache:null,_transitions:null}),n.mode==="hidden"){if((t.flags&128)!==0){if(s=s!==null?s.baseLanes|a:a,e!==null){for(n=t.child=e.child,i=0;n!==null;)i=i|n.lanes|n.childLanes,n=n.sibling;n=i&~s}else n=0,t.child=null;return Du(e,t,s,a,n)}if((a&536870912)!==0)t.memoizedState={baseLanes:0,cachePool:null},e!==null&&Yi(t,s!==null?s.cachePool:null),s!==null?zc(t,s):Mo(),Pc(t);else return n=t.lanes=536870912,Du(e,t,s!==null?s.baseLanes|a:a,a,n)}else s!==null?(Yi(t,s.cachePool),zc(t,s),ra(),t.memoizedState=null):(e!==null&&Yi(t,null),Mo(),ra());return Me(e,t,i,a),t.child}function ai(e,t){return e!==null&&e.tag===22||t.stateNode!==null||(t.stateNode={_visibility:1,_pendingMarkers:null,_retryCache:null,_transitions:null}),t.sibling}function Du(e,t,a,n,i){var s=Po();return s=s===null?null:{parent:qe._currentValue,pool:s},t.memoizedState={baseLanes:a,cachePool:s},e!==null&&Yi(t,null),Mo(),Pc(t),e!==null&&rn(e,t,n,!0),t.childLanes=i,null}function cs(e,t){return t=ds({mode:t.mode,children:t.children},e.mode),t.ref=e.ref,e.child=t,t.return=e,t}function Iu(e,t,a){return Ba(t,e.child,null,a),e=cs(t,t.pendingProps),e.flags|=2,rt(t),t.memoizedState=null,e}function Xp(e,t,a){var n=t.pendingProps,i=(t.flags&128)!==0;if(t.flags&=-129,e===null){if($){if(n.mode==="hidden")return e=cs(t,n),t.lanes=536870912,ai(null,e);if(_o(t),(e=ge)?(e=Qd(e,yt),e=e!==null&&e.data==="&"?e:null,e!==null&&(t.memoizedState={dehydrated:e,treeContext:$t!==null?{id:Lt,overflow:Rt}:null,retryLane:536870912,hydrationErrors:null},a=mc(e),a.return=t,t.child=a,He=t,ge=null)):e=null,e===null)throw ta(t);return t.lanes=536870912,null}return cs(t,n)}var s=e.memoizedState;if(s!==null){var o=s.dehydrated;if(_o(t),i)if(t.flags&256)t.flags&=-257,t=Iu(e,t,a);else if(t.memoizedState!==null)t.child=e.child,t.flags|=128,t=null;else throw Error(h(558));else if(Re||rn(e,t,a,!1),i=(a&e.childLanes)!==0,Re||i){if(n=me,n!==null&&(o=Tl(n,a),o!==0&&o!==s.retryLane))throw s.retryLane=o,Ca(e,o),et(n,e,o),cr;ws(),t=Iu(e,t,a)}else e=s.treeContext,ge=vt(o.nextSibling),He=t,$=!0,ea=null,yt=!1,e!==null&&bc(t,e),t=cs(t,n),t.flags|=4096;return t}return e=Et(e.child,{mode:n.mode,children:n.children}),e.ref=t.ref,t.child=e,e.return=t,e}function us(e,t){var a=t.ref;if(a===null)e!==null&&e.ref!==null&&(t.flags|=4194816);else{if(typeof a!="function"&&typeof a!="object")throw Error(h(284));(e===null||e.ref!==a)&&(t.flags|=4194816)}}function ur(e,t,a,n,i){return za(t),a=Qo(e,t,a,n,void 0,i),n=Go(),e!==null&&!Re?(Ko(e,t,i),Ot(e,t,i)):($&&n&&So(t),t.flags|=1,Me(e,t,a,i),t.child)}function zu(e,t,a,n,i,s){return za(t),t.updateQueue=null,a=Nc(t,n,a,i),Ec(e),n=Go(),e!==null&&!Re?(Ko(e,t,s),Ot(e,t,s)):($&&n&&So(t),t.flags|=1,Me(e,t,a,s),t.child)}function Pu(e,t,a,n,i){if(za(t),t.stateNode===null){var s=an,o=a.contextType;typeof o=="object"&&o!==null&&(s=We(o)),s=new a(n,s),t.memoizedState=s.state!==null&&s.state!==void 0?s.state:null,s.updater=rr,t.stateNode=s,s._reactInternals=t,s=t.stateNode,s.props=n,s.state=t.memoizedState,s.refs={},No(t),o=a.contextType,s.context=typeof o=="object"&&o!==null?We(o):an,s.state=t.memoizedState,o=a.getDerivedStateFromProps,typeof o=="function"&&(or(t,a,o,n),s.state=t.memoizedState),typeof a.getDerivedStateFromProps=="function"||typeof s.getSnapshotBeforeUpdate=="function"||typeof s.UNSAFE_componentWillMount!="function"&&typeof s.componentWillMount!="function"||(o=s.state,typeof s.componentWillMount=="function"&&s.componentWillMount(),typeof s.UNSAFE_componentWillMount=="function"&&s.UNSAFE_componentWillMount(),o!==s.state&&rr.enqueueReplaceState(s,s.state,null),Fn(t,n,s,i),Vn(),s.state=t.memoizedState),typeof s.componentDidMount=="function"&&(t.flags|=4194308),n=!0}else if(e===null){s=t.stateNode;var r=t.memoizedProps,l=Wa(a,r);s.props=l;var f=s.context,b=a.contextType;o=an,typeof b=="object"&&b!==null&&(o=We(b));var T=a.getDerivedStateFromProps;b=typeof T=="function"||typeof s.getSnapshotBeforeUpdate=="function",r=t.pendingProps!==r,b||typeof s.UNSAFE_componentWillReceiveProps!="function"&&typeof s.componentWillReceiveProps!="function"||(r||f!==o)&&wu(t,s,n,o),na=!1;var m=t.memoizedState;s.state=m,Fn(t,n,s,i),Vn(),f=t.memoizedState,r||m!==f||na?(typeof T=="function"&&(or(t,a,T,n),f=t.memoizedState),(l=na||vu(t,a,l,n,m,f,o))?(b||typeof s.UNSAFE_componentWillMount!="function"&&typeof s.componentWillMount!="function"||(typeof s.componentWillMount=="function"&&s.componentWillMount(),typeof s.UNSAFE_componentWillMount=="function"&&s.UNSAFE_componentWillMount()),typeof s.componentDidMount=="function"&&(t.flags|=4194308)):(typeof s.componentDidMount=="function"&&(t.flags|=4194308),t.memoizedProps=n,t.memoizedState=f),s.props=n,s.state=f,s.context=o,n=l):(typeof s.componentDidMount=="function"&&(t.flags|=4194308),n=!1)}else{s=t.stateNode,Bo(e,t),o=t.memoizedProps,b=Wa(a,o),s.props=b,T=t.pendingProps,m=s.context,f=a.contextType,l=an,typeof f=="object"&&f!==null&&(l=We(f)),r=a.getDerivedStateFromProps,(f=typeof r=="function"||typeof s.getSnapshotBeforeUpdate=="function")||typeof s.UNSAFE_componentWillReceiveProps!="function"&&typeof s.componentWillReceiveProps!="function"||(o!==T||m!==l)&&wu(t,s,n,l),na=!1,m=t.memoizedState,s.state=m,Fn(t,n,s,i),Vn();var g=t.memoizedState;o!==T||m!==g||na||e!==null&&e.dependencies!==null&&Gi(e.dependencies)?(typeof r=="function"&&(or(t,a,r,n),g=t.memoizedState),(b=na||vu(t,a,b,n,m,g,l)||e!==null&&e.dependencies!==null&&Gi(e.dependencies))?(f||typeof s.UNSAFE_componentWillUpdate!="function"&&typeof s.componentWillUpdate!="function"||(typeof s.componentWillUpdate=="function"&&s.componentWillUpdate(n,g,l),typeof s.UNSAFE_componentWillUpdate=="function"&&s.UNSAFE_componentWillUpdate(n,g,l)),typeof s.componentDidUpdate=="function"&&(t.flags|=4),typeof s.getSnapshotBeforeUpdate=="function"&&(t.flags|=1024)):(typeof s.componentDidUpdate!="function"||o===e.memoizedProps&&m===e.memoizedState||(t.flags|=4),typeof s.getSnapshotBeforeUpdate!="function"||o===e.memoizedProps&&m===e.memoizedState||(t.flags|=1024),t.memoizedProps=n,t.memoizedState=g),s.props=n,s.state=g,s.context=l,n=b):(typeof s.componentDidUpdate!="function"||o===e.memoizedProps&&m===e.memoizedState||(t.flags|=4),typeof s.getSnapshotBeforeUpdate!="function"||o===e.memoizedProps&&m===e.memoizedState||(t.flags|=1024),n=!1)}return s=n,us(e,t),n=(t.flags&128)!==0,s||n?(s=t.stateNode,a=n&&typeof a.getDerivedStateFromError!="function"?null:s.render(),t.flags|=1,e!==null&&n?(t.child=Ba(t,e.child,null,i),t.child=Ba(t,null,a,i)):Me(e,t,a,i),t.memoizedState=s.state,e=t.child):e=Ot(e,t,i),e}function Eu(e,t,a,n){return Da(),t.flags|=256,Me(e,t,a,n),t.child}var dr={dehydrated:null,treeContext:null,retryLane:0,hydrationErrors:null};function hr(e){return{baseLanes:e,cachePool:Ac()}}function pr(e,t,a){return e=e!==null?e.childLanes&~a:0,t&&(e|=ct),e}function Nu(e,t,a){var n=t.pendingProps,i=!1,s=(t.flags&128)!==0,o;if((o=s)||(o=e!==null&&e.memoizedState===null?!1:(Ae.current&2)!==0),o&&(i=!0,t.flags&=-129),o=(t.flags&32)!==0,t.flags&=-33,e===null){if($){if(i?oa(t):ra(),(e=ge)?(e=Qd(e,yt),e=e!==null&&e.data!=="&"?e:null,e!==null&&(t.memoizedState={dehydrated:e,treeContext:$t!==null?{id:Lt,overflow:Rt}:null,retryLane:536870912,hydrationErrors:null},a=mc(e),a.return=t,t.child=a,He=t,ge=null)):e=null,e===null)throw ta(t);return Xr(e)?t.lanes=32:t.lanes=536870912,null}var r=n.children;return n=n.fallback,i?(ra(),i=t.mode,r=ds({mode:"hidden",children:r},i),n=Ua(n,i,a,null),r.return=t,n.return=t,r.sibling=n,t.child=r,n=t.child,n.memoizedState=hr(a),n.childLanes=pr(e,o,a),t.memoizedState=dr,ai(null,n)):(oa(t),fr(t,r))}var l=e.memoizedState;if(l!==null&&(r=l.dehydrated,r!==null)){if(s)t.flags&256?(oa(t),t.flags&=-257,t=mr(e,t,a)):t.memoizedState!==null?(ra(),t.child=e.child,t.flags|=128,t=null):(ra(),r=n.fallback,i=t.mode,n=ds({mode:"visible",children:n.children},i),r=Ua(r,i,a,null),r.flags|=2,n.return=t,r.return=t,n.sibling=r,t.child=n,Ba(t,e.child,null,a),n=t.child,n.memoizedState=hr(a),n.childLanes=pr(e,o,a),t.memoizedState=dr,t=ai(null,n));else if(oa(t),Xr(r)){if(o=r.nextSibling&&r.nextSibling.dataset,o)var f=o.dgst;o=f,n=Error(h(419)),n.stack="",n.digest=o,Qn({value:n,source:null,stack:null}),t=mr(e,t,a)}else if(Re||rn(e,t,a,!1),o=(a&e.childLanes)!==0,Re||o){if(o=me,o!==null&&(n=Tl(o,a),n!==0&&n!==l.retryLane))throw l.retryLane=n,Ca(e,n),et(o,e,n),cr;Zr(r)||ws(),t=mr(e,t,a)}else Zr(r)?(t.flags|=192,t.child=e.child,t=null):(e=l.treeContext,ge=vt(r.nextSibling),He=t,$=!0,ea=null,yt=!1,e!==null&&bc(t,e),t=fr(t,n.children),t.flags|=4096);return t}return i?(ra(),r=n.fallback,i=t.mode,l=e.child,f=l.sibling,n=Et(l,{mode:"hidden",children:n.children}),n.subtreeFlags=l.subtreeFlags&65011712,f!==null?r=Et(f,r):(r=Ua(r,i,a,null),r.flags|=2),r.return=t,n.return=t,n.sibling=r,t.child=n,ai(null,n),n=t.child,r=e.child.memoizedState,r===null?r=hr(a):(i=r.cachePool,i!==null?(l=qe._currentValue,i=i.parent!==l?{parent:l,pool:l}:i):i=Ac(),r={baseLanes:r.baseLanes|a,cachePool:i}),n.memoizedState=r,n.childLanes=pr(e,o,a),t.memoizedState=dr,ai(e.child,n)):(oa(t),a=e.child,e=a.sibling,a=Et(a,{mode:"visible",children:n.children}),a.return=t,a.sibling=null,e!==null&&(o=t.deletions,o===null?(t.deletions=[e],t.flags|=16):o.push(e)),t.child=a,t.memoizedState=null,a)}function fr(e,t){return t=ds({mode:"visible",children:t},e.mode),t.return=e,e.child=t}function ds(e,t){return e=st(22,e,null,t),e.lanes=0,e}function mr(e,t,a){return Ba(t,e.child,null,a),e=fr(t,t.pendingProps.children),e.flags|=2,t.memoizedState=null,e}function Bu(e,t,a){e.lanes|=t;var n=e.alternate;n!==null&&(n.lanes|=t),Uo(e.return,t,a)}function gr(e,t,a,n,i,s){var o=e.memoizedState;o===null?e.memoizedState={isBackwards:t,rendering:null,renderingStartTime:0,last:n,tail:a,tailMode:i,treeForkCount:s}:(o.isBackwards=t,o.rendering=null,o.renderingStartTime=0,o.last=n,o.tail=a,o.tailMode=i,o.treeForkCount=s)}function Hu(e,t,a){var n=t.pendingProps,i=n.revealOrder,s=n.tail;n=n.children;var o=Ae.current,r=(o&2)!==0;if(r?(o=o&1|2,t.flags|=128):o&=1,q(Ae,o),Me(e,t,n,a),n=$?jn:0,!r&&e!==null&&(e.flags&128)!==0)e:for(e=t.child;e!==null;){if(e.tag===13)e.memoizedState!==null&&Bu(e,a,t);else if(e.tag===19)Bu(e,a,t);else if(e.child!==null){e.child.return=e,e=e.child;continue}if(e===t)break e;for(;e.sibling===null;){if(e.return===null||e.return===t)break e;e=e.return}e.sibling.return=e.return,e=e.sibling}switch(i){case"forwards":for(a=t.child,i=null;a!==null;)e=a.alternate,e!==null&&$i(e)===null&&(i=a),a=a.sibling;a=i,a===null?(i=t.child,t.child=null):(i=a.sibling,a.sibling=null),gr(t,!1,i,a,s,n);break;case"backwards":case"unstable_legacy-backwards":for(a=null,i=t.child,t.child=null;i!==null;){if(e=i.alternate,e!==null&&$i(e)===null){t.child=i;break}e=i.sibling,i.sibling=a,a=i,i=e}gr(t,!0,a,null,s,n);break;case"together":gr(t,!1,null,null,void 0,n);break;default:t.memoizedState=null}return t.child}function Ot(e,t,a){if(e!==null&&(t.dependencies=e.dependencies),ua|=t.lanes,(a&t.childLanes)===0)if(e!==null){if(rn(e,t,a,!1),(a&t.childLanes)===0)return null}else return null;if(e!==null&&t.child!==e.child)throw Error(h(153));if(t.child!==null){for(e=t.child,a=Et(e,e.pendingProps),t.child=a,a.return=t;e.sibling!==null;)e=e.sibling,a=a.sibling=Et(e,e.pendingProps),a.return=t;a.sibling=null}return t.child}function yr(e,t){return(e.lanes&t)!==0?!0:(e=e.dependencies,!!(e!==null&&Gi(e)))}function Vp(e,t,a){switch(t.tag){case 3:Ge(t,t.stateNode.containerInfo),aa(t,qe,e.memoizedState.cache),Da();break;case 27:case 5:Rn(t);break;case 4:Ge(t,t.stateNode.containerInfo);break;case 10:aa(t,t.type,t.memoizedProps.value);break;case 31:if(t.memoizedState!==null)return t.flags|=128,_o(t),null;break;case 13:var n=t.memoizedState;if(n!==null)return n.dehydrated!==null?(oa(t),t.flags|=128,null):(a&t.child.childLanes)!==0?Nu(e,t,a):(oa(t),e=Ot(e,t,a),e!==null?e.sibling:null);oa(t);break;case 19:var i=(e.flags&128)!==0;if(n=(a&t.childLanes)!==0,n||(rn(e,t,a,!1),n=(a&t.childLanes)!==0),i){if(n)return Hu(e,t,a);t.flags|=128}if(i=t.memoizedState,i!==null&&(i.rendering=null,i.tail=null,i.lastEffect=null),q(Ae,Ae.current),n)break;return null;case 22:return t.lanes=0,Uu(e,t,a,t.pendingProps);case 24:aa(t,qe,e.memoizedState.cache)}return Ot(e,t,a)}function Wu(e,t,a){if(e!==null)if(e.memoizedProps!==t.pendingProps)Re=!0;else{if(!yr(e,a)&&(t.flags&128)===0)return Re=!1,Vp(e,t,a);Re=(e.flags&131072)!==0}else Re=!1,$&&(t.flags&1048576)!==0&&yc(t,jn,t.index);switch(t.lanes=0,t.tag){case 16:e:{var n=t.pendingProps;if(e=Ea(t.elementType),t.type=e,typeof e=="function")To(e)?(n=Wa(e,n),t.tag=1,t=Pu(null,t,e,n,a)):(t.tag=0,t=ur(null,t,e,n,a));else{if(e!=null){var i=e.$$typeof;if(i===pe){t.tag=11,t=Lu(null,t,e,n,a);break e}else if(i===Y){t.tag=14,t=Ru(null,t,e,n,a);break e}}throw t=Dt(e)||e,Error(h(306,t,""))}}return t;case 0:return ur(e,t,t.type,t.pendingProps,a);case 1:return n=t.type,i=Wa(n,t.pendingProps),Pu(e,t,n,i,a);case 3:e:{if(Ge(t,t.stateNode.containerInfo),e===null)throw Error(h(387));n=t.pendingProps;var s=t.memoizedState;i=s.element,Bo(e,t),Fn(t,n,null,a);var o=t.memoizedState;if(n=o.cache,aa(t,qe,n),n!==s.cache&&Do(t,[qe],a,!0),Vn(),n=o.element,s.isDehydrated)if(s={element:n,isDehydrated:!1,cache:o.cache},t.updateQueue.baseState=s,t.memoizedState=s,t.flags&256){t=Eu(e,t,n,a);break e}else if(n!==i){i=ft(Error(h(424)),t),Qn(i),t=Eu(e,t,n,a);break e}else{switch(e=t.stateNode.containerInfo,e.nodeType){case 9:e=e.body;break;default:e=e.nodeName==="HTML"?e.ownerDocument.body:e}for(ge=vt(e.firstChild),He=t,$=!0,ea=null,yt=!0,a=Uc(t,null,n,a),t.child=a;a;)a.flags=a.flags&-3|4096,a=a.sibling}else{if(Da(),n===i){t=Ot(e,t,a);break e}Me(e,t,n,a)}t=t.child}return t;case 26:return us(e,t),e===null?(a=Vd(t.type,null,t.pendingProps,null))?t.memoizedState=a:$||(a=t.type,e=t.pendingProps,n=Ls(Z.current).createElement(a),n[Be]=t,n[Ze]=e,Oe(n,a,e),Ie(n),t.stateNode=n):t.memoizedState=Vd(t.type,e.memoizedProps,t.pendingProps,e.memoizedState),null;case 27:return Rn(t),e===null&&$&&(n=t.stateNode=Yd(t.type,t.pendingProps,Z.current),He=t,yt=!0,i=ge,ma(t.type)?(Vr=i,ge=vt(n.firstChild)):ge=i),Me(e,t,t.pendingProps.children,a),us(e,t),e===null&&(t.flags|=4194304),t.child;case 5:return e===null&&$&&((i=n=ge)&&(n=Lf(n,t.type,t.pendingProps,yt),n!==null?(t.stateNode=n,He=t,ge=vt(n.firstChild),yt=!1,i=!0):i=!1),i||ta(t)),Rn(t),i=t.type,s=t.pendingProps,o=e!==null?e.memoizedProps:null,n=s.children,Gr(i,s)?n=null:o!==null&&Gr(i,o)&&(t.flags|=32),t.memoizedState!==null&&(i=Qo(e,t,Op,null,null,a),yi._currentValue=i),us(e,t),Me(e,t,n,a),t.child;case 6:return e===null&&$&&((e=a=ge)&&(a=Rf(a,t.pendingProps,yt),a!==null?(t.stateNode=a,He=t,ge=null,e=!0):e=!1),e||ta(t)),null;case 13:return Nu(e,t,a);case 4:return Ge(t,t.stateNode.containerInfo),n=t.pendingProps,e===null?t.child=Ba(t,null,n,a):Me(e,t,n,a),t.child;case 11:return Lu(e,t,t.type,t.pendingProps,a);case 7:return Me(e,t,t.pendingProps,a),t.child;case 8:return Me(e,t,t.pendingProps.children,a),t.child;case 12:return Me(e,t,t.pendingProps.children,a),t.child;case 10:return n=t.pendingProps,aa(t,t.type,n.value),Me(e,t,n.children,a),t.child;case 9:return i=t.type._context,n=t.pendingProps.children,za(t),i=We(i),n=n(i),t.flags|=1,Me(e,t,n,a),t.child;case 14:return Ru(e,t,t.type,t.pendingProps,a);case 15:return Cu(e,t,t.type,t.pendingProps,a);case 19:return Hu(e,t,a);case 31:return Xp(e,t,a);case 22:return Uu(e,t,a,t.pendingProps);case 24:return za(t),n=We(qe),e===null?(i=Po(),i===null&&(i=me,s=Io(),i.pooledCache=s,s.refCount++,s!==null&&(i.pooledCacheLanes|=a),i=s),t.memoizedState={parent:n,cache:i},No(t),aa(t,qe,i)):((e.lanes&a)!==0&&(Bo(e,t),Fn(t,null,null,a),Vn()),i=e.memoizedState,s=t.memoizedState,i.parent!==n?(i={parent:n,cache:n},t.memoizedState=i,t.lanes===0&&(t.memoizedState=t.updateQueue.baseState=i),aa(t,qe,n)):(n=s.cache,aa(t,qe,n),n!==i.cache&&Do(t,[qe],a,!0))),Me(e,t,t.pendingProps.children,a),t.child;case 29:throw t.pendingProps}throw Error(h(156,t.tag))}function _t(e){e.flags|=4}function br(e,t,a,n,i){if((t=(e.mode&32)!==0)&&(t=!1),t){if(e.flags|=16777216,(i&335544128)===i)if(e.stateNode.complete)e.flags|=8192;else if(hd())e.flags|=8192;else throw Na=Xi,Eo}else e.flags&=-16777217}function Mu(e,t){if(t.type!=="stylesheet"||(t.state.loading&4)!==0)e.flags&=-16777217;else if(e.flags|=16777216,!th(t))if(hd())e.flags|=8192;else throw Na=Xi,Eo}function hs(e,t){t!==null&&(e.flags|=4),e.flags&16384&&(t=e.tag!==22?vl():536870912,e.lanes|=t,vn|=t)}function ni(e,t){if(!$)switch(e.tailMode){case"hidden":t=e.tail;for(var a=null;t!==null;)t.alternate!==null&&(a=t),t=t.sibling;a===null?e.tail=null:a.sibling=null;break;case"collapsed":a=e.tail;for(var n=null;a!==null;)a.alternate!==null&&(n=a),a=a.sibling;n===null?t||e.tail===null?e.tail=null:e.tail.sibling=null:n.sibling=null}}function ye(e){var t=e.alternate!==null&&e.alternate.child===e.child,a=0,n=0;if(t)for(var i=e.child;i!==null;)a|=i.lanes|i.childLanes,n|=i.subtreeFlags&65011712,n|=i.flags&65011712,i.return=e,i=i.sibling;else for(i=e.child;i!==null;)a|=i.lanes|i.childLanes,n|=i.subtreeFlags,n|=i.flags,i.return=e,i=i.sibling;return e.subtreeFlags|=n,e.childLanes=a,t}function Fp(e,t,a){var n=t.pendingProps;switch(qo(t),t.tag){case 16:case 15:case 0:case 11:case 7:case 8:case 12:case 9:case 14:return ye(t),null;case 1:return ye(t),null;case 3:return a=t.stateNode,n=null,e!==null&&(n=e.memoizedState.cache),t.memoizedState.cache!==n&&(t.flags|=2048),Ht(qe),ke(),a.pendingContext&&(a.context=a.pendingContext,a.pendingContext=null),(e===null||e.child===null)&&(on(t)?_t(t):e===null||e.memoizedState.isDehydrated&&(t.flags&256)===0||(t.flags|=1024,Ro())),ye(t),null;case 26:var i=t.type,s=t.memoizedState;return e===null?(_t(t),s!==null?(ye(t),Mu(t,s)):(ye(t),br(t,i,null,n,a))):s?s!==e.memoizedState?(_t(t),ye(t),Mu(t,s)):(ye(t),t.flags&=-16777217):(e=e.memoizedProps,e!==n&&_t(t),ye(t),br(t,i,e,n,a)),null;case 27:if(ki(t),a=Z.current,i=t.type,e!==null&&t.stateNode!=null)e.memoizedProps!==n&&_t(t);else{if(!n){if(t.stateNode===null)throw Error(h(166));return ye(t),null}e=U.current,on(t)?vc(t):(e=Yd(i,n,a),t.stateNode=e,_t(t))}return ye(t),null;case 5:if(ki(t),i=t.type,e!==null&&t.stateNode!=null)e.memoizedProps!==n&&_t(t);else{if(!n){if(t.stateNode===null)throw Error(h(166));return ye(t),null}if(s=U.current,on(t))vc(t);else{var o=Ls(Z.current);switch(s){case 1:s=o.createElementNS("http://www.w3.org/2000/svg",i);break;case 2:s=o.createElementNS("http://www.w3.org/1998/Math/MathML",i);break;default:switch(i){case"svg":s=o.createElementNS("http://www.w3.org/2000/svg",i);break;case"math":s=o.createElementNS("http://www.w3.org/1998/Math/MathML",i);break;case"script":s=o.createElement("div"),s.innerHTML="<script><\/script>",s=s.removeChild(s.firstChild);break;case"select":s=typeof n.is=="string"?o.createElement("select",{is:n.is}):o.createElement("select"),n.multiple?s.multiple=!0:n.size&&(s.size=n.size);break;default:s=typeof n.is=="string"?o.createElement(i,{is:n.is}):o.createElement(i)}}s[Be]=t,s[Ze]=n;e:for(o=t.child;o!==null;){if(o.tag===5||o.tag===6)s.appendChild(o.stateNode);else if(o.tag!==4&&o.tag!==27&&o.child!==null){o.child.return=o,o=o.child;continue}if(o===t)break e;for(;o.sibling===null;){if(o.return===null||o.return===t)break e;o=o.return}o.sibling.return=o.return,o=o.sibling}t.stateNode=s;e:switch(Oe(s,i,n),i){case"button":case"input":case"select":case"textarea":n=!!n.autoFocus;break e;case"img":n=!0;break e;default:n=!1}n&&_t(t)}}return ye(t),br(t,t.type,e===null?null:e.memoizedProps,t.pendingProps,a),null;case 6:if(e&&t.stateNode!=null)e.memoizedProps!==n&&_t(t);else{if(typeof n!="string"&&t.stateNode===null)throw Error(h(166));if(e=Z.current,on(t)){if(e=t.stateNode,a=t.memoizedProps,n=null,i=He,i!==null)switch(i.tag){case 27:case 5:n=i.memoizedProps}e[Be]=t,e=!!(e.nodeValue===a||n!==null&&n.suppressHydrationWarning===!0||Nd(e.nodeValue,a)),e||ta(t,!0)}else e=Ls(e).createTextNode(n),e[Be]=t,t.stateNode=e}return ye(t),null;case 31:if(a=t.memoizedState,e===null||e.memoizedState!==null){if(n=on(t),a!==null){if(e===null){if(!n)throw Error(h(318));if(e=t.memoizedState,e=e!==null?e.dehydrated:null,!e)throw Error(h(557));e[Be]=t}else Da(),(t.flags&128)===0&&(t.memoizedState=null),t.flags|=4;ye(t),e=!1}else a=Ro(),e!==null&&e.memoizedState!==null&&(e.memoizedState.hydrationErrors=a),e=!0;if(!e)return t.flags&256?(rt(t),t):(rt(t),null);if((t.flags&128)!==0)throw Error(h(558))}return ye(t),null;case 13:if(n=t.memoizedState,e===null||e.memoizedState!==null&&e.memoizedState.dehydrated!==null){if(i=on(t),n!==null&&n.dehydrated!==null){if(e===null){if(!i)throw Error(h(318));if(i=t.memoizedState,i=i!==null?i.dehydrated:null,!i)throw Error(h(317));i[Be]=t}else Da(),(t.flags&128)===0&&(t.memoizedState=null),t.flags|=4;ye(t),i=!1}else i=Ro(),e!==null&&e.memoizedState!==null&&(e.memoizedState.hydrationErrors=i),i=!0;if(!i)return t.flags&256?(rt(t),t):(rt(t),null)}return rt(t),(t.flags&128)!==0?(t.lanes=a,t):(a=n!==null,e=e!==null&&e.memoizedState!==null,a&&(n=t.child,i=null,n.alternate!==null&&n.alternate.memoizedState!==null&&n.alternate.memoizedState.cachePool!==null&&(i=n.alternate.memoizedState.cachePool.pool),s=null,n.memoizedState!==null&&n.memoizedState.cachePool!==null&&(s=n.memoizedState.cachePool.pool),s!==i&&(n.flags|=2048)),a!==e&&a&&(t.child.flags|=8192),hs(t,t.updateQueue),ye(t),null);case 4:return ke(),e===null&&Mr(t.stateNode.containerInfo),ye(t),null;case 10:return Ht(t.type),ye(t),null;case 19:if(k(Ae),n=t.memoizedState,n===null)return ye(t),null;if(i=(t.flags&128)!==0,s=n.rendering,s===null)if(i)ni(n,!1);else{if(Te!==0||e!==null&&(e.flags&128)!==0)for(e=t.child;e!==null;){if(s=$i(e),s!==null){for(t.flags|=128,ni(n,!1),e=s.updateQueue,t.updateQueue=e,hs(t,e),t.subtreeFlags=0,e=a,a=t.child;a!==null;)fc(a,e),a=a.sibling;return q(Ae,Ae.current&1|2),$&&Nt(t,n.treeForkCount),t.child}e=e.sibling}n.tail!==null&&tt()>ys&&(t.flags|=128,i=!0,ni(n,!1),t.lanes=4194304)}else{if(!i)if(e=$i(s),e!==null){if(t.flags|=128,i=!0,e=e.updateQueue,t.updateQueue=e,hs(t,e),ni(n,!0),n.tail===null&&n.tailMode==="hidden"&&!s.alternate&&!$)return ye(t),null}else 2*tt()-n.renderingStartTime>ys&&a!==536870912&&(t.flags|=128,i=!0,ni(n,!1),t.lanes=4194304);n.isBackwards?(s.sibling=t.child,t.child=s):(e=n.last,e!==null?e.sibling=s:t.child=s,n.last=s)}return n.tail!==null?(e=n.tail,n.rendering=e,n.tail=e.sibling,n.renderingStartTime=tt(),e.sibling=null,a=Ae.current,q(Ae,i?a&1|2:a&1),$&&Nt(t,n.treeForkCount),e):(ye(t),null);case 22:case 23:return rt(t),Oo(),n=t.memoizedState!==null,e!==null?e.memoizedState!==null!==n&&(t.flags|=8192):n&&(t.flags|=8192),n?(a&536870912)!==0&&(t.flags&128)===0&&(ye(t),t.subtreeFlags&6&&(t.flags|=8192)):ye(t),a=t.updateQueue,a!==null&&hs(t,a.retryQueue),a=null,e!==null&&e.memoizedState!==null&&e.memoizedState.cachePool!==null&&(a=e.memoizedState.cachePool.pool),n=null,t.memoizedState!==null&&t.memoizedState.cachePool!==null&&(n=t.memoizedState.cachePool.pool),n!==a&&(t.flags|=2048),e!==null&&k(Pa),null;case 24:return a=null,e!==null&&(a=e.memoizedState.cache),t.memoizedState.cache!==a&&(t.flags|=2048),Ht(qe),ye(t),null;case 25:return null;case 30:return null}throw Error(h(156,t.tag))}function Jp(e,t){switch(qo(t),t.tag){case 1:return e=t.flags,e&65536?(t.flags=e&-65537|128,t):null;case 3:return Ht(qe),ke(),e=t.flags,(e&65536)!==0&&(e&128)===0?(t.flags=e&-65537|128,t):null;case 26:case 27:case 5:return ki(t),null;case 31:if(t.memoizedState!==null){if(rt(t),t.alternate===null)throw Error(h(340));Da()}return e=t.flags,e&65536?(t.flags=e&-65537|128,t):null;case 13:if(rt(t),e=t.memoizedState,e!==null&&e.dehydrated!==null){if(t.alternate===null)throw Error(h(340));Da()}return e=t.flags,e&65536?(t.flags=e&-65537|128,t):null;case 19:return k(Ae),null;case 4:return ke(),null;case 10:return Ht(t.type),null;case 22:case 23:return rt(t),Oo(),e!==null&&k(Pa),e=t.flags,e&65536?(t.flags=e&-65537|128,t):null;case 24:return Ht(qe),null;case 25:return null;default:return null}}function Ou(e,t){switch(qo(t),t.tag){case 3:Ht(qe),ke();break;case 26:case 27:case 5:ki(t);break;case 4:ke();break;case 31:t.memoizedState!==null&&rt(t);break;case 13:rt(t);break;case 19:k(Ae);break;case 10:Ht(t.type);break;case 22:case 23:rt(t),Oo(),e!==null&&k(Pa);break;case 24:Ht(qe)}}function ii(e,t){try{var a=t.updateQueue,n=a!==null?a.lastEffect:null;if(n!==null){var i=n.next;a=i;do{if((a.tag&e)===e){n=void 0;var s=a.create,o=a.inst;n=s(),o.destroy=n}a=a.next}while(a!==i)}}catch(r){oe(t,t.return,r)}}function la(e,t,a){try{var n=t.updateQueue,i=n!==null?n.lastEffect:null;if(i!==null){var s=i.next;n=s;do{if((n.tag&e)===e){var o=n.inst,r=o.destroy;if(r!==void 0){o.destroy=void 0,i=t;var l=a,f=r;try{f()}catch(b){oe(i,l,b)}}}n=n.next}while(n!==s)}}catch(b){oe(t,t.return,b)}}function _u(e){var t=e.updateQueue;if(t!==null){var a=e.stateNode;try{Ic(t,a)}catch(n){oe(e,e.return,n)}}}function ju(e,t,a){a.props=Wa(e.type,e.memoizedProps),a.state=e.memoizedState;try{a.componentWillUnmount()}catch(n){oe(e,t,n)}}function si(e,t){try{var a=e.ref;if(a!==null){switch(e.tag){case 26:case 27:case 5:var n=e.stateNode;break;case 30:n=e.stateNode;break;default:n=e.stateNode}typeof a=="function"?e.refCleanup=a(n):a.current=n}}catch(i){oe(e,t,i)}}function Ct(e,t){var a=e.ref,n=e.refCleanup;if(a!==null)if(typeof n=="function")try{n()}catch(i){oe(e,t,i)}finally{e.refCleanup=null,e=e.alternate,e!=null&&(e.refCleanup=null)}else if(typeof a=="function")try{a(null)}catch(i){oe(e,t,i)}else a.current=null}function Qu(e){var t=e.type,a=e.memoizedProps,n=e.stateNode;try{e:switch(t){case"button":case"input":case"select":case"textarea":a.autoFocus&&n.focus();break e;case"img":a.src?n.src=a.src:a.srcSet&&(n.srcset=a.srcSet)}}catch(i){oe(e,e.return,i)}}function vr(e,t,a){try{var n=e.stateNode;xf(n,e.type,a,t),n[Ze]=t}catch(i){oe(e,e.return,i)}}function Gu(e){return e.tag===5||e.tag===3||e.tag===26||e.tag===27&&ma(e.type)||e.tag===4}function wr(e){e:for(;;){for(;e.sibling===null;){if(e.return===null||Gu(e.return))return null;e=e.return}for(e.sibling.return=e.return,e=e.sibling;e.tag!==5&&e.tag!==6&&e.tag!==18;){if(e.tag===27&&ma(e.type)||e.flags&2||e.child===null||e.tag===4)continue e;e.child.return=e,e=e.child}if(!(e.flags&2))return e.stateNode}}function xr(e,t,a){var n=e.tag;if(n===5||n===6)e=e.stateNode,t?(a.nodeType===9?a.body:a.nodeName==="HTML"?a.ownerDocument.body:a).insertBefore(e,t):(t=a.nodeType===9?a.body:a.nodeName==="HTML"?a.ownerDocument.body:a,t.appendChild(e),a=a._reactRootContainer,a!=null||t.onclick!==null||(t.onclick=zt));else if(n!==4&&(n===27&&ma(e.type)&&(a=e.stateNode,t=null),e=e.child,e!==null))for(xr(e,t,a),e=e.sibling;e!==null;)xr(e,t,a),e=e.sibling}function ps(e,t,a){var n=e.tag;if(n===5||n===6)e=e.stateNode,t?a.insertBefore(e,t):a.appendChild(e);else if(n!==4&&(n===27&&ma(e.type)&&(a=e.stateNode),e=e.child,e!==null))for(ps(e,t,a),e=e.sibling;e!==null;)ps(e,t,a),e=e.sibling}function Ku(e){var t=e.stateNode,a=e.memoizedProps;try{for(var n=e.type,i=t.attributes;i.length;)t.removeAttributeNode(i[0]);Oe(t,n,a),t[Be]=e,t[Ze]=a}catch(s){oe(e,e.return,s)}}var jt=!1,Ce=!1,Tr=!1,Yu=typeof WeakSet=="function"?WeakSet:Set,ze=null;function $p(e,t){if(e=e.containerInfo,jr=Ps,e=sc(e),mo(e)){if("selectionStart"in e)var a={start:e.selectionStart,end:e.selectionEnd};else e:{a=(a=e.ownerDocument)&&a.defaultView||window;var n=a.getSelection&&a.getSelection();if(n&&n.rangeCount!==0){a=n.anchorNode;var i=n.anchorOffset,s=n.focusNode;n=n.focusOffset;try{a.nodeType,s.nodeType}catch{a=null;break e}var o=0,r=-1,l=-1,f=0,b=0,T=e,m=null;t:for(;;){for(var g;T!==a||i!==0&&T.nodeType!==3||(r=o+i),T!==s||n!==0&&T.nodeType!==3||(l=o+n),T.nodeType===3&&(o+=T.nodeValue.length),(g=T.firstChild)!==null;)m=T,T=g;for(;;){if(T===e)break t;if(m===a&&++f===i&&(r=o),m===s&&++b===n&&(l=o),(g=T.nextSibling)!==null)break;T=m,m=T.parentNode}T=g}a=r===-1||l===-1?null:{start:r,end:l}}else a=null}a=a||{start:0,end:0}}else a=null;for(Qr={focusedElem:e,selectionRange:a},Ps=!1,ze=t;ze!==null;)if(t=ze,e=t.child,(t.subtreeFlags&1028)!==0&&e!==null)e.return=t,ze=e;else for(;ze!==null;){switch(t=ze,s=t.alternate,e=t.flags,t.tag){case 0:if((e&4)!==0&&(e=t.updateQueue,e=e!==null?e.events:null,e!==null))for(a=0;a<e.length;a++)i=e[a],i.ref.impl=i.nextImpl;break;case 11:case 15:break;case 1:if((e&1024)!==0&&s!==null){e=void 0,a=t,i=s.memoizedProps,s=s.memoizedState,n=a.stateNode;try{var R=Wa(a.type,i);e=n.getSnapshotBeforeUpdate(R,s),n.__reactInternalSnapshotBeforeUpdate=e}catch(N){oe(a,a.return,N)}}break;case 3:if((e&1024)!==0){if(e=t.stateNode.containerInfo,a=e.nodeType,a===9)Yr(e);else if(a===1)switch(e.nodeName){case"HEAD":case"HTML":case"BODY":Yr(e);break;default:e.textContent=""}}break;case 5:case 26:case 27:case 6:case 4:case 17:break;default:if((e&1024)!==0)throw Error(h(163))}if(e=t.sibling,e!==null){e.return=t.return,ze=e;break}ze=t.return}}function Zu(e,t,a){var n=a.flags;switch(a.tag){case 0:case 11:case 15:Gt(e,a),n&4&&ii(5,a);break;case 1:if(Gt(e,a),n&4)if(e=a.stateNode,t===null)try{e.componentDidMount()}catch(o){oe(a,a.return,o)}else{var i=Wa(a.type,t.memoizedProps);t=t.memoizedState;try{e.componentDidUpdate(i,t,e.__reactInternalSnapshotBeforeUpdate)}catch(o){oe(a,a.return,o)}}n&64&&_u(a),n&512&&si(a,a.return);break;case 3:if(Gt(e,a),n&64&&(e=a.updateQueue,e!==null)){if(t=null,a.child!==null)switch(a.child.tag){case 27:case 5:t=a.child.stateNode;break;case 1:t=a.child.stateNode}try{Ic(e,t)}catch(o){oe(a,a.return,o)}}break;case 27:t===null&&n&4&&Ku(a);case 26:case 5:Gt(e,a),t===null&&n&4&&Qu(a),n&512&&si(a,a.return);break;case 12:Gt(e,a);break;case 31:Gt(e,a),n&4&&Fu(e,a);break;case 13:Gt(e,a),n&4&&Ju(e,a),n&64&&(e=a.memoizedState,e!==null&&(e=e.dehydrated,e!==null&&(a=cf.bind(null,a),Cf(e,a))));break;case 22:if(n=a.memoizedState!==null||jt,!n){t=t!==null&&t.memoizedState!==null||Ce,i=jt;var s=Ce;jt=n,(Ce=t)&&!s?Kt(e,a,(a.subtreeFlags&8772)!==0):Gt(e,a),jt=i,Ce=s}break;case 30:break;default:Gt(e,a)}}function Xu(e){var t=e.alternate;t!==null&&(e.alternate=null,Xu(t)),e.child=null,e.deletions=null,e.sibling=null,e.tag===5&&(t=e.stateNode,t!==null&&Fs(t)),e.stateNode=null,e.return=null,e.dependencies=null,e.memoizedProps=null,e.memoizedState=null,e.pendingProps=null,e.stateNode=null,e.updateQueue=null}var ve=null,Ve=!1;function Qt(e,t,a){for(a=a.child;a!==null;)Vu(e,t,a),a=a.sibling}function Vu(e,t,a){if(at&&typeof at.onCommitFiberUnmount=="function")try{at.onCommitFiberUnmount(Cn,a)}catch{}switch(a.tag){case 26:Ce||Ct(a,t),Qt(e,t,a),a.memoizedState?a.memoizedState.count--:a.stateNode&&(a=a.stateNode,a.parentNode.removeChild(a));break;case 27:Ce||Ct(a,t);var n=ve,i=Ve;ma(a.type)&&(ve=a.stateNode,Ve=!1),Qt(e,t,a),fi(a.stateNode),ve=n,Ve=i;break;case 5:Ce||Ct(a,t);case 6:if(n=ve,i=Ve,ve=null,Qt(e,t,a),ve=n,Ve=i,ve!==null)if(Ve)try{(ve.nodeType===9?ve.body:ve.nodeName==="HTML"?ve.ownerDocument.body:ve).removeChild(a.stateNode)}catch(s){oe(a,t,s)}else try{ve.removeChild(a.stateNode)}catch(s){oe(a,t,s)}break;case 18:ve!==null&&(Ve?(e=ve,_d(e.nodeType===9?e.body:e.nodeName==="HTML"?e.ownerDocument.body:e,a.stateNode),Ln(e)):_d(ve,a.stateNode));break;case 4:n=ve,i=Ve,ve=a.stateNode.containerInfo,Ve=!0,Qt(e,t,a),ve=n,Ve=i;break;case 0:case 11:case 14:case 15:la(2,a,t),Ce||la(4,a,t),Qt(e,t,a);break;case 1:Ce||(Ct(a,t),n=a.stateNode,typeof n.componentWillUnmount=="function"&&ju(a,t,n)),Qt(e,t,a);break;case 21:Qt(e,t,a);break;case 22:Ce=(n=Ce)||a.memoizedState!==null,Qt(e,t,a),Ce=n;break;default:Qt(e,t,a)}}function Fu(e,t){if(t.memoizedState===null&&(e=t.alternate,e!==null&&(e=e.memoizedState,e!==null))){e=e.dehydrated;try{Ln(e)}catch(a){oe(t,t.return,a)}}}function Ju(e,t){if(t.memoizedState===null&&(e=t.alternate,e!==null&&(e=e.memoizedState,e!==null&&(e=e.dehydrated,e!==null))))try{Ln(e)}catch(a){oe(t,t.return,a)}}function ef(e){switch(e.tag){case 31:case 13:case 19:var t=e.stateNode;return t===null&&(t=e.stateNode=new Yu),t;case 22:return e=e.stateNode,t=e._retryCache,t===null&&(t=e._retryCache=new Yu),t;default:throw Error(h(435,e.tag))}}function fs(e,t){var a=ef(e);t.forEach(function(n){if(!a.has(n)){a.add(n);var i=uf.bind(null,e,n);n.then(i,i)}})}function Fe(e,t){var a=t.deletions;if(a!==null)for(var n=0;n<a.length;n++){var i=a[n],s=e,o=t,r=o;e:for(;r!==null;){switch(r.tag){case 27:if(ma(r.type)){ve=r.stateNode,Ve=!1;break e}break;case 5:ve=r.stateNode,Ve=!1;break e;case 3:case 4:ve=r.stateNode.containerInfo,Ve=!0;break e}r=r.return}if(ve===null)throw Error(h(160));Vu(s,o,i),ve=null,Ve=!1,s=i.alternate,s!==null&&(s.return=null),i.return=null}if(t.subtreeFlags&13886)for(t=t.child;t!==null;)$u(t,e),t=t.sibling}var kt=null;function $u(e,t){var a=e.alternate,n=e.flags;switch(e.tag){case 0:case 11:case 14:case 15:Fe(t,e),Je(e),n&4&&(la(3,e,e.return),ii(3,e),la(5,e,e.return));break;case 1:Fe(t,e),Je(e),n&512&&(Ce||a===null||Ct(a,a.return)),n&64&&jt&&(e=e.updateQueue,e!==null&&(n=e.callbacks,n!==null&&(a=e.shared.hiddenCallbacks,e.shared.hiddenCallbacks=a===null?n:a.concat(n))));break;case 26:var i=kt;if(Fe(t,e),Je(e),n&512&&(Ce||a===null||Ct(a,a.return)),n&4){var s=a!==null?a.memoizedState:null;if(n=e.memoizedState,a===null)if(n===null)if(e.stateNode===null){e:{n=e.type,a=e.memoizedProps,i=i.ownerDocument||i;t:switch(n){case"title":s=i.getElementsByTagName("title")[0],(!s||s[In]||s[Be]||s.namespaceURI==="http://www.w3.org/2000/svg"||s.hasAttribute("itemprop"))&&(s=i.createElement(n),i.head.insertBefore(s,i.querySelector("head > title"))),Oe(s,n,a),s[Be]=e,Ie(s),n=s;break e;case"link":var o=$d("link","href",i).get(n+(a.href||""));if(o){for(var r=0;r<o.length;r++)if(s=o[r],s.getAttribute("href")===(a.href==null||a.href===""?null:a.href)&&s.getAttribute("rel")===(a.rel==null?null:a.rel)&&s.getAttribute("title")===(a.title==null?null:a.title)&&s.getAttribute("crossorigin")===(a.crossOrigin==null?null:a.crossOrigin)){o.splice(r,1);break t}}s=i.createElement(n),Oe(s,n,a),i.head.appendChild(s);break;case"meta":if(o=$d("meta","content",i).get(n+(a.content||""))){for(r=0;r<o.length;r++)if(s=o[r],s.getAttribute("content")===(a.content==null?null:""+a.content)&&s.getAttribute("name")===(a.name==null?null:a.name)&&s.getAttribute("property")===(a.property==null?null:a.property)&&s.getAttribute("http-equiv")===(a.httpEquiv==null?null:a.httpEquiv)&&s.getAttribute("charset")===(a.charSet==null?null:a.charSet)){o.splice(r,1);break t}}s=i.createElement(n),Oe(s,n,a),i.head.appendChild(s);break;default:throw Error(h(468,n))}s[Be]=e,Ie(s),n=s}e.stateNode=n}else eh(i,e.type,e.stateNode);else e.stateNode=Jd(i,n,e.memoizedProps);else s!==n?(s===null?a.stateNode!==null&&(a=a.stateNode,a.parentNode.removeChild(a)):s.count--,n===null?eh(i,e.type,e.stateNode):Jd(i,n,e.memoizedProps)):n===null&&e.stateNode!==null&&vr(e,e.memoizedProps,a.memoizedProps)}break;case 27:Fe(t,e),Je(e),n&512&&(Ce||a===null||Ct(a,a.return)),a!==null&&n&4&&vr(e,e.memoizedProps,a.memoizedProps);break;case 5:if(Fe(t,e),Je(e),n&512&&(Ce||a===null||Ct(a,a.return)),e.flags&32){i=e.stateNode;try{Xa(i,"")}catch(R){oe(e,e.return,R)}}n&4&&e.stateNode!=null&&(i=e.memoizedProps,vr(e,i,a!==null?a.memoizedProps:i)),n&1024&&(Tr=!0);break;case 6:if(Fe(t,e),Je(e),n&4){if(e.stateNode===null)throw Error(h(162));n=e.memoizedProps,a=e.stateNode;try{a.nodeValue=n}catch(R){oe(e,e.return,R)}}break;case 3:if(Us=null,i=kt,kt=Rs(t.containerInfo),Fe(t,e),kt=i,Je(e),n&4&&a!==null&&a.memoizedState.isDehydrated)try{Ln(t.containerInfo)}catch(R){oe(e,e.return,R)}Tr&&(Tr=!1,ed(e));break;case 4:n=kt,kt=Rs(e.stateNode.containerInfo),Fe(t,e),Je(e),kt=n;break;case 12:Fe(t,e),Je(e);break;case 31:Fe(t,e),Je(e),n&4&&(n=e.updateQueue,n!==null&&(e.updateQueue=null,fs(e,n)));break;case 13:Fe(t,e),Je(e),e.child.flags&8192&&e.memoizedState!==null!=(a!==null&&a.memoizedState!==null)&&(gs=tt()),n&4&&(n=e.updateQueue,n!==null&&(e.updateQueue=null,fs(e,n)));break;case 22:i=e.memoizedState!==null;var l=a!==null&&a.memoizedState!==null,f=jt,b=Ce;if(jt=f||i,Ce=b||l,Fe(t,e),Ce=b,jt=f,Je(e),n&8192)e:for(t=e.stateNode,t._visibility=i?t._visibility&-2:t._visibility|1,i&&(a===null||l||jt||Ce||Ma(e)),a=null,t=e;;){if(t.tag===5||t.tag===26){if(a===null){l=a=t;try{if(s=l.stateNode,i)o=s.style,typeof o.setProperty=="function"?o.setProperty("display","none","important"):o.display="none";else{r=l.stateNode;var T=l.memoizedProps.style,m=T!=null&&T.hasOwnProperty("display")?T.display:null;r.style.display=m==null||typeof m=="boolean"?"":(""+m).trim()}}catch(R){oe(l,l.return,R)}}}else if(t.tag===6){if(a===null){l=t;try{l.stateNode.nodeValue=i?"":l.memoizedProps}catch(R){oe(l,l.return,R)}}}else if(t.tag===18){if(a===null){l=t;try{var g=l.stateNode;i?jd(g,!0):jd(l.stateNode,!1)}catch(R){oe(l,l.return,R)}}}else if((t.tag!==22&&t.tag!==23||t.memoizedState===null||t===e)&&t.child!==null){t.child.return=t,t=t.child;continue}if(t===e)break e;for(;t.sibling===null;){if(t.return===null||t.return===e)break e;a===t&&(a=null),t=t.return}a===t&&(a=null),t.sibling.return=t.return,t=t.sibling}n&4&&(n=e.updateQueue,n!==null&&(a=n.retryQueue,a!==null&&(n.retryQueue=null,fs(e,a))));break;case 19:Fe(t,e),Je(e),n&4&&(n=e.updateQueue,n!==null&&(e.updateQueue=null,fs(e,n)));break;case 30:break;case 21:break;default:Fe(t,e),Je(e)}}function Je(e){var t=e.flags;if(t&2){try{for(var a,n=e.return;n!==null;){if(Gu(n)){a=n;break}n=n.return}if(a==null)throw Error(h(160));switch(a.tag){case 27:var i=a.stateNode,s=wr(e);ps(e,s,i);break;case 5:var o=a.stateNode;a.flags&32&&(Xa(o,""),a.flags&=-33);var r=wr(e);ps(e,r,o);break;case 3:case 4:var l=a.stateNode.containerInfo,f=wr(e);xr(e,f,l);break;default:throw Error(h(161))}}catch(b){oe(e,e.return,b)}e.flags&=-3}t&4096&&(e.flags&=-4097)}function ed(e){if(e.subtreeFlags&1024)for(e=e.child;e!==null;){var t=e;ed(t),t.tag===5&&t.flags&1024&&t.stateNode.reset(),e=e.sibling}}function Gt(e,t){if(t.subtreeFlags&8772)for(t=t.child;t!==null;)Zu(e,t.alternate,t),t=t.sibling}function Ma(e){for(e=e.child;e!==null;){var t=e;switch(t.tag){case 0:case 11:case 14:case 15:la(4,t,t.return),Ma(t);break;case 1:Ct(t,t.return);var a=t.stateNode;typeof a.componentWillUnmount=="function"&&ju(t,t.return,a),Ma(t);break;case 27:fi(t.stateNode);case 26:case 5:Ct(t,t.return),Ma(t);break;case 22:t.memoizedState===null&&Ma(t);break;case 30:Ma(t);break;default:Ma(t)}e=e.sibling}}function Kt(e,t,a){for(a=a&&(t.subtreeFlags&8772)!==0,t=t.child;t!==null;){var n=t.alternate,i=e,s=t,o=s.flags;switch(s.tag){case 0:case 11:case 15:Kt(i,s,a),ii(4,s);break;case 1:if(Kt(i,s,a),n=s,i=n.stateNode,typeof i.componentDidMount=="function")try{i.componentDidMount()}catch(f){oe(n,n.return,f)}if(n=s,i=n.updateQueue,i!==null){var r=n.stateNode;try{var l=i.shared.hiddenCallbacks;if(l!==null)for(i.shared.hiddenCallbacks=null,i=0;i<l.length;i++)Dc(l[i],r)}catch(f){oe(n,n.return,f)}}a&&o&64&&_u(s),si(s,s.return);break;case 27:Ku(s);case 26:case 5:Kt(i,s,a),a&&n===null&&o&4&&Qu(s),si(s,s.return);break;case 12:Kt(i,s,a);break;case 31:Kt(i,s,a),a&&o&4&&Fu(i,s);break;case 13:Kt(i,s,a),a&&o&4&&Ju(i,s);break;case 22:s.memoizedState===null&&Kt(i,s,a),si(s,s.return);break;case 30:break;default:Kt(i,s,a)}t=t.sibling}}function kr(e,t){var a=null;e!==null&&e.memoizedState!==null&&e.memoizedState.cachePool!==null&&(a=e.memoizedState.cachePool.pool),e=null,t.memoizedState!==null&&t.memoizedState.cachePool!==null&&(e=t.memoizedState.cachePool.pool),e!==a&&(e!=null&&e.refCount++,a!=null&&Gn(a))}function Ar(e,t){e=null,t.alternate!==null&&(e=t.alternate.memoizedState.cache),t=t.memoizedState.cache,t!==e&&(t.refCount++,e!=null&&Gn(e))}function At(e,t,a,n){if(t.subtreeFlags&10256)for(t=t.child;t!==null;)td(e,t,a,n),t=t.sibling}function td(e,t,a,n){var i=t.flags;switch(t.tag){case 0:case 11:case 15:At(e,t,a,n),i&2048&&ii(9,t);break;case 1:At(e,t,a,n);break;case 3:At(e,t,a,n),i&2048&&(e=null,t.alternate!==null&&(e=t.alternate.memoizedState.cache),t=t.memoizedState.cache,t!==e&&(t.refCount++,e!=null&&Gn(e)));break;case 12:if(i&2048){At(e,t,a,n),e=t.stateNode;try{var s=t.memoizedProps,o=s.id,r=s.onPostCommit;typeof r=="function"&&r(o,t.alternate===null?"mount":"update",e.passiveEffectDuration,-0)}catch(l){oe(t,t.return,l)}}else At(e,t,a,n);break;case 31:At(e,t,a,n);break;case 13:At(e,t,a,n);break;case 23:break;case 22:s=t.stateNode,o=t.alternate,t.memoizedState!==null?s._visibility&2?At(e,t,a,n):oi(e,t):s._visibility&2?At(e,t,a,n):(s._visibility|=2,gn(e,t,a,n,(t.subtreeFlags&10256)!==0||!1)),i&2048&&kr(o,t);break;case 24:At(e,t,a,n),i&2048&&Ar(t.alternate,t);break;default:At(e,t,a,n)}}function gn(e,t,a,n,i){for(i=i&&((t.subtreeFlags&10256)!==0||!1),t=t.child;t!==null;){var s=e,o=t,r=a,l=n,f=o.flags;switch(o.tag){case 0:case 11:case 15:gn(s,o,r,l,i),ii(8,o);break;case 23:break;case 22:var b=o.stateNode;o.memoizedState!==null?b._visibility&2?gn(s,o,r,l,i):oi(s,o):(b._visibility|=2,gn(s,o,r,l,i)),i&&f&2048&&kr(o.alternate,o);break;case 24:gn(s,o,r,l,i),i&&f&2048&&Ar(o.alternate,o);break;default:gn(s,o,r,l,i)}t=t.sibling}}function oi(e,t){if(t.subtreeFlags&10256)for(t=t.child;t!==null;){var a=e,n=t,i=n.flags;switch(n.tag){case 22:oi(a,n),i&2048&&kr(n.alternate,n);break;case 24:oi(a,n),i&2048&&Ar(n.alternate,n);break;default:oi(a,n)}t=t.sibling}}var ri=8192;function yn(e,t,a){if(e.subtreeFlags&ri)for(e=e.child;e!==null;)ad(e,t,a),e=e.sibling}function ad(e,t,a){switch(e.tag){case 26:yn(e,t,a),e.flags&ri&&e.memoizedState!==null&&Of(a,kt,e.memoizedState,e.memoizedProps);break;case 5:yn(e,t,a);break;case 3:case 4:var n=kt;kt=Rs(e.stateNode.containerInfo),yn(e,t,a),kt=n;break;case 22:e.memoizedState===null&&(n=e.alternate,n!==null&&n.memoizedState!==null?(n=ri,ri=16777216,yn(e,t,a),ri=n):yn(e,t,a));break;default:yn(e,t,a)}}function nd(e){var t=e.alternate;if(t!==null&&(e=t.child,e!==null)){t.child=null;do t=e.sibling,e.sibling=null,e=t;while(e!==null)}}function li(e){var t=e.deletions;if((e.flags&16)!==0){if(t!==null)for(var a=0;a<t.length;a++){var n=t[a];ze=n,sd(n,e)}nd(e)}if(e.subtreeFlags&10256)for(e=e.child;e!==null;)id(e),e=e.sibling}function id(e){switch(e.tag){case 0:case 11:case 15:li(e),e.flags&2048&&la(9,e,e.return);break;case 3:li(e);break;case 12:li(e);break;case 22:var t=e.stateNode;e.memoizedState!==null&&t._visibility&2&&(e.return===null||e.return.tag!==13)?(t._visibility&=-3,ms(e)):li(e);break;default:li(e)}}function ms(e){var t=e.deletions;if((e.flags&16)!==0){if(t!==null)for(var a=0;a<t.length;a++){var n=t[a];ze=n,sd(n,e)}nd(e)}for(e=e.child;e!==null;){switch(t=e,t.tag){case 0:case 11:case 15:la(8,t,t.return),ms(t);break;case 22:a=t.stateNode,a._visibility&2&&(a._visibility&=-3,ms(t));break;default:ms(t)}e=e.sibling}}function sd(e,t){for(;ze!==null;){var a=ze;switch(a.tag){case 0:case 11:case 15:la(8,a,t);break;case 23:case 22:if(a.memoizedState!==null&&a.memoizedState.cachePool!==null){var n=a.memoizedState.cachePool.pool;n!=null&&n.refCount++}break;case 24:Gn(a.memoizedState.cache)}if(n=a.child,n!==null)n.return=a,ze=n;else e:for(a=e;ze!==null;){n=ze;var i=n.sibling,s=n.return;if(Xu(n),n===a){ze=null;break e}if(i!==null){i.return=s,ze=i;break e}ze=s}}}var tf={getCacheForType:function(e){var t=We(qe),a=t.data.get(e);return a===void 0&&(a=e(),t.data.set(e,a)),a},cacheSignal:function(){return We(qe).controller.signal}},af=typeof WeakMap=="function"?WeakMap:Map,ae=0,me=null,X=null,F=0,se=0,lt=null,ca=!1,bn=!1,Sr=!1,Yt=0,Te=0,ua=0,Oa=0,qr=0,ct=0,vn=0,ci=null,$e=null,Lr=!1,gs=0,od=0,ys=1/0,bs=null,da=null,Ue=0,ha=null,wn=null,Zt=0,Rr=0,Cr=null,rd=null,ui=0,Ur=null;function ut(){return(ae&2)!==0&&F!==0?F&-F:w.T!==null?Nr():kl()}function ld(){if(ct===0)if((F&536870912)===0||$){var e=qi;qi<<=1,(qi&3932160)===0&&(qi=262144),ct=e}else ct=536870912;return e=ot.current,e!==null&&(e.flags|=32),ct}function et(e,t,a){(e===me&&(se===2||se===9)||e.cancelPendingCommit!==null)&&(xn(e,0),pa(e,F,ct,!1)),Dn(e,a),((ae&2)===0||e!==me)&&(e===me&&((ae&2)===0&&(Oa|=a),Te===4&&pa(e,F,ct,!1)),Ut(e))}function cd(e,t,a){if((ae&6)!==0)throw Error(h(327));var n=!a&&(t&127)===0&&(t&e.expiredLanes)===0||Un(e,t),i=n?of(e,t):Ir(e,t,!0),s=n;do{if(i===0){bn&&!n&&pa(e,t,0,!1);break}else{if(a=e.current.alternate,s&&!nf(a)){i=Ir(e,t,!1),s=!1;continue}if(i===2){if(s=t,e.errorRecoveryDisabledLanes&s)var o=0;else o=e.pendingLanes&-536870913,o=o!==0?o:o&536870912?536870912:0;if(o!==0){t=o;e:{var r=e;i=ci;var l=r.current.memoizedState.isDehydrated;if(l&&(xn(r,o).flags|=256),o=Ir(r,o,!1),o!==2){if(Sr&&!l){r.errorRecoveryDisabledLanes|=s,Oa|=s,i=4;break e}s=$e,$e=i,s!==null&&($e===null?$e=s:$e.push.apply($e,s))}i=o}if(s=!1,i!==2)continue}}if(i===1){xn(e,0),pa(e,t,0,!0);break}e:{switch(n=e,s=i,s){case 0:case 1:throw Error(h(345));case 4:if((t&4194048)!==t)break;case 6:pa(n,t,ct,!ca);break e;case 2:$e=null;break;case 3:case 5:break;default:throw Error(h(329))}if((t&62914560)===t&&(i=gs+300-tt(),10<i)){if(pa(n,t,ct,!ca),Ri(n,0,!0)!==0)break e;Zt=t,n.timeoutHandle=Md(ud.bind(null,n,a,$e,bs,Lr,t,ct,Oa,vn,ca,s,"Throttled",-0,0),i);break e}ud(n,a,$e,bs,Lr,t,ct,Oa,vn,ca,s,null,-0,0)}}break}while(!0);Ut(e)}function ud(e,t,a,n,i,s,o,r,l,f,b,T,m,g){if(e.timeoutHandle=-1,T=t.subtreeFlags,T&8192||(T&16785408)===16785408){T={stylesheets:null,count:0,imgCount:0,imgBytes:0,suspenseyImages:[],waitingForImages:!0,waitingForViewTransition:!1,unsuspend:zt},ad(t,s,T);var R=(s&62914560)===s?gs-tt():(s&4194048)===s?od-tt():0;if(R=_f(T,R),R!==null){Zt=s,e.cancelPendingCommit=R(bd.bind(null,e,t,s,a,n,i,o,r,l,b,T,null,m,g)),pa(e,s,o,!f);return}}bd(e,t,s,a,n,i,o,r,l)}function nf(e){for(var t=e;;){var a=t.tag;if((a===0||a===11||a===15)&&t.flags&16384&&(a=t.updateQueue,a!==null&&(a=a.stores,a!==null)))for(var n=0;n<a.length;n++){var i=a[n],s=i.getSnapshot;i=i.value;try{if(!it(s(),i))return!1}catch{return!1}}if(a=t.child,t.subtreeFlags&16384&&a!==null)a.return=t,t=a;else{if(t===e)break;for(;t.sibling===null;){if(t.return===null||t.return===e)return!0;t=t.return}t.sibling.return=t.return,t=t.sibling}}return!0}function pa(e,t,a,n){t&=~qr,t&=~Oa,e.suspendedLanes|=t,e.pingedLanes&=~t,n&&(e.warmLanes|=t),n=e.expirationTimes;for(var i=t;0<i;){var s=31-nt(i),o=1<<s;n[s]=-1,i&=~o}a!==0&&wl(e,a,t)}function vs(){return(ae&6)===0?(di(0),!1):!0}function Dr(){if(X!==null){if(se===0)var e=X.return;else e=X,Bt=Ia=null,Yo(e),dn=null,Yn=0,e=X;for(;e!==null;)Ou(e.alternate,e),e=e.return;X=null}}function xn(e,t){var a=e.timeoutHandle;a!==-1&&(e.timeoutHandle=-1,Af(a)),a=e.cancelPendingCommit,a!==null&&(e.cancelPendingCommit=null,a()),Zt=0,Dr(),me=e,X=a=Et(e.current,null),F=t,se=0,lt=null,ca=!1,bn=Un(e,t),Sr=!1,vn=ct=qr=Oa=ua=Te=0,$e=ci=null,Lr=!1,(t&8)!==0&&(t|=t&32);var n=e.entangledLanes;if(n!==0)for(e=e.entanglements,n&=t;0<n;){var i=31-nt(n),s=1<<i;t|=e[i],n&=~s}return Yt=t,Mi(),a}function dd(e,t){Q=null,w.H=ti,t===un||t===Zi?(t=Lc(),se=3):t===Eo?(t=Lc(),se=4):se=t===cr?8:t!==null&&typeof t=="object"&&typeof t.then=="function"?6:1,lt=t,X===null&&(Te=1,ls(e,ft(t,e.current)))}function hd(){var e=ot.current;return e===null?!0:(F&4194048)===F?bt===null:(F&62914560)===F||(F&536870912)!==0?e===bt:!1}function pd(){var e=w.H;return w.H=ti,e===null?ti:e}function fd(){var e=w.A;return w.A=tf,e}function ws(){Te=4,ca||(F&4194048)!==F&&ot.current!==null||(bn=!0),(ua&134217727)===0&&(Oa&134217727)===0||me===null||pa(me,F,ct,!1)}function Ir(e,t,a){var n=ae;ae|=2;var i=pd(),s=fd();(me!==e||F!==t)&&(bs=null,xn(e,t)),t=!1;var o=Te;e:do try{if(se!==0&&X!==null){var r=X,l=lt;switch(se){case 8:Dr(),o=6;break e;case 3:case 2:case 9:case 6:ot.current===null&&(t=!0);var f=se;if(se=0,lt=null,Tn(e,r,l,f),a&&bn){o=0;break e}break;default:f=se,se=0,lt=null,Tn(e,r,l,f)}}sf(),o=Te;break}catch(b){dd(e,b)}while(!0);return t&&e.shellSuspendCounter++,Bt=Ia=null,ae=n,w.H=i,w.A=s,X===null&&(me=null,F=0,Mi()),o}function sf(){for(;X!==null;)md(X)}function of(e,t){var a=ae;ae|=2;var n=pd(),i=fd();me!==e||F!==t?(bs=null,ys=tt()+500,xn(e,t)):bn=Un(e,t);e:do try{if(se!==0&&X!==null){t=X;var s=lt;t:switch(se){case 1:se=0,lt=null,Tn(e,t,s,1);break;case 2:case 9:if(Sc(s)){se=0,lt=null,gd(t);break}t=function(){se!==2&&se!==9||me!==e||(se=7),Ut(e)},s.then(t,t);break e;case 3:se=7;break e;case 4:se=5;break e;case 7:Sc(s)?(se=0,lt=null,gd(t)):(se=0,lt=null,Tn(e,t,s,7));break;case 5:var o=null;switch(X.tag){case 26:o=X.memoizedState;case 5:case 27:var r=X;if(o?th(o):r.stateNode.complete){se=0,lt=null;var l=r.sibling;if(l!==null)X=l;else{var f=r.return;f!==null?(X=f,xs(f)):X=null}break t}}se=0,lt=null,Tn(e,t,s,5);break;case 6:se=0,lt=null,Tn(e,t,s,6);break;case 8:Dr(),Te=6;break e;default:throw Error(h(462))}}rf();break}catch(b){dd(e,b)}while(!0);return Bt=Ia=null,w.H=n,w.A=i,ae=a,X!==null?0:(me=null,F=0,Mi(),Te)}function rf(){for(;X!==null&&!Ch();)md(X)}function md(e){var t=Wu(e.alternate,e,Yt);e.memoizedProps=e.pendingProps,t===null?xs(e):X=t}function gd(e){var t=e,a=t.alternate;switch(t.tag){case 15:case 0:t=zu(a,t,t.pendingProps,t.type,void 0,F);break;case 11:t=zu(a,t,t.pendingProps,t.type.render,t.ref,F);break;case 5:Yo(t);default:Ou(a,t),t=X=fc(t,Yt),t=Wu(a,t,Yt)}e.memoizedProps=e.pendingProps,t===null?xs(e):X=t}function Tn(e,t,a,n){Bt=Ia=null,Yo(t),dn=null,Yn=0;var i=t.return;try{if(Zp(e,i,t,a,F)){Te=1,ls(e,ft(a,e.current)),X=null;return}}catch(s){if(i!==null)throw X=i,s;Te=1,ls(e,ft(a,e.current)),X=null;return}t.flags&32768?($||n===1?e=!0:bn||(F&536870912)!==0?e=!1:(ca=e=!0,(n===2||n===9||n===3||n===6)&&(n=ot.current,n!==null&&n.tag===13&&(n.flags|=16384))),yd(t,e)):xs(t)}function xs(e){var t=e;do{if((t.flags&32768)!==0){yd(t,ca);return}e=t.return;var a=Fp(t.alternate,t,Yt);if(a!==null){X=a;return}if(t=t.sibling,t!==null){X=t;return}X=t=e}while(t!==null);Te===0&&(Te=5)}function yd(e,t){do{var a=Jp(e.alternate,e);if(a!==null){a.flags&=32767,X=a;return}if(a=e.return,a!==null&&(a.flags|=32768,a.subtreeFlags=0,a.deletions=null),!t&&(e=e.sibling,e!==null)){X=e;return}X=e=a}while(e!==null);Te=6,X=null}function bd(e,t,a,n,i,s,o,r,l){e.cancelPendingCommit=null;do Ts();while(Ue!==0);if((ae&6)!==0)throw Error(h(327));if(t!==null){if(t===e.current)throw Error(h(177));if(s=t.lanes|t.childLanes,s|=wo,Wh(e,a,s,o,r,l),e===me&&(X=me=null,F=0),wn=t,ha=e,Zt=a,Rr=s,Cr=i,rd=n,(t.subtreeFlags&10256)!==0||(t.flags&10256)!==0?(e.callbackNode=null,e.callbackPriority=0,df(Ai,function(){return kd(),null})):(e.callbackNode=null,e.callbackPriority=0),n=(t.flags&13878)!==0,(t.subtreeFlags&13878)!==0||n){n=w.T,w.T=null,i=S.p,S.p=2,o=ae,ae|=4;try{$p(e,t,a)}finally{ae=o,S.p=i,w.T=n}}Ue=1,vd(),wd(),xd()}}function vd(){if(Ue===1){Ue=0;var e=ha,t=wn,a=(t.flags&13878)!==0;if((t.subtreeFlags&13878)!==0||a){a=w.T,w.T=null;var n=S.p;S.p=2;var i=ae;ae|=4;try{$u(t,e);var s=Qr,o=sc(e.containerInfo),r=s.focusedElem,l=s.selectionRange;if(o!==r&&r&&r.ownerDocument&&ic(r.ownerDocument.documentElement,r)){if(l!==null&&mo(r)){var f=l.start,b=l.end;if(b===void 0&&(b=f),"selectionStart"in r)r.selectionStart=f,r.selectionEnd=Math.min(b,r.value.length);else{var T=r.ownerDocument||document,m=T&&T.defaultView||window;if(m.getSelection){var g=m.getSelection(),R=r.textContent.length,N=Math.min(l.start,R),he=l.end===void 0?N:Math.min(l.end,R);!g.extend&&N>he&&(o=he,he=N,N=o);var d=nc(r,N),c=nc(r,he);if(d&&c&&(g.rangeCount!==1||g.anchorNode!==d.node||g.anchorOffset!==d.offset||g.focusNode!==c.node||g.focusOffset!==c.offset)){var p=T.createRange();p.setStart(d.node,d.offset),g.removeAllRanges(),N>he?(g.addRange(p),g.extend(c.node,c.offset)):(p.setEnd(c.node,c.offset),g.addRange(p))}}}}for(T=[],g=r;g=g.parentNode;)g.nodeType===1&&T.push({element:g,left:g.scrollLeft,top:g.scrollTop});for(typeof r.focus=="function"&&r.focus(),r=0;r<T.length;r++){var x=T[r];x.element.scrollLeft=x.left,x.element.scrollTop=x.top}}Ps=!!jr,Qr=jr=null}finally{ae=i,S.p=n,w.T=a}}e.current=t,Ue=2}}function wd(){if(Ue===2){Ue=0;var e=ha,t=wn,a=(t.flags&8772)!==0;if((t.subtreeFlags&8772)!==0||a){a=w.T,w.T=null;var n=S.p;S.p=2;var i=ae;ae|=4;try{Zu(e,t.alternate,t)}finally{ae=i,S.p=n,w.T=a}}Ue=3}}function xd(){if(Ue===4||Ue===3){Ue=0,Uh();var e=ha,t=wn,a=Zt,n=rd;(t.subtreeFlags&10256)!==0||(t.flags&10256)!==0?Ue=5:(Ue=0,wn=ha=null,Td(e,e.pendingLanes));var i=e.pendingLanes;if(i===0&&(da=null),Xs(a),t=t.stateNode,at&&typeof at.onCommitFiberRoot=="function")try{at.onCommitFiberRoot(Cn,t,void 0,(t.current.flags&128)===128)}catch{}if(n!==null){t=w.T,i=S.p,S.p=2,w.T=null;try{for(var s=e.onRecoverableError,o=0;o<n.length;o++){var r=n[o];s(r.value,{componentStack:r.stack})}}finally{w.T=t,S.p=i}}(Zt&3)!==0&&Ts(),Ut(e),i=e.pendingLanes,(a&261930)!==0&&(i&42)!==0?e===Ur?ui++:(ui=0,Ur=e):ui=0,di(0)}}function Td(e,t){(e.pooledCacheLanes&=t)===0&&(t=e.pooledCache,t!=null&&(e.pooledCache=null,Gn(t)))}function Ts(){return vd(),wd(),xd(),kd()}function kd(){if(Ue!==5)return!1;var e=ha,t=Rr;Rr=0;var a=Xs(Zt),n=w.T,i=S.p;try{S.p=32>a?32:a,w.T=null,a=Cr,Cr=null;var s=ha,o=Zt;if(Ue=0,wn=ha=null,Zt=0,(ae&6)!==0)throw Error(h(331));var r=ae;if(ae|=4,id(s.current),td(s,s.current,o,a),ae=r,di(0,!1),at&&typeof at.onPostCommitFiberRoot=="function")try{at.onPostCommitFiberRoot(Cn,s)}catch{}return!0}finally{S.p=i,w.T=n,Td(e,t)}}function Ad(e,t,a){t=ft(a,t),t=lr(e.stateNode,t,2),e=sa(e,t,2),e!==null&&(Dn(e,2),Ut(e))}function oe(e,t,a){if(e.tag===3)Ad(e,e,a);else for(;t!==null;){if(t.tag===3){Ad(t,e,a);break}else if(t.tag===1){var n=t.stateNode;if(typeof t.type.getDerivedStateFromError=="function"||typeof n.componentDidCatch=="function"&&(da===null||!da.has(n))){e=ft(a,e),a=Su(2),n=sa(t,a,2),n!==null&&(qu(a,n,t,e),Dn(n,2),Ut(n));break}}t=t.return}}function zr(e,t,a){var n=e.pingCache;if(n===null){n=e.pingCache=new af;var i=new Set;n.set(t,i)}else i=n.get(t),i===void 0&&(i=new Set,n.set(t,i));i.has(a)||(Sr=!0,i.add(a),e=lf.bind(null,e,t,a),t.then(e,e))}function lf(e,t,a){var n=e.pingCache;n!==null&&n.delete(t),e.pingedLanes|=e.suspendedLanes&a,e.warmLanes&=~a,me===e&&(F&a)===a&&(Te===4||Te===3&&(F&62914560)===F&&300>tt()-gs?(ae&2)===0&&xn(e,0):qr|=a,vn===F&&(vn=0)),Ut(e)}function Sd(e,t){t===0&&(t=vl()),e=Ca(e,t),e!==null&&(Dn(e,t),Ut(e))}function cf(e){var t=e.memoizedState,a=0;t!==null&&(a=t.retryLane),Sd(e,a)}function uf(e,t){var a=0;switch(e.tag){case 31:case 13:var n=e.stateNode,i=e.memoizedState;i!==null&&(a=i.retryLane);break;case 19:n=e.stateNode;break;case 22:n=e.stateNode._retryCache;break;default:throw Error(h(314))}n!==null&&n.delete(t),Sd(e,a)}function df(e,t){return Gs(e,t)}var ks=null,kn=null,Pr=!1,As=!1,Er=!1,fa=0;function Ut(e){e!==kn&&e.next===null&&(kn===null?ks=kn=e:kn=kn.next=e),As=!0,Pr||(Pr=!0,pf())}function di(e,t){if(!Er&&As){Er=!0;do for(var a=!1,n=ks;n!==null;){if(e!==0){var i=n.pendingLanes;if(i===0)var s=0;else{var o=n.suspendedLanes,r=n.pingedLanes;s=(1<<31-nt(42|e)+1)-1,s&=i&~(o&~r),s=s&201326741?s&201326741|1:s?s|2:0}s!==0&&(a=!0,Cd(n,s))}else s=F,s=Ri(n,n===me?s:0,n.cancelPendingCommit!==null||n.timeoutHandle!==-1),(s&3)===0||Un(n,s)||(a=!0,Cd(n,s));n=n.next}while(a);Er=!1}}function hf(){qd()}function qd(){As=Pr=!1;var e=0;fa!==0&&kf()&&(e=fa);for(var t=tt(),a=null,n=ks;n!==null;){var i=n.next,s=Ld(n,t);s===0?(n.next=null,a===null?ks=i:a.next=i,i===null&&(kn=a)):(a=n,(e!==0||(s&3)!==0)&&(As=!0)),n=i}Ue!==0&&Ue!==5||di(e),fa!==0&&(fa=0)}function Ld(e,t){for(var a=e.suspendedLanes,n=e.pingedLanes,i=e.expirationTimes,s=e.pendingLanes&-62914561;0<s;){var o=31-nt(s),r=1<<o,l=i[o];l===-1?((r&a)===0||(r&n)!==0)&&(i[o]=Hh(r,t)):l<=t&&(e.expiredLanes|=r),s&=~r}if(t=me,a=F,a=Ri(e,e===t?a:0,e.cancelPendingCommit!==null||e.timeoutHandle!==-1),n=e.callbackNode,a===0||e===t&&(se===2||se===9)||e.cancelPendingCommit!==null)return n!==null&&n!==null&&Ks(n),e.callbackNode=null,e.callbackPriority=0;if((a&3)===0||Un(e,a)){if(t=a&-a,t===e.callbackPriority)return t;switch(n!==null&&Ks(n),Xs(a)){case 2:case 8:a=yl;break;case 32:a=Ai;break;case 268435456:a=bl;break;default:a=Ai}return n=Rd.bind(null,e),a=Gs(a,n),e.callbackPriority=t,e.callbackNode=a,t}return n!==null&&n!==null&&Ks(n),e.callbackPriority=2,e.callbackNode=null,2}function Rd(e,t){if(Ue!==0&&Ue!==5)return e.callbackNode=null,e.callbackPriority=0,null;var a=e.callbackNode;if(Ts()&&e.callbackNode!==a)return null;var n=F;return n=Ri(e,e===me?n:0,e.cancelPendingCommit!==null||e.timeoutHandle!==-1),n===0?null:(cd(e,n,t),Ld(e,tt()),e.callbackNode!=null&&e.callbackNode===a?Rd.bind(null,e):null)}function Cd(e,t){if(Ts())return null;cd(e,t,!0)}function pf(){Sf(function(){(ae&6)!==0?Gs(gl,hf):qd()})}function Nr(){if(fa===0){var e=ln;e===0&&(e=Si,Si<<=1,(Si&261888)===0&&(Si=256)),fa=e}return fa}function Ud(e){return e==null||typeof e=="symbol"||typeof e=="boolean"?null:typeof e=="function"?e:Ii(""+e)}function Dd(e,t){var a=t.ownerDocument.createElement("input");return a.name=t.name,a.value=t.value,e.id&&a.setAttribute("form",e.id),t.parentNode.insertBefore(a,t),e=new FormData(e),a.parentNode.removeChild(a),e}function ff(e,t,a,n,i){if(t==="submit"&&a&&a.stateNode===i){var s=Ud((i[Ze]||null).action),o=n.submitter;o&&(t=(t=o[Ze]||null)?Ud(t.formAction):o.getAttribute("formAction"),t!==null&&(s=t,o=null));var r=new Ni("action","action",null,n,i);e.push({event:r,listeners:[{instance:null,listener:function(){if(n.defaultPrevented){if(fa!==0){var l=o?Dd(i,o):new FormData(i);ar(a,{pending:!0,data:l,method:i.method,action:s},null,l)}}else typeof s=="function"&&(r.preventDefault(),l=o?Dd(i,o):new FormData(i),ar(a,{pending:!0,data:l,method:i.method,action:s},s,l))},currentTarget:i}]})}}for(var Br=0;Br<vo.length;Br++){var Hr=vo[Br],mf=Hr.toLowerCase(),gf=Hr[0].toUpperCase()+Hr.slice(1);Tt(mf,"on"+gf)}Tt(lc,"onAnimationEnd"),Tt(cc,"onAnimationIteration"),Tt(uc,"onAnimationStart"),Tt("dblclick","onDoubleClick"),Tt("focusin","onFocus"),Tt("focusout","onBlur"),Tt(Dp,"onTransitionRun"),Tt(Ip,"onTransitionStart"),Tt(zp,"onTransitionCancel"),Tt(dc,"onTransitionEnd"),Ya("onMouseEnter",["mouseout","mouseover"]),Ya("onMouseLeave",["mouseout","mouseover"]),Ya("onPointerEnter",["pointerout","pointerover"]),Ya("onPointerLeave",["pointerout","pointerover"]),Sa("onChange","change click focusin focusout input keydown keyup selectionchange".split(" ")),Sa("onSelect","focusout contextmenu dragend focusin keydown keyup mousedown mouseup selectionchange".split(" ")),Sa("onBeforeInput",["compositionend","keypress","textInput","paste"]),Sa("onCompositionEnd","compositionend focusout keydown keypress keyup mousedown".split(" ")),Sa("onCompositionStart","compositionstart focusout keydown keypress keyup mousedown".split(" ")),Sa("onCompositionUpdate","compositionupdate focusout keydown keypress keyup mousedown".split(" "));var hi="abort canplay canplaythrough durationchange emptied encrypted ended error loadeddata loadedmetadata loadstart pause play playing progress ratechange resize seeked seeking stalled suspend timeupdate volumechange waiting".split(" "),yf=new Set("beforetoggle cancel close invalid load scroll scrollend toggle".split(" ").concat(hi));function Id(e,t){t=(t&4)!==0;for(var a=0;a<e.length;a++){var n=e[a],i=n.event;n=n.listeners;e:{var s=void 0;if(t)for(var o=n.length-1;0<=o;o--){var r=n[o],l=r.instance,f=r.currentTarget;if(r=r.listener,l!==s&&i.isPropagationStopped())break e;s=r,i.currentTarget=f;try{s(i)}catch(b){Wi(b)}i.currentTarget=null,s=l}else for(o=0;o<n.length;o++){if(r=n[o],l=r.instance,f=r.currentTarget,r=r.listener,l!==s&&i.isPropagationStopped())break e;s=r,i.currentTarget=f;try{s(i)}catch(b){Wi(b)}i.currentTarget=null,s=l}}}}function V(e,t){var a=t[Vs];a===void 0&&(a=t[Vs]=new Set);var n=e+"__bubble";a.has(n)||(zd(t,e,2,!1),a.add(n))}function Wr(e,t,a){var n=0;t&&(n|=4),zd(a,e,n,t)}var Ss="_reactListening"+Math.random().toString(36).slice(2);function Mr(e){if(!e[Ss]){e[Ss]=!0,ql.forEach(function(a){a!=="selectionchange"&&(yf.has(a)||Wr(a,!1,e),Wr(a,!0,e))});var t=e.nodeType===9?e:e.ownerDocument;t===null||t[Ss]||(t[Ss]=!0,Wr("selectionchange",!1,t))}}function zd(e,t,a,n){switch(lh(t)){case 2:var i=Gf;break;case 8:i=Kf;break;default:i=tl}a=i.bind(null,t,a,e),i=void 0,!so||t!=="touchstart"&&t!=="touchmove"&&t!=="wheel"||(i=!0),n?i!==void 0?e.addEventListener(t,a,{capture:!0,passive:i}):e.addEventListener(t,a,!0):i!==void 0?e.addEventListener(t,a,{passive:i}):e.addEventListener(t,a,!1)}function Or(e,t,a,n,i){var s=n;if((t&1)===0&&(t&2)===0&&n!==null)e:for(;;){if(n===null)return;var o=n.tag;if(o===3||o===4){var r=n.stateNode.containerInfo;if(r===i)break;if(o===4)for(o=n.return;o!==null;){var l=o.tag;if((l===3||l===4)&&o.stateNode.containerInfo===i)return;o=o.return}for(;r!==null;){if(o=Qa(r),o===null)return;if(l=o.tag,l===5||l===6||l===26||l===27){n=s=o;continue e}r=r.parentNode}}n=n.return}Hl(function(){var f=s,b=no(a),T=[];e:{var m=hc.get(e);if(m!==void 0){var g=Ni,R=e;switch(e){case"keypress":if(Pi(a)===0)break e;case"keydown":case"keyup":g=cp;break;case"focusin":R="focus",g=co;break;case"focusout":R="blur",g=co;break;case"beforeblur":case"afterblur":g=co;break;case"click":if(a.button===2)break e;case"auxclick":case"dblclick":case"mousedown":case"mousemove":case"mouseup":case"mouseout":case"mouseover":case"contextmenu":g=Ol;break;case"drag":case"dragend":case"dragenter":case"dragexit":case"dragleave":case"dragover":case"dragstart":case"drop":g=Fh;break;case"touchcancel":case"touchend":case"touchmove":case"touchstart":g=hp;break;case lc:case cc:case uc:g=ep;break;case dc:g=fp;break;case"scroll":case"scrollend":g=Xh;break;case"wheel":g=gp;break;case"copy":case"cut":case"paste":g=ap;break;case"gotpointercapture":case"lostpointercapture":case"pointercancel":case"pointerdown":case"pointermove":case"pointerout":case"pointerover":case"pointerup":g=jl;break;case"toggle":case"beforetoggle":g=bp}var N=(t&4)!==0,he=!N&&(e==="scroll"||e==="scrollend"),d=N?m!==null?m+"Capture":null:m;N=[];for(var c=f,p;c!==null;){var x=c;if(p=x.stateNode,x=x.tag,x!==5&&x!==26&&x!==27||p===null||d===null||(x=Pn(c,d),x!=null&&N.push(pi(c,x,p))),he)break;c=c.return}0<N.length&&(m=new g(m,R,null,a,b),T.push({event:m,listeners:N}))}}if((t&7)===0){e:{if(m=e==="mouseover"||e==="pointerover",g=e==="mouseout"||e==="pointerout",m&&a!==ao&&(R=a.relatedTarget||a.fromElement)&&(Qa(R)||R[ja]))break e;if((g||m)&&(m=b.window===b?b:(m=b.ownerDocument)?m.defaultView||m.parentWindow:window,g?(R=a.relatedTarget||a.toElement,g=f,R=R?Qa(R):null,R!==null&&(he=E(R),N=R.tag,R!==he||N!==5&&N!==27&&N!==6)&&(R=null)):(g=null,R=f),g!==R)){if(N=Ol,x="onMouseLeave",d="onMouseEnter",c="mouse",(e==="pointerout"||e==="pointerover")&&(N=jl,x="onPointerLeave",d="onPointerEnter",c="pointer"),he=g==null?m:zn(g),p=R==null?m:zn(R),m=new N(x,c+"leave",g,a,b),m.target=he,m.relatedTarget=p,x=null,Qa(b)===f&&(N=new N(d,c+"enter",R,a,b),N.target=p,N.relatedTarget=he,x=N),he=x,g&&R)t:{for(N=bf,d=g,c=R,p=0,x=d;x;x=N(x))p++;x=0;for(var z=c;z;z=N(z))x++;for(;0<p-x;)d=N(d),p--;for(;0<x-p;)c=N(c),x--;for(;p--;){if(d===c||c!==null&&d===c.alternate){N=d;break t}d=N(d),c=N(c)}N=null}else N=null;g!==null&&Pd(T,m,g,N,!1),R!==null&&he!==null&&Pd(T,he,R,N,!0)}}e:{if(m=f?zn(f):window,g=m.nodeName&&m.nodeName.toLowerCase(),g==="select"||g==="input"&&m.type==="file")var ee=Fl;else if(Xl(m))if(Jl)ee=Rp;else{ee=qp;var D=Sp}else g=m.nodeName,!g||g.toLowerCase()!=="input"||m.type!=="checkbox"&&m.type!=="radio"?f&&to(f.elementType)&&(ee=Fl):ee=Lp;if(ee&&(ee=ee(e,f))){Vl(T,ee,a,b);break e}D&&D(e,m,f),e==="focusout"&&f&&m.type==="number"&&f.memoizedProps.value!=null&&eo(m,"number",m.value)}switch(D=f?zn(f):window,e){case"focusin":(Xl(D)||D.contentEditable==="true")&&($a=D,go=f,_n=null);break;case"focusout":_n=go=$a=null;break;case"mousedown":yo=!0;break;case"contextmenu":case"mouseup":case"dragend":yo=!1,oc(T,a,b);break;case"selectionchange":if(Up)break;case"keydown":case"keyup":oc(T,a,b)}var G;if(ho)e:{switch(e){case"compositionstart":var J="onCompositionStart";break e;case"compositionend":J="onCompositionEnd";break e;case"compositionupdate":J="onCompositionUpdate";break e}J=void 0}else Ja?Yl(e,a)&&(J="onCompositionEnd"):e==="keydown"&&a.keyCode===229&&(J="onCompositionStart");J&&(Ql&&a.locale!=="ko"&&(Ja||J!=="onCompositionStart"?J==="onCompositionEnd"&&Ja&&(G=Wl()):(Jt=b,oo="value"in Jt?Jt.value:Jt.textContent,Ja=!0)),D=qs(f,J),0<D.length&&(J=new _l(J,e,null,a,b),T.push({event:J,listeners:D}),G?J.data=G:(G=Zl(a),G!==null&&(J.data=G)))),(G=wp?xp(e,a):Tp(e,a))&&(J=qs(f,"onBeforeInput"),0<J.length&&(D=new _l("onBeforeInput","beforeinput",null,a,b),T.push({event:D,listeners:J}),D.data=G)),ff(T,e,f,a,b)}Id(T,t)})}function pi(e,t,a){return{instance:e,listener:t,currentTarget:a}}function qs(e,t){for(var a=t+"Capture",n=[];e!==null;){var i=e,s=i.stateNode;if(i=i.tag,i!==5&&i!==26&&i!==27||s===null||(i=Pn(e,a),i!=null&&n.unshift(pi(e,i,s)),i=Pn(e,t),i!=null&&n.push(pi(e,i,s))),e.tag===3)return n;e=e.return}return[]}function bf(e){if(e===null)return null;do e=e.return;while(e&&e.tag!==5&&e.tag!==27);return e||null}function Pd(e,t,a,n,i){for(var s=t._reactName,o=[];a!==null&&a!==n;){var r=a,l=r.alternate,f=r.stateNode;if(r=r.tag,l!==null&&l===n)break;r!==5&&r!==26&&r!==27||f===null||(l=f,i?(f=Pn(a,s),f!=null&&o.unshift(pi(a,f,l))):i||(f=Pn(a,s),f!=null&&o.push(pi(a,f,l)))),a=a.return}o.length!==0&&e.push({event:t,listeners:o})}var vf=/\r\n?/g,wf=/\u0000|\uFFFD/g;function Ed(e){return(typeof e=="string"?e:""+e).replace(vf,`
`).replace(wf,"")}function Nd(e,t){return t=Ed(t),Ed(e)===t}function de(e,t,a,n,i,s){switch(a){case"children":typeof n=="string"?t==="body"||t==="textarea"&&n===""||Xa(e,n):(typeof n=="number"||typeof n=="bigint")&&t!=="body"&&Xa(e,""+n);break;case"className":Ui(e,"class",n);break;case"tabIndex":Ui(e,"tabindex",n);break;case"dir":case"role":case"viewBox":case"width":case"height":Ui(e,a,n);break;case"style":Nl(e,n,s);break;case"data":if(t!=="object"){Ui(e,"data",n);break}case"src":case"href":if(n===""&&(t!=="a"||a!=="href")){e.removeAttribute(a);break}if(n==null||typeof n=="function"||typeof n=="symbol"||typeof n=="boolean"){e.removeAttribute(a);break}n=Ii(""+n),e.setAttribute(a,n);break;case"action":case"formAction":if(typeof n=="function"){e.setAttribute(a,"javascript:throw new Error('A React form was unexpectedly submitted. If you called form.submit() manually, consider using form.requestSubmit() instead. If you\\'re trying to use event.stopPropagation() in a submit event handler, consider also calling event.preventDefault().')");break}else typeof s=="function"&&(a==="formAction"?(t!=="input"&&de(e,t,"name",i.name,i,null),de(e,t,"formEncType",i.formEncType,i,null),de(e,t,"formMethod",i.formMethod,i,null),de(e,t,"formTarget",i.formTarget,i,null)):(de(e,t,"encType",i.encType,i,null),de(e,t,"method",i.method,i,null),de(e,t,"target",i.target,i,null)));if(n==null||typeof n=="symbol"||typeof n=="boolean"){e.removeAttribute(a);break}n=Ii(""+n),e.setAttribute(a,n);break;case"onClick":n!=null&&(e.onclick=zt);break;case"onScroll":n!=null&&V("scroll",e);break;case"onScrollEnd":n!=null&&V("scrollend",e);break;case"dangerouslySetInnerHTML":if(n!=null){if(typeof n!="object"||!("__html"in n))throw Error(h(61));if(a=n.__html,a!=null){if(i.children!=null)throw Error(h(60));e.innerHTML=a}}break;case"multiple":e.multiple=n&&typeof n!="function"&&typeof n!="symbol";break;case"muted":e.muted=n&&typeof n!="function"&&typeof n!="symbol";break;case"suppressContentEditableWarning":case"suppressHydrationWarning":case"defaultValue":case"defaultChecked":case"innerHTML":case"ref":break;case"autoFocus":break;case"xlinkHref":if(n==null||typeof n=="function"||typeof n=="boolean"||typeof n=="symbol"){e.removeAttribute("xlink:href");break}a=Ii(""+n),e.setAttributeNS("http://www.w3.org/1999/xlink","xlink:href",a);break;case"contentEditable":case"spellCheck":case"draggable":case"value":case"autoReverse":case"externalResourcesRequired":case"focusable":case"preserveAlpha":n!=null&&typeof n!="function"&&typeof n!="symbol"?e.setAttribute(a,""+n):e.removeAttribute(a);break;case"inert":case"allowFullScreen":case"async":case"autoPlay":case"controls":case"default":case"defer":case"disabled":case"disablePictureInPicture":case"disableRemotePlayback":case"formNoValidate":case"hidden":case"loop":case"noModule":case"noValidate":case"open":case"playsInline":case"readOnly":case"required":case"reversed":case"scoped":case"seamless":case"itemScope":n&&typeof n!="function"&&typeof n!="symbol"?e.setAttribute(a,""):e.removeAttribute(a);break;case"capture":case"download":n===!0?e.setAttribute(a,""):n!==!1&&n!=null&&typeof n!="function"&&typeof n!="symbol"?e.setAttribute(a,n):e.removeAttribute(a);break;case"cols":case"rows":case"size":case"span":n!=null&&typeof n!="function"&&typeof n!="symbol"&&!isNaN(n)&&1<=n?e.setAttribute(a,n):e.removeAttribute(a);break;case"rowSpan":case"start":n==null||typeof n=="function"||typeof n=="symbol"||isNaN(n)?e.removeAttribute(a):e.setAttribute(a,n);break;case"popover":V("beforetoggle",e),V("toggle",e),Ci(e,"popover",n);break;case"xlinkActuate":It(e,"http://www.w3.org/1999/xlink","xlink:actuate",n);break;case"xlinkArcrole":It(e,"http://www.w3.org/1999/xlink","xlink:arcrole",n);break;case"xlinkRole":It(e,"http://www.w3.org/1999/xlink","xlink:role",n);break;case"xlinkShow":It(e,"http://www.w3.org/1999/xlink","xlink:show",n);break;case"xlinkTitle":It(e,"http://www.w3.org/1999/xlink","xlink:title",n);break;case"xlinkType":It(e,"http://www.w3.org/1999/xlink","xlink:type",n);break;case"xmlBase":It(e,"http://www.w3.org/XML/1998/namespace","xml:base",n);break;case"xmlLang":It(e,"http://www.w3.org/XML/1998/namespace","xml:lang",n);break;case"xmlSpace":It(e,"http://www.w3.org/XML/1998/namespace","xml:space",n);break;case"is":Ci(e,"is",n);break;case"innerText":case"textContent":break;default:(!(2<a.length)||a[0]!=="o"&&a[0]!=="O"||a[1]!=="n"&&a[1]!=="N")&&(a=Yh.get(a)||a,Ci(e,a,n))}}function _r(e,t,a,n,i,s){switch(a){case"style":Nl(e,n,s);break;case"dangerouslySetInnerHTML":if(n!=null){if(typeof n!="object"||!("__html"in n))throw Error(h(61));if(a=n.__html,a!=null){if(i.children!=null)throw Error(h(60));e.innerHTML=a}}break;case"children":typeof n=="string"?Xa(e,n):(typeof n=="number"||typeof n=="bigint")&&Xa(e,""+n);break;case"onScroll":n!=null&&V("scroll",e);break;case"onScrollEnd":n!=null&&V("scrollend",e);break;case"onClick":n!=null&&(e.onclick=zt);break;case"suppressContentEditableWarning":case"suppressHydrationWarning":case"innerHTML":case"ref":break;case"innerText":case"textContent":break;default:if(!Ll.hasOwnProperty(a))e:{if(a[0]==="o"&&a[1]==="n"&&(i=a.endsWith("Capture"),t=a.slice(2,i?a.length-7:void 0),s=e[Ze]||null,s=s!=null?s[a]:null,typeof s=="function"&&e.removeEventListener(t,s,i),typeof n=="function")){typeof s!="function"&&s!==null&&(a in e?e[a]=null:e.hasAttribute(a)&&e.removeAttribute(a)),e.addEventListener(t,n,i);break e}a in e?e[a]=n:n===!0?e.setAttribute(a,""):Ci(e,a,n)}}}function Oe(e,t,a){switch(t){case"div":case"span":case"svg":case"path":case"a":case"g":case"p":case"li":break;case"img":V("error",e),V("load",e);var n=!1,i=!1,s;for(s in a)if(a.hasOwnProperty(s)){var o=a[s];if(o!=null)switch(s){case"src":n=!0;break;case"srcSet":i=!0;break;case"children":case"dangerouslySetInnerHTML":throw Error(h(137,t));default:de(e,t,s,o,a,null)}}i&&de(e,t,"srcSet",a.srcSet,a,null),n&&de(e,t,"src",a.src,a,null);return;case"input":V("invalid",e);var r=s=o=i=null,l=null,f=null;for(n in a)if(a.hasOwnProperty(n)){var b=a[n];if(b!=null)switch(n){case"name":i=b;break;case"type":o=b;break;case"checked":l=b;break;case"defaultChecked":f=b;break;case"value":s=b;break;case"defaultValue":r=b;break;case"children":case"dangerouslySetInnerHTML":if(b!=null)throw Error(h(137,t));break;default:de(e,t,n,b,a,null)}}Il(e,s,r,l,f,o,i,!1);return;case"select":V("invalid",e),n=o=s=null;for(i in a)if(a.hasOwnProperty(i)&&(r=a[i],r!=null))switch(i){case"value":s=r;break;case"defaultValue":o=r;break;case"multiple":n=r;default:de(e,t,i,r,a,null)}t=s,a=o,e.multiple=!!n,t!=null?Za(e,!!n,t,!1):a!=null&&Za(e,!!n,a,!0);return;case"textarea":V("invalid",e),s=i=n=null;for(o in a)if(a.hasOwnProperty(o)&&(r=a[o],r!=null))switch(o){case"value":n=r;break;case"defaultValue":i=r;break;case"children":s=r;break;case"dangerouslySetInnerHTML":if(r!=null)throw Error(h(91));break;default:de(e,t,o,r,a,null)}Pl(e,n,i,s);return;case"option":for(l in a)if(a.hasOwnProperty(l)&&(n=a[l],n!=null))switch(l){case"selected":e.selected=n&&typeof n!="function"&&typeof n!="symbol";break;default:de(e,t,l,n,a,null)}return;case"dialog":V("beforetoggle",e),V("toggle",e),V("cancel",e),V("close",e);break;case"iframe":case"object":V("load",e);break;case"video":case"audio":for(n=0;n<hi.length;n++)V(hi[n],e);break;case"image":V("error",e),V("load",e);break;case"details":V("toggle",e);break;case"embed":case"source":case"link":V("error",e),V("load",e);case"area":case"base":case"br":case"col":case"hr":case"keygen":case"meta":case"param":case"track":case"wbr":case"menuitem":for(f in a)if(a.hasOwnProperty(f)&&(n=a[f],n!=null))switch(f){case"children":case"dangerouslySetInnerHTML":throw Error(h(137,t));default:de(e,t,f,n,a,null)}return;default:if(to(t)){for(b in a)a.hasOwnProperty(b)&&(n=a[b],n!==void 0&&_r(e,t,b,n,a,void 0));return}}for(r in a)a.hasOwnProperty(r)&&(n=a[r],n!=null&&de(e,t,r,n,a,null))}function xf(e,t,a,n){switch(t){case"div":case"span":case"svg":case"path":case"a":case"g":case"p":case"li":break;case"input":var i=null,s=null,o=null,r=null,l=null,f=null,b=null;for(g in a){var T=a[g];if(a.hasOwnProperty(g)&&T!=null)switch(g){case"checked":break;case"value":break;case"defaultValue":l=T;default:n.hasOwnProperty(g)||de(e,t,g,null,n,T)}}for(var m in n){var g=n[m];if(T=a[m],n.hasOwnProperty(m)&&(g!=null||T!=null))switch(m){case"type":s=g;break;case"name":i=g;break;case"checked":f=g;break;case"defaultChecked":b=g;break;case"value":o=g;break;case"defaultValue":r=g;break;case"children":case"dangerouslySetInnerHTML":if(g!=null)throw Error(h(137,t));break;default:g!==T&&de(e,t,m,g,n,T)}}$s(e,o,r,l,f,b,s,i);return;case"select":g=o=r=m=null;for(s in a)if(l=a[s],a.hasOwnProperty(s)&&l!=null)switch(s){case"value":break;case"multiple":g=l;default:n.hasOwnProperty(s)||de(e,t,s,null,n,l)}for(i in n)if(s=n[i],l=a[i],n.hasOwnProperty(i)&&(s!=null||l!=null))switch(i){case"value":m=s;break;case"defaultValue":r=s;break;case"multiple":o=s;default:s!==l&&de(e,t,i,s,n,l)}t=r,a=o,n=g,m!=null?Za(e,!!a,m,!1):!!n!=!!a&&(t!=null?Za(e,!!a,t,!0):Za(e,!!a,a?[]:"",!1));return;case"textarea":g=m=null;for(r in a)if(i=a[r],a.hasOwnProperty(r)&&i!=null&&!n.hasOwnProperty(r))switch(r){case"value":break;case"children":break;default:de(e,t,r,null,n,i)}for(o in n)if(i=n[o],s=a[o],n.hasOwnProperty(o)&&(i!=null||s!=null))switch(o){case"value":m=i;break;case"defaultValue":g=i;break;case"children":break;case"dangerouslySetInnerHTML":if(i!=null)throw Error(h(91));break;default:i!==s&&de(e,t,o,i,n,s)}zl(e,m,g);return;case"option":for(var R in a)if(m=a[R],a.hasOwnProperty(R)&&m!=null&&!n.hasOwnProperty(R))switch(R){case"selected":e.selected=!1;break;default:de(e,t,R,null,n,m)}for(l in n)if(m=n[l],g=a[l],n.hasOwnProperty(l)&&m!==g&&(m!=null||g!=null))switch(l){case"selected":e.selected=m&&typeof m!="function"&&typeof m!="symbol";break;default:de(e,t,l,m,n,g)}return;case"img":case"link":case"area":case"base":case"br":case"col":case"embed":case"hr":case"keygen":case"meta":case"param":case"source":case"track":case"wbr":case"menuitem":for(var N in a)m=a[N],a.hasOwnProperty(N)&&m!=null&&!n.hasOwnProperty(N)&&de(e,t,N,null,n,m);for(f in n)if(m=n[f],g=a[f],n.hasOwnProperty(f)&&m!==g&&(m!=null||g!=null))switch(f){case"children":case"dangerouslySetInnerHTML":if(m!=null)throw Error(h(137,t));break;default:de(e,t,f,m,n,g)}return;default:if(to(t)){for(var he in a)m=a[he],a.hasOwnProperty(he)&&m!==void 0&&!n.hasOwnProperty(he)&&_r(e,t,he,void 0,n,m);for(b in n)m=n[b],g=a[b],!n.hasOwnProperty(b)||m===g||m===void 0&&g===void 0||_r(e,t,b,m,n,g);return}}for(var d in a)m=a[d],a.hasOwnProperty(d)&&m!=null&&!n.hasOwnProperty(d)&&de(e,t,d,null,n,m);for(T in n)m=n[T],g=a[T],!n.hasOwnProperty(T)||m===g||m==null&&g==null||de(e,t,T,m,n,g)}function Bd(e){switch(e){case"css":case"script":case"font":case"img":case"image":case"input":case"link":return!0;default:return!1}}function Tf(){if(typeof performance.getEntriesByType=="function"){for(var e=0,t=0,a=performance.getEntriesByType("resource"),n=0;n<a.length;n++){var i=a[n],s=i.transferSize,o=i.initiatorType,r=i.duration;if(s&&r&&Bd(o)){for(o=0,r=i.responseEnd,n+=1;n<a.length;n++){var l=a[n],f=l.startTime;if(f>r)break;var b=l.transferSize,T=l.initiatorType;b&&Bd(T)&&(l=l.responseEnd,o+=b*(l<r?1:(r-f)/(l-f)))}if(--n,t+=8*(s+o)/(i.duration/1e3),e++,10<e)break}}if(0<e)return t/e/1e6}return navigator.connection&&(e=navigator.connection.downlink,typeof e=="number")?e:5}var jr=null,Qr=null;function Ls(e){return e.nodeType===9?e:e.ownerDocument}function Hd(e){switch(e){case"http://www.w3.org/2000/svg":return 1;case"http://www.w3.org/1998/Math/MathML":return 2;default:return 0}}function Wd(e,t){if(e===0)switch(t){case"svg":return 1;case"math":return 2;default:return 0}return e===1&&t==="foreignObject"?0:e}function Gr(e,t){return e==="textarea"||e==="noscript"||typeof t.children=="string"||typeof t.children=="number"||typeof t.children=="bigint"||typeof t.dangerouslySetInnerHTML=="object"&&t.dangerouslySetInnerHTML!==null&&t.dangerouslySetInnerHTML.__html!=null}var Kr=null;function kf(){var e=window.event;return e&&e.type==="popstate"?e===Kr?!1:(Kr=e,!0):(Kr=null,!1)}var Md=typeof setTimeout=="function"?setTimeout:void 0,Af=typeof clearTimeout=="function"?clearTimeout:void 0,Od=typeof Promise=="function"?Promise:void 0,Sf=typeof queueMicrotask=="function"?queueMicrotask:typeof Od<"u"?function(e){return Od.resolve(null).then(e).catch(qf)}:Md;function qf(e){setTimeout(function(){throw e})}function ma(e){return e==="head"}function _d(e,t){var a=t,n=0;do{var i=a.nextSibling;if(e.removeChild(a),i&&i.nodeType===8)if(a=i.data,a==="/$"||a==="/&"){if(n===0){e.removeChild(i),Ln(t);return}n--}else if(a==="$"||a==="$?"||a==="$~"||a==="$!"||a==="&")n++;else if(a==="html")fi(e.ownerDocument.documentElement);else if(a==="head"){a=e.ownerDocument.head,fi(a);for(var s=a.firstChild;s;){var o=s.nextSibling,r=s.nodeName;s[In]||r==="SCRIPT"||r==="STYLE"||r==="LINK"&&s.rel.toLowerCase()==="stylesheet"||a.removeChild(s),s=o}}else a==="body"&&fi(e.ownerDocument.body);a=i}while(a);Ln(t)}function jd(e,t){var a=e;e=0;do{var n=a.nextSibling;if(a.nodeType===1?t?(a._stashedDisplay=a.style.display,a.style.display="none"):(a.style.display=a._stashedDisplay||"",a.getAttribute("style")===""&&a.removeAttribute("style")):a.nodeType===3&&(t?(a._stashedText=a.nodeValue,a.nodeValue=""):a.nodeValue=a._stashedText||""),n&&n.nodeType===8)if(a=n.data,a==="/$"){if(e===0)break;e--}else a!=="$"&&a!=="$?"&&a!=="$~"&&a!=="$!"||e++;a=n}while(a)}function Yr(e){var t=e.firstChild;for(t&&t.nodeType===10&&(t=t.nextSibling);t;){var a=t;switch(t=t.nextSibling,a.nodeName){case"HTML":case"HEAD":case"BODY":Yr(a),Fs(a);continue;case"SCRIPT":case"STYLE":continue;case"LINK":if(a.rel.toLowerCase()==="stylesheet")continue}e.removeChild(a)}}function Lf(e,t,a,n){for(;e.nodeType===1;){var i=a;if(e.nodeName.toLowerCase()!==t.toLowerCase()){if(!n&&(e.nodeName!=="INPUT"||e.type!=="hidden"))break}else if(n){if(!e[In])switch(t){case"meta":if(!e.hasAttribute("itemprop"))break;return e;case"link":if(s=e.getAttribute("rel"),s==="stylesheet"&&e.hasAttribute("data-precedence"))break;if(s!==i.rel||e.getAttribute("href")!==(i.href==null||i.href===""?null:i.href)||e.getAttribute("crossorigin")!==(i.crossOrigin==null?null:i.crossOrigin)||e.getAttribute("title")!==(i.title==null?null:i.title))break;return e;case"style":if(e.hasAttribute("data-precedence"))break;return e;case"script":if(s=e.getAttribute("src"),(s!==(i.src==null?null:i.src)||e.getAttribute("type")!==(i.type==null?null:i.type)||e.getAttribute("crossorigin")!==(i.crossOrigin==null?null:i.crossOrigin))&&s&&e.hasAttribute("async")&&!e.hasAttribute("itemprop"))break;return e;default:return e}}else if(t==="input"&&e.type==="hidden"){var s=i.name==null?null:""+i.name;if(i.type==="hidden"&&e.getAttribute("name")===s)return e}else return e;if(e=vt(e.nextSibling),e===null)break}return null}function Rf(e,t,a){if(t==="")return null;for(;e.nodeType!==3;)if((e.nodeType!==1||e.nodeName!=="INPUT"||e.type!=="hidden")&&!a||(e=vt(e.nextSibling),e===null))return null;return e}function Qd(e,t){for(;e.nodeType!==8;)if((e.nodeType!==1||e.nodeName!=="INPUT"||e.type!=="hidden")&&!t||(e=vt(e.nextSibling),e===null))return null;return e}function Zr(e){return e.data==="$?"||e.data==="$~"}function Xr(e){return e.data==="$!"||e.data==="$?"&&e.ownerDocument.readyState!=="loading"}function Cf(e,t){var a=e.ownerDocument;if(e.data==="$~")e._reactRetry=t;else if(e.data!=="$?"||a.readyState!=="loading")t();else{var n=function(){t(),a.removeEventListener("DOMContentLoaded",n)};a.addEventListener("DOMContentLoaded",n),e._reactRetry=n}}function vt(e){for(;e!=null;e=e.nextSibling){var t=e.nodeType;if(t===1||t===3)break;if(t===8){if(t=e.data,t==="$"||t==="$!"||t==="$?"||t==="$~"||t==="&"||t==="F!"||t==="F")break;if(t==="/$"||t==="/&")return null}}return e}var Vr=null;function Gd(e){e=e.nextSibling;for(var t=0;e;){if(e.nodeType===8){var a=e.data;if(a==="/$"||a==="/&"){if(t===0)return vt(e.nextSibling);t--}else a!=="$"&&a!=="$!"&&a!=="$?"&&a!=="$~"&&a!=="&"||t++}e=e.nextSibling}return null}function Kd(e){e=e.previousSibling;for(var t=0;e;){if(e.nodeType===8){var a=e.data;if(a==="$"||a==="$!"||a==="$?"||a==="$~"||a==="&"){if(t===0)return e;t--}else a!=="/$"&&a!=="/&"||t++}e=e.previousSibling}return null}function Yd(e,t,a){switch(t=Ls(a),e){case"html":if(e=t.documentElement,!e)throw Error(h(452));return e;case"head":if(e=t.head,!e)throw Error(h(453));return e;case"body":if(e=t.body,!e)throw Error(h(454));return e;default:throw Error(h(451))}}function fi(e){for(var t=e.attributes;t.length;)e.removeAttributeNode(t[0]);Fs(e)}var wt=new Map,Zd=new Set;function Rs(e){return typeof e.getRootNode=="function"?e.getRootNode():e.nodeType===9?e:e.ownerDocument}var Xt=S.d;S.d={f:Uf,r:Df,D:If,C:zf,L:Pf,m:Ef,X:Bf,S:Nf,M:Hf};function Uf(){var e=Xt.f(),t=vs();return e||t}function Df(e){var t=Ga(e);t!==null&&t.tag===5&&t.type==="form"?du(t):Xt.r(e)}var An=typeof document>"u"?null:document;function Xd(e,t,a){var n=An;if(n&&typeof t=="string"&&t){var i=ht(t);i='link[rel="'+e+'"][href="'+i+'"]',typeof a=="string"&&(i+='[crossorigin="'+a+'"]'),Zd.has(i)||(Zd.add(i),e={rel:e,crossOrigin:a,href:t},n.querySelector(i)===null&&(t=n.createElement("link"),Oe(t,"link",e),Ie(t),n.head.appendChild(t)))}}function If(e){Xt.D(e),Xd("dns-prefetch",e,null)}function zf(e,t){Xt.C(e,t),Xd("preconnect",e,t)}function Pf(e,t,a){Xt.L(e,t,a);var n=An;if(n&&e&&t){var i='link[rel="preload"][as="'+ht(t)+'"]';t==="image"&&a&&a.imageSrcSet?(i+='[imagesrcset="'+ht(a.imageSrcSet)+'"]',typeof a.imageSizes=="string"&&(i+='[imagesizes="'+ht(a.imageSizes)+'"]')):i+='[href="'+ht(e)+'"]';var s=i;switch(t){case"style":s=Sn(e);break;case"script":s=qn(e)}wt.has(s)||(e=I({rel:"preload",href:t==="image"&&a&&a.imageSrcSet?void 0:e,as:t},a),wt.set(s,e),n.querySelector(i)!==null||t==="style"&&n.querySelector(mi(s))||t==="script"&&n.querySelector(gi(s))||(t=n.createElement("link"),Oe(t,"link",e),Ie(t),n.head.appendChild(t)))}}function Ef(e,t){Xt.m(e,t);var a=An;if(a&&e){var n=t&&typeof t.as=="string"?t.as:"script",i='link[rel="modulepreload"][as="'+ht(n)+'"][href="'+ht(e)+'"]',s=i;switch(n){case"audioworklet":case"paintworklet":case"serviceworker":case"sharedworker":case"worker":case"script":s=qn(e)}if(!wt.has(s)&&(e=I({rel:"modulepreload",href:e},t),wt.set(s,e),a.querySelector(i)===null)){switch(n){case"audioworklet":case"paintworklet":case"serviceworker":case"sharedworker":case"worker":case"script":if(a.querySelector(gi(s)))return}n=a.createElement("link"),Oe(n,"link",e),Ie(n),a.head.appendChild(n)}}}function Nf(e,t,a){Xt.S(e,t,a);var n=An;if(n&&e){var i=Ka(n).hoistableStyles,s=Sn(e);t=t||"default";var o=i.get(s);if(!o){var r={loading:0,preload:null};if(o=n.querySelector(mi(s)))r.loading=5;else{e=I({rel:"stylesheet",href:e,"data-precedence":t},a),(a=wt.get(s))&&Fr(e,a);var l=o=n.createElement("link");Ie(l),Oe(l,"link",e),l._p=new Promise(function(f,b){l.onload=f,l.onerror=b}),l.addEventListener("load",function(){r.loading|=1}),l.addEventListener("error",function(){r.loading|=2}),r.loading|=4,Cs(o,t,n)}o={type:"stylesheet",instance:o,count:1,state:r},i.set(s,o)}}}function Bf(e,t){Xt.X(e,t);var a=An;if(a&&e){var n=Ka(a).hoistableScripts,i=qn(e),s=n.get(i);s||(s=a.querySelector(gi(i)),s||(e=I({src:e,async:!0},t),(t=wt.get(i))&&Jr(e,t),s=a.createElement("script"),Ie(s),Oe(s,"link",e),a.head.appendChild(s)),s={type:"script",instance:s,count:1,state:null},n.set(i,s))}}function Hf(e,t){Xt.M(e,t);var a=An;if(a&&e){var n=Ka(a).hoistableScripts,i=qn(e),s=n.get(i);s||(s=a.querySelector(gi(i)),s||(e=I({src:e,async:!0,type:"module"},t),(t=wt.get(i))&&Jr(e,t),s=a.createElement("script"),Ie(s),Oe(s,"link",e),a.head.appendChild(s)),s={type:"script",instance:s,count:1,state:null},n.set(i,s))}}function Vd(e,t,a,n){var i=(i=Z.current)?Rs(i):null;if(!i)throw Error(h(446));switch(e){case"meta":case"title":return null;case"style":return typeof a.precedence=="string"&&typeof a.href=="string"?(t=Sn(a.href),a=Ka(i).hoistableStyles,n=a.get(t),n||(n={type:"style",instance:null,count:0,state:null},a.set(t,n)),n):{type:"void",instance:null,count:0,state:null};case"link":if(a.rel==="stylesheet"&&typeof a.href=="string"&&typeof a.precedence=="string"){e=Sn(a.href);var s=Ka(i).hoistableStyles,o=s.get(e);if(o||(i=i.ownerDocument||i,o={type:"stylesheet",instance:null,count:0,state:{loading:0,preload:null}},s.set(e,o),(s=i.querySelector(mi(e)))&&!s._p&&(o.instance=s,o.state.loading=5),wt.has(e)||(a={rel:"preload",as:"style",href:a.href,crossOrigin:a.crossOrigin,integrity:a.integrity,media:a.media,hrefLang:a.hrefLang,referrerPolicy:a.referrerPolicy},wt.set(e,a),s||Wf(i,e,a,o.state))),t&&n===null)throw Error(h(528,""));return o}if(t&&n!==null)throw Error(h(529,""));return null;case"script":return t=a.async,a=a.src,typeof a=="string"&&t&&typeof t!="function"&&typeof t!="symbol"?(t=qn(a),a=Ka(i).hoistableScripts,n=a.get(t),n||(n={type:"script",instance:null,count:0,state:null},a.set(t,n)),n):{type:"void",instance:null,count:0,state:null};default:throw Error(h(444,e))}}function Sn(e){return'href="'+ht(e)+'"'}function mi(e){return'link[rel="stylesheet"]['+e+"]"}function Fd(e){return I({},e,{"data-precedence":e.precedence,precedence:null})}function Wf(e,t,a,n){e.querySelector('link[rel="preload"][as="style"]['+t+"]")?n.loading=1:(t=e.createElement("link"),n.preload=t,t.addEventListener("load",function(){return n.loading|=1}),t.addEventListener("error",function(){return n.loading|=2}),Oe(t,"link",a),Ie(t),e.head.appendChild(t))}function qn(e){return'[src="'+ht(e)+'"]'}function gi(e){return"script[async]"+e}function Jd(e,t,a){if(t.count++,t.instance===null)switch(t.type){case"style":var n=e.querySelector('style[data-href~="'+ht(a.href)+'"]');if(n)return t.instance=n,Ie(n),n;var i=I({},a,{"data-href":a.href,"data-precedence":a.precedence,href:null,precedence:null});return n=(e.ownerDocument||e).createElement("style"),Ie(n),Oe(n,"style",i),Cs(n,a.precedence,e),t.instance=n;case"stylesheet":i=Sn(a.href);var s=e.querySelector(mi(i));if(s)return t.state.loading|=4,t.instance=s,Ie(s),s;n=Fd(a),(i=wt.get(i))&&Fr(n,i),s=(e.ownerDocument||e).createElement("link"),Ie(s);var o=s;return o._p=new Promise(function(r,l){o.onload=r,o.onerror=l}),Oe(s,"link",n),t.state.loading|=4,Cs(s,a.precedence,e),t.instance=s;case"script":return s=qn(a.src),(i=e.querySelector(gi(s)))?(t.instance=i,Ie(i),i):(n=a,(i=wt.get(s))&&(n=I({},a),Jr(n,i)),e=e.ownerDocument||e,i=e.createElement("script"),Ie(i),Oe(i,"link",n),e.head.appendChild(i),t.instance=i);case"void":return null;default:throw Error(h(443,t.type))}else t.type==="stylesheet"&&(t.state.loading&4)===0&&(n=t.instance,t.state.loading|=4,Cs(n,a.precedence,e));return t.instance}function Cs(e,t,a){for(var n=a.querySelectorAll('link[rel="stylesheet"][data-precedence],style[data-precedence]'),i=n.length?n[n.length-1]:null,s=i,o=0;o<n.length;o++){var r=n[o];if(r.dataset.precedence===t)s=r;else if(s!==i)break}s?s.parentNode.insertBefore(e,s.nextSibling):(t=a.nodeType===9?a.head:a,t.insertBefore(e,t.firstChild))}function Fr(e,t){e.crossOrigin==null&&(e.crossOrigin=t.crossOrigin),e.referrerPolicy==null&&(e.referrerPolicy=t.referrerPolicy),e.title==null&&(e.title=t.title)}function Jr(e,t){e.crossOrigin==null&&(e.crossOrigin=t.crossOrigin),e.referrerPolicy==null&&(e.referrerPolicy=t.referrerPolicy),e.integrity==null&&(e.integrity=t.integrity)}var Us=null;function $d(e,t,a){if(Us===null){var n=new Map,i=Us=new Map;i.set(a,n)}else i=Us,n=i.get(a),n||(n=new Map,i.set(a,n));if(n.has(e))return n;for(n.set(e,null),a=a.getElementsByTagName(e),i=0;i<a.length;i++){var s=a[i];if(!(s[In]||s[Be]||e==="link"&&s.getAttribute("rel")==="stylesheet")&&s.namespaceURI!=="http://www.w3.org/2000/svg"){var o=s.getAttribute(t)||"";o=e+o;var r=n.get(o);r?r.push(s):n.set(o,[s])}}return n}function eh(e,t,a){e=e.ownerDocument||e,e.head.insertBefore(a,t==="title"?e.querySelector("head > title"):null)}function Mf(e,t,a){if(a===1||t.itemProp!=null)return!1;switch(e){case"meta":case"title":return!0;case"style":if(typeof t.precedence!="string"||typeof t.href!="string"||t.href==="")break;return!0;case"link":if(typeof t.rel!="string"||typeof t.href!="string"||t.href===""||t.onLoad||t.onError)break;switch(t.rel){case"stylesheet":return e=t.disabled,typeof t.precedence=="string"&&e==null;default:return!0}case"script":if(t.async&&typeof t.async!="function"&&typeof t.async!="symbol"&&!t.onLoad&&!t.onError&&t.src&&typeof t.src=="string")return!0}return!1}function th(e){return!(e.type==="stylesheet"&&(e.state.loading&3)===0)}function Of(e,t,a,n){if(a.type==="stylesheet"&&(typeof n.media!="string"||matchMedia(n.media).matches!==!1)&&(a.state.loading&4)===0){if(a.instance===null){var i=Sn(n.href),s=t.querySelector(mi(i));if(s){t=s._p,t!==null&&typeof t=="object"&&typeof t.then=="function"&&(e.count++,e=Ds.bind(e),t.then(e,e)),a.state.loading|=4,a.instance=s,Ie(s);return}s=t.ownerDocument||t,n=Fd(n),(i=wt.get(i))&&Fr(n,i),s=s.createElement("link"),Ie(s);var o=s;o._p=new Promise(function(r,l){o.onload=r,o.onerror=l}),Oe(s,"link",n),a.instance=s}e.stylesheets===null&&(e.stylesheets=new Map),e.stylesheets.set(a,t),(t=a.state.preload)&&(a.state.loading&3)===0&&(e.count++,a=Ds.bind(e),t.addEventListener("load",a),t.addEventListener("error",a))}}var $r=0;function _f(e,t){return e.stylesheets&&e.count===0&&zs(e,e.stylesheets),0<e.count||0<e.imgCount?function(a){var n=setTimeout(function(){if(e.stylesheets&&zs(e,e.stylesheets),e.unsuspend){var s=e.unsuspend;e.unsuspend=null,s()}},6e4+t);0<e.imgBytes&&$r===0&&($r=62500*Tf());var i=setTimeout(function(){if(e.waitingForImages=!1,e.count===0&&(e.stylesheets&&zs(e,e.stylesheets),e.unsuspend)){var s=e.unsuspend;e.unsuspend=null,s()}},(e.imgBytes>$r?50:800)+t);return e.unsuspend=a,function(){e.unsuspend=null,clearTimeout(n),clearTimeout(i)}}:null}function Ds(){if(this.count--,this.count===0&&(this.imgCount===0||!this.waitingForImages)){if(this.stylesheets)zs(this,this.stylesheets);else if(this.unsuspend){var e=this.unsuspend;this.unsuspend=null,e()}}}var Is=null;function zs(e,t){e.stylesheets=null,e.unsuspend!==null&&(e.count++,Is=new Map,t.forEach(jf,e),Is=null,Ds.call(e))}function jf(e,t){if(!(t.state.loading&4)){var a=Is.get(e);if(a)var n=a.get(null);else{a=new Map,Is.set(e,a);for(var i=e.querySelectorAll("link[data-precedence],style[data-precedence]"),s=0;s<i.length;s++){var o=i[s];(o.nodeName==="LINK"||o.getAttribute("media")!=="not all")&&(a.set(o.dataset.precedence,o),n=o)}n&&a.set(null,n)}i=t.instance,o=i.getAttribute("data-precedence"),s=a.get(o)||n,s===n&&a.set(null,i),a.set(o,i),this.count++,n=Ds.bind(this),i.addEventListener("load",n),i.addEventListener("error",n),s?s.parentNode.insertBefore(i,s.nextSibling):(e=e.nodeType===9?e.head:e,e.insertBefore(i,e.firstChild)),t.state.loading|=4}}var yi={$$typeof:B,Provider:null,Consumer:null,_currentValue:W,_currentValue2:W,_threadCount:0};function Qf(e,t,a,n,i,s,o,r,l){this.tag=1,this.containerInfo=e,this.pingCache=this.current=this.pendingChildren=null,this.timeoutHandle=-1,this.callbackNode=this.next=this.pendingContext=this.context=this.cancelPendingCommit=null,this.callbackPriority=0,this.expirationTimes=Ys(-1),this.entangledLanes=this.shellSuspendCounter=this.errorRecoveryDisabledLanes=this.expiredLanes=this.warmLanes=this.pingedLanes=this.suspendedLanes=this.pendingLanes=0,this.entanglements=Ys(0),this.hiddenUpdates=Ys(null),this.identifierPrefix=n,this.onUncaughtError=i,this.onCaughtError=s,this.onRecoverableError=o,this.pooledCache=null,this.pooledCacheLanes=0,this.formState=l,this.incompleteTransitions=new Map}function ah(e,t,a,n,i,s,o,r,l,f,b,T){return e=new Qf(e,t,a,o,l,f,b,T,r),t=1,s===!0&&(t|=24),s=st(3,null,null,t),e.current=s,s.stateNode=e,t=Io(),t.refCount++,e.pooledCache=t,t.refCount++,s.memoizedState={element:n,isDehydrated:a,cache:t},No(s),e}function nh(e){return e?(e=an,e):an}function ih(e,t,a,n,i,s){i=nh(i),n.context===null?n.context=i:n.pendingContext=i,n=ia(t),n.payload={element:a},s=s===void 0?null:s,s!==null&&(n.callback=s),a=sa(e,n,t),a!==null&&(et(a,e,t),Xn(a,e,t))}function sh(e,t){if(e=e.memoizedState,e!==null&&e.dehydrated!==null){var a=e.retryLane;e.retryLane=a!==0&&a<t?a:t}}function el(e,t){sh(e,t),(e=e.alternate)&&sh(e,t)}function oh(e){if(e.tag===13||e.tag===31){var t=Ca(e,67108864);t!==null&&et(t,e,67108864),el(e,67108864)}}function rh(e){if(e.tag===13||e.tag===31){var t=ut();t=Zs(t);var a=Ca(e,t);a!==null&&et(a,e,t),el(e,t)}}var Ps=!0;function Gf(e,t,a,n){var i=w.T;w.T=null;var s=S.p;try{S.p=2,tl(e,t,a,n)}finally{S.p=s,w.T=i}}function Kf(e,t,a,n){var i=w.T;w.T=null;var s=S.p;try{S.p=8,tl(e,t,a,n)}finally{S.p=s,w.T=i}}function tl(e,t,a,n){if(Ps){var i=al(n);if(i===null)Or(e,t,n,Es,a),ch(e,n);else if(Zf(i,e,t,a,n))n.stopPropagation();else if(ch(e,n),t&4&&-1<Yf.indexOf(e)){for(;i!==null;){var s=Ga(i);if(s!==null)switch(s.tag){case 3:if(s=s.stateNode,s.current.memoizedState.isDehydrated){var o=Aa(s.pendingLanes);if(o!==0){var r=s;for(r.pendingLanes|=2,r.entangledLanes|=2;o;){var l=1<<31-nt(o);r.entanglements[1]|=l,o&=~l}Ut(s),(ae&6)===0&&(ys=tt()+500,di(0))}}break;case 31:case 13:r=Ca(s,2),r!==null&&et(r,s,2),vs(),el(s,2)}if(s=al(n),s===null&&Or(e,t,n,Es,a),s===i)break;i=s}i!==null&&n.stopPropagation()}else Or(e,t,n,null,a)}}function al(e){return e=no(e),nl(e)}var Es=null;function nl(e){if(Es=null,e=Qa(e),e!==null){var t=E(e);if(t===null)e=null;else{var a=t.tag;if(a===13){if(e=P(t),e!==null)return e;e=null}else if(a===31){if(e=K(t),e!==null)return e;e=null}else if(a===3){if(t.stateNode.current.memoizedState.isDehydrated)return t.tag===3?t.stateNode.containerInfo:null;e=null}else t!==e&&(e=null)}}return Es=e,null}function lh(e){switch(e){case"beforetoggle":case"cancel":case"click":case"close":case"contextmenu":case"copy":case"cut":case"auxclick":case"dblclick":case"dragend":case"dragstart":case"drop":case"focusin":case"focusout":case"input":case"invalid":case"keydown":case"keypress":case"keyup":case"mousedown":case"mouseup":case"paste":case"pause":case"play":case"pointercancel":case"pointerdown":case"pointerup":case"ratechange":case"reset":case"resize":case"seeked":case"submit":case"toggle":case"touchcancel":case"touchend":case"touchstart":case"volumechange":case"change":case"selectionchange":case"textInput":case"compositionstart":case"compositionend":case"compositionupdate":case"beforeblur":case"afterblur":case"beforeinput":case"blur":case"fullscreenchange":case"focus":case"hashchange":case"popstate":case"select":case"selectstart":return 2;case"drag":case"dragenter":case"dragexit":case"dragleave":case"dragover":case"mousemove":case"mouseout":case"mouseover":case"pointermove":case"pointerout":case"pointerover":case"scroll":case"touchmove":case"wheel":case"mouseenter":case"mouseleave":case"pointerenter":case"pointerleave":return 8;case"message":switch(Dh()){case gl:return 2;case yl:return 8;case Ai:case Ih:return 32;case bl:return 268435456;default:return 32}default:return 32}}var il=!1,ga=null,ya=null,ba=null,bi=new Map,vi=new Map,va=[],Yf="mousedown mouseup touchcancel touchend touchstart auxclick dblclick pointercancel pointerdown pointerup dragend dragstart drop compositionend compositionstart keydown keypress keyup input textInput copy cut paste click change contextmenu reset".split(" ");function ch(e,t){switch(e){case"focusin":case"focusout":ga=null;break;case"dragenter":case"dragleave":ya=null;break;case"mouseover":case"mouseout":ba=null;break;case"pointerover":case"pointerout":bi.delete(t.pointerId);break;case"gotpointercapture":case"lostpointercapture":vi.delete(t.pointerId)}}function wi(e,t,a,n,i,s){return e===null||e.nativeEvent!==s?(e={blockedOn:t,domEventName:a,eventSystemFlags:n,nativeEvent:s,targetContainers:[i]},t!==null&&(t=Ga(t),t!==null&&oh(t)),e):(e.eventSystemFlags|=n,t=e.targetContainers,i!==null&&t.indexOf(i)===-1&&t.push(i),e)}function Zf(e,t,a,n,i){switch(t){case"focusin":return ga=wi(ga,e,t,a,n,i),!0;case"dragenter":return ya=wi(ya,e,t,a,n,i),!0;case"mouseover":return ba=wi(ba,e,t,a,n,i),!0;case"pointerover":var s=i.pointerId;return bi.set(s,wi(bi.get(s)||null,e,t,a,n,i)),!0;case"gotpointercapture":return s=i.pointerId,vi.set(s,wi(vi.get(s)||null,e,t,a,n,i)),!0}return!1}function uh(e){var t=Qa(e.target);if(t!==null){var a=E(t);if(a!==null){if(t=a.tag,t===13){if(t=P(a),t!==null){e.blockedOn=t,Al(e.priority,function(){rh(a)});return}}else if(t===31){if(t=K(a),t!==null){e.blockedOn=t,Al(e.priority,function(){rh(a)});return}}else if(t===3&&a.stateNode.current.memoizedState.isDehydrated){e.blockedOn=a.tag===3?a.stateNode.containerInfo:null;return}}}e.blockedOn=null}function Ns(e){if(e.blockedOn!==null)return!1;for(var t=e.targetContainers;0<t.length;){var a=al(e.nativeEvent);if(a===null){a=e.nativeEvent;var n=new a.constructor(a.type,a);ao=n,a.target.dispatchEvent(n),ao=null}else return t=Ga(a),t!==null&&oh(t),e.blockedOn=a,!1;t.shift()}return!0}function dh(e,t,a){Ns(e)&&a.delete(t)}function Xf(){il=!1,ga!==null&&Ns(ga)&&(ga=null),ya!==null&&Ns(ya)&&(ya=null),ba!==null&&Ns(ba)&&(ba=null),bi.forEach(dh),vi.forEach(dh)}function Bs(e,t){e.blockedOn===t&&(e.blockedOn=null,il||(il=!0,v.unstable_scheduleCallback(v.unstable_NormalPriority,Xf)))}var Hs=null;function hh(e){Hs!==e&&(Hs=e,v.unstable_scheduleCallback(v.unstable_NormalPriority,function(){Hs===e&&(Hs=null);for(var t=0;t<e.length;t+=3){var a=e[t],n=e[t+1],i=e[t+2];if(typeof n!="function"){if(nl(n||a)===null)continue;break}var s=Ga(a);s!==null&&(e.splice(t,3),t-=3,ar(s,{pending:!0,data:i,method:a.method,action:n},n,i))}}))}function Ln(e){function t(l){return Bs(l,e)}ga!==null&&Bs(ga,e),ya!==null&&Bs(ya,e),ba!==null&&Bs(ba,e),bi.forEach(t),vi.forEach(t);for(var a=0;a<va.length;a++){var n=va[a];n.blockedOn===e&&(n.blockedOn=null)}for(;0<va.length&&(a=va[0],a.blockedOn===null);)uh(a),a.blockedOn===null&&va.shift();if(a=(e.ownerDocument||e).$$reactFormReplay,a!=null)for(n=0;n<a.length;n+=3){var i=a[n],s=a[n+1],o=i[Ze]||null;if(typeof s=="function")o||hh(a);else if(o){var r=null;if(s&&s.hasAttribute("formAction")){if(i=s,o=s[Ze]||null)r=o.formAction;else if(nl(i)!==null)continue}else r=o.action;typeof r=="function"?a[n+1]=r:(a.splice(n,3),n-=3),hh(a)}}}function ph(){function e(s){s.canIntercept&&s.info==="react-transition"&&s.intercept({handler:function(){return new Promise(function(o){return i=o})},focusReset:"manual",scroll:"manual"})}function t(){i!==null&&(i(),i=null),n||setTimeout(a,20)}function a(){if(!n&&!navigation.transition){var s=navigation.currentEntry;s&&s.url!=null&&navigation.navigate(s.url,{state:s.getState(),info:"react-transition",history:"replace"})}}if(typeof navigation=="object"){var n=!1,i=null;return navigation.addEventListener("navigate",e),navigation.addEventListener("navigatesuccess",t),navigation.addEventListener("navigateerror",t),setTimeout(a,100),function(){n=!0,navigation.removeEventListener("navigate",e),navigation.removeEventListener("navigatesuccess",t),navigation.removeEventListener("navigateerror",t),i!==null&&(i(),i=null)}}}function sl(e){this._internalRoot=e}Ws.prototype.render=sl.prototype.render=function(e){var t=this._internalRoot;if(t===null)throw Error(h(409));var a=t.current,n=ut();ih(a,n,e,t,null,null)},Ws.prototype.unmount=sl.prototype.unmount=function(){var e=this._internalRoot;if(e!==null){this._internalRoot=null;var t=e.containerInfo;ih(e.current,2,null,e,null,null),vs(),t[ja]=null}};function Ws(e){this._internalRoot=e}Ws.prototype.unstable_scheduleHydration=function(e){if(e){var t=kl();e={blockedOn:null,target:e,priority:t};for(var a=0;a<va.length&&t!==0&&t<va[a].priority;a++);va.splice(a,0,e),a===0&&uh(e)}};var fh=H.version;if(fh!=="19.2.4")throw Error(h(527,fh,"19.2.4"));S.findDOMNode=function(e){var t=e._reactInternals;if(t===void 0)throw typeof e.render=="function"?Error(h(188)):(e=Object.keys(e).join(","),Error(h(268,e)));return e=y(t),e=e!==null?_(e):null,e=e===null?null:e.stateNode,e};var Vf={bundleType:0,version:"19.2.4",rendererPackageName:"react-dom",currentDispatcherRef:w,reconcilerVersion:"19.2.4"};if(typeof __REACT_DEVTOOLS_GLOBAL_HOOK__<"u"){var Ms=__REACT_DEVTOOLS_GLOBAL_HOOK__;if(!Ms.isDisabled&&Ms.supportsFiber)try{Cn=Ms.inject(Vf),at=Ms}catch{}}return Ti.createRoot=function(e,t){if(!C(e))throw Error(h(299));var a=!1,n="",i=xu,s=Tu,o=ku;return t!=null&&(t.unstable_strictMode===!0&&(a=!0),t.identifierPrefix!==void 0&&(n=t.identifierPrefix),t.onUncaughtError!==void 0&&(i=t.onUncaughtError),t.onCaughtError!==void 0&&(s=t.onCaughtError),t.onRecoverableError!==void 0&&(o=t.onRecoverableError)),t=ah(e,1,!1,null,null,a,n,null,i,s,o,ph),e[ja]=t.current,Mr(e),new sl(t)},Ti.hydrateRoot=function(e,t,a){if(!C(e))throw Error(h(299));var n=!1,i="",s=xu,o=Tu,r=ku,l=null;return a!=null&&(a.unstable_strictMode===!0&&(n=!0),a.identifierPrefix!==void 0&&(i=a.identifierPrefix),a.onUncaughtError!==void 0&&(s=a.onUncaughtError),a.onCaughtError!==void 0&&(o=a.onCaughtError),a.onRecoverableError!==void 0&&(r=a.onRecoverableError),a.formState!==void 0&&(l=a.formState)),t=ah(e,1,!0,t,a??null,n,i,l,s,o,r,ph),t.context=nh(null),a=t.current,n=ut(),n=Zs(n),i=ia(n),i.callback=null,sa(a,i,n),a=n,t.current.lanes=a,Dn(t,a),Ut(t),e[ja]=t.current,Mr(e),new Ws(t)},Ti.version="19.2.4",Ti}var Ah;function rm(){if(Ah)return ll.exports;Ah=1;function v(){if(!(typeof __REACT_DEVTOOLS_GLOBAL_HOOK__>"u"||typeof __REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE!="function"))try{__REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE(v)}catch(H){console.error(H)}}return v(),ll.exports=om(),ll.exports}var lm=rm();const cm=Lh(lm),um=[{question:"What are the three guarantees described by the CAP theorem?",options:["Consistency, Availability, Partition Tolerance","Concurrency, Atomicity, Performance","Caching, Authentication, Persistence","Consensus, Agreement, Propagation"],correctIndex:0,explanation:"The CAP theorem, proposed by Eric Brewer in 2000, states that a distributed system can only guarantee two out of three properties: Consistency (every read receives the most recent write), Availability (every request receives a non-error response), and Partition Tolerance (the system continues to operate despite network partitions). The other options mix unrelated distributed systems concepts. This theorem is foundational because it forces architects to make explicit trade-offs when designing systems that span multiple nodes."},{question:"In the context of CAP theorem, what does 'Consistency' mean?",options:["All nodes see the same data at the same time (linearizability)","Data is never lost even if a node crashes","The system always returns a successful response","Transactions follow ACID properties"],correctIndex:0,explanation:"CAP Consistency means linearizability  every read returns the value of the most recent completed write, as if there's a single copy of the data. This is different from ACID consistency, which refers to database invariants being maintained. Durability (option B) is about persistence, availability (option C) is a separate CAP property, and ACID (option D) is a transaction model concept. In practice, achieving CAP consistency requires coordination between nodes, which adds latency  this is why many systems choose eventual consistency instead."},{question:"What does 'Partition Tolerance' mean in CAP theorem?",options:["The system can split data across multiple partitions","The system continues to function despite network communication failures between nodes","The system tolerates disk partitioning failures","The system can handle partitioned database tables"],correctIndex:1,explanation:"Partition tolerance means the system continues to operate even when network messages between nodes are lost or delayed  essentially, when the network 'partitions' into groups that can't communicate. This is about network failures, not data partitioning (sharding) or disk failures. In real distributed systems, network partitions are inevitable (cables get cut, switches fail, cloud availability zones lose connectivity), so partition tolerance is not really optional  you must handle it. This is why the real CAP choice is between CP and AP during a partition event."},{question:"Why is a 'CA' system (Consistency + Availability, no Partition Tolerance) considered impractical in distributed systems?",options:["Because network partitions are unavoidable in distributed systems","Because consistency and availability are mutually exclusive","Because CA systems require infinite bandwidth","Because no database supports both consistency and availability"],correctIndex:0,explanation:"Network partitions are a fact of life in distributed systems  switches fail, cables get cut, packets get dropped. Since you can't prevent partitions, you must tolerate them, making P mandatory. A CA system would only work on a single node or a network that never fails, which doesn't exist in practice. This is why the real CAP trade-off is between CP (sacrifice availability during partitions) and AP (sacrifice consistency during partitions). A traditional single-node PostgreSQL instance is technically CA, but the moment you distribute it, you must choose CP or AP."},{question:"Which of the following is a CP (Consistency + Partition Tolerance) system?",options:["Amazon DynamoDB (default config)","Apache Cassandra (default config)","Apache HBase","CouchDB"],correctIndex:2,explanation:"HBase is a CP system  during a network partition, it will refuse to serve requests from partitioned regions rather than risk returning stale data. It relies on ZooKeeper for coordination and will make a region unavailable if it can't confirm consistency. DynamoDB (default) and Cassandra (default) are AP systems  they prioritize availability and accept eventual consistency. CouchDB is also AP, designed for eventual consistency with multi-master replication. The key insight: CP systems choose correctness over responsiveness when the network misbehaves."},{question:"Which of the following is an AP (Availability + Partition Tolerance) system?",options:["Google Spanner","Apache ZooKeeper","Amazon DynamoDB (default configuration)","etcd"],correctIndex:2,explanation:"DynamoDB in its default configuration is an AP system  it uses eventual consistency to ensure that reads always return a response, even during network partitions, though the data might be slightly stale. Google Spanner is CP (it uses TrueTime and Paxos for strong consistency). ZooKeeper and etcd are both CP systems used for coordination  they use consensus protocols (ZAB and Raft respectively) and will become unavailable if they lose quorum. The trade-off DynamoDB makes is accepting occasional stale reads in exchange for always being responsive."},{question:"What happens in a CP system during a network partition?",options:["It returns stale data to maintain availability","It becomes unavailable rather than risk inconsistent data","It automatically resolves the partition","It switches to eventual consistency mode"],correctIndex:1,explanation:"A CP system prioritizes consistency over availability. During a network partition, nodes that can't confirm they have the latest data will refuse to serve requests  returning errors or timing out rather than potentially serving stale or inconsistent data. This is the behavior you see in systems like HBase, ZooKeeper, and etcd. For example, if a ZooKeeper ensemble loses quorum (majority of nodes can't communicate), the remaining nodes stop accepting writes. This is critical for use cases like distributed locks or configuration management where stale data could cause catastrophic errors."},{question:"What happens in an AP system during a network partition?",options:["It stops accepting writes until the partition heals","All nodes shut down to prevent data corruption","It continues serving requests but may return stale or divergent data","It automatically promotes a new leader"],correctIndex:2,explanation:"An AP system continues serving both reads and writes during a partition, accepting that different nodes might have divergent data. For example, Cassandra will accept writes on both sides of a partition, and when the partition heals, it uses techniques like last-write-wins or vector clocks to reconcile conflicts. This is ideal for systems where availability is more important than immediate consistency  like a shopping cart (Amazon's original Dynamo paper) where it's better to accept a potentially stale cart than to show the user an error page. The reconciliation after partition healing is where the complexity lives."},{question:"Who originally proposed the CAP theorem?",options:["Leslie Lamport","Eric Brewer","Martin Fowler","Jeff Dean"],correctIndex:1,explanation:"Eric Brewer proposed the CAP theorem as a conjecture at the ACM Symposium on Principles of Distributed Computing (PODC) in 2000. It was later formally proved by Seth Gilbert and Nancy Lynch in 2002. Leslie Lamport is famous for Paxos, Lamport clocks, and LaTeX. Martin Fowler is known for software architecture patterns and refactoring. Jeff Dean is the legendary Google engineer behind MapReduce, BigTable, and TensorFlow. Brewer's insight fundamentally changed how engineers reason about distributed system design trade-offs."},{question:"DynamoDB offers both eventually consistent and strongly consistent reads. When using strongly consistent reads, what CAP trade-off is being made?",options:["It becomes a CA system","It trades some availability for consistency (moves toward CP)","It gains partition tolerance it didn't have before","No trade-off  strong consistency is free in DynamoDB"],correctIndex:1,explanation:"When you request strongly consistent reads in DynamoDB, you're moving along the CAP spectrum toward CP. The read must go to the leader node of the partition, and if that leader is unreachable due to a network partition, the read will fail  sacrificing availability for consistency. With eventually consistent reads (default), DynamoDB can serve from any replica, giving you availability even during partitions but potentially stale data. This demonstrates that CAP isn't a binary choice but a spectrum  systems can offer different consistency levels for different operations. Strong consistent reads in DynamoDB also have higher latency and lower throughput."},{question:"What is the PACELC theorem?",options:["An alternative to CAP that ignores partition tolerance","An extension of CAP: during Partition choose A or C; Else (no partition) choose Latency or Consistency","A theorem about parallel computing efficiency","A consensus protocol for distributed databases"],correctIndex:1,explanation:"PACELC extends CAP by addressing what happens when there is NO partition (the normal case). It says: if there's a Partition, choose Availability or Consistency (PAC); Else, choose Latency or Consistency (ELC). This is important because CAP only describes behavior during partitions, but most of the time the network is fine. Even without partitions, there's still a trade-off: strong consistency requires coordination (adding latency), while relaxed consistency can be faster. For example, DynamoDB is PA/EL (available during partitions, low latency normally), while Spanner is PC/EC (consistent during partitions, consistent normally but with higher latency)."},{question:"In PACELC, what does a PA/EL system prioritize?",options:["Consistency during partitions, latency during normal operation","Availability during partitions, low latency during normal operation","Consistency always, regardless of partition status","Availability during partitions, consistency during normal operation"],correctIndex:1,explanation:"A PA/EL system chooses Availability during Partitions and Low Latency when there's no partition (Else). This describes systems like Cassandra and DynamoDB (default)  they always prioritize responsiveness, whether or not there's a partition. During partitions they serve potentially stale data (PA), and during normal operation they use eventual consistency to minimize latency (EL). Contrast this with a PC/EC system like Google Spanner, which always prioritizes consistency at the cost of higher latency. The PACELC model is more nuanced than CAP because it captures behavior during the 99.99% of time when there's no partition."},{question:"Which consistency model guarantees that once a write is confirmed, all subsequent reads will see that write?",options:["Eventual consistency","Strong consistency (linearizability)","Causal consistency","Read-your-writes consistency"],correctIndex:1,explanation:"Strong consistency (linearizability) guarantees that once a write completes, ALL subsequent reads from ANY client will see that write  as if there's a single copy of the data updated atomically. Eventual consistency only guarantees that reads will EVENTUALLY see the write, but there's no bound on when. Causal consistency preserves causally related operation order but allows concurrent operations to be seen in different orders. Read-your-writes only guarantees that the client that performed the write sees it  other clients might still see stale data. Linearizability is the strongest guarantee but the most expensive in terms of latency and availability."},{question:"A social media platform needs to display a user's post count. The count doesn't need to be perfectly accurate in real-time. Which CAP trade-off is most appropriate?",options:["CP  count must always be exactly correct","AP  eventual consistency is acceptable for display counts","CA  the count should never be unavailable or wrong","None  this doesn't involve CAP trade-offs"],correctIndex:1,explanation:"For a non-critical metric like a post count display, AP with eventual consistency is the right trade-off. Users won't notice if their post count is off by one for a few seconds, but they WILL notice if the page fails to load (unavailability). Facebook and Twitter use this approach  counters like likes, followers, and post counts are eventually consistent and may temporarily show different values to different users. This is a classic example of choosing the right consistency level based on business requirements rather than defaulting to the strongest guarantee. Overengineering consistency for non-critical data wastes resources and hurts availability."},{question:"A banking system processes fund transfers between accounts. Which CAP trade-off is most appropriate?",options:["AP  the system must always be available for transfers","CP  consistency is critical to prevent double-spending","CA  both consistency and availability are mandatory","AP with conflict resolution after partitions heal"],correctIndex:1,explanation:"For financial transactions like fund transfers, consistency is non-negotiable  you cannot risk double-spending or lost money due to stale reads. A CP approach ensures that during a network partition, the system will refuse the transfer rather than potentially process it incorrectly. This is why traditional banking systems use strong consistency and would rather show 'service temporarily unavailable' than process an inconsistent transaction. Banks like HSBC and JPMorgan use CP databases for core transaction processing. The cost of an inconsistency (regulatory fines, lost money) far outweighs the cost of brief unavailability."},{question:"What is a 'split-brain' scenario in distributed systems?",options:["When a database table is split across multiple shards","When network partition causes two groups of nodes to independently believe they are the active cluster","When a query execution plan is split into parallel sub-plans","When a leader node's CPU is overloaded and splits tasks"],correctIndex:1,explanation:"Split-brain occurs during a network partition when two (or more) groups of nodes can't communicate and each group independently believes it's the legitimate active cluster. Both sides may accept writes, leading to conflicting data. For example, in a two-node cluster, if the network between them fails, both nodes might promote themselves to leader and accept writes  creating divergent state that's extremely difficult to reconcile. This is why consensus protocols like Raft and Paxos require a majority quorum  only the side with the majority can continue operating, preventing split-brain. This is a critical failure mode that CP systems are designed to prevent."},{question:"How does a quorum-based system prevent split-brain during a network partition?",options:["By requiring all nodes to agree on every operation","By requiring a majority of nodes to agree before proceeding","By using timestamps to order conflicting operations","By shutting down all nodes when a partition is detected"],correctIndex:1,explanation:"Quorum-based systems require a majority (more than half) of nodes to agree before an operation can proceed. Since a majority can only exist on one side of any partition, this prevents split-brain  only one partition can form a quorum and accept operations. For example, in a 5-node cluster, you need 3 nodes to agree. If the network splits into groups of 3 and 2, only the group of 3 can form a quorum and continue operating. The group of 2 becomes unavailable. This is fundamentally why consensus protocols like Raft, Paxos, and ZAB use odd numbers of nodes (3, 5, 7)  it maximizes the chance that one side can achieve quorum."},{question:"Cassandra uses tunable consistency with settings like ONE, QUORUM, and ALL. If you set both read and write consistency to QUORUM, what behavior do you get?",options:["Eventual consistency only","Strong consistency for operations where R + W > N","The same as setting consistency to ALL","No consistency guarantees at all"],correctIndex:1,explanation:"When R (read replicas) + W (write replicas) > N (total replicas), you get strong consistency because there must be at least one node that participated in both the latest write and the current read. With QUORUM on both reads and writes in a replication factor of 3, R=2 and W=2, so R+W=4 > N=3  guaranteeing overlap. Setting both to ALL would give even stronger guarantees but at the cost of availability (any single node failure blocks the operation). Setting just one to ONE would break the overlap guarantee. This tunable consistency is what makes Cassandra flexible  you can choose per-query whether you need strong or eventual consistency."},{question:"What is 'eventual consistency'?",options:["Data is consistent after every write operation","If no new updates are made, all replicas will eventually converge to the same value","Data becomes consistent only at scheduled intervals","Consistency is guaranteed within a fixed time window"],correctIndex:1,explanation:"Eventual consistency means that if no new writes occur, all replicas will eventually converge to the same value  but there's no guarantee on how long this takes. It's not about scheduled intervals or fixed windows; it's a convergence guarantee without a time bound. In practice, convergence usually happens within milliseconds to seconds, but during partitions it could take longer. DNS is a classic example  when you update a DNS record, it propagates eventually but different resolvers may see the old value for varying durations (TTL-based). Amazon's Dynamo paper popularized this model, arguing that many applications can tolerate brief inconsistency in exchange for better availability and performance."},{question:"What consistency model does DNS use?",options:["Strong consistency","Eventual consistency with TTL-based propagation","Causal consistency","Sequential consistency"],correctIndex:1,explanation:"DNS uses eventual consistency with TTL (Time-To-Live) based propagation. When you update a DNS record, the change doesn't instantly propagate to all DNS resolvers worldwide. Each resolver caches the record for its TTL duration and only fetches the new value when the TTL expires. This means different clients might resolve the same domain to different IPs for a period. This is by design  DNS prioritizes availability and performance (billions of queries per day) over immediate consistency. The TTL mechanism gives a rough upper bound on propagation time, but in practice, some resolvers may not respect TTLs. This is why DNS migrations often involve lowering TTLs beforehand."},{question:"Google Spanner achieves external consistency (strongest form of consistency) in a distributed system. What technology makes this possible?",options:["Blockchain-based verification","TrueTime API with GPS and atomic clocks","Eventual consistency with automatic conflict resolution","Single-leader replication with synchronous followers"],correctIndex:1,explanation:"Google Spanner uses TrueTime, an API that provides globally consistent timestamps using GPS receivers and atomic clocks in every Google datacenter. TrueTime gives a bounded uncertainty interval for the current time, and Spanner waits out this uncertainty before committing transactions  ensuring that transaction timestamps reflect their real-time order. This allows Spanner to achieve external consistency (even stronger than linearizability) across globally distributed nodes. No other production system has replicated this approach because it requires Google's custom hardware infrastructure. This is why Spanner is often called the 'impossibility-defying' database  it achieves properties that the CAP theorem suggests shouldn't coexist."},{question:"What is a vector clock, and which CAP concern does it address?",options:["A physical timestamp mechanism that ensures strong consistency","A logical clock that tracks causality to help resolve conflicts in AP systems","A scheduling algorithm for distributed task execution","A technique for synchronizing wall clocks across data centers"],correctIndex:1,explanation:"A vector clock is a logical clock mechanism where each node maintains a vector of counters (one per node). When events happen, counters are incremented and exchanged, allowing the system to determine causal ordering  whether event A happened before event B, or if they're concurrent (and thus conflicting). This is crucial in AP systems where concurrent writes during partitions create conflicts that need resolution. Amazon's original Dynamo used vector clocks to detect conflicting versions and present them to the application for resolution (e.g., merging shopping carts). Vector clocks don't prevent inconsistency  they help detect and resolve it after the fact."},{question:"What conflict resolution strategy does 'last-write-wins' (LWW) use?",options:["It merges all conflicting writes into a single combined value","It keeps the write with the highest timestamp and discards others","It asks the user to manually resolve the conflict","It rejects all writes that conflict"],correctIndex:1,explanation:"Last-write-wins (LWW) resolves conflicts by simply keeping the write with the highest timestamp and discarding all others. This is used by Cassandra by default and is simple to implement, but it can silently lose data  if two clients write different values concurrently, one write is silently dropped. For example, if client A writes 'blue' and client B writes 'red' at nearly the same time, only the one with the later timestamp survives. This is acceptable for some use cases (like session data or caches) but dangerous for others (like shopping carts, where you'd lose items). The simplicity of LWW comes at the cost of potential data loss, which is why some systems like Riak offer alternative strategies like merge functions."},{question:"A system uses Raft consensus protocol. What CAP classification does it fall under?",options:["AP  Raft ensures all nodes can serve requests","CP  Raft requires a quorum and sacrifices availability during partitions","CA  Raft prevents partitions from occurring","None  consensus protocols are unrelated to CAP"],correctIndex:1,explanation:"Raft is a CP consensus protocol. It requires a majority quorum to elect a leader and commit log entries. During a network partition, the minority side cannot form a quorum and becomes unavailable  sacrificing availability for consistency. The majority side can still operate with full consistency guarantees. Systems built on Raft (like etcd, CockroachDB, TiKV) inherit this CP characteristic. Raft was designed by Diego Ongaro as an understandable alternative to Paxos, and it explicitly prioritizes safety (consistency) over liveness (availability). This makes it ideal for coordination services, configuration stores, and metadata management where correctness is paramount."},{question:"What is the difference between 'consistency' in CAP theorem and 'consistency' in ACID?",options:["They are exactly the same concept","CAP consistency is about linearizability across nodes; ACID consistency is about maintaining database invariants","ACID consistency is stronger than CAP consistency","CAP consistency only applies to NoSQL databases"],correctIndex:1,explanation:"These are fundamentally different concepts that unfortunately share a name. CAP consistency (linearizability) means all nodes see the same data at the same time  it's about replica agreement in a distributed system. ACID consistency means a transaction moves the database from one valid state to another, maintaining all defined rules (constraints, cascades, triggers). You could have a single-node database with perfect ACID consistency but no CAP consistency concerns (since there's only one node). Conversely, you could have a distributed system with CAP consistency but no ACID transactions. This naming collision causes endless confusion in system design discussions  always clarify which 'consistency' you mean."},{question:"ZooKeeper is classified as a CP system. In what scenario would ZooKeeper become unavailable?",options:["When any single node fails","When the leader node fails (before a new leader is elected)","When a minority of nodes are partitioned from the majority","Both B and C"],correctIndex:3,explanation:"ZooKeeper becomes unavailable in two scenarios: (1) When the leader fails, there's a brief unavailability window while a new leader is elected via the ZAB protocol (typically a few seconds). (2) When nodes are partitioned such that some are in the minority  those minority nodes cannot serve consistent reads or accept writes because they can't confirm they have the latest state. The majority partition will elect a new leader and continue operating. A single node failure in a 3+ node ensemble is tolerable as long as quorum is maintained. This is the CP trade-off in action  ZooKeeper would rather be unavailable than serve potentially inconsistent data, which is exactly what you want from a coordination service."},{question:"What is 'read-your-writes' consistency?",options:["A guarantee that all clients see writes in the order they were issued","A guarantee that a client will always see its own previous writes","A guarantee that reads are always served from the node that processed the write","A guarantee that all writes are immediately visible to all readers"],correctIndex:1,explanation:"Read-your-writes consistency guarantees that if a client writes a value, subsequent reads BY THAT SAME CLIENT will see the write (or a more recent value). Other clients might still see stale data. This is weaker than strong consistency but stronger than eventual consistency, and it's crucial for user experience  imagine posting a tweet and then not seeing it on your own timeline. Many systems implement this by routing a user's reads to the same replica that handled their writes (sticky sessions), or by tracking the latest write timestamp and ensuring reads go to replicas that are at least that up-to-date. Facebook uses a version of this for their social graph."},{question:"What is 'monotonic reads' consistency?",options:["Reads always return the latest value","Once a client reads a value, subsequent reads will never return an older value","All reads are served in the order they were issued","Reads always return values in alphabetical order"],correctIndex:1,explanation:"Monotonic reads guarantees that if a client reads a value at time T, any subsequent read by that client will return that value or a newer one  never an older value. Without this guarantee, a user might see a post, refresh the page, and the post disappears (because they hit a less-up-to-date replica), then see it again on the next refresh. This 'time travel' effect is confusing and breaks user expectations. Monotonic reads is typically implemented by ensuring a client always reads from the same replica (or one that's at least as up-to-date). It's a session-level guarantee that's weaker than strong consistency but prevents the most jarring user experience issues."},{question:"A system uses eventual consistency with a conflict resolution policy. During a network partition, two clients update the same record simultaneously on different sides of the partition. What is this called?",options:["A race condition","A write-write conflict (conflicting writes / siblings)","A deadlock","A dirty read"],correctIndex:1,explanation:"When two clients update the same data on different sides of a network partition, it creates a write-write conflict (also called 'siblings' in Riak terminology). Both writes succeed locally because the nodes can't coordinate across the partition. When the partition heals, the system must reconcile these conflicting versions. This is different from a race condition (which is a timing-dependent bug), a deadlock (which is about resource locking), or a dirty read (which is about reading uncommitted data). AP systems must have a strategy for handling these conflicts  LWW, vector clocks, CRDTs, or application-level resolution. The choice of conflict resolution strategy is one of the most important design decisions in AP systems."},{question:"What are CRDTs, and how do they relate to CAP theorem?",options:["Conflict-free Replicated Data Types  data structures that can be merged without conflicts, enabling AP systems","Consistent Replication Data Transfers  a protocol for CP systems","Concurrent Read Data Transactions  a transaction isolation mechanism","Checkpoint Recovery Data Techniques  a durability mechanism"],correctIndex:0,explanation:"CRDTs (Conflict-free Replicated Data Types) are data structures mathematically designed to always converge when replicas are merged, without requiring coordination. They enable AP systems to handle concurrent updates without conflicts  the merge operation is commutative, associative, and idempotent. Examples include G-Counters (grow-only counters), OR-Sets (observed-remove sets), and LWW-Registers. Redis Enterprise uses CRDTs for active-active geo-replication, and collaborative editors like Figma use CRDT-like structures. They're powerful because they give you the availability benefits of AP systems while eliminating the need for explicit conflict resolution  the math guarantees convergence."},{question:"What is 'tunable consistency' as implemented in Apache Cassandra?",options:["The ability to change the consistency model of the cluster at runtime","The ability to specify per-query how many replicas must respond for reads and writes","A feature that automatically adjusts consistency based on network conditions","The ability to set different consistency levels for different tables"],correctIndex:1,explanation:"Cassandra's tunable consistency allows you to specify per-query how many replicas must acknowledge a read or write. With consistency level ONE, only one replica needs to respond (fast but weakly consistent). With QUORUM, a majority must respond. With ALL, every replica must respond (slowest but strongest). The key insight is that when Read replicas + Write replicas > Total replicas (R + W > N), you get strong consistency because at least one node overlaps. This lets you make different trade-offs for different queries  strong consistency for critical operations like payments, eventual consistency for non-critical ones like analytics. No other database popularized this concept as effectively as Cassandra."},{question:"What is the significance of the formula R + W > N in quorum-based systems?",options:["It calculates the minimum number of nodes needed","It ensures at least one node has the latest write when reading, providing strong consistency","It determines the maximum throughput of the system","It calculates the replication factor needed for fault tolerance"],correctIndex:1,explanation:"When R (read quorum) + W (write quorum) > N (replication factor), there's guaranteed overlap  at least one node that handled the write will also be queried during the read. This node has the latest value, so the system can return consistent data. For example, with N=3, R=2, W=2: any 2 nodes you read from must include at least 1 of the 2 nodes that acknowledged the write (since 2+2=4 > 3). If R+W  N, there's no guaranteed overlap, and reads might miss the latest write. This formula is the mathematical foundation of quorum-based consistency in systems like Cassandra, DynamoDB, and Riak. It lets you trade off consistency for latency by adjusting R and W."},{question:"In a Cassandra cluster with replication factor 3, what is the minimum consistency level for BOTH reads and writes to guarantee strong consistency?",options:["ONE for both","TWO for reads, ONE for writes","QUORUM for both","ALL for both"],correctIndex:2,explanation:"With replication factor N=3, QUORUM means 2 nodes (3/2 + 1 = 2). If both reads and writes use QUORUM, then R + W = 2 + 2 = 4 > 3 = N, satisfying the strong consistency condition. ALL for both would also work but is unnecessarily restrictive  a single node failure would block all operations. ONE for both gives R + W = 2, which is less than N=3, so there's no overlap guarantee. TWO reads + ONE write gives R + W = 3 = N, which is borderline  it works only if there are no concurrent writes. QUORUM for both is the standard strong consistency configuration in Cassandra that also tolerates one node failure."},{question:"What is 'hinted handoff' in distributed databases like Cassandra and DynamoDB?",options:["A technique where a node stores writes intended for an unavailable node and forwards them when it recovers","A method for transferring leadership during leader election","A way to redirect client requests to the nearest data center","A protocol for negotiating consistency levels between client and server"],correctIndex:0,explanation:"Hinted handoff is a technique used in AP systems where, if a target replica is down, another node temporarily stores the write as a 'hint.' When the unavailable node comes back online, the hint is replayed, bringing it up to date. This improves availability (writes don't fail just because one replica is down) and helps achieve eventual consistency faster after node recovery. However, hints are stored temporarily and have limits  if a node is down for too long, hints may expire, requiring anti-entropy repair mechanisms (like Merkle trees in Cassandra) to fully synchronize. Hinted handoff is a pragmatic trade-off: it sacrifices some consistency guarantees for better availability and faster convergence."},{question:"What is 'anti-entropy' in the context of distributed systems?",options:["A mechanism to prevent data corruption from hardware failures","A background process that compares and synchronizes data between replicas to ensure they converge","A technique to prevent entropy-based encryption attacks","A method for reducing the randomness in load balancer routing"],correctIndex:1,explanation:"Anti-entropy is a background process where nodes periodically compare their data and synchronize any differences. In Cassandra, this is called 'repair' and uses Merkle trees  hash trees that efficiently identify which data ranges differ between replicas. Dynamo-style systems use anti-entropy to handle cases where hinted handoff isn't sufficient (e.g., a node was down too long and hints expired). The term comes from information theory  entropy represents disorder, so anti-entropy reduces disorder (data inconsistency) between replicas. It's like periodically reconciling two copies of a spreadsheet to ensure they match. This is a key mechanism for achieving eventual consistency in AP systems."},{question:"What is a 'sloppy quorum' as described in the Amazon Dynamo paper?",options:["A quorum that includes nodes outside the preference list when some preferred nodes are unavailable","A quorum that requires fewer nodes than a strict majority","A quorum that allows reads from any node without coordination","A quorum that uses approximate rather than exact counts"],correctIndex:0,explanation:"In a sloppy quorum, if the designated replica nodes for a key are unavailable, the system writes to other 'healthy' nodes that aren't normally responsible for that key. These temporary holders use hinted handoff to forward the data back when the proper nodes recover. This maximizes availability because writes can always succeed as long as any W nodes in the cluster are reachable  not necessarily the 'correct' W nodes. The trade-off is that this breaks the R + W > N strong consistency guarantee since the read and write sets may not overlap. Sloppy quorums are a key innovation in the Dynamo paper that enables always-writable behavior, making DynamoDB an AP system."},{question:"What is 'causal consistency' and where does it fall in the consistency spectrum?",options:["It's stronger than strong consistency  it guarantees global ordering of all operations","It's between strong and eventual consistency  it preserves the order of causally related operations","It's the weakest form of consistency  weaker than eventual","It's identical to sequential consistency"],correctIndex:1,explanation:"Causal consistency guarantees that operations that are causally related (one depends on or is influenced by another) are seen in the correct order by all nodes. Operations that are NOT causally related (concurrent) can be seen in any order. This is stronger than eventual consistency (which provides no ordering guarantees) but weaker than strong consistency (which orders ALL operations). For example, if Alice posts a message and Bob replies, causal consistency ensures everyone sees Alice's message before Bob's reply. But two unrelated posts by different users might appear in different orders on different nodes. MongoDB offers causal consistency through causal sessions, and it's often a sweet spot  providing meaningful guarantees without the performance cost of linearizability."},{question:"Which of the following is NOT a valid way to handle conflicts in an AP system after a partition heals?",options:["Last-write-wins using timestamps","Application-level conflict resolution","Using CRDTs for automatic merge","Rolling back all writes from both sides of the partition"],correctIndex:3,explanation:"Rolling back ALL writes from both partition sides would mean losing all data written during the partition  this defeats the purpose of an AP system, which accepts writes during partitions to maintain availability. The whole point of AP is that writes succeed during partitions and are reconciled afterwards. LWW (option A) picks one write and discards others  simple but lossy. Application-level resolution (option B) lets the app decide how to merge (e.g., Amazon's shopping cart merges items from both sides). CRDTs (option C) mathematically guarantee conflict-free merging. The challenge of AP systems is designing good conflict resolution  it's the price you pay for availability during partitions."},{question:"Netflix uses Cassandra as its primary data store. What CAP trade-off does this reflect about Netflix's priorities?",options:["Netflix prioritizes consistency  users must always see the exact same catalog","Netflix prioritizes availability  it's better to show a slightly stale catalog than show an error","Netflix doesn't need partition tolerance because it runs on a single server","Netflix uses Cassandra in CP mode for all operations"],correctIndex:1,explanation:"Netflix chose Cassandra (an AP system) because for their use case, availability is paramount. If a user opens Netflix, it's far better to show a slightly outdated 'continue watching' list than to show an error page. A few seconds of stale data in a recommendation or viewing history is invisible to the user, but an error page means a lost customer. Netflix operates across multiple AWS regions, where network partitions between regions are not uncommon. Cassandra's AP design with tunable consistency lets Netflix serve requests from any region regardless of partition status. This is a textbook example of choosing your CAP trade-off based on business impact rather than technical purity."},{question:"What is the CAP classification of Redis Sentinel?",options:["AP  Redis always serves reads even during failover","CP  Redis Sentinel ensures consistency through leader election","Neither  Redis Sentinel doesn't fit neatly into CAP","CA  Redis prevents partitions through fast networking"],correctIndex:2,explanation:"Redis Sentinel is actually difficult to classify cleanly in CAP terms. During normal operation, it's single-leader (CP-ish), but during failover it can lose acknowledged writes (not fully consistent). It uses asynchronous replication by default, so writes acknowledged by the leader may be lost if the leader fails before replicating. During a partition, the old leader might still accept writes (split-brain), though Sentinel tries to mitigate this with 'min-replicas-to-write' configuration. This illustrates an important point: many real-world systems don't fit neatly into CAP categories. CAP is a simplified model  real systems exist on a spectrum and may behave differently depending on configuration and failure modes."},{question:"What does 'linearizability' mean, and how is it different from 'serializability'?",options:["They are the same thing  both ensure operations appear to happen in order","Linearizability is about single-object real-time ordering; serializability is about multi-object transaction ordering","Linearizability applies only to reads; serializability applies only to writes","Linearizability is weaker than serializability"],correctIndex:1,explanation:"Linearizability and serializability are both correctness conditions but for different contexts. Linearizability ensures that operations on a SINGLE object appear to take effect instantaneously at some point between their invocation and response, respecting real-time ordering. Serializability ensures that the execution of MULTIPLE concurrent transactions is equivalent to some serial (sequential) execution of those transactions, but doesn't require the serial order to respect real-time ordering. Strict serializability (or 'linearizable + serializable') combines both  Google Spanner offers this. The confusion between these terms is one of the most common misunderstandings in distributed systems. Linearizability is a recency guarantee; serializability is a transaction isolation guarantee."},{question:"In a 5-node Raft cluster, what is the maximum number of node failures the system can tolerate while remaining available?",options:["1 node","2 nodes","3 nodes","4 nodes"],correctIndex:1,explanation:"A Raft cluster requires a majority quorum to operate. In a 5-node cluster, the quorum size is 3 (5/2 + 1). This means the system can tolerate 2 node failures and still have 3 nodes available to form a quorum. With 3 failures, only 2 nodes remain, which cannot form a majority of 5  the cluster becomes unavailable. This is the fundamental CP trade-off: more nodes improve fault tolerance but increase the coordination overhead. A 5-node cluster is the sweet spot for many production systems  it tolerates 2 failures (including during rolling upgrades) while keeping quorum size manageable. etcd, used by Kubernetes, typically runs as a 3 or 5-node cluster for this reason."},{question:"What is 'session consistency'?",options:["A guarantee that all sessions across all clients see the same data","A guarantee that within a single client session, the client sees a consistent view of the data","A guarantee that sessions are never lost during server failures","A guarantee that all database sessions use the same consistency level"],correctIndex:1,explanation:"Session consistency provides guarantees within the scope of a single client session. It typically combines read-your-writes, monotonic reads, and monotonic writes  ensuring that within a session, a client sees a progressively more up-to-date view of the data, never going backwards. Different sessions from different clients may see different data states. This is implemented by tracking the session's read/write timestamps and ensuring subsequent operations go to replicas that are at least as up-to-date. Azure Cosmos DB explicitly offers session consistency as one of its five consistency levels. It's a practical middle ground  strong enough for good UX, weak enough for reasonable performance in geo-distributed systems."},{question:"What is the 'FLP impossibility result' and how does it relate to CAP?",options:["It proves that distributed consensus is impossible in a purely asynchronous system with even one faulty node","It proves that CAP is wrong and all three properties can be achieved","It proves that network partitions can always be prevented","It proves that eventual consistency always converges within a bounded time"],correctIndex:0,explanation:"The FLP (Fischer, Lynch, Paterson) impossibility result from 1985 proves that in a purely asynchronous distributed system (no timeouts, no clock bounds), deterministic consensus is impossible if even one process can crash. This complements CAP  while CAP says you can't have all three properties during partitions, FLP says you can't even solve consensus reliably in asynchronous networks. In practice, systems work around FLP by using partial synchrony assumptions (timeouts), randomization, or failure detectors. Raft and Paxos, for example, assume partial synchrony  they're guaranteed safe always, but only make progress when the network is behaving. FLP and CAP together form the theoretical bedrock explaining why distributed systems are fundamentally hard."},{question:"You're designing a global e-commerce inventory system. The business requires that items are never oversold. Which approach is most appropriate?",options:["AP with eventual consistency  reconcile oversells later with customer apologies","CP with strong consistency on inventory decrements, accepting brief unavailability during partitions","AP with optimistic concurrency and CRDTs for inventory counts","Eventual consistency with last-write-wins for all inventory operations"],correctIndex:1,explanation:"When business rules demand 'never oversold,' you need CP with strong consistency for inventory decrements. An oversell means shipping product you don't have or canceling orders  both are costly in money and reputation. CP ensures that during a network partition, the system refuses to process sales rather than risk selling the same item twice. This is exactly the approach that high-value e-commerce uses for limited inventory items. Options A and D risk overselling during partitions. Option C with CRDTs doesn't work for inventory because a counter CRDT can't enforce 'must be  0'  that's a constraint requiring coordination. For high-volume, non-limited items, you might accept some overselling, but the question specifies 'never oversold.'"},{question:"Cosmos DB offers five consistency levels. From strongest to weakest, what is the correct order?",options:["Strong, Session, Bounded Staleness, Consistent Prefix, Eventual","Strong, Bounded Staleness, Session, Consistent Prefix, Eventual","Eventual, Consistent Prefix, Session, Bounded Staleness, Strong","Strong, Bounded Staleness, Consistent Prefix, Session, Eventual"],correctIndex:1,explanation:"Azure Cosmos DB offers five consistency levels from strongest to weakest: Strong  Bounded Staleness  Session  Consistent Prefix  Eventual. Strong provides linearizability. Bounded Staleness guarantees reads lag behind writes by at most K versions or T seconds. Session guarantees consistency within a client session (read-your-writes, monotonic reads). Consistent Prefix guarantees that reads never see out-of-order writes. Eventual provides no ordering guarantees but maximum performance. This five-level spectrum is Cosmos DB's answer to the false binary of 'strong vs eventual'  real applications need different consistency levels for different operations. Most Cosmos DB users use Session consistency as their default, which provides a good balance of correctness and performance."},{question:"What is 'bounded staleness' consistency?",options:["Reads return data that is at most K versions or T time units behind the latest write","Data is never stale  it's always the latest version","Staleness is bounded by the network latency between data centers","Reads are guaranteed fresh within one second of the write"],correctIndex:0,explanation:"Bounded staleness provides a quantitative guarantee on how stale data can be  reads will see data that is at most K versions old or T time units behind the latest write. This is stronger than eventual consistency (which provides no staleness bound) but weaker than strong consistency (which requires zero staleness). It's useful when you can tolerate some lag but need to guarantee it's limited. For example, Cosmos DB's bounded staleness with T=5 seconds means a read is guaranteed to see data no more than 5 seconds old. This is valuable for regulatory or business requirements that need a concrete SLA on data freshness without paying the full cost of strong consistency."},{question:"In the context of distributed databases, what is a 'read repair'?",options:["Fixing corrupted data during a read by checking parity bits","When a coordinator detects stale data on some replicas during a read and triggers an update to those replicas","Repairing broken read connections to the database","A scheduled job that re-reads all data to verify integrity"],correctIndex:1,explanation:"Read repair is an opportunistic consistency mechanism in Dynamo-style databases. When a coordinator sends a read request to multiple replicas and detects that some replicas have stale data (older versions), it sends the latest version back to the stale replicas to bring them up to date. This happens 'for free' during the read path  no extra coordination needed. Cassandra uses read repair extensively. The coordinator compares digests (hashes) from replicas, and if they differ, it fetches full data from all replicas, determines the latest version, and pushes it to the out-of-date ones. It's an elegant mechanism that gradually improves consistency through normal read traffic, though it only repairs data that's actually being read  rarely accessed data may remain inconsistent until anti-entropy repair runs."},{question:"What is a 'Merkle tree' and how is it used for anti-entropy in distributed systems?",options:["A B-tree variant used for indexing in distributed databases","A hash tree where each leaf is a hash of a data block, and parent nodes are hashes of their children, used to efficiently detect data differences between replicas","A tree structure for routing queries in distributed hash tables","A binary tree used for sorting data during map-reduce operations"],correctIndex:1,explanation:"A Merkle tree is a binary tree of hashes where each leaf node is the hash of a data block, and each parent node is the hash of its children's hashes. To compare data between two replicas, you only need to compare root hashes. If they differ, you traverse down the tree to find exactly which data blocks differ  a process that requires O(log n) comparisons instead of comparing all data. Cassandra uses Merkle trees during 'nodetool repair' to efficiently identify and synchronize data differences between replicas. This was described in the original Dynamo paper and is crucial for making anti-entropy practical at scale  without Merkle trees, comparing terabytes of data between replicas would be prohibitively expensive."},{question:"What is the 'consensus number' concept, and why does it matter for distributed systems?",options:["The minimum number of nodes needed to reach consensus","The maximum number of concurrent processes for which an object can solve consensus in a wait-free manner","The number of rounds needed for Paxos to converge","The percentage of nodes that must agree in a quorum"],correctIndex:1,explanation:"The consensus number (from Herlihy's 1991 paper) defines the maximum number of concurrent processes for which a synchronization primitive can solve wait-free consensus. Read-write registers have consensus number 1, test-and-set has consensus number 2, and compare-and-swap (CAS) has consensus number infinity (can solve consensus for any number of processes). This matters because it establishes a hierarchy of synchronization primitives  you can't build stronger primitives from weaker ones. This is why CAS is the foundation of most lock-free data structures and why modern CPUs include CAS instructions. It's a fundamental impossibility result that guides the design of concurrent and distributed algorithms."},{question:"What is the Paxos consensus algorithm, and which CAP property does it sacrifice?",options:["A consensus protocol that sacrifices consistency for availability","A consensus protocol that sacrifices availability for consistency (CP)","A consensus protocol that achieves all three CAP properties","A consensus protocol that only works without network partitions (CA)"],correctIndex:1,explanation:"Paxos, invented by Leslie Lamport, is a CP consensus protocol. It ensures that a group of nodes can agree on a single value even if some nodes fail or messages are lost. During a network partition, Paxos requires a majority quorum to make progress  the minority side becomes unavailable. Paxos guarantees safety (consistency) always but only guarantees liveness (availability) when a majority of nodes can communicate. It's notoriously difficult to understand and implement correctly  Lamport's original paper used a Greek parliament metaphor that confused many readers. Google's Chubby lock service uses Paxos internally, and many modern systems use Raft instead because it's easier to understand while providing equivalent guarantees."},{question:"What is 'consistent hashing' and how does it relate to partition tolerance?",options:["A hashing algorithm that guarantees consistent hash values across different programming languages","A technique for distributing data across nodes where adding/removing nodes only redistributes a minimal amount of data","A method for ensuring hash-based partitions are always consistent","A consensus algorithm for hash-partitioned databases"],correctIndex:1,explanation:"Consistent hashing maps both data and nodes onto a conceptual ring (hash space). Each node is responsible for the data between it and its predecessor on the ring. When a node joins or leaves, only the data in its immediate neighborhood needs to be redistributed  not the entire dataset. This is critical for partition tolerance because it minimizes data movement during topology changes (node failures, scaling). DynamoDB, Cassandra, and many CDNs use consistent hashing. Without it, adding a node to a traditional hash-based system (hash(key) % N) would require redistributing nearly all data because N changes. Consistent hashing makes the system resilient to node changes, which is a form of handling partitions gracefully."},{question:"What is a 'gossip protocol' and which CAP-related property does it support?",options:["A protocol for achieving strong consistency through message passing","A protocol for leader election in CP systems","A protocol where nodes periodically exchange state with random peers, supporting eventual consistency (AP)","A protocol for encrypting inter-node communication"],correctIndex:2,explanation:"A gossip protocol (also called epidemic protocol) is where each node periodically selects a random peer and exchanges state information. Over time, information propagates to all nodes  like how rumors spread in a social network. This supports AP systems because it doesn't require coordination or quorum  nodes spread information opportunistically, and the system eventually converges. Cassandra uses gossip for cluster membership and failure detection. Gossip protocols are inherently eventually consistent  there's no guarantee on when all nodes will have the same information, but they will converge. They're highly scalable (O(log N) rounds to propagate to all N nodes) and robust to node failures, making them ideal for large-scale distributed systems."},{question:"What does the 'C' in PACELC's 'ELC' portion represent differently from the 'C' in 'PAC'?",options:["They represent the same thing  linearizable consistency","ELC's C represents the consistency-latency trade-off during normal operation, while PAC's C is about consistency during partitions","ELC's C means caching, PAC's C means consistency","They are unrelated properties that happen to share a letter"],correctIndex:1,explanation:"While both use 'C' for consistency, the trade-off context differs. In PAC (during a Partition), choosing C means the system becomes unavailable to maintain consistency  it's a binary choice of serving or not serving. In ELC (Else, no partition), choosing C means accepting higher latency to ensure consistency through coordination between nodes  it's a continuous trade-off between response time and consistency strength. For example, even without partitions, a system like Spanner adds latency for TrueTime uncertainty waits to ensure external consistency (PC/EC). DynamoDB normally returns fast but potentially stale data (PA/EL). The PACELC model captures that consistency costs are different during partitions (availability cost) versus normal operation (latency cost)."},{question:"MongoDB uses a single-leader replication model. During a leader failover, what CAP behavior does MongoDB exhibit?",options:["It continues serving reads and writes from secondaries (AP behavior)","It becomes briefly unavailable for writes until a new primary is elected (CP behavior)","It maintains full availability and consistency throughout the failover","It switches to eventual consistency for all operations"],correctIndex:1,explanation:"During a MongoDB primary (leader) failover, writes are unavailable for 10-30 seconds while the replica set elects a new primary via Raft-like consensus. Reads can continue from secondaries but may be stale. This makes MongoDB exhibit CP behavior during failovers  it sacrifices write availability to maintain consistency (only one primary can accept writes at a time, preventing split-brain). The election uses a majority vote among replica set members, ensuring only one primary exists. This is a common pattern in single-leader systems: the failover window is a brief availability sacrifice. MongoDB applications must handle 'not master' errors during this window, which is why drivers have built-in retry logic."},{question:"What is a 'witness replica' or 'arbiter' in a consensus system?",options:["A replica that stores full data and serves client requests","A lightweight node that participates in voting/quorum but doesn't store full data","A node that monitors system health but doesn't participate in consensus","A backup node that only activates when the leader fails"],correctIndex:1,explanation:"A witness (or arbiter in MongoDB terminology) is a node that participates in quorum voting for leader election and consensus but doesn't store a full copy of the data. Its purpose is to provide an odd number of voters to prevent tied elections, without the cost of storing and replicating all the data. For example, if you have two data-bearing nodes in different data centers, adding a lightweight witness in a third location gives you 3 voters for quorum. MongoDB uses arbiters in replica sets, and CockroachDB supports witness replicas. This is cost-effective: you get better fault tolerance for consensus without tripling your storage costs, though you still need at least two full replicas for data durability."},{question:"What is the difference between 'safety' and 'liveness' in distributed systems, and how does it relate to CAP?",options:["Safety means data is encrypted; liveness means the system is running","Safety means nothing bad happens (e.g., inconsistency); liveness means something good eventually happens (e.g., requests complete)","Safety is about hardware; liveness is about software","They are the same thing expressed differently"],correctIndex:1,explanation:"Safety properties guarantee that 'nothing bad happens'  for example, consistency means you never read incorrect data, and agreement means nodes never decide different values. Liveness properties guarantee that 'something good eventually happens'  for example, availability means requests eventually get responses, and termination means consensus eventually completes. In CAP terms, consistency is a safety property and availability is a liveness property. The FLP impossibility result shows you can't guarantee both in asynchronous systems. Raft and Paxos always maintain safety (consistency) but may sacrifice liveness (availability) during partitions. This safety/liveness framework is fundamental to reasoning about correctness in distributed systems."},{question:"What is a 'network partition' at the infrastructure level? Give a realistic example.",options:["A deliberate splitting of data into shards for scalability","A failure where some nodes can communicate with each other but not with other groups of nodes","A DNS failure that prevents name resolution","A firewall rule that blocks all external traffic"],correctIndex:1,explanation:"A network partition is when the network breaks into two or more groups that can communicate within their group but not across groups. Real examples: a switch failure between two racks in a data center, a severed fiber optic cable between two AWS availability zones, or a misconfigured firewall rule. The critical characteristic is that nodes on both sides are healthy and running  they just can't reach each other. This is different from a total network failure (where nothing works) because both sides continue operating independently, potentially accepting writes and diverging in state. GitHub experienced a famous 24-second network partition between their primary MySQL and replica in 2018 that caused widespread data inconsistency and a multi-hour outage."},{question:"CockroachDB claims to be both consistent and available. How does it achieve this, and what's the catch?",options:["It violates the CAP theorem through a novel algorithm","It's CP but optimizes for availability during normal operation, becoming unavailable only during actual partitions","It achieves CA by running in a single data center","It uses eventual consistency with very fast convergence that appears consistent"],correctIndex:1,explanation:"CockroachDB is a CP system that uses Raft consensus for strong consistency. The 'catch' is that CAP only applies during network partitions  which are rare. During normal operation (the vast majority of time), CockroachDB provides both consistency AND high availability through multi-replica Raft groups, automatic failover, and optimized consensus. During an actual partition, it will sacrifice availability (minority partition becomes unavailable) to maintain consistency. This highlights a key misconception about CAP: it's about behavior during partitions, not during normal operation. CockroachDB, Spanner, and YugabyteDB all take this approach  be CP during partitions but design for high availability during the 99.99% of time when the network is healthy."},{question:"What is 'quorum intersection' and why is it essential for consistency?",options:["The physical location where quorum servers are co-located","The guarantee that any two quorums share at least one common member, ensuring data overlap","The time when two quorum votes happen simultaneously","The intersection of read and write timeout settings"],correctIndex:1,explanation:"Quorum intersection means that any two quorums (sets of nodes that can independently make decisions) must share at least one node in common. This is what guarantees consistency in quorum-based systems  the shared node(s) have participated in both the write quorum and the read quorum, so they can provide the latest data. With majority quorums in a set of N nodes, any two majorities must overlap by at least one node (since two groups each larger than N/2 must share members). If quorums could be disjoint, the system could have two independent writes without any node knowing about both, breaking consistency. This is the mathematical foundation of why R + W > N works."},{question:"What is the 'Two Generals' Problem and how does it relate to distributed consensus?",options:["A problem about optimizing military strategy with two leaders","A thought experiment proving that reaching agreement over an unreliable communication channel is impossible","A technique for dual-leader replication in databases","An algorithm for achieving consensus with exactly two nodes"],correctIndex:1,explanation:"The Two Generals' Problem illustrates that two parties communicating over an unreliable channel can never be certain they've reached agreement. General A sends 'attack at dawn' to General B, but the messenger might be captured. Even if B confirms, A can't be sure the confirmation arrived. This creates an infinite regress of confirmations. It was the first computer communication problem proved to be unsolvable (1975). It demonstrates why distributed consensus is fundamentally hard  messages can be lost, and you can never be 100% certain the other party received your message. Real systems work around this with timeouts, retries, and probabilistic guarantees (like TCP's three-way handshake), but the fundamental impossibility remains."},{question:"In a geo-distributed system spanning US-East and EU-West regions, a network partition occurs between the regions. Under CP semantics, what happens to requests in each region?",options:["Both regions continue serving all requests normally","Both regions become completely unavailable","The region with the majority of replicas continues; the other becomes unavailable for writes","Both regions switch to eventual consistency automatically"],correctIndex:2,explanation:"Under CP semantics, the region that can form a quorum (majority of replicas) continues operating normally. The region in the minority becomes unavailable for writes and potentially for consistent reads. For example, if you have 3 replicas (2 in US-East, 1 in EU-West) and the inter-region link goes down, US-East has 2 out of 3 nodes (quorum) and continues serving requests. EU-West has only 1 node and cannot form a quorum  it will refuse writes and return errors. This is why replica placement is a critical design decision in geo-distributed systems. Google Spanner, for instance, carefully places replicas to ensure quorum can be maintained even if one region is isolated."},{question:"What is 'lease-based consistency' and which systems use it?",options:["A consistency model where nodes 'lease' data from a central server for a fixed duration","A technique where a leader holds a time-bounded lease, ensuring only one leader exists and reads can be served locally during the lease","A subscription model for database access","A method for renting cloud database instances"],correctIndex:1,explanation:"Lease-based consistency uses time-bounded leases to ensure that at most one node acts as leader for a given data partition. The leader can serve consistent reads locally (without quorum) during its lease period, because no other node can become leader until the lease expires. If the leader fails, the system waits for the lease to expire before electing a new one  preventing split-brain. Google's Chubby lock service uses leases extensively, and Raft implementations often use leader leases to optimize read performance. The key trade-off is that after a leader failure, there's a mandatory wait period (lease duration) before a new leader can be elected, which is a bounded unavailability window. This is a practical optimization that reduces read latency in CP systems."},{question:"You're designing a collaborative document editor (like Google Docs). Which consistency model and conflict resolution approach would you use?",options:["Strong consistency with pessimistic locking  lock the document for each editor","Eventual consistency with OT (Operational Transformation) or CRDTs for real-time conflict-free merging","CP with single-writer at a time  queue edits and apply sequentially","Last-write-wins  whoever saves last, their version is kept"],correctIndex:1,explanation:"Collaborative editors like Google Docs use eventual consistency with conflict resolution algorithms  either OT (Operational Transformation, which Google Docs uses) or CRDTs (which Figma and some newer editors use). Multiple users edit simultaneously on their local copies, and the system automatically merges changes in a way that converges to the same document. Strong consistency with locks would make the experience terrible  only one person could type at a time. Single-writer would queue users. LWW would lose entire edits. OT transforms operations based on concurrent changes (if user A inserts at position 5 and user B deletes at position 3, A's position is adjusted to 4). CRDTs offer a mathematically cleaner approach where merge is always safe. Both provide AP-style availability with automatic conflict resolution."},{question:"What is the role of a 'coordinator node' in Dynamo-style distributed databases?",options:["A permanent leader that handles all reads and writes for the cluster","A node that receives a client request and coordinates the operation across the relevant replicas","A node dedicated to monitoring cluster health","A node that handles schema changes and DDL operations"],correctIndex:1,explanation:"In Dynamo-style systems (Cassandra, DynamoDB, Riak), any node can act as a coordinator for a given request. When a client sends a request, the receiving node becomes the coordinator for that operation. It determines which replica nodes hold the data (using consistent hashing), forwards the request to those replicas, collects responses, and returns the result to the client based on the configured consistency level. This is different from single-leader systems where one node is permanently the leader. The coordinator role is per-request, enabling any node to serve any request  this is key to the AP nature of these systems. If one node is down, clients can connect to any other node. This leaderless architecture eliminates the single point of failure inherent in leader-based systems."},{question:"What is 'Jepsen testing' and what does it validate about distributed systems?",options:["A performance benchmarking tool for measuring throughput","A framework that tests distributed systems under network partitions and other faults to verify consistency and safety claims","A unit testing framework for distributed applications","A security testing tool for finding vulnerabilities in databases"],correctIndex:1,explanation:"Jepsen, created by Kyle Kingsbury (Aphyr), is a testing framework that subjects distributed databases to network partitions, clock skew, process crashes, and other faults while checking whether the system maintains its claimed consistency guarantees. It has found critical bugs in nearly every distributed database tested  including MongoDB, Cassandra, CockroachDB, and Elasticsearch. Jepsen uses a formal model to verify properties like linearizability and serializability against actual observed histories. Its findings have been so impactful that 'passing Jepsen testing' has become a credibility milestone for distributed database vendors. It directly validates CAP-related claims  does the system actually maintain consistency during partitions as it claims?"},{question:"In Amazon's original Dynamo paper, what was the 'shopping cart' problem that motivated the AP design?",options:["Shopping carts needed ACID transactions for payment processing","Shopping carts needed to always be available for adds/removes, even during partitions, with conflicts merged by union","Shopping carts needed strong consistency to prevent duplicate items","Shopping carts needed to support millions of concurrent users with CP guarantees"],correctIndex:1,explanation:"The Dynamo paper was motivated by Amazon's need for an always-available shopping cart. The insight was that it's better to accept an add-to-cart request during a partition (even if it creates a conflict) than to reject it  a rejected add means a lost sale. If conflicts arise (e.g., two versions of the cart after a partition), the resolution is to merge by union (keep all items from both versions). The worst case is a deleted item reappearing, which is far less costly than a customer being unable to add items. This was a groundbreaking business-driven architecture decision: the cost of unavailability (lost revenue) was quantified and found to be much higher than the cost of occasional inconsistency (a reappearing cart item). This thinking spawned an entire generation of AP databases."},{question:"What is 'external consistency' as defined by Google Spanner?",options:["Consistency between the database and external APIs","Consistency that respects real-world time ordering  if transaction T1 commits before T2 starts, T1's commit timestamp is earlier","Consistency enforced by an external validation service","The ability to query consistent data from outside the cluster"],correctIndex:1,explanation:"External consistency (or strict serializability) is the strongest consistency guarantee  it means that if transaction T1 commits before transaction T2 starts in real (wall-clock) time, then T1's timestamp will be less than T2's timestamp in the database. This is stronger than linearizability because it applies to transactions, not just single operations. Google Spanner achieves this using TrueTime  GPS and atomic clocks that give bounded clock uncertainty. Spanner waits out the uncertainty interval before committing, ensuring timestamps reflect real-time order. This means Spanner's globally distributed database behaves as if it were a single machine processing transactions sequentially in real-time order  a remarkable achievement that requires specialized hardware (atomic clocks in every datacenter)."},{question:"A system uses asynchronous replication from a primary to secondaries. During a primary failure, acknowledged but unreplicated writes are lost. What CAP implication does this have?",options:["The system is strongly consistent (CP)","The system is NOT strongly consistent  it may lose acknowledged writes, breaking consistency guarantees","This has no CAP implications  data loss is a durability concern, not consistency","The system achieves perfect availability (AP)"],correctIndex:1,explanation:"If the system acknowledges a write to the client but the write hasn't been replicated when the primary fails, the client believes the write succeeded but the data is gone  this violates the consistency guarantee (linearizability requires that once a write is acknowledged, all subsequent reads see it). This is a common issue with asynchronous replication in systems like Redis Sentinel, MongoDB (default), and PostgreSQL streaming replication. The system appears consistent during normal operation but reveals its inconsistency during failures. To achieve true CP behavior, you need synchronous replication or consensus-based replication (like Raft), where writes are only acknowledged after a majority of replicas have them. The CAP classification of a system should be evaluated based on its behavior during failures, not just during normal operation."},{question:"What does it mean for a system to provide 'monotonic writes' consistency?",options:["All writes must be larger than the previous write value","Writes from the same client are applied in the order they were issued","Writes are applied in alphabetical order across all clients","Each write must complete before the next write can begin"],correctIndex:1,explanation:"Monotonic writes guarantees that writes from the same process are serialized  if a client writes A then writes B, all replicas will apply A before B. Without this guarantee, a replica might see B before A due to message reordering, leading to inconsistent state. For example, if you update a user's email (write A) then send a notification to the new email (write B), monotonic writes ensures no replica processes the notification before the email update. This is weaker than strong consistency (it doesn't order writes across different clients) but prevents a specific class of anomalies. Session-based systems often provide this guarantee by routing all writes from a session through the same replica or by including sequence numbers."},{question:"What is the relationship between consensus and state machine replication (SMR)?",options:["They are completely unrelated concepts","Consensus is used to agree on the order of commands, which SMR applies to replicas to keep them in sync","SMR replaces the need for consensus","Consensus handles reads, SMR handles writes"],correctIndex:1,explanation:"State Machine Replication (SMR) is the technique of replicating a deterministic state machine across multiple nodes. If all replicas start in the same state and apply the same commands in the same order, they'll end up in the same state. Consensus protocols (Paxos, Raft) provide the mechanism to agree on the order of commands  this is the hard part in a distributed system. Raft explicitly implements SMR: the leader appends commands to a replicated log, consensus ensures all nodes agree on the log order, and each node applies the log to its state machine. This is the foundation of nearly all CP distributed systems  etcd, ZooKeeper, CockroachDB, and TiKV all use consensus-based SMR. The key insight is that ordering + determinism = consistency."},{question:"What is 'chain replication' and how does it differ from quorum-based replication in terms of CAP?",options:["It's the same as quorum replication but with a different name","Writes go to the head of a chain and propagate to the tail; reads are served from the tail  providing strong consistency with different availability trade-offs","Data is replicated in a blockchain-like immutable chain","It uses hash chains for data integrity verification"],correctIndex:1,explanation:"In chain replication, nodes are arranged in a chain. Writes enter at the head and propagate sequentially to each node until reaching the tail, which sends the acknowledgment. Reads are served only from the tail, which has the latest committed data. This provides strong consistency (the tail always has all committed writes) with high read throughput (reads only hit one node). The trade-off versus quorum replication: chain replication has higher write latency (must traverse the entire chain) and is more sensitive to individual node failures (any node failure breaks the chain until reconfigured). HDFS NameNode and Microsoft's Azure Storage use variants of chain replication. It's CP like quorum systems but with different performance characteristics  better read throughput but worse write latency and failure handling."},{question:"A distributed system uses 'leader leases' where the leader can serve reads locally without consulting followers. What happens if the leader's clock is significantly ahead of other nodes?",options:["No impact  clock skew doesn't affect leader leases","The leader might serve stale reads after its lease has actually expired (from other nodes' perspective) but it still thinks it's valid","The system automatically corrects for clock skew","Other nodes will also advance their clocks to match"],correctIndex:1,explanation:"If the leader's clock is fast, its lease will expire (by its own clock) before other nodes think it should. This is actually the safe direction  the leader stops serving before others could elect a new one. The dangerous case is if the leader's clock is SLOW  it thinks its lease is still valid when other nodes have already started a new election. In this case, two nodes might both think they're the leader, causing split-brain and inconsistent reads. This is why Google's TrueTime is so valuable  it provides bounded clock uncertainty, and Spanner waits out the uncertainty before relying on timestamps. Without hardware clock synchronization, leader leases depend on the assumption that clock drift is bounded, which NTP tries to provide but can't guarantee."},{question:"What is a 'tombstone' in eventually consistent systems, and why is it needed?",options:["A marker indicating a record has been logically deleted, preventing its resurrection during replica synchronization","A backup copy of deleted data for recovery purposes","A log entry marking the end of a database transaction","A timestamp indicating when a node was last seen alive"],correctIndex:0,explanation:"In eventually consistent systems, you can't simply delete a record because during synchronization, another replica that still has the record would re-introduce it (thinking the deleting replica just missed the write). Tombstones solve this by replacing the deleted record with a special marker that says 'this was intentionally deleted.' During sync, when a node sees a tombstone, it knows to delete its copy rather than propagate the live version. Cassandra uses tombstones extensively, and they have a configurable retention period (gc_grace_seconds, default 10 days). Tombstones can accumulate and cause performance issues if not cleaned up  a common operational challenge in Cassandra. This is a non-obvious consequence of choosing AP/eventual consistency: even deletes are more complex."},{question:"What is the 'harvest and yield' framework proposed by Eric Brewer as an alternative way to think about CAP?",options:["A farming analogy for database scaling","Yield is the probability of completing a request; harvest is the fraction of complete data in the response  systems can trade between them","A method for calculating database throughput","A framework for deciding when to scale up vs scale out"],correctIndex:1,explanation:"Brewer proposed 'harvest and yield' as a more nuanced way to think about CAP trade-offs. Yield is the probability that a request completes (related to availability). Harvest is the completeness of the answer  the fraction of total data reflected in the response. Instead of a binary available/unavailable choice, systems can degrade gracefully: return partial results (lower harvest) while maintaining high yield. For example, a search engine might return results from 99 out of 100 index shards if one is down  high yield (the request completes) with slightly reduced harvest (missing some results). Google Search does exactly this. This framework is more practical than binary CAP thinking because it allows for partial degradation rather than complete failure."},{question:"What is the 'CAP theorem proof' by Gilbert and Lynch (2002) based on?",options:["An empirical study of distributed database failures","A formal proof showing that in an asynchronous network, it's impossible to implement a read/write data object that guarantees availability and atomic consistency in all executions including partitions","A mathematical proof based on information theory","A simulation of network partition scenarios"],correctIndex:1,explanation:"Gilbert and Lynch formally proved Brewer's conjecture by constructing an asynchronous network model and showing that no algorithm can implement an atomic (linearizable) read/write register that is both available (always responds) and consistent (linearizable) in all executions where messages can be lost (partitions). The proof is by contradiction: they show that if a partition occurs and both sides must respond (availability), at least one side must return a stale value (violating consistency), or one side must not respond (violating availability). The formal proof is important because it moved CAP from a conjecture ('I think this is true') to a theorem ('this is mathematically proven'). It also precisely defined the terms, reducing the ambiguity in Brewer's original talk."},{question:"etcd is used by Kubernetes for cluster state. Why is a CP system appropriate for this use case?",options:["Because Kubernetes doesn't need high availability","Because cluster state (pod scheduling, service discovery) must be consistent to prevent conflicting assignments","Because etcd is the fastest available key-value store","Because Kubernetes only runs in a single data center"],correctIndex:1,explanation:"Kubernetes uses etcd for storing all cluster state  pod assignments, service definitions, ConfigMaps, Secrets, etc. This data must be strongly consistent because inconsistency could cause catastrophic problems: two nodes might both think they own the same pod, or a scheduler might assign work to a node that's been removed. It's better for the control plane to be briefly unavailable (CP) than to make conflicting scheduling decisions. Kubernetes can tolerate brief control plane unavailability because running workloads continue operating even if the control plane is down  it's the control plane decisions that need consistency. This is a textbook CP use case: metadata and coordination data where correctness trumps availability."},{question:"What is a 'phi accrual failure detector' used in systems like Cassandra?",options:["A mechanism that gives a continuous suspicion level (phi value) for each node rather than a binary alive/dead determination","A cryptographic method for detecting data tampering","A load balancing algorithm that detects hot spots","A technique for detecting network partitions using packet analysis"],correctIndex:0,explanation:"The phi accrual failure detector outputs a continuous suspicion level (phi) rather than a binary alive/dead verdict. Phi represents the likelihood that a node has failed, based on the statistical distribution of heartbeat inter-arrival times. A higher phi means more suspicion. Applications can choose their own phi threshold based on their tolerance for false positives vs. detection speed. This is more nuanced than simple timeout-based detection: a node that usually responds in 10ms but hasn't responded in 100ms gets a high phi, while in a system where responses normally take 500ms, 100ms of silence barely registers. Cassandra uses this to adapt to varying network conditions and avoid falsely marking healthy-but-slow nodes as down, which would cause unnecessary data movement."},{question:"What are the implications of the CAP theorem for microservices architectures?",options:["Each microservice must choose the same CAP trade-off for consistency across the system","Different microservices can make different CAP trade-offs based on their specific requirements","CAP theorem doesn't apply to microservices, only to databases","Microservices avoid CAP issues by using REST APIs"],correctIndex:1,explanation:"One of the key benefits of microservices is that each service can make independent CAP trade-offs based on its domain requirements. A payment service might use a CP database (PostgreSQL with synchronous replication) because financial consistency is critical. A product catalog service might use an AP database (Cassandra) because availability is more important than immediate consistency for product listings. A session store might use Redis with eventual consistency because session data is ephemeral. This per-service optimization is impossible in a monolith with a single database. However, this creates complexity at service boundaries  when a workflow spans multiple services with different consistency models, you need patterns like sagas, eventual consistency, or compensation transactions to maintain overall system correctness."},{question:"What is the 'saga pattern' and how does it relate to consistency across AP and CP services?",options:["A Norse mythology-inspired naming convention for database tables","A pattern for managing distributed transactions across services through a sequence of local transactions with compensating actions for rollback","A protocol for achieving strong consistency in AP systems","A logging pattern for tracking data changes across services"],correctIndex:1,explanation:"The saga pattern manages distributed transactions by breaking them into a sequence of local transactions, each in its own service. If one step fails, compensating transactions undo the previous steps. For example, an order saga: (1) Reserve inventory  (2) Charge payment  (3) Ship order. If payment fails, a compensating transaction releases the reserved inventory. Sagas provide eventual consistency across services without requiring distributed locking (2PC), which would create tight coupling and reduce availability. This is essential in microservices where different services may use AP or CP databases. The trade-off: sagas are eventually consistent (there's a window where the system is in a partially completed state), and compensating logic can be complex. Orchestrated sagas use a central coordinator; choreographed sagas use events."},{question:"What is 'strict quorum' vs 'sloppy quorum' in Dynamo-style systems?",options:["Strict quorum uses exactly N nodes; sloppy quorum uses fewer than N nodes","Strict quorum only counts designated replica nodes; sloppy quorum can count any healthy node toward the quorum","Strict quorum requires all nodes to respond; sloppy quorum requires only a majority","They are the same thing with different names"],correctIndex:1,explanation:"A strict quorum only counts the designated replica nodes (the N nodes responsible for a key based on consistent hashing) toward the R or W threshold. If some designated nodes are down, the operation fails. A sloppy quorum allows the system to count any healthy node toward the quorum  if a designated node is down, the write goes to another node (with a hint for later handoff). Sloppy quorums increase availability (writes succeed even if designated nodes are down) but break the R + W > N consistency guarantee because reads and writes might go to entirely different sets of nodes. DynamoDB uses sloppy quorums to achieve its always-writable design. Cassandra uses strict quorums by default. The choice between them is fundamentally a CP vs AP trade-off."},{question:"How does the 'Raft' consensus protocol handle a leader failure?",options:["A human operator manually promotes a new leader","Followers detect leader absence via heartbeat timeout, start an election with randomized timeouts to prevent split votes, and the first candidate to win majority votes becomes leader","The node with the most data automatically becomes leader","All nodes stop processing until the old leader recovers"],correctIndex:1,explanation:"In Raft, the leader periodically sends heartbeats to followers. If a follower doesn't receive a heartbeat within its election timeout (randomized to prevent simultaneous elections), it increments its term, transitions to 'candidate' state, votes for itself, and requests votes from other nodes. A candidate wins if it receives votes from a majority. The randomized timeout is crucial  without it, multiple nodes would start elections simultaneously, splitting votes and delaying leader election. The new leader must have all committed log entries (candidates with incomplete logs are rejected). This entire process typically completes in milliseconds, making the unavailability window very brief. Raft's clear separation of leader election, log replication, and safety makes it much easier to understand and implement correctly compared to Paxos."},{question:"What is 'multi-leader replication' (also called 'multi-master'), and what CAP classification does it typically have?",options:["CP  multiple leaders ensure stronger consistency","AP  multiple leaders can accept writes independently, tolerating partitions at the cost of potential conflicts","CA  multiple leaders prevent both partitions and unavailability","It doesn't have a CAP classification"],correctIndex:1,explanation:"Multi-leader replication allows multiple nodes to accept writes independently and asynchronously replicate changes to each other. This is an AP design because during a network partition, leaders on both sides continue accepting writes  maintaining availability at the cost of consistency (conflicting writes may need resolution later). CouchDB, MySQL Group Replication (multi-primary mode), and DynamoDB global tables use multi-leader replication. The main use case is geo-distributed systems where users write to their nearest leader for low latency. The downside is write conflicts when different leaders modify the same data  requiring conflict resolution strategies (LWW, merge functions, CRDTs). Multi-leader is essentially choosing availability and low latency over consistency."},{question:"Google Bigtable is often described as a CP system. What mechanism provides its consistency guarantee?",options:["Multi-leader replication with conflict resolution","Single-leader design where each tablet is served by exactly one tablet server, using Chubby for leader election","Paxos consensus across all tablet servers","Eventual consistency with automatic reconciliation"],correctIndex:1,explanation:"Bigtable achieves strong consistency through a simple but effective mechanism: each tablet (data partition) is assigned to exactly one tablet server at a time. All reads and writes for that tablet go to its assigned server, eliminating the possibility of conflicting writes. If the tablet server fails, Chubby (Google's distributed lock service, which uses Paxos internally) detects the failure and reassigns the tablet to another server. During reassignment, the tablet is briefly unavailable  the CP trade-off. This single-server-per-tablet design makes consistency trivial (no concurrent writers) at the cost of availability during failures. HBase, the open-source Bigtable clone, uses ZooKeeper instead of Chubby for the same purpose."},{question:"What is 'conflict-free' about CRDTs, and what types of CRDTs exist?",options:["CRDTs prevent conflicts by using locks  there are read-CRDTs and write-CRDTs","CRDTs are mathematically designed so concurrent updates can always be merged without conflicts  the two types are state-based (CvRDT) and operation-based (CmRDT)","CRDTs avoid conflicts by only allowing append operations  there are log-CRDTs and queue-CRDTs","CRDTs resolve conflicts using timestamps  there are LWW-CRDTs and MWW-CRDTs"],correctIndex:1,explanation:"CRDTs achieve conflict-freedom through mathematical properties: the merge function is commutative (order doesn't matter), associative (grouping doesn't matter), and idempotent (merging the same state twice has no effect). State-based CRDTs (CvRDTs - Convergent) transmit their full state, and any two states can be merged. Operation-based CRDTs (CmRDTs - Commutative) transmit operations, which must be commutative. Examples include G-Counter (grow-only counter), PN-Counter (increment/decrement), G-Set (grow-only set), OR-Set (observed-remove set), and LWW-Register. The key insight is that by constraining the data structure's operations, you can guarantee convergence without coordination  no locks, no consensus, no conflicts. This makes CRDTs ideal for AP systems and collaborative applications."},{question:"What is the practical impact of CAP theorem on system design decisions in a real-world company?",options:["Teams must choose one database technology for the entire company","It forces engineers to have explicit conversations about consistency vs availability trade-offs for each use case and choose appropriate technologies","It means distributed systems are impossible to build correctly","It only matters for database vendors, not application developers"],correctIndex:1,explanation:"CAP's greatest practical impact is forcing explicit trade-off discussions. When designing a feature, engineers must ask: 'What happens during a network partition? Is it worse to show stale data or to show an error?' This leads to polyglot persistence  using different databases for different use cases based on their CAP properties. A company might use PostgreSQL (CP) for billing, Cassandra (AP) for user activity feeds, Redis (typically CP with caveats) for caching, and Elasticsearch (AP-ish) for search. CAP also influences API design (should this endpoint return stale data or error?), SLA definitions, and incident response procedures. The theorem's real value isn't the mathematical proof  it's the vocabulary and framework it gives teams to reason about and communicate distributed system trade-offs."},{question:"What happens to a Cassandra cluster if ALL nodes for a particular partition key go down?",options:["Other nodes automatically take over the partition","The data is served from a cache layer","Read and write requests for that partition key fail, regardless of consistency level","Cassandra reconstructs the data from parity information"],correctIndex:2,explanation:"Even in AP systems like Cassandra, if ALL replicas for a key are down, requests for that key will fail. AP guarantees availability only if at least one replica is reachable. With a replication factor of 3, losing all 3 replicas for a token range makes that data unavailable. This is why Cassandra recommends replication factor of 3 across multiple racks/data centers  it makes simultaneous failure of all replicas extremely unlikely. Cassandra doesn't use parity (like RAID) or automatic data reconstruction. It also doesn't dynamically reassign token ranges to other nodes (unlike some systems with virtual nodes that support range takeover). The data is simply unavailable until at least one replica recovers. This illustrates that 'available' in CAP means 'every non-failing node responds,' not 'the system handles any number of failures.'"},{question:"What is the difference between 'synchronous' and 'asynchronous' replication in terms of CAP?",options:["Synchronous replication is always AP; asynchronous is always CP","Synchronous replication provides stronger consistency (CP-leaning) by waiting for replicas to confirm before acknowledging; asynchronous is faster but may lose data (AP-leaning)","There is no difference  both provide the same CAP guarantees","Synchronous replication doesn't involve the network; asynchronous does"],correctIndex:1,explanation:"Synchronous replication waits for one or more replicas to confirm the write before acknowledging it to the client. This provides stronger durability and consistency (the data exists on multiple nodes before the client considers the write complete) but increases latency and reduces availability (if a replica is slow or down, the write is delayed or fails). Asynchronous replication acknowledges the write immediately after the primary stores it, then replicates in the background. This is faster and more available but risks data loss during primary failure. PostgreSQL offers both: synchronous_commit=on waits for replica confirmation, synchronous_commit=off doesn't. The choice is a direct CAP/PACELC trade-off  synchronous replication is CP/EC behavior, asynchronous is AP/EL behavior."},{question:"What is the 'impossibility of exactly-once delivery' and how does it relate to CAP?",options:["Messages can always be delivered exactly once with the right protocol","In a distributed system with possible failures, you can only guarantee at-most-once or at-least-once delivery, not exactly-once  making idempotency crucial for AP systems","Exactly-once delivery is trivially achievable with TCP","This impossibility only applies to email systems"],correctIndex:1,explanation:"In a distributed system where messages can be lost or duplicated, you can guarantee at-most-once delivery (send once, don't retry  might be lost) or at-least-once delivery (retry until acknowledged  might be duplicated), but not exactly-once delivery at the network level. This is related to CAP because AP systems that retry writes for availability may create duplicates. The practical solution is 'effectively exactly-once' through idempotent operations  designing operations so that applying them multiple times has the same effect as applying them once. For example, 'set balance to $100' is idempotent, but 'add $10 to balance' is not. Kafka achieves 'exactly-once semantics' through idempotent producers and transactional consumers  it's still at-least-once at the network level, but deduplication makes it appear exactly-once to the application."},{question:"What is the impact of CAP theorem on global database deployments spanning multiple continents?",options:["Global deployments are impossible due to CAP constraints","Cross-continent latency (~100-300ms) amplifies the consistency-latency trade-off: CP means high write latency for coordination; AP means fast writes but potential conflicts","CAP doesn't apply to global deployments","All global databases must use eventual consistency"],correctIndex:1,explanation:"In global deployments, the physics of light speed creates ~100-300ms round-trip latency between continents. For CP systems, every write requires cross-continent coordination (consensus), adding this latency to every write. Spanner accepts this cost for external consistency. For AP systems, writes can complete locally and replicate asynchronously, giving low latency but risking conflicts. The PACELC model is especially relevant here: even without partitions (which are more common across continents), the latency-consistency trade-off is severe. This is why many global systems use a hybrid approach: CP for critical data (like user identity), AP for non-critical data (like activity feeds), and careful data placement to keep most operations within a single region."},{question:"What is 'data sovereignty' and how does it intersect with CAP theorem decisions?",options:["A marketing term for data encryption","Legal requirements that data must reside in specific geographic regions, constraining replica placement and affecting CAP trade-offs","The right of a database to refuse queries it doesn't want to answer","A technique for preventing data replication"],correctIndex:1,explanation:"Data sovereignty laws (GDPR in Europe, LGPD in Brazil, etc.) require that certain data physically resides within specific geographic boundaries. This constrains where you can place replicas, directly affecting CAP trade-offs. If EU user data can only be in EU data centers, you can't have replicas in US-East for faster quorum  limiting your options for fault tolerance and latency. A partition between EU data centers has no US-based backup. Some systems (like CockroachDB and Spanner) support geo-partitioning to pin data to specific regions while maintaining global replication for other data. This intersection of legal requirements and distributed systems theory is increasingly important as more countries enact data localization laws, forcing architects to make CAP trade-offs within geographic constraints."},{question:"What is the 'Dynamo' model and how has it influenced modern databases?",options:["A relational database model from Oracle","A design philosophy from Amazon's Dynamo paper: leaderless replication, consistent hashing, sloppy quorums, vector clocks, hinted handoff, and eventual consistency","A specific AWS database product","A consensus-based replication model"],correctIndex:1,explanation:"Amazon's 2007 Dynamo paper introduced a revolutionary AP database architecture featuring: leaderless replication (any node can handle any request), consistent hashing (for data distribution), sloppy quorums (for high availability), vector clocks (for conflict detection), hinted handoff (for handling unavailable nodes), and gossip protocols (for membership and failure detection). This design prioritized availability and scalability over strong consistency. Its influence is massive: Cassandra (Facebook, based on Dynamo + Bigtable), Riak (Basho), Voldemort (LinkedIn), and DynamoDB (Amazon's managed version) all descend from this model. The paper's key insight  that many applications can tolerate eventual consistency and benefit enormously from the resulting availability  fundamentally changed distributed database design."},{question:"What is 'convergent conflict resolution' and why is it important?",options:["Resolving conflicts by choosing the value that converges to zero","A conflict resolution strategy where all replicas are guaranteed to reach the same final state regardless of the order they receive updates","A method for resolving network routing conflicts","An algorithm for merging sorted data from multiple sources"],correctIndex:1,explanation:"Convergent conflict resolution ensures that no matter what order replicas receive and process conflicting updates, they all arrive at the same final state. This is critical in AP systems where different replicas may receive updates in different orders. LWW (Last-Write-Wins) is convergent because all replicas will keep the same write (the one with the highest timestamp). CRDTs are convergent by mathematical design. Non-convergent resolution (like 'first-write-wins' in an uncoordinated system) could leave replicas permanently diverged. Convergence is the minimum viable correctness property for AP systems  without it, replicas might never agree, and clients would get different answers depending on which replica they hit, with no path to consistency even after partitions heal."},{question:"What is the 'write-ahead log' (WAL) pattern and which CAP-related property does it primarily support?",options:["Partition tolerance  it helps nodes recover from partitions","Availability  it keeps the system available during failures","It primarily supports durability (related to the 'D' in ACID), not directly a CAP property, but is essential for recovery in both CP and AP systems","Consistency  it ensures all nodes have the same data"],correctIndex:2,explanation:"Write-ahead logging (WAL) writes changes to a sequential log on disk before applying them to the main data structure. This ensures durability  if a node crashes mid-operation, it can replay the log on startup to recover its state. WAL is orthogonal to CAP (which is about distributed consistency/availability trade-offs) and is instead about single-node durability and recovery. However, WAL is essential infrastructure for both CP and AP systems: Raft and Paxos use replicated logs (WALs across nodes), and single-node databases use WAL for crash recovery. PostgreSQL, MySQL, etcd, and Cassandra all use WAL. Without WAL, a crash could leave data in a corrupt, partially-written state. It's one of the most fundamental patterns in database engineering."},{question:"How do service meshes like Istio relate to CAP theorem concerns?",options:["Service meshes eliminate CAP trade-offs by providing perfect networking","Service meshes provide the infrastructure layer (retries, timeouts, circuit breakers) that implements the chosen CAP trade-offs at the application level","Service meshes only handle security, not consistency","Service meshes enforce strong consistency across all services"],correctIndex:1,explanation:"Service meshes don't eliminate CAP trade-offs  they provide the infrastructure to implement and manage them. Circuit breakers implement availability trade-offs (fail fast vs retry). Retry policies affect consistency (retrying a non-idempotent operation could cause duplicates). Timeouts determine when a service considers a downstream service 'partitioned.' Traffic routing can direct reads to nearby replicas (AP-style) or to the primary (CP-style). For example, Istio's retry and timeout configuration directly affects whether your system behaves more CP (strict timeouts, no retries, fail-closed) or AP (generous retries, graceful degradation). The service mesh makes these policies configurable and observable, but the fundamental CAP trade-off decisions still rest with the architect."},{question:"What is 'optimistic replication' and which CAP property does it favor?",options:["Replication that assumes the best case and uses strong consistency","Replication that allows replicas to diverge temporarily and reconcile later, favoring availability (AP)","Replication that optimizes for speed by skipping error checking","Replication that only works when network conditions are optimal"],correctIndex:1,explanation:"Optimistic replication (also called lazy replication) allows replicas to accept updates independently without coordinating with other replicas first, on the 'optimistic' assumption that conflicts are rare. Conflicting updates are detected and resolved after the fact. This favors availability (AP) because nodes can always accept writes, even during partitions. Git is a great analogy  you commit locally (optimistic, no coordination) and merge/resolve conflicts later when you push/pull. Pessimistic replication (like synchronous replication) coordinates before accepting writes, favoring consistency (CP). Most AP databases use optimistic replication: Cassandra, DynamoDB, CouchDB. The trade-off is that conflict resolution can be complex and application-specific, but the availability and latency benefits are significant for many use cases."},{question:"In CAP terms, what is the difference between a 'partition' and a 'node failure'?",options:["They are the same thing  both mean a node is unreachable","A partition means groups of nodes can't communicate with EACH OTHER but are individually healthy; a node failure means a node is actually down","A partition only affects data; a node failure affects processing","A partition is permanent; a node failure is temporary"],correctIndex:1,explanation:"A network partition means the communication link between groups of nodes is broken, but the nodes themselves are healthy and running. Both sides might continue accepting client requests independently. A node failure means a specific node has actually crashed or become unresponsive. The distinction matters because partitions can cause split-brain (both sides independently serving requests) while node failures typically don't (the failed node isn't doing anything). In practice, it's often impossible to distinguish between the two from the perspective of a remote node  if you can't reach a node, is it down or is the network broken? This ambiguity is why timeout-based failure detection is inherently imperfect, and why quorum systems are designed to handle both scenarios safely."},{question:"What is 'active-active' vs 'active-passive' replication, and how do they map to CAP?",options:["Active-active is CP; active-passive is AP","Active-active (multi-leader, AP-leaning) has all replicas serving writes; active-passive (single-leader, CP-leaning) has one primary and standby replicas","They're the same thing with different marketing names","Active-active requires consensus; active-passive doesn't"],correctIndex:1,explanation:"Active-active replication has multiple nodes accepting writes simultaneously (multi-leader), which is AP-leaning because during a partition, any active node can accept writes independently. However, this creates conflict resolution challenges. Active-passive has one primary accepting writes and one or more standby replicas that take over if the primary fails. This is CP-leaning because there's always one source of truth. During a partition, if you can't reach the primary, writes are unavailable (CP behavior). Active-active is used for geo-distributed systems where latency matters (users write to their nearest leader)  Redis Enterprise, CouchDB, and DynamoDB Global Tables support this. Active-passive is common in traditional RDBMS setups  PostgreSQL streaming replication, MySQL replication."},{question:"Why did Amazon build DynamoDB as an AP system rather than a CP system?",options:["Because CP systems hadn't been invented yet in 2007","Because Amazon's SLA research showed that even 100ms of added latency reduced sales by 1%, making availability the top priority over consistency","Because CP systems can't scale to Amazon's size","Because AP systems are always faster than CP systems"],correctIndex:1,explanation:"Amazon's research quantified the business impact of latency and unavailability on e-commerce revenue. Even small increases in page load time (100ms) measurably reduced sales. An unavailable shopping cart or product page is a guaranteed lost sale. In contrast, a slightly stale product recommendation or shopping cart is barely noticeable to users. This business analysis drove the architectural decision: always be available and responsive, even at the cost of occasional inconsistency. The Dynamo paper explicitly states that Amazon's services needed an 'always-on' experience. This is a masterclass in letting business requirements drive technical trade-offs  the CAP choice wasn't made in a vacuum but based on quantified revenue impact. It demonstrates that system design decisions should start with 'what does the business need?' not 'what's the theoretically correct approach?'"},{question:"What is a 'read quorum' and 'write quorum' in the context of Dynamo-style systems?",options:["The number of nodes that must be consulted during read and write operations respectively","The minimum disk space required for reads and writes","The CPU quota allocated for read and write operations","The maximum number of concurrent reads and writes allowed"],correctIndex:0,explanation:"In Dynamo-style systems, R (read quorum) is the number of replicas that must respond to a read request, and W (write quorum) is the number of replicas that must acknowledge a write. With N replicas, common configurations are: R=1, W=N (fast reads, slow writes); R=N, W=1 (slow reads, fast writes); R=W=QUORUM (balanced). The fundamental trade-off is captured by R + W > N for strong consistency. Lower R means faster reads but potentially stale data. Lower W means faster writes but less durable. Cassandra exposes this as consistency levels (ONE, QUORUM, ALL) per query. This tunability is powerful because different access patterns within the same application can use different quorum settings  a product search might use R=1 for speed while a checkout might use R=QUORUM for correctness."}],dm=[{question:"What is the primary difference between SQL and NoSQL databases?",options:["SQL is faster than NoSQL","SQL enforces a fixed schema with structured tables; NoSQL offers flexible schemas with various data models","NoSQL is newer and always better","SQL doesn't support indexing"],correctIndex:1,explanation:"SQL databases enforce a predefined schema with structured tables, rows, and columns, ensuring data consistency through constraints and relationships. NoSQL databases provide flexible schemas, allowing different records to have different structures  this includes document stores (MongoDB), key-value stores (Redis), column-family (Cassandra), and graph databases (Neo4j). The choice depends on your needs: SQL excels at complex queries and transactions with strong consistency, while NoSQL shines for flexible schemas, horizontal scaling, and high write throughput. Instagram uses both PostgreSQL (for user/relationship data needing consistency) and Cassandra (for high-volume feed data needing scale)."},{question:"What does ACID stand for in database transactions?",options:["Automated, Consistent, Isolated, Durable","Atomicity, Consistency, Isolation, Durability","Atomic, Cached, Indexed, Distributed","Available, Consistent, Isolated, Durable"],correctIndex:1,explanation:"ACID stands for Atomicity (all operations in a transaction succeed or all fail), Consistency (the database moves from one valid state to another), Isolation (concurrent transactions don't interfere with each other), and Durability (committed data survives system failures). These properties are the foundation of reliable relational databases like PostgreSQL and MySQL. For example, in a bank transfer, atomicity ensures money is deducted from one account AND added to another  never just one. NoSQL databases often relax some ACID properties (particularly isolation and consistency) in exchange for better performance and availability, following the BASE model instead."},{question:"What is a B-tree index and why is it the default index type in most relational databases?",options:["A binary tree for fast sorting","A balanced tree structure that keeps data sorted and allows searches, insertions, and deletions in O(log n) time","A tree that stores only boolean values","A backup tree for redundancy"],correctIndex:1,explanation:"A B-tree is a self-balancing tree data structure that maintains sorted data and allows searches, sequential access, insertions, and deletions in O(log n) time. It's the default index in PostgreSQL, MySQL, and most relational databases because it handles both equality lookups (WHERE id = 5) and range queries (WHERE age BETWEEN 20 AND 30) efficiently. B-trees have high fan-out (many children per node), which minimizes the number of disk reads  a B-tree with branching factor 500 can index billions of rows in just 3-4 levels. Each level requires one disk read, so even very large tables need only a few disk accesses. This is why B-tree indexes dramatically improve query performance on large tables."},{question:"What is an LSM-tree and which databases use it?",options:["A tree for log storage management","Log-Structured Merge-tree that optimizes write performance by buffering writes in memory then flushing sorted runs to disk","A tree for least-recently-used data","A tree structure for read optimization"],correctIndex:1,explanation:"An LSM-tree (Log-Structured Merge-tree) optimizes write performance by first writing data to an in-memory sorted structure (memtable), then periodically flushing it to immutable sorted files (SSTables) on disk. Reads check the memtable first, then progressively older SSTables. Background compaction merges SSTables to reduce read amplification. LSM-trees are used by Cassandra, RocksDB, LevelDB, and HBase. They excel at write-heavy workloads because writes are sequential (appending to memtable and writing sorted runs) rather than random (as with B-tree page updates). The tradeoff is that reads can be slower because they may check multiple SSTables, which is mitigated by Bloom filters that quickly determine if a key exists in a given SSTable."},{question:"What is database normalization?",options:["Making the database run at normal speed","Organizing data to minimize redundancy by decomposing tables according to normal forms","Converting data to a normal distribution","Resetting the database to defaults"],correctIndex:1,explanation:"Normalization is the process of organizing a relational database to reduce data redundancy and improve data integrity by decomposing tables according to a series of normal forms (1NF, 2NF, 3NF, BCNF, etc.). For example, instead of storing a customer's address in every order row, you store it once in a customers table and reference it via a foreign key. This prevents update anomalies (changing an address in one order but not others), insertion anomalies, and deletion anomalies. The tradeoff is that normalized databases require more JOINs to reconstruct data, which can impact read performance. Most production systems normalize to 3NF as a good balance between data integrity and query performance."},{question:"When would you choose denormalization over normalization?",options:["Always, because it's faster","When read performance is critical and the cost of JOINs is too high for your query patterns","When you want better data integrity","When disk space is limited"],correctIndex:1,explanation:"Denormalization intentionally introduces redundancy by storing derived or copied data to avoid expensive JOINs at query time. It's chosen when read performance is the primary concern and the application can tolerate some data redundancy. For example, storing a user's name directly in the orders table (instead of joining to users table) eliminates a JOIN for the common 'show order with customer name' query. The cost is increased storage, more complex updates (must update the name in multiple places), and potential inconsistency. Social media feeds, analytics dashboards, and e-commerce product pages are common use cases where denormalization significantly improves read latency. NoSQL databases often embrace denormalization as a primary design pattern."},{question:"What is a document store database?",options:["A database for storing PDF documents","A NoSQL database that stores data as semi-structured documents (JSON/BSON) with flexible schemas","A database that stores documentation","A relational database with text columns"],correctIndex:1,explanation:"A document store is a type of NoSQL database that stores data as semi-structured documents, typically in JSON or BSON format. Each document can have a different structure  one user document might have an 'address' field while another doesn't. This flexibility is ideal for evolving schemas and varied data. MongoDB is the most popular document store, using BSON (Binary JSON) internally. Documents are grouped into collections (analogous to tables), and you can query on any field without predefined schemas. Document stores excel when your data naturally fits a hierarchical/nested structure, like a blog post with embedded comments. The tradeoff is that cross-document queries (equivalent to JOINs) are less efficient than in relational databases."},{question:"What is a column-family database?",options:["A database that only stores columns","A NoSQL database that stores data in columns grouped by column families, optimized for queries over large datasets","A relational database with many columns","A database for spreadsheet data"],correctIndex:1,explanation:"Column-family databases (also called wide-column stores) organize data by columns rather than rows, with columns grouped into families. Unlike relational databases where a row stores all columns together, column-family stores keep each column family's data contiguous on disk. This makes them extremely efficient for queries that read specific columns across many rows (like analytics) and for sparse data where most columns are empty. Cassandra and HBase are the main examples. Cassandra's data model uses partition keys for distribution and clustering columns for sorting within partitions. Column-family databases excel at time-series data, event logging, and analytics workloads where you typically query specific attributes across millions of records."},{question:"What is the CAP theorem?",options:["Caching And Processing theorem","A distributed system can only guarantee two of three: Consistency, Availability, and Partition tolerance","Concurrency, Atomicity, and Persistence theorem","A theorem about database capacity"],correctIndex:1,explanation:"The CAP theorem states that in a distributed system, you can only guarantee two of three properties simultaneously: Consistency (all nodes see the same data at the same time), Availability (every request receives a response), and Partition tolerance (the system continues operating despite network failures). Since network partitions are inevitable in distributed systems, the real choice is between CP (consistency + partition tolerance, like HBase) and AP (availability + partition tolerance, like Cassandra with eventual consistency). For example, during a network partition, a CP system might refuse to serve requests to maintain consistency, while an AP system serves potentially stale data to remain available. It's important to note that CAP applies only during partition events."},{question:"What is the difference between OLTP and OLAP?",options:["They are the same thing with different names","OLTP handles real-time transactional operations; OLAP handles analytical queries on large datasets","OLTP is for old systems; OLAP is for new systems","OLTP is faster than OLAP"],correctIndex:1,explanation:"OLTP (Online Transaction Processing) systems handle high volumes of short, real-time transactions like inserts, updates, and deletes  think of a bank processing transfers or an e-commerce site processing orders. OLAP (Online Analytical Processing) systems are optimized for complex analytical queries that scan large amounts of data  think of a data warehouse generating quarterly revenue reports. OLTP uses row-oriented storage for fast writes and point lookups, while OLAP uses columnar storage for efficient aggregation queries. PostgreSQL and MySQL are OLTP databases; Snowflake, BigQuery, and ClickHouse are OLAP systems. Many architectures use both: OLTP for the application and ETL pipelines feeding data into an OLAP system for analytics."},{question:"What is a composite index and when should you use one?",options:["An index made of composite materials","An index on multiple columns that speeds up queries filtering on those columns together","An index that combines multiple tables","An index that's created automatically"],correctIndex:1,explanation:"A composite (compound) index covers multiple columns in a specific order, optimizing queries that filter or sort on those columns together. For example, an index on (country, city) speeds up queries like 'WHERE country = 'US' AND city = 'NYC'' and also 'WHERE country = 'US'' (leftmost prefix). However, it doesn't help 'WHERE city = 'NYC'' alone because indexes follow left-to-right column order. The column order matters enormously  put the highest-cardinality or most-frequently-filtered column first. In PostgreSQL, a composite index on (user_id, created_at) is perfect for 'SELECT * FROM orders WHERE user_id = 123 ORDER BY created_at DESC' because it satisfies both the filter and the sort in a single index scan."},{question:"What is a covering index?",options:["An index that covers the entire table","An index that contains all columns needed by a query, avoiding table lookups entirely","An index with encryption coverage","An index used for covering edge cases"],correctIndex:1,explanation:"A covering index includes all the columns that a query needs, so the database can satisfy the query entirely from the index without reading the actual table rows (an 'index-only scan'). For example, if you frequently run 'SELECT name, email FROM users WHERE user_id = ?', an index on (user_id, name, email) covers this query completely. PostgreSQL shows this as 'Index Only Scan' in EXPLAIN output. Covering indexes can dramatically improve performance for read-heavy queries because index structures are typically smaller and more cache-friendly than full table pages. The tradeoff is that wider indexes consume more disk space and slow down writes since each INSERT/UPDATE must update the index."},{question:"What is database sharding?",options:["Breaking a database into pieces for disposal","Horizontally partitioning data across multiple database instances to distribute load","Creating read replicas","Backing up the database"],correctIndex:1,explanation:"Sharding horizontally partitions data across multiple independent database instances (shards), each holding a subset of the total data. For example, users with IDs 1-1M go to shard 1, 1M-2M to shard 2, etc. This enables horizontal scaling beyond a single machine's capacity for both storage and throughput. Instagram sharded their PostgreSQL database by user ID to handle billions of photos. The key challenges are choosing a good shard key (to avoid hot shards), handling cross-shard queries (JOINs across shards are very expensive), and rebalancing data when adding shards. Many modern databases like CockroachDB and Vitess (MySQL) handle sharding automatically, while others like MongoDB provide built-in sharding support."},{question:"What is the difference between horizontal and vertical partitioning?",options:["They are the same","Horizontal partitioning splits rows across tables/databases; vertical partitioning splits columns across tables","Horizontal is for NoSQL; vertical is for SQL","Horizontal adds more servers; vertical adds more storage"],correctIndex:1,explanation:"Horizontal partitioning (sharding) distributes rows across multiple tables or database instances  each partition has the same columns but different rows. For example, orders from January in one partition, February in another. Vertical partitioning splits columns across tables  frequently accessed columns stay in a 'hot' table while rarely accessed large columns (like BLOBs) go to a separate table. For example, separating user profile data (name, email  read often) from user preferences (JSON blob  read rarely). Horizontal partitioning enables scaling across machines, while vertical partitioning optimizes I/O by keeping hot data compact. PostgreSQL supports both via declarative partitioning (horizontal) and manual table decomposition (vertical)."},{question:"What is eventual consistency?",options:["The database will eventually become consistent when you fix bugs","All replicas will converge to the same state over time, but reads may return stale data temporarily","The database is always consistent","Consistency that applies only to eventual queries"],correctIndex:1,explanation:"Eventual consistency is a consistency model where, given enough time without new updates, all replicas will converge to the same state. During the convergence period, different replicas may return different values for the same data. This is the model used by AP systems like Cassandra and DynamoDB (by default). For example, after updating a user's profile, one replica might return the old name while another returns the new name for a brief period. Eventually, all replicas sync up. The benefit is higher availability and lower latency (no need to wait for all replicas to acknowledge). For many use cases like social media likes or product view counts, eventual consistency is perfectly acceptable  users won't notice a count being off by one for a few seconds."},{question:"What is a graph database best suited for?",options:["Storing graphs and charts","Data with complex many-to-many relationships where traversal queries are common","Time-series data","Simple key-value lookups"],correctIndex:1,explanation:"Graph databases store data as nodes (entities) and edges (relationships), making them ideal for highly connected data where relationship traversal is the primary query pattern. They excel at queries like 'find all friends of friends who work at company X' or 'what's the shortest path between person A and person B'  queries that would require multiple expensive JOINs in a relational database. Neo4j is the most popular graph database. Real-world use cases include social networks (Facebook's TAO), fraud detection (finding circular money transfers), recommendation engines (products bought by similar users), and knowledge graphs. The key advantage is that traversing relationships is O(1) per hop regardless of total data size, unlike SQL JOINs which degrade with table size."},{question:"What is a write-ahead log (WAL) in databases?",options:["A log of future writes planned","A durability mechanism that logs changes before applying them to the database, enabling crash recovery","A log that's written ahead of time","A performance optimization log"],correctIndex:1,explanation:"A write-ahead log (WAL) records all changes to a log file before they're applied to the actual database pages. If the database crashes, it can replay the WAL to recover committed transactions that weren't yet written to the data files, and roll back incomplete transactions. This is how databases achieve durability (the 'D' in ACID) without writing every change synchronously to data files (which would be very slow due to random I/O). PostgreSQL's WAL writes sequentially to disk (fast) while data file updates happen asynchronously in the background. WAL also enables replication  replicas can stream and replay the WAL to stay in sync. MySQL's equivalent is the InnoDB redo log."},{question:"What is the difference between PostgreSQL and MySQL?",options:["They are identical","PostgreSQL is more standards-compliant with advanced features (JSONB, CTE, window functions); MySQL is simpler with historically better replication","PostgreSQL is NoSQL; MySQL is SQL","MySQL is always faster"],correctIndex:1,explanation:"PostgreSQL is known for strict SQL standards compliance, advanced data types (JSONB, arrays, hstore), sophisticated query planner, extensibility (custom types, functions, operators), and features like CTEs, window functions, and full-text search. MySQL (especially with InnoDB) has historically offered simpler setup, better replication support, and faster performance for simple queries. PostgreSQL supports MVCC more completely and handles complex queries better, while MySQL's simpler architecture made it the default for web applications. In modern versions, the gap has narrowed significantly  MySQL 8.0 added CTEs and window functions. Companies like Instagram and Discord use PostgreSQL for its JSONB support and reliability, while many legacy web apps (WordPress, Drupal) use MySQL."}],hm=[{question:"What is the primary advantage of using base62 encoding for URL shortener keys?",options:["It produces the shortest possible keys","It uses only URL-safe characters (a-z, A-Z, 0-9)","It prevents hash collisions entirely","It enables automatic expiration"],correctIndex:1,explanation:"Base62 encoding uses characters a-z (26), A-Z (26), and 0-9 (10), totaling 62 URL-safe characters. This is critical because URL shortener keys appear directly in URLs, so they must not contain special characters that could break URL parsing or require percent-encoding. Base64, by contrast, includes '+' and '/' which are not URL-safe without modification. While base62 does produce compact keys, the primary advantage is URL-safety  a 7-character base62 string gives 62^7  3.5 trillion unique URLs, which is more than sufficient for most services."},{question:"How many unique short URLs can a 7-character base62 key generate?",options:["About 3.5 million","About 3.5 billion","About 3.5 trillion","About 3.5 quadrillion"],correctIndex:2,explanation:"A 7-character base62 key generates 62^7 = 3,521,614,606,208, which is approximately 3.5 trillion unique combinations. This calculation is straightforward: each of the 7 positions can hold any of 62 characters. For context, bit.ly processes billions of links but is nowhere near this limit, making 7 characters a practical choice. If you needed fewer characters, 6 gives only ~57 billion combinations, while 8 gives ~218 trillion  so 7 is the sweet spot for most URL shortening services."},{question:"What HTTP status code should a URL shortener return for temporary redirects where analytics tracking is important?",options:["200 OK","301 Moved Permanently","302 Found","404 Not Found"],correctIndex:2,explanation:"A 302 (Found) redirect tells the browser that the redirect is temporary, meaning the browser will NOT cache the redirect and will hit the URL shortener server on every request. This is critical for analytics because each request passes through your server, allowing you to log click counts, referrers, geolocation, and timestamps. A 301 (Moved Permanently) would cause browsers to cache the redirect and skip your server on subsequent visits, making your analytics incomplete. Most URL shorteners like bit.ly use 302 redirects (or 307) to ensure accurate click tracking, even though 301 would be slightly faster for end users."},{question:"What is the main problem with using MD5 or SHA-256 hashing directly for generating short URLs?",options:["They are too slow to compute","They produce fixed-length outputs that are too long for short URLs","They cannot handle Unicode input","They require a salt"],correctIndex:1,explanation:"MD5 produces a 128-bit (32 hex characters) hash, and SHA-256 produces a 256-bit (64 hex characters) hash  both far too long for a 'short' URL. Even if you take just the first 7 characters of the hash, you dramatically increase collision probability compared to using all the bits. The birthday paradox means collisions become likely much sooner than you'd expect  with 7 characters of hex (16^7  268 million), you'd expect a collision after roughly 23,000 URLs. A Key Generation Service (KGS) that pre-generates unique keys avoids this collision problem entirely, which is why it's the preferred approach in system design interviews."},{question:"What is a Key Generation Service (KGS) in the context of a URL shortener?",options:["A service that encrypts URLs before storage","A service that pre-generates unique short keys and stores them in a database","A service that manages API authentication keys","A service that compresses URLs"],correctIndex:1,explanation:"A Key Generation Service (KGS) pre-generates a large pool of unique random keys (e.g., 7-character base62 strings) and stores them in a database with two tables: one for unused keys and one for used keys. When a new short URL is needed, KGS simply moves a key from the unused table to the used table, guaranteeing uniqueness without any collision checking at creation time. This is far more efficient than hash-then-check-collision approaches because the uniqueness guarantee is built into the pre-generation step. Services like bit.ly and TinyURL use similar approaches  pre-generation decouples the key uniqueness problem from the URL creation hot path, making the write operation O(1) with no retry logic needed."},{question:"In a URL shortener, why is the read-to-write ratio typically very high (e.g., 100:1)?",options:["Because URLs expire frequently","Because each short URL is created once but redirected many times","Because the database performs read replicas","Because caching is not used"],correctIndex:1,explanation:"A short URL is created exactly once (one write) but may be clicked hundreds or thousands of times (many reads). For example, a popular tweet with a shortened link might generate millions of clicks from a single creation event. This 100:1 or even 1000:1 read-to-write ratio fundamentally shapes the system architecture: you should optimize for reads with extensive caching (Redis/Memcached), use read replicas for the database, and consider a CDN for geographic distribution. The write path can be simpler and slower since it's invoked far less frequently  this is why many URL shortener designs focus their optimization efforts on the redirect (read) path."},{question:"Which caching strategy is most appropriate for a URL shortener's redirect service?",options:["Write-through cache","Write-around cache with LRU eviction","Write-back cache","No caching needed"],correctIndex:1,explanation:"Write-around cache with LRU (Least Recently Used) eviction is ideal because URL shortener access patterns follow a power-law distribution  a small percentage of URLs receive the vast majority of traffic. With write-around, new URL mappings are written directly to the database, and the cache is populated only on read misses. LRU eviction ensures hot (frequently accessed) URLs stay in cache while rarely-accessed URLs are evicted. Write-through would unnecessarily cache every newly created URL, most of which may never be accessed. A Redis or Memcached layer with this strategy can serve 99%+ of redirect requests from memory, reducing database load dramatically  bit.ly reportedly caches its hottest URLs this way."},{question:"What database type is most commonly recommended for a URL shortener's primary storage?",options:["Graph database like Neo4j","NoSQL key-value store like DynamoDB","Time-series database like InfluxDB","Document database like MongoDB"],correctIndex:1,explanation:"A URL shortener's data model is fundamentally a key-value mapping: short_key  long_url. This maps perfectly to a NoSQL key-value store like DynamoDB, Cassandra, or Riak, which provide O(1) lookups by key, horizontal scalability, and high throughput. Relational databases work but add overhead for features (joins, transactions, schemas) that aren't needed for this simple mapping. The access pattern is almost entirely point lookups (get URL by key) with no complex queries, making key-value stores the natural fit. Companies like Pinterest (which shortened billions of URLs) chose similar NoSQL approaches for their URL shortening infrastructure."},{question:"How should you handle hash collisions when using a hash-based approach for short URL generation?",options:["Ignore them  collisions are extremely rare","Append a sequence number to the URL before re-hashing","Use a longer hash output","Switch to a different hashing algorithm"],correctIndex:1,explanation:"When a collision is detected (the generated short key already exists in the database), the standard approach is to append a monotonically increasing sequence number or timestamp to the original URL and re-hash. For example, if hash('example.com') = 'abc1234' and that key is taken, you try hash('example.com1'), then hash('example.com2'), etc. This is preferable to using a longer hash (which defeats the purpose of short URLs) or switching algorithms (which doesn't prevent future collisions). However, this retry approach has performance implications under high write load, which is exactly why a KGS is preferred  it eliminates collisions entirely by pre-generating guaranteed-unique keys."},{question:"What is the purpose of database sharding in a URL shortener, and what is the best sharding key?",options:["Shard by user ID to keep each user's URLs together","Shard by the hash/short key for even distribution","Shard by creation date for time-based queries","Shard by the long URL's domain"],correctIndex:1,explanation:"Sharding by the short key (hash) provides the most even distribution of data across shards because short keys are random (especially with base62 encoding or pre-generated random keys). This avoids hot spots  if you sharded by user ID, a single power user could overload one shard. Date-based sharding would create hot spots on the current day's shard. The short key is also the primary lookup key for redirects, so sharding by it means every redirect request hits exactly one shard with no scatter-gather needed. Consistent hashing is often used to map short keys to shards, allowing smooth scaling when adding or removing nodes  this is the approach used by large-scale URL shorteners."},{question:"What happens when a user requests a custom alias that already exists?",options:["Silently assign a different alias","Return an error indicating the alias is taken","Overwrite the existing alias","Add a random suffix to make it unique"],correctIndex:1,explanation:"When a custom alias collides with an existing one, the correct behavior is to return a clear error (e.g., HTTP 409 Conflict) telling the user the alias is taken and to choose another. Silently changing the alias would violate the user's intent and break their expected URL. Overwriting would be a security disaster  someone could hijack existing short URLs. Adding random suffixes defeats the purpose of a custom alias. This check requires a database lookup before creation, and for KGS-based systems, you need to also check that the custom alias doesn't conflict with pre-generated keys  some systems use separate namespaces or length conventions to avoid this (e.g., custom aliases must be 8+ characters while auto-generated are exactly 7)."},{question:"Why might a URL shortener use 301 redirects instead of 302?",options:["To reduce server load since browsers cache the redirect","To improve SEO for the short URL","To prevent analytics tracking","To support HTTPS"],correctIndex:0,explanation:"A 301 (Moved Permanently) redirect tells the browser to cache the redirect mapping locally, so subsequent clicks on the same short URL bypass the shortener's server entirely and go directly to the destination. This dramatically reduces server load and improves user experience with faster redirects. The trade-off is that you lose analytics visibility for repeat visitors. Services that prioritize performance and low infrastructure costs over detailed analytics may prefer 301. Google's goo.gl (now discontinued) originally used 301 redirects, while bit.ly uses 302 to maintain analytics  the choice depends on whether your product prioritizes speed or tracking."},{question:"How should a URL shortener handle URL expiration?",options:["Delete expired URLs immediately from the database","Use a lazy cleanup approach  check expiration on read and run periodic batch cleanup","Set database TTL and let the database handle it","Never expire URLs  storage is cheap"],correctIndex:1,explanation:"A lazy cleanup approach is most practical: when a redirect request comes in, check if the URL has expired, and if so, return a 410 Gone or 404. Separately, run a periodic batch job (e.g., nightly) to delete expired entries and reclaim keys. Immediate deletion on expiration would require a scheduled task per URL, which doesn't scale to billions of URLs. While database-level TTL (like DynamoDB's) can work, it provides less control and doesn't allow graceful error responses. The lazy approach ensures no redirect latency impact for non-expired URLs while still eventually cleaning up storage  this is similar to how Redis handles key expiration with lazy + periodic deletion strategies."},{question:"What rate limiting strategy should a URL shortener use for its creation endpoint?",options:["No rate limiting needed","Rate limit by IP address with a token bucket algorithm","Rate limit only authenticated users","Rate limit based on URL domain"],correctIndex:1,explanation:"Rate limiting the creation endpoint by IP address (for anonymous users) or API key (for authenticated users) using a token bucket algorithm is essential to prevent abuse. Without rate limiting, attackers could exhaust your key space, use your service for spam/phishing link distribution, or simply overwhelm your write infrastructure. Token bucket is preferred because it allows burst traffic (a user creating several URLs quickly) while enforcing a sustained rate limit. The redirect endpoint typically doesn't need aggressive rate limiting since it's designed for high-throughput reads. Real-world services like bit.ly enforce strict creation rate limits  free tier users get limited link creation per month, which serves both business and infrastructure protection purposes."},{question:"In a URL shortener system, what is the role of a CDN?",options:["To store the database closer to users","To cache redirect responses at edge locations for faster response times","To compress the short URLs","To generate short keys at edge locations"],correctIndex:1,explanation:"A CDN (Content Delivery Network) caches the redirect responses at edge servers distributed globally, so a user in Tokyo clicking a short URL gets the redirect response from a nearby edge server rather than a data center in Virginia. With 302 redirects, CDN caching requires careful configuration (short TTLs or using the Cache-Control header) to balance performance with analytics accuracy. With 301 redirects, the CDN can aggressively cache since the redirect is permanent. This is especially impactful for viral links that receive millions of clicks from diverse geographic locations. Services like Cloudflare or AWS CloudFront can reduce redirect latency from hundreds of milliseconds to single-digit milliseconds for cached URLs."},{question:"What is the birthday paradox and how does it relate to URL shortener design?",options:["It's about generating unique URLs for users born on the same day","It means hash collisions become probable much sooner than intuition suggests, affecting key space sizing","It's a method for generating random keys","It's related to URL expiration timing"],correctIndex:1,explanation:"The birthday paradox states that in a set of n randomly chosen values from a space of N possibilities, collisions become likely when n  N. For a URL shortener, if your key space has N possible keys, you'll likely see your first collision after approximately N URLs. For example, with a 7-character hex key (16^7  268M possibilities), collisions become probable after only ~16,000 URLs  far sooner than the intuitive 268M/2. This is why base62 with 7 characters (62^7  3.5T) is preferred  collisions become likely only after ~1.87M URLs. Understanding this paradox is critical for sizing your key space correctly and justifying the use of KGS over hash-based approaches in system design interviews."},{question:"How should a URL shortener validate the destination URL before creating a short link?",options:["Only check that it starts with http:// or https://","Validate URL format, check against malware/phishing blacklists, and optionally verify the URL is reachable","No validation needed  just store whatever the user provides","Only check URL length"],correctIndex:1,explanation:"Comprehensive URL validation involves multiple layers: format validation (proper URL syntax), protocol checking (restrict to http/https), blacklist checking against known malware/phishing databases (like Google Safe Browsing API), and optionally an HTTP HEAD request to verify the destination is reachable. This is crucial because URL shorteners are frequently abused for phishing  a legitimate-looking short URL can hide a malicious destination. Bit.ly and other major shorteners integrate with threat intelligence feeds and scan destinations periodically (not just at creation time, since a destination could become malicious later). Skipping validation exposes your service to abuse and could get your domain blacklisted by browsers and email providers."},{question:"What is the advantage of using a NoSQL database over a relational database for a URL shortener?",options:["NoSQL supports SQL queries","NoSQL provides horizontal scalability and handles the simple key-value access pattern efficiently","NoSQL is always faster than relational databases","NoSQL provides ACID transactions"],correctIndex:1,explanation:"A URL shortener's primary data access pattern is a simple key-value lookup: given a short key, return the long URL. NoSQL databases like DynamoDB or Cassandra are purpose-built for this pattern, offering O(1) lookups and linear horizontal scalability by adding more nodes. Relational databases would work but carry overhead from features like enforcing schemas, supporting complex joins, and maintaining ACID transactions  none of which are needed for a key-value mapping. NoSQL isn't 'always faster' (that's a myth), but for this specific access pattern at scale (billions of URLs, millions of reads/second), it's the more natural and cost-effective choice. Companies like Instagram and Pinterest have successfully used NoSQL stores for similar simple-mapping use cases."},{question:"How can a URL shortener track analytics without significantly impacting redirect latency?",options:["Log analytics synchronously before redirecting","Write analytics events to a message queue (like Kafka) asynchronously and process them separately","Store analytics in the same database as URL mappings","Skip analytics for high-traffic URLs"],correctIndex:1,explanation:"Writing analytics events asynchronously to a message queue like Kafka or Amazon Kinesis allows the redirect response to return immediately (within milliseconds) while analytics data is processed separately by downstream consumers. Synchronous logging would add database write latency to every redirect, directly impacting user experience. Storing analytics in the same database as URL mappings would create write contention on the read-optimized URL store. The async approach also enables rich analytics processing  a stream processor can compute real-time dashboards, aggregate by geography, referrer, device type, and time windows. This is a classic CQRS (Command Query Responsibility Segregation) pattern used by virtually all production URL shorteners."},{question:"What is consistent hashing and why is it useful for a URL shortener?",options:["It ensures all hash values are the same length","It minimizes key redistribution when adding or removing database shards","It prevents hash collisions","It ensures URLs are distributed alphabetically"],correctIndex:1,explanation:"Consistent hashing maps both keys and servers onto a hash ring, so each key is assigned to the nearest server clockwise on the ring. When a server is added or removed, only the keys that map to the affected segment of the ring need to be redistributed  typically only K/N keys (where K is total keys and N is number of servers) rather than remapping everything. For a URL shortener with billions of entries across many database shards, this means adding a new shard to handle growth only requires migrating a fraction of existing data. Without consistent hashing, adding a shard with simple modular hashing (key % N) would require redistributing nearly all keys. Virtual nodes (multiple points per server on the ring) further improve distribution uniformity."},{question:"What is the difference between a URL shortener using random key generation vs. sequential counter-based keys?",options:["Random keys are shorter than sequential ones","Random keys don't reveal information about URL creation order or volume, while sequential keys are predictable","Sequential keys are impossible to implement at scale","Random keys never collide"],correctIndex:1,explanation:"Random keys (e.g., 'xK9mQ2z') provide security through unpredictability  an attacker cannot guess other valid short URLs by incrementing a counter. Sequential keys (e.g., '0000001', '0000002') reveal business metrics (total URL count, creation rate) and allow enumeration attacks where someone scrapes all URLs by iterating through the sequence. However, sequential keys have the advantage of guaranteed uniqueness without collision checking, which is why KGS uses pre-generated random keys  combining the uniqueness guarantee of sequential generation with the unpredictability of random keys. In practice, most production URL shorteners use random or pseudo-random keys, sometimes with a distributed ID generator like Twitter's Snowflake as a seed."},{question:"How should a URL shortener handle the same long URL being shortened multiple times?",options:["Always return the same short URL (deduplication)","Always generate a new short URL each time","It depends on the product requirements  dedup saves space but prevents per-context analytics","Return an error for duplicate URLs"],correctIndex:2,explanation:"This is a product decision with engineering trade-offs. Deduplication (returning the same short URL for the same long URL) saves storage and key space but means you can't track analytics separately for each context where the URL was shared. Generating a new short URL each time uses more storage but enables per-link analytics  a marketer sharing the same article in an email vs. a tweet wants separate click counts. Most commercial shorteners like bit.ly create a new short URL each time because analytics per-link is their core value proposition. If you do want dedup, you'd need an index on the long URL column, which adds write overhead and storage for the index itself. The interviewer wants to hear you articulate this trade-off."},{question:"What is the purpose of the X-Forwarded-For header in URL shortener analytics?",options:["To forward the short URL to another server","To identify the original client IP address when the request passes through proxies or load balancers","To forward the URL to the destination","To encrypt the client's IP address"],correctIndex:1,explanation:"When a request passes through load balancers, reverse proxies, or CDN edge servers before reaching the URL shortener's application server, the source IP seen by the application is the proxy's IP, not the end user's. The X-Forwarded-For header preserves the original client IP chain, which is essential for geo-analytics (determining where clicks come from), rate limiting, and fraud detection. Without this header, all analytics would show your load balancer's IP as the source, making geographic and per-user analytics useless. In production, you must validate this header carefully  it can be spoofed by clients, so you typically trust only the rightmost IP added by your own infrastructure. This is a common detail that differentiates senior engineers in system design discussions."},{question:"Why is eventual consistency acceptable for a URL shortener's read path?",options:["Because URLs change frequently","Because a short delay in propagating new URL mappings to read replicas is tolerable  the URL will resolve correctly within seconds","Because consistency doesn't matter for any system","Because users don't notice errors"],correctIndex:1,explanation:"When a user creates a short URL, there may be a brief delay (milliseconds to seconds) before the mapping propagates to all read replicas and cache nodes. This is acceptable because users rarely create a short URL and click it within the same second  typically they share it and others click it later. The trade-off is worth it because strong consistency would require synchronous replication to all nodes before returning the creation response, significantly increasing write latency and reducing availability. This is a textbook application of the CAP theorem  for a URL shortener, we prefer availability and partition tolerance (AP) over strict consistency. In the rare case a click arrives before replication completes, a cache miss falls through to the primary database, which has the data."},{question:"How would you design the database schema for a URL shortener?",options:["Single table with columns: short_key (PK), long_url, user_id, created_at, expires_at","Normalized schema with separate tables for URLs, users, clicks, and tags","Store everything in a single JSON document","Use a graph schema with URL nodes and user edges"],correctIndex:0,explanation:"A simple single-table design with short_key as the primary key is ideal because the dominant access pattern is point lookups by short key. The table contains: short_key (partition key), long_url, user_id (who created it), created_at (timestamp), and expires_at (optional TTL). Analytics data should be stored separately (in a time-series database or data warehouse) rather than in the URL mapping table to avoid write contention. A normalized schema with joins would add unnecessary latency to the redirect hot path. Additional secondary indexes on user_id enable 'list my URLs' functionality but aren't needed for the core redirect path. This denormalized design is a perfect fit for DynamoDB or Cassandra."},{question:"What is the role of a load balancer in a URL shortener architecture?",options:["To shorten URLs faster","To distribute incoming redirect requests across multiple application servers for high availability and throughput","To cache URL mappings","To generate short keys"],correctIndex:1,explanation:"A load balancer sits between clients and the URL shortener's application servers, distributing incoming requests (both URL creation and redirect) across multiple server instances. This provides horizontal scalability (add more servers to handle more traffic), high availability (if one server dies, traffic routes to others), and even load distribution. For a URL shortener with potentially millions of redirects per second, a single server cannot handle the load. Layer 7 (application) load balancers can also perform health checks, SSL termination, and request routing. In production, you'd typically use multiple layers  a DNS-based global load balancer (like AWS Route 53) for geographic routing, and an L7 load balancer (like nginx or AWS ALB) within each region."},{question:"What is the significance of using PUT vs POST for creating short URLs in a RESTful API?",options:["PUT is faster than POST","PUT is idempotent (same request = same result), making it suitable for custom aliases, while POST is for server-generated keys","PUT supports longer URLs","There is no difference"],correctIndex:1,explanation:"In REST semantics, PUT is idempotent  sending the same PUT request multiple times produces the same result. This maps well to custom aliases: PUT /urls/my-custom-alias with a body containing the long URL always creates or updates the same mapping. POST is non-idempotent, meaning each call can create a different resource  appropriate when the server generates the short key, as each POST creates a new unique short URL. This distinction matters for retry logic: if a PUT request times out, the client can safely retry without creating duplicates. With POST, a retry might create a duplicate entry. In system design interviews, discussing this shows understanding of API design principles and their implications for distributed systems reliability."},{question:"How would you implement a URL shortener that needs to support 1 billion URLs with 10,000 redirects per second?",options:["Single powerful server with lots of RAM","Microservices architecture with separate read/write services, NoSQL storage, caching layer, and CDN","Serverless functions for each redirect","Peer-to-peer architecture"],correctIndex:1,explanation:"At this scale, you need a distributed architecture with clear separation of concerns. The write service handles URL creation through a KGS, storing mappings in a horizontally-sharded NoSQL database (like DynamoDB or Cassandra). The read service handles redirects, first checking a distributed cache layer (Redis cluster) that holds hot URLs  with a typical 80/20 distribution, caching the top 20% of URLs serves 80% of requests. A CDN layer in front handles geographically distributed traffic. 10,000 redirects/second is modest for this architecture  each Redis node can handle 100K+ operations/second. A single server would be a single point of failure and couldn't scale. Serverless has cold start issues that would add unacceptable latency to redirects. This layered approach is how real URL shorteners operate at scale."},{question:"What is URL canonicalization and why is it important for a URL shortener?",options:["Making URLs prettier","Converting equivalent URLs to a standard form to prevent duplicate entries for the same destination","Encrypting URLs for security","Compressing URLs to save space"],correctIndex:1,explanation:"URL canonicalization converts URLs to a standard form so that equivalent URLs map to the same short URL (if deduplication is desired). For example, 'HTTP://Example.COM/page' and 'http://example.com/page' are equivalent but textually different. Canonicalization includes: lowercasing the scheme and host, removing default ports (80 for HTTP, 443 for HTTPS), removing trailing slashes, sorting query parameters, and removing unnecessary fragments. Without canonicalization, the same destination could consume multiple short keys unnecessarily. However, over-aggressive canonicalization can be dangerous  removing query parameters or fragments might change the page content. The implementation should be configurable and well-tested against edge cases like URLs with authentication credentials or unusual encoding."},{question:"What security concern arises from URL shorteners and how can it be mitigated?",options:["Short URLs use too much bandwidth","URL shorteners can be used to disguise malicious/phishing URLs behind trusted-looking short domains","Short URLs expire too quickly","URL shorteners slow down page loads"],correctIndex:1,explanation:"URL shorteners fundamentally hide the destination behind a short, opaque link, making them ideal tools for phishing attacks. An attacker can shorten a malicious URL (e.g., a fake banking login page) and share the innocent-looking short link. Mitigations include: scanning destination URLs against malware/phishing blacklists (Google Safe Browsing, PhishTank) at creation time AND periodically re-scanning existing URLs, implementing preview pages (like bit.ly's '+' suffix) that show the destination before redirecting, adding interstitial warning pages for suspicious destinations, and flagging URLs with multiple reports. Some browsers and email clients also preview short URLs before following them. This is why URL shortener services bear significant responsibility for internet safety."},{question:"How does a distributed counter work for URL shortener analytics?",options:["Each server maintains its own counter independently","Use a centralized counter with locks","Approximate counting with HyperLogLog for unique visitors and append-only logs for total clicks","Counters are stored in the URL database"],correctIndex:2,explanation:"At scale, maintaining exact real-time counters for every short URL is impractical because it would require a synchronized write for every redirect. Instead, production systems use approximate counting structures like HyperLogLog for unique visitor counts (which uses only ~12KB of memory per counter regardless of cardinality) and append-only event logs (Kafka/Kinesis) for total click streams. Click events are asynchronously consumed and aggregated in batch or stream processing systems (like Apache Flink or Spark Streaming) to update dashboards. For rough real-time counts, Redis INCR commands with eventual aggregation work well. This separation of the real-time redirect path from the analytics aggregation path is a fundamental principle of scalable system design."},{question:"What is the advantage of using a Bloom filter in a URL shortener?",options:["It compresses URLs","It provides a space-efficient probabilistic check for whether a short key already exists, reducing database lookups","It encrypts URL mappings","It speeds up URL redirects"],correctIndex:1,explanation:"A Bloom filter is a probabilistic data structure that can tell you with certainty when an element is NOT in a set, and with high probability when it IS in a set (allowing small false positive rates). For a URL shortener using hash-based key generation, before checking the database for a collision, you first check the Bloom filter  if it says the key doesn't exist, you skip the database lookup entirely. This dramatically reduces database reads during URL creation. The false positive rate can be tuned by adjusting the filter size (typically a few bits per element). For 1 billion URLs, a Bloom filter with 1% false positive rate requires only about 1.2 GB of memory. However, Bloom filters cannot delete entries, which complicates URL expiration  counting Bloom filters or cuckoo filters address this limitation."},{question:"What is the purpose of using a separate read replica database in a URL shortener?",options:["To store different types of data","To handle high read throughput by distributing redirect queries across multiple database copies","To provide backup in case of data loss","To speed up write operations"],correctIndex:1,explanation:"Read replicas are copies of the primary database that handle read queries (redirect lookups) while the primary handles writes (URL creation). Given the extreme read-to-write ratio (100:1 or higher) of URL shorteners, this architecture allows you to scale read capacity independently by adding more replicas. Each replica can serve redirect queries, and a load balancer distributes read traffic across them. While replicas also provide some backup capability, that's not their primary purpose  dedicated backup solutions are separate. Write operations aren't sped up by replicas; in fact, writes may be slightly slower due to replication overhead. This is a standard pattern for read-heavy workloads and is used extensively in production systems at companies like Twitter and Pinterest."},{question:"How would you handle a database failure in a URL shortener?",options:["Show a 500 error for all requests","Use circuit breaker pattern with fallback to cache, and redirect to a status page for cache misses","Retry indefinitely until the database recovers","Queue all requests until the database comes back"],correctIndex:1,explanation:"The circuit breaker pattern detects database failures and stops sending requests to the failing database, preventing cascade failures. For redirects, the system falls back to the cache layer  if the short URL is cached (likely for hot URLs), the redirect works normally despite the database being down. For cache misses, return a friendly error page or redirect to a status page. For URL creation, requests can be queued in a message queue (like Kafka) for processing when the database recovers. Infinite retries would overwhelm the recovering database and degrade the entire system. This graceful degradation approach ensures that the majority of redirects (those hitting cache) continue working during database outages, maintaining high availability where it matters most."},{question:"What is the trade-off between using auto-increment IDs vs. random IDs for short URL keys?",options:["Auto-increment is always better because it's simpler","Auto-increment reveals business metrics and is predictable; random IDs are secure but require collision handling","Random IDs are always better because they're faster","There is no meaningful trade-off"],correctIndex:1,explanation:"Auto-increment IDs (1, 2, 3...) converted to base62 are simple and guarantee uniqueness, but they expose business information  anyone can estimate total URLs created and creation rate by decoding the latest short URL. They're also sequentially predictable, enabling enumeration attacks. Random IDs provide security through unpredictability but require collision detection (checking if the random key already exists). The KGS approach solves this by pre-generating random unique keys, giving you both unpredictability and guaranteed uniqueness. In a distributed system, auto-increment also requires coordination (like Twitter's Snowflake) to avoid duplicate IDs across servers, while random keys from KGS can be distributed to multiple servers from a shared pool."},{question:"Why is it important to URL-encode the long URL before storing it in a URL shortener?",options:["To make it shorter","To ensure special characters in the URL don't break storage, retrieval, or redirect operations","To encrypt the URL","To compress the URL"],correctIndex:1,explanation:"URLs can contain special characters (spaces, Unicode, ampersands, quotes) that could break database queries, HTTP redirect headers, or JSON API responses if not properly encoded. Storing the properly encoded form ensures the URL is faithfully preserved and can be safely included in the HTTP Location header during redirect. For example, a URL containing a space should be stored with %20 encoding. Additionally, different databases handle special characters differently  proper encoding provides a consistent representation regardless of the storage backend. However, you should also store the original (decoded) URL for display purposes, as showing '%20' instead of spaces in analytics dashboards provides a poor user experience."},{question:"What monitoring metrics are most important for a URL shortener service?",options:["Only tracking total number of URLs created","Redirect latency (p50/p95/p99), cache hit ratio, creation throughput, error rates, and database query latency","Only monitoring disk space usage","Only tracking the number of active users"],correctIndex:1,explanation:"Comprehensive monitoring is critical for a URL shortener's reliability. Redirect latency percentiles (p50, p95, p99) tell you about user experience  p99 matters because 1% of millions of daily users is still thousands of people. Cache hit ratio indicates cache effectiveness  a drop might signal a change in traffic patterns or cache issues. Creation throughput and error rates detect abuse or system problems. Database query latency detects storage layer issues before they become user-facing. Additional metrics include: KGS key pool size (to prevent running out of keys), queue depth for analytics events, and per-URL redirect rates for detecting abuse. Setting up alerts on these metrics enables proactive incident response  for example, if cache hit ratio drops below 90%, the on-call engineer investigates immediately."},{question:"How would you implement URL shortener expiration at database level in DynamoDB?",options:["Use a cron job to scan the entire table","Use DynamoDB's built-in TTL (Time-To-Live) feature which automatically deletes expired items","Manually delete items on each read","Set up a separate expiration service"],correctIndex:1,explanation:"DynamoDB's TTL feature allows you to specify an attribute containing an expiration timestamp (Unix epoch). DynamoDB automatically scans for and deletes expired items in the background at no additional cost, without consuming write capacity. This is ideal for URL shortener expiration because it requires zero application-level code for cleanup. The deletion is eventually consistent  items may persist briefly after their TTL  so your application should still check expiration on reads. A full table scan would be prohibitively expensive at scale (billions of items), and manual deletion on reads would only clean up accessed items, leaving orphaned expired URLs consuming storage. The freed-up short keys can be recycled back to the KGS pool through a DynamoDB Streams trigger that captures deletion events."},{question:"What is the role of a message queue in a URL shortener architecture?",options:["To queue redirect requests during peak traffic","To decouple URL creation from analytics processing and handle asynchronous tasks like link scanning and expiration","To store URL mappings temporarily","To communicate between frontend and backend"],correctIndex:1,explanation:"A message queue (like Kafka, RabbitMQ, or SQS) serves multiple asynchronous purposes in a URL shortener. First, click analytics: every redirect publishes an event to the queue, and separate analytics consumers aggregate the data without impacting redirect latency. Second, link safety scanning: newly created URLs are queued for asynchronous malware/phishing analysis, so the creation API returns quickly while scanning happens in the background. Third, expired URL cleanup: expiration events trigger key recycling back to the KGS pool. This decoupling is fundamental to building scalable systems  the synchronous redirect path stays fast and simple while complex processing happens asynchronously. Without a message queue, you'd need to perform all these operations synchronously, dramatically increasing latency and coupling between components."},{question:"How do you handle the 'thundering herd' problem for a popular short URL?",options:["Block all requests until one completes","Use request coalescing (single-flight) where concurrent requests for the same key share one database lookup","Increase database capacity","Disable the URL temporarily"],correctIndex:1,explanation:"When a viral short URL's cache entry expires, hundreds of concurrent requests might simultaneously miss the cache and hit the database  this is the 'thundering herd' or 'cache stampede' problem. Request coalescing (also called single-flight or request collapsing) ensures that only the first request actually queries the database, while all concurrent requests for the same key wait for and share that single result. Libraries like Go's singleflight or custom implementations with distributed locks (Redis SETNX) achieve this. Additional mitigations include: staggered cache TTLs (adding random jitter to prevent synchronized expiration), cache-aside with early refresh (refreshing entries before they expire), and background cache warming for known hot URLs. This is a critical concept that shows deep understanding of caching challenges in system design interviews."},{question:"What is the CAP theorem trade-off for a URL shortener?",options:["URL shorteners need strong consistency above all","URL shorteners typically choose AP (Availability + Partition Tolerance) with eventual consistency, since brief stale reads are acceptable","URL shorteners need CP (Consistency + Partition Tolerance)","The CAP theorem doesn't apply to URL shorteners"],correctIndex:1,explanation:"The CAP theorem states that during a network partition, a distributed system must choose between Consistency (all nodes see the same data) and Availability (every request gets a response). For a URL shortener, availability is paramount  returning a 503 error for a redirect is a worse user experience than a brief period where a newly-created URL isn't yet resolvable on all nodes. Eventual consistency means that after a new URL is created, it might take milliseconds to seconds for the mapping to propagate to all read replicas and cache nodes, which is acceptable because users rarely click a short URL within milliseconds of creating it. This AP choice aligns with DynamoDB's and Cassandra's default modes, which is another reason these databases are popular choices for URL shorteners."},{question:"How should a URL shortener handle internationalized (non-ASCII) long URLs?",options:["Reject non-ASCII URLs","Accept and store them with proper percent-encoding (Punycode for domains, UTF-8 percent-encoding for paths)","Convert them to ASCII by removing non-ASCII characters","Store them as-is without any encoding"],correctIndex:1,explanation:"Internationalized URLs contain non-ASCII characters in domain names (IDN) and paths. The correct approach is to convert domain names using Punycode (e.g., 'mnchen.de'  'xn--mnchen-3ya.de') and percent-encode path segments using UTF-8 encoding (e.g., ''  '%D0%BF%D1%83%D1%82%D1%8C'). This produces a valid ASCII URL that browsers and HTTP clients can handle. Rejecting non-ASCII URLs would exclude a huge portion of the international web. Storing them as-is could cause issues with HTTP headers (which must be ASCII) and database encoding. The display URL shown to users should preserve the original Unicode characters for readability, while the stored/redirected URL uses the encoded form. This internationalization support is increasingly important as internet usage grows globally."},{question:"What happens if the Key Generation Service (KGS) becomes unavailable?",options:["All URL creation stops immediately","Pre-fetched key batches on application servers allow URL creation to continue temporarily until KGS recovers or local batches are exhausted","The system switches to hash-based generation as a fallback","Nothing  KGS is not a critical service"],correctIndex:1,explanation:"To avoid KGS being a single point of failure, application servers pre-fetch batches of keys from KGS (e.g., 1000 keys at a time) and store them in local memory. When KGS goes down, servers continue creating URLs using their local key batches, buying time for KGS recovery. Once local batches are exhausted, the system can either fail gracefully with a 503 or fall back to a hash-based approach with collision detection. For high availability, KGS itself should be replicated with a primary-standby setup and use a persistent database (not just in-memory) for the unused key pool. The key batch size represents a trade-off: larger batches provide more buffer during outages but mean more wasted keys if a server crashes before using its batch."},{question:"Why is it important to log the User-Agent header for URL shortener analytics?",options:["To verify the URL format","To identify the device type, browser, and OS of users clicking short URLs, enabling device-specific analytics","To authenticate users","To speed up redirects"],correctIndex:1,explanation:"The User-Agent header contains information about the client's browser, operating system, and device type (mobile, desktop, bot). For URL shortener analytics, this data enables: device breakdown charts (60% mobile, 35% desktop, 5% tablet), browser statistics, OS distribution, and critically, bot detection  distinguishing real human clicks from automated crawlers and link preview generators (like Facebook's, Twitter's, and Slack's preview bots). Without filtering bot traffic, click analytics would be severely inflated and misleading. Modern User-Agent parsing libraries (like ua-parser) can extract structured device information from the raw header string. This granular analytics is often the premium feature that URL shortener services monetize."},{question:"How would you design the URL shortener's API for creating short URLs?",options:["GET /create?url=<long_url>","POST /api/v1/urls with JSON body containing the long URL, optional custom alias, and expiration","PUT /shorten/<long_url>","DELETE /api/v1/urls"],correctIndex:1,explanation:"A well-designed REST API uses POST for resource creation (since each call creates a new short URL), versioned endpoints (/api/v1/) for backward compatibility, and a JSON request body for structured parameters. The body should include: 'long_url' (required), 'custom_alias' (optional), 'expires_at' (optional), and any metadata. The response should return the short URL, creation timestamp, and expiration details. Using GET for creation is an anti-pattern because GET should be idempotent and safe (no side effects), and URLs have length limits. API versioning prevents breaking existing clients when you evolve the API. Authentication via API key or OAuth token in the Authorization header enables per-user rate limiting and analytics. This RESTful design is what interviewers expect in system design discussions."},{question:"What is the benefit of using Redis as a caching layer for a URL shortener?",options:["Redis supports complex SQL queries","Redis provides sub-millisecond read latency, built-in TTL support, and data structures like HyperLogLog for analytics","Redis automatically scales horizontally without configuration","Redis provides strong ACID guarantees"],correctIndex:1,explanation:"Redis is an in-memory data store that provides sub-millisecond read latency  critical for URL shortener redirects where every millisecond matters. Its built-in TTL (Time-To-Live) support naturally handles cache expiration, and it supports data structures beyond simple key-value: HyperLogLog for approximate unique visitor counting (using only 12KB per counter), sorted sets for top-URL leaderboards, and bitmaps for daily active URL tracking. Redis Cluster provides horizontal scaling through hash-slot-based sharding. While Redis doesn't 'automatically' scale (it requires cluster configuration), it's operationally simpler than many alternatives. Combined with the URL shortener's simple key-value access pattern, Redis achieves cache hit response times of 0.1-0.5ms, compared to 5-20ms for database lookups. This order-of-magnitude improvement is why Redis is ubiquitous in URL shortener architectures."},{question:"How would you implement a 'link preview' feature for a URL shortener?",options:["Scrape the destination page in real-time when the short URL is accessed","Pre-fetch and cache Open Graph metadata (title, description, image) when the short URL is created, with periodic refresh","Embed the destination page in an iframe","Ask the user to provide preview information manually"],correctIndex:1,explanation:"Pre-fetching Open Graph (og:title, og:description, og:image) and Twitter Card metadata at URL creation time provides instant link previews without impacting redirect latency. The metadata is stored alongside the URL mapping and served via an API endpoint (e.g., GET /api/v1/urls/{key}/preview). Periodic background refreshes ensure metadata stays current if the destination page changes. Real-time scraping at access time would add seconds of latency and create a DoS vector. Iframes have cross-origin security restrictions and poor mobile experience. Requiring manual input adds friction to URL creation. This pre-fetch approach is exactly how Slack, Discord, and social media platforms generate link previews  they fetch the destination's metadata and cache it for fast display."},{question:"What is the difference between URL shortening and URL aliasing?",options:["They are exactly the same thing","Shortening generates a random/auto key; aliasing lets users choose a custom, meaningful short path (e.g., bit.ly/my-brand)","Aliasing only works with certain URLs","Shortening is faster than aliasing"],correctIndex:1,explanation:"URL shortening generates an opaque, system-assigned key (like 'xK9mQ2z') optimized for brevity, while URL aliasing (also called custom short links or vanity URLs) lets users choose a meaningful, branded path (like 'bit.ly/summer-sale-2024'). Aliasing requires additional validation: checking that the custom alias is available, doesn't conflict with reserved words or system paths, meets length/character requirements, and isn't offensive. Custom aliases are a premium feature in most URL shortener business models because they provide brand value. The technical implementation differs too: auto-generated keys come from KGS, while custom aliases require a real-time uniqueness check against the database. Both map to the same underlying redirect mechanism, but the creation paths are distinct."},{question:"How would you prevent a URL shortener from being used as an open redirect vulnerability?",options:["Only allow URLs from a whitelist of approved domains","Validate destination URLs, implement abuse detection, add warning interstitials for suspicious URLs, and support user reporting","Block all external URLs","Require CAPTCHA for every redirect"],correctIndex:1,explanation:"Open redirect vulnerabilities occur when an attacker uses your trusted short URL domain to redirect users to malicious sites, bypassing security filters that trust your domain. A multi-layered defense includes: URL validation at creation time (checking against phishing/malware databases), real-time abuse detection (monitoring for patterns like bulk creation of URLs pointing to the same suspicious domain), interstitial warning pages for URLs flagged as potentially dangerous, and a user reporting mechanism for URLs that bypass automated detection. Domain whitelisting is too restrictive for a general-purpose shortener, and CAPTCHAs on redirects would destroy user experience. This defense-in-depth approach is standard practice  Google, Microsoft, and other major shortener operators continuously scan and block millions of malicious URLs."},{question:"In a URL shortener, what is a 'warm-up' cache strategy?",options:["Heating up the server hardware","Pre-loading frequently accessed URL mappings into cache before traffic arrives, typically after a deployment or cache flush","Gradually increasing rate limits","Slowly starting the application server"],correctIndex:1,explanation:"Cache warm-up involves pre-populating the cache with known hot URL mappings before the cache starts receiving production traffic. This is critical after events like: deploying a new cache cluster, recovering from a cache failure, or performing cache maintenance. Without warm-up, the cold cache would cause all requests to hit the database simultaneously (a thundering herd), potentially overwhelming it. Warm-up strategies include: replaying recent access logs to identify hot keys, querying the database for the most-accessed URLs, or gradually shifting traffic to the new cache while the old one is still serving. Some systems maintain a 'hot key list' specifically for this purpose. Cache warm-up is an operational detail that demonstrates production experience in system design interviews."},{question:"What is the advantage of using a CDN with anycast for a URL shortener?",options:["It makes URLs shorter","Anycast routes users to the nearest CDN edge server automatically via BGP, minimizing redirect latency globally","It prevents DDoS attacks","It compresses URL data"],correctIndex:1,explanation:"Anycast is a network routing technique where multiple servers share the same IP address, and BGP routing directs each client to the topologically nearest server. For a URL shortener CDN, this means a user in Sydney automatically reaches an edge server in Australia rather than a server in the US, reducing redirect latency from ~200ms to ~5ms. Unlike DNS-based geographic routing which has TTL-based delays, anycast routing is immediate and transparent. Additionally, anycast provides natural DDoS resilience by distributing attack traffic across all edge locations. While DDoS protection is a side benefit, the primary advantage is latency reduction. Cloudflare and other major CDN providers use anycast extensively, and URL shortener services running on these CDNs automatically benefit from this routing optimization."},{question:"How do you handle database migration or schema changes in a running URL shortener service?",options:["Shut down the service and perform the migration","Use online schema migration tools (like pt-online-schema-change or gh-ost) with blue-green or rolling deployments","Create a new database and copy all data","Schema changes are not needed for URL shorteners"],correctIndex:1,explanation:"Zero-downtime schema migrations are essential for a URL shortener serving millions of requests per second. Tools like gh-ost (GitHub's online schema tool) or pt-online-schema-change create a shadow copy of the table, apply the schema change, copy data incrementally, and swap tables atomically. For NoSQL databases, schema evolution is handled at the application level with backward-compatible changes (adding optional fields, handling missing fields gracefully). Blue-green deployments maintain two production environments and switch traffic after verification. Shutting down is unacceptable for a service that's part of the web's redirect infrastructure  a major shortener outage breaks millions of links across the internet. This operational awareness is what distinguishes senior engineers in system design interviews."},{question:"What is the impact of HTTPS on URL shortener performance?",options:["HTTPS has no impact on performance","TLS handshakes add latency to each new connection, but connection reuse and TLS 1.3 minimize the overhead","HTTPS makes URLs shorter","HTTPS prevents URL shortening"],correctIndex:1,explanation:"HTTPS requires a TLS handshake before any data is exchanged, adding 1-2 round trips (with TLS 1.2) or 1 round trip (with TLS 1.3's 0-RTT) to each new connection. For a URL shortener where each redirect is often a new connection, this overhead is significant  potentially adding 50-200ms per redirect depending on geographic distance. Mitigations include: TLS 1.3 with 0-RTT resumption, TLS session tickets for returning visitors, HTTP/2 connection pooling, and CDN edge termination (which terminates TLS close to the user). Despite the overhead, HTTPS is mandatory for modern URL shorteners  browsers increasingly warn about HTTP-only sites, and HTTPS prevents intermediate network manipulation of redirects. Certificate management at scale (via Let's Encrypt or AWS ACM) is also an operational consideration."},{question:"How would you design a URL shortener to support A/B testing with split redirects?",options:["Create two separate short URLs","Allow a single short URL to redirect to different destinations based on configurable traffic percentages","Randomly change the destination URL","Use cookies to track A/B assignments"],correctIndex:1,explanation:"Split redirect (also called split URL testing) allows a single short URL to redirect different percentages of traffic to different destination URLs. For example, 50% of clicks on 'sho.rt/campaign' go to page A and 50% to page B. Implementation requires: storing multiple destination URLs with traffic weights per short key, a deterministic assignment mechanism (hashing the request IP or generating a random number per request), and analytics that track conversions per variant. This is a powerful marketing feature  marketers can test different landing pages without creating multiple short URLs. Cookie-based tracking helps maintain consistent assignment for returning users but requires the redirect response to set a cookie before redirecting, adding complexity. This feature differentiates premium URL shortener services from basic ones."},{question:"What are the implications of GDPR on URL shortener design?",options:["GDPR doesn't apply to URL shorteners","URL shorteners must handle personal data (IP addresses, analytics) with consent, retention limits, and data deletion capabilities","GDPR only requires HTTPS","GDPR requires URLs to be encrypted"],correctIndex:1,explanation:"GDPR considers IP addresses and device information as personal data, which URL shorteners collect during redirects for analytics. Compliance requires: a clear privacy policy explaining what data is collected and why, a legal basis for processing (typically legitimate interest for basic functionality, consent for detailed analytics), data retention limits (not storing click data indefinitely), data deletion capabilities (right to erasure  users can request deletion of their data), and data processing agreements with third-party analytics providers. For EU users, you may need to avoid logging IP addresses or anonymize them (truncating the last octet). The 'right to be forgotten' means your analytics pipeline must support deleting specific user data retroactively. These privacy requirements significantly impact the analytics architecture and should be discussed in system design interviews."},{question:"How would you implement URL shortener high availability across multiple data centers?",options:["Run everything in one data center with backups","Deploy in multiple regions with active-active configuration, cross-region database replication, and global DNS routing","Use a single database shared across regions","Only replicate the cache layer across regions"],correctIndex:1,explanation:"Multi-region active-active deployment ensures the URL shortener remains available even if an entire data center or region fails. Each region runs a complete stack: load balancers, application servers, cache layer, and database nodes. Cross-region database replication (using Cassandra's multi-DC replication or DynamoDB Global Tables) keeps URL mappings synchronized. Global DNS routing (AWS Route 53 latency-based routing or Cloudflare's anycast) directs users to the nearest healthy region. Write conflicts are handled with last-writer-wins or vector clocks depending on the database. This architecture provides both high availability (surviving regional outages) and low latency (serving users from nearby regions). A single data center, no matter how well-provisioned, represents a single point of failure  natural disasters, network partitions, or power outages can take down an entire region."},{question:"What is the purpose of the HTTP 307 status code in URL shortening?",options:["It means the URL was not found","It's a temporary redirect that preserves the HTTP method (POST stays POST), unlike 302 which may change POST to GET","It's a permanent redirect like 301","It means the server is unavailable"],correctIndex:1,explanation:"HTTP 307 (Temporary Redirect) is similar to 302 but with a crucial difference: 307 guarantees that the HTTP method and body are preserved across the redirect. With 302, browsers historically changed POST requests to GET requests when following the redirect (despite the spec saying they shouldn't). For a URL shortener, this distinction matters if short URLs are used for API endpoints or form submissions where the POST method must be preserved. For simple click redirects (GET requests), 302 and 307 behave identically. The 308 status code similarly preserves the method but for permanent redirects (equivalent to 301). Understanding these subtle HTTP semantics demonstrates deep web protocol knowledge in system design interviews."},{question:"How would you implement rate limiting for the URL shortener's redirect endpoint?",options:["Apply the same strict limits as the creation endpoint","Use lenient limits (e.g., 1000 requests/minute/IP) to protect against DDoS while allowing normal viral traffic patterns","No rate limiting needed for redirects","Block IPs after 10 redirects"],correctIndex:1,explanation:"The redirect endpoint needs rate limiting but with much higher thresholds than the creation endpoint because legitimate use cases include: a popular link being clicked thousands of times per minute from the same corporate proxy IP, automated monitoring checking link health, and social media platforms generating link previews. Overly aggressive limits would block legitimate traffic and degrade the service's core value proposition. A reasonable approach uses tiered rate limiting: lenient per-IP limits (e.g., 1000/min) with stricter per-URL limits if a single URL receives suspicious traffic patterns (like exactly 100 requests/second from different IPs  a botnet pattern). DDoS protection should be handled at the infrastructure layer (CDN/WAF) rather than the application layer for the redirect path, as the goal is to keep redirect latency minimal."},{question:"Why might you choose Cassandra over MongoDB for a URL shortener's primary database?",options:["Cassandra supports SQL queries","Cassandra provides linear horizontal scalability, tunable consistency, and no single point of failure, ideal for the simple partition-key lookup pattern","MongoDB is not a NoSQL database","Cassandra is always faster for all workloads"],correctIndex:1,explanation:"Cassandra excels at the URL shortener's access pattern for several reasons: its partition-key-based architecture maps perfectly to short-key lookups (O(1) reads), it scales linearly by adding nodes (no master node bottleneck), it supports multi-datacenter replication natively, and it offers tunable consistency (use ONE for fast reads, QUORUM for consistent writes). MongoDB would work but uses a primary-secondary architecture that creates a single point of failure for writes and requires more complex sharding configuration. Cassandra's ring-based architecture distributes data evenly and handles node failures gracefully  any node can serve any request. However, Cassandra's write-optimized LSM-tree storage is overkill for a read-heavy URL shortener, and it lacks the rich query capabilities of MongoDB, which aren't needed for this use case anyway."},{question:"What is the difference between URL shortener keys generated with counter-based vs. random approaches in a distributed system?",options:["Counter-based is impossible in distributed systems","Counter-based requires coordination between servers (like Snowflake IDs) to avoid duplicates, while random approaches need collision detection but require no coordination","Random approaches never produce duplicates","Counter-based is always preferred"],correctIndex:1,explanation:"In a distributed system, a simple auto-increment counter doesn't work because multiple servers would generate the same numbers. Coordination mechanisms like Twitter's Snowflake solve this by combining: a timestamp, a worker ID (unique per server), and a per-server sequence number  guaranteeing globally unique IDs without inter-server communication. The trade-off is that Snowflake IDs are 64-bit (12+ characters in base62)  longer than desired for a URL shortener. Random approaches (random 7-character base62 strings) produce shorter keys but require collision detection (check if key exists before using it). KGS bridges both worlds by pre-generating random unique keys centrally. The key insight for interviews: understand the coordination vs. collision trade-off and why KGS is the elegant solution that avoids both problems."},{question:"How does a URL shortener handle the 'hot key' problem in a sharded database?",options:["Ignore it  all keys are accessed equally","Replicate hot keys across multiple shards and use caching to absorb the majority of reads for popular URLs","Move hot keys to a separate database","Delete hot keys after they become too popular"],correctIndex:1,explanation:"In a sharded database, a viral URL can create a 'hot key'  a single shard receiving disproportionate traffic while others sit idle. The primary defense is caching: a Redis cluster absorbs the vast majority of reads for hot URLs, ensuring most requests never reach the database. For extreme cases (a URL going viral on the homepage of Reddit), you can: replicate the hot key's data across multiple cache nodes (read from any), use client-side caching on application servers, or prepend the cache key with a random prefix (e.g., 'shard-1-abc123', 'shard-2-abc123') to distribute the key across multiple cache nodes. This is the same problem Twitter faces with celebrity tweets and Instagram faces with popular accounts  caching with key replication is the universal solution."},{question:"What encryption should a URL shortener use for storing URLs at rest?",options:["No encryption needed  URLs are public data","Encrypt sensitive metadata (user IDs, analytics data) but URL mappings themselves may not need encryption since the URLs are publicly accessible","Encrypt everything with AES-256","Use homomorphic encryption"],correctIndex:1,explanation:"This is a nuanced question. The URL mappings themselves (short key  long URL) are essentially public data  anyone with the short URL can discover the long URL by clicking it. However, associated metadata deserves encryption: user account information, detailed analytics (who clicked what, when, from where), API keys, and any personally identifiable information. For compliance-sensitive deployments (healthcare, finance), encrypting everything at rest with AES-256 via database-level encryption (like DynamoDB's or RDS's encryption at rest) provides defense-in-depth with minimal performance impact. The key point for interviews: don't apply a one-size-fits-all approach. Threat model first, then decide what needs protection. Encrypting everything adds operational complexity (key management, rotation) that should be justified by actual security requirements."},{question:"How would you implement URL shortener search functionality (e.g., searching through created URLs)?",options:["Full table scan on every search","Use Elasticsearch or similar search engine indexed on long URL, custom alias, and creation metadata","Store URLs in alphabetical order","Use LIKE queries on the primary database"],correctIndex:1,explanation:"Full-text search across millions of URL mappings requires a dedicated search engine like Elasticsearch or Apache Solr. When a URL is created, an event is published to index the mapping in Elasticsearch, including: the long URL, custom alias (if any), creation timestamp, user ID, and any tags/metadata. This enables searching by destination domain, partial URL matches, tags, and creation date ranges. LIKE queries on the primary database (NoSQL or SQL) would be catastrophically slow at scale and could impact production traffic. Elasticsearch provides full-text search, fuzzy matching, and aggregations with sub-second response times even over billions of documents. This is a classic polyglot persistence pattern  use the right storage engine for each access pattern: key-value store for redirects, Elasticsearch for search, time-series for analytics."},{question:"What is base62 encoding?",options:["An encoding that uses 62 characters: a-z, A-Z, 0-9","An encoding that uses 62 bits","A compression algorithm","A hashing algorithm"],correctIndex:0,explanation:"Base62 encoding maps numbers to a 62-character alphabet consisting of lowercase letters (a-z, 26 chars), uppercase letters (A-Z, 26 chars), and digits (0-9, 10 chars), totaling 62 characters. Unlike base64, base62 excludes '+' and '/' which are not URL-safe, making it ideal for URL shortener keys. A numeric ID like 125 would be converted by repeatedly dividing by 62 and mapping remainders to characters. This encoding is reversible  given a base62 string, you can decode it back to the original number, which is useful when the short key is derived from a database auto-increment ID."},{question:"What is the main bottleneck in a URL shortener system?",options:["CPU processing","Database reads during redirect operations at peak traffic","Network bandwidth","Disk storage"],correctIndex:1,explanation:"The primary bottleneck is the database read path during redirects, especially at peak traffic. Each redirect requires looking up the short key to find the long URL, and at millions of redirects per second, even fast databases struggle. This is why caching is the single most important optimization  a well-tuned Redis cache with high hit ratios can reduce database load by 95%+. CPU is minimal (just a lookup and redirect), network bandwidth is small (redirect responses are tiny), and disk storage grows slowly. The read-heavy workload pattern (100:1 read-to-write ratio) means optimizing the read path has 100x more impact than optimizing writes."},{question:"What is the purpose of a 410 Gone HTTP response in a URL shortener?",options:["The server is temporarily unavailable","It indicates the short URL existed but has been permanently removed or expired","The URL was redirected","Authentication is required"],correctIndex:1,explanation:"HTTP 410 (Gone) explicitly tells clients and search engines that the resource existed previously but has been intentionally and permanently removed. This is more informative than a 404 (Not Found), which doesn't distinguish between 'never existed' and 'existed but was deleted.' For expired or manually deleted short URLs, 410 is semantically correct and helps search engines remove the URL from their index faster. This matters for SEO  if someone published the short URL and it's been shared widely, search engines need to know to stop indexing it. Returning 404 might cause search engines to keep retrying, while 410 is a clear signal to give up."},{question:"How many database reads does a single redirect operation require in an optimized URL shortener?",options:["At least 3 reads (key lookup, user lookup, analytics lookup)","One read  a single key-value lookup by the short key, or zero if cached","At least 5 reads including authentication","It varies based on URL length"],correctIndex:1,explanation:"In an optimized URL shortener, the redirect path involves a single key-value lookup: given the short key, retrieve the long URL. If the result is cached in Redis, it's zero database reads. The operation is O(1) in both the cache and a key-value database. There's no need to look up user information (that's only for the management API), no authentication required (short URLs are public), and analytics are written asynchronously rather than read. This simplicity is by design  the redirect path must be as fast as possible, and every additional lookup adds latency. This is why the data model should be denormalized for the redirect use case, with all necessary redirect information in a single record."},{question:"What is the purpose of the Location header in a URL shortener redirect response?",options:["It specifies the server's geographic location","It contains the destination URL that the browser should navigate to","It contains the short URL","It specifies the cache location"],correctIndex:1,explanation:"The HTTP Location header is a required component of 3xx redirect responses. When a URL shortener returns a 301 or 302 response, the Location header contains the full destination URL (the original long URL). The browser reads this header and automatically navigates to the specified URL. Without the Location header, the browser wouldn't know where to redirect. The response body is typically empty or contains a small HTML page with a meta refresh tag as a fallback for older clients that don't handle redirects properly. The Location header must contain a valid, absolute URL  relative URLs are technically allowed by HTTP/1.1 but can cause issues with some clients."},{question:"What is the impact of using a SQL database with indexes for a URL shortener at scale?",options:["SQL databases are always better than NoSQL","SQL databases work but B-tree indexes add write overhead and JOIN capabilities go unused for the simple key-value pattern","SQL databases cannot handle URL shortener workloads","SQL databases don't support indexing on string columns"],correctIndex:1,explanation:"SQL databases like PostgreSQL or MySQL can handle URL shortener workloads at moderate scale (millions of URLs). However, B-tree indexes on the short_key column add write amplification (each insert updates the index), and the relational features (JOINs, foreign keys, transactions) add overhead without providing value for the simple key-value access pattern. At billions of URLs with thousands of reads per second, SQL database sharding becomes complex (no native support in most SQL databases) compared to NoSQL databases that shard automatically. That said, if your scale doesn't require billions of entries, PostgreSQL with proper indexing and connection pooling (PgBouncer) is a perfectly valid choice  over-engineering with NoSQL for a small-scale service is equally problematic."},{question:"How does DNS resolution affect URL shortener latency?",options:["DNS has no impact on URL shortener latency","Each new connection requires DNS resolution of the short URL domain, adding 10-100ms that can be mitigated with low TTLs and DNS prefetching","DNS only affects the creation of short URLs","DNS resolution is handled by the database"],correctIndex:1,explanation:"Before a browser can connect to your URL shortener server, it must resolve the short URL domain name to an IP address via DNS. This DNS lookup adds 10-100ms depending on whether the result is cached in the user's DNS resolver. For a URL shortener where speed is critical, DNS optimization matters: use low but reasonable TTL values (300 seconds), ensure DNS records are available from multiple authoritative nameservers, and use a DNS provider with global anycast (like Cloudflare or Route 53). Websites can also add DNS prefetch hints (<link rel='dns-prefetch'>) for their short URL domains. After the first resolution, DNS caching makes subsequent lookups near-instant  but the first click on a short URL from a new device always pays the DNS tax."},{question:"What is the maximum URL length that a URL shortener should accept?",options:["No limit","A practical limit of 2,000-8,000 characters, matching browser and server constraints","Exactly 255 characters","Exactly 100 characters"],correctIndex:1,explanation:"While HTTP doesn't define a maximum URL length, practical limits exist: Internet Explorer historically supported only 2,083 characters, most modern browsers support 8,000-65,000 characters, and many web servers default to 8KB maximum header size (which includes the URL). A URL shortener should enforce a reasonable limit (e.g., 2,048 or 8,192 characters) that works across all major browsers and servers. Accepting unlimited-length URLs could be exploited for storage abuse or buffer overflow attacks. The limit should be documented in the API specification and enforced with a clear 414 (URI Too Long) error response. In practice, URLs longer than 2,000 characters are rare and often indicate encoding issues or tracking parameter bloat."},{question:"What is the role of connection pooling in a URL shortener's database layer?",options:["It pools multiple databases together","It reuses database connections across requests to avoid the overhead of creating new connections for each redirect","It compresses database queries","It caches query results"],correctIndex:1,explanation:"Creating a new database connection for each request involves TCP handshake, authentication, and SSL negotiation  adding 5-50ms of overhead per request. Connection pooling (using tools like PgBouncer for PostgreSQL, or built-in pools in application frameworks) maintains a pool of pre-established connections that are reused across requests. For a URL shortener handling thousands of redirects per second, this eliminates connection creation overhead entirely. The pool size should be tuned based on the database's max_connections setting and the number of application server instances. A common mistake is setting pool size too high, which can overwhelm the database with too many concurrent connections. A good starting point is: pool_size = (database_max_connections - reserved) / number_of_app_servers."},{question:"How should a URL shortener handle concurrent requests to create the same custom alias?",options:["Allow both requests to succeed","Use database-level unique constraints and optimistic locking  the first request succeeds, subsequent ones get a conflict error","Queue all requests and process them sequentially","Ignore the conflict and overwrite"],correctIndex:1,explanation:"Database-level unique constraints on the custom alias column provide atomic conflict detection: when two concurrent requests try to insert the same alias, the database guarantees exactly one succeeds while the other receives a unique constraint violation. The application catches this error and returns HTTP 409 (Conflict) to the losing request. This approach is more efficient than pessimistic locking (which would serialize all creation requests) or application-level checking (which has a race condition between check and insert). Optimistic concurrency is the standard pattern for handling concurrent writes in web applications. For distributed databases without unique constraints (like Cassandra), you can use lightweight transactions (IF NOT EXISTS) to achieve the same atomicity, though with higher latency."},{question:"What is the purpose of a WAF (Web Application Firewall) for a URL shortener?",options:["To shorten URLs faster","To protect against common web attacks like SQL injection, XSS, DDoS, and bot abuse targeting the shortener's API","To compress HTTP responses","To cache URL mappings"],correctIndex:1,explanation:"A WAF sits in front of the URL shortener's API and inspects incoming requests for malicious patterns. It protects against: SQL injection in URL parameters (even with NoSQL, injection attacks exist), cross-site scripting (XSS) in custom alias fields, DDoS attacks by rate-limiting at the edge, bot abuse (automated bulk URL creation for spam), and request smuggling attacks. Cloud WAF services (like Cloudflare WAF or AWS WAF) can also enforce geographic restrictions, block known malicious IPs, and implement custom rules (e.g., blocking requests containing known phishing domains). For a URL shortener that's publicly accessible on the internet, a WAF is a critical security layer that handles threats before they reach the application code."},{question:"How do you test a URL shortener system for correctness and performance?",options:["Only manual testing is needed","Unit tests for encoding logic, integration tests for redirect flows, load tests simulating realistic traffic patterns, and chaos engineering for resilience","Only load testing matters","Testing isn't important for URL shorteners"],correctIndex:1,explanation:"A comprehensive testing strategy includes multiple layers: unit tests verify base62 encoding/decoding, URL validation, and key generation logic. Integration tests verify the full redirect flow (create  redirect  analytics), custom alias creation, expiration behavior, and error handling. Load tests using tools like k6, Locust, or Gatling simulate realistic traffic patterns  high read-to-write ratios, burst traffic during viral events, and geographic distribution. Chaos engineering (killing cache nodes, simulating database failures, introducing network latency) verifies graceful degradation. Performance benchmarks establish baselines for redirect latency (target: <50ms p99), cache hit ratios (target: >95%), and throughput (target: sustain 10x average load). This multi-layered approach catches bugs at every level and gives confidence in the system's behavior under stress."},{question:"What is the 'stampede' problem when a cache entry for a popular URL expires?",options:["Users rush to create the same short URL simultaneously","Many concurrent requests miss the expired cache entry and simultaneously query the database, potentially overwhelming it","The database runs out of storage","Short URLs expire too quickly"],correctIndex:1,explanation:"Cache stampede occurs when a cached entry expires and many concurrent requests for the same key arrive before the cache is repopulated. All requests find a cache miss and query the database simultaneously. For a viral short URL with 10,000 requests/second, a cache expiry could instantly send 10,000 queries to the database for the same key. Mitigations include: probabilistic early expiration (each request has a small random chance of refreshing the cache before TTL expires), distributed locking (only one request fetches from DB while others wait for the cache to be repopulated), and staggered TTLs (adding random jitter to TTL values so entries don't expire simultaneously). This is also known as the 'thundering herd' problem and is a critical concern for any read-heavy cached system."},{question:"How does a URL shortener handle Unicode/emoji in custom aliases?",options:["Reject all non-ASCII characters","Allow Unicode with proper percent-encoding, or use Punycode-like encoding to support emoji/international aliases in a URL-safe way","Convert to ASCII by removing special characters","Store as-is without encoding"],correctIndex:1,explanation:"Supporting Unicode custom aliases (like 'bit.ly/party' or 'bit.ly/') requires careful handling. The alias must be percent-encoded for the actual URL (emoji  %F0%9F%8E%89...) but displayed in its readable form in the UI. You need to normalize Unicode (NFC normalization) to prevent visually-identical-but-different-byte-sequence aliases from coexisting. Security concerns include homograph attacks (using Cyrillic '' that looks like Latin 'a'). Some shorteners restrict aliases to ASCII to avoid these complexities, while others embrace Unicode as a differentiating feature. If supporting Unicode, validate against confusable characters, normalize consistently, and store both the display form and the encoded form."},{question:"What is the benefit of using HTTP/2 for a URL shortener's server?",options:["HTTP/2 creates shorter URLs","HTTP/2's multiplexing, header compression, and server push can reduce connection overhead for clients making multiple requests","HTTP/2 is required for redirects","HTTP/2 encrypts URLs"],correctIndex:1,explanation:"HTTP/2 provides several performance benefits for URL shorteners: header compression (HPACK) reduces the overhead of repeated headers across requests, which matters when a page contains multiple short URLs. Multiplexing allows multiple redirect requests over a single TCP connection, avoiding head-of-line blocking. For the shortener's API (creating URLs, checking analytics), HTTP/2's binary framing and multiplexing reduce latency compared to HTTP/1.1's sequential request processing. While a single redirect doesn't benefit much from multiplexing, analytics API endpoints returning multiple data streams do. Most modern CDNs and load balancers support HTTP/2 automatically. The practical impact is modest for simple redirect operations but meaningful for API-heavy interactions."},{question:"What is the trade-off between storing analytics data in the same database as URL mappings vs. a separate analytics store?",options:["Always store everything together for simplicity","Separating analytics from URL mappings prevents write-heavy analytics from degrading read-heavy redirect performance","Analytics data should never be stored","The same database is always better for consistency"],correctIndex:1,explanation:"URL mappings are read-heavy (millions of lookups/second) while analytics data is write-heavy (every redirect generates an analytics event). Storing both in the same database creates resource contention  analytics writes compete with redirect reads for I/O, CPU, and memory. Separating them allows each database to be optimized for its workload: the URL mapping store uses read-optimized settings (large buffer pool, read replicas), while the analytics store uses write-optimized settings (write-ahead log, batch inserts, columnar storage for aggregation queries). Time-series databases (InfluxDB, TimescaleDB) or columnar analytics databases (ClickHouse, BigQuery) are ideal for analytics data. This polyglot persistence approach is standard in production systems at scale."},{question:"How would you implement a URL shortener preview feature (like bit.ly's '+' suffix)?",options:["Create a separate short URL for the preview","Append a special character (like '+') to the short URL path that serves an HTML page showing destination URL and metadata instead of redirecting","Add a query parameter for preview mode","Preview is not possible with short URLs"],correctIndex:1,explanation:"The preview feature works by reserving a URL pattern (e.g., 'sho.rt/abc123+' or 'sho.rt/abc123/info') that returns an HTML page showing: the destination URL, page title and description (from Open Graph metadata), click statistics, creation date, and a 'proceed to destination' button. The server routes these requests differently from redirect requests  matching the '+' suffix pattern returns an HTML page rather than a 302 redirect. This feature is important for security-conscious users who want to verify the destination before being redirected. Query parameters (like '?preview=true') would also work technically but could conflict with URLs that contain similar parameters. Bit.ly popularized the '+' convention, and it has become a de facto standard in the URL shortener space."},{question:"What is the role of a reverse proxy (like Nginx) in front of a URL shortener application?",options:["To generate short URLs","To handle SSL termination, static content serving, request buffering, and act as a gateway before requests reach the application servers","To store URL mappings","To generate analytics reports"],correctIndex:1,explanation:"A reverse proxy like Nginx sits between clients and application servers, providing several critical functions. SSL/TLS termination handles HTTPS decryption at the proxy level so application servers process plain HTTP, reducing their CPU load. Request buffering absorbs slow client connections so application servers aren't tied up waiting for slow uploads. Connection pooling maintains a smaller number of persistent connections to backend servers. Static content serving returns error pages and status pages without hitting the application. Rate limiting and access control provide a first line of defense against abuse. For URL shorteners, Nginx can also handle the redirect logic directly for cached URLs using its built-in proxy_cache module, making redirects extremely fast without involving the application server at all."},{question:"How would you design a URL shortener to be serverless?",options:["Serverless architectures can't support URL shorteners","Use API Gateway for routing, Lambda/Cloud Functions for logic, DynamoDB for storage, and CloudFront as CDN  but cold starts may increase redirect latency","Replace all servers with a single large instance","Use only edge computing"],correctIndex:1,explanation:"A serverless URL shortener architecture uses: API Gateway (AWS API Gateway or Cloudflare Workers) for request routing, Lambda or Cloud Functions for creation logic and analytics processing, DynamoDB or FaunaDB for URL storage with built-in TTL for expiration, and CloudFront or another CDN for caching redirect responses. Benefits include zero infrastructure management, automatic scaling, and pay-per-request pricing. The main concern is cold start latency  a Lambda function that hasn't been invoked recently may take 100-500ms to start, which is unacceptable for redirect latency. Mitigations include provisioned concurrency (keeping functions warm) and edge-based execution (Cloudflare Workers run on the edge with near-zero cold starts). For low-to-moderate traffic URL shorteners, serverless is cost-effective; at high scale, dedicated infrastructure may be cheaper."},{question:"What is the difference between soft delete and hard delete for expired URLs?",options:["There is no difference","Soft delete marks the URL as inactive (preserving data for analytics/legal), while hard delete permanently removes the record from the database","Soft delete is slower","Hard delete preserves data"],correctIndex:1,explanation:"Soft delete adds a 'deleted_at' timestamp or 'is_active' flag to the record without removing it from the database. The redirect logic checks this flag and returns 410 Gone for soft-deleted URLs. This preserves historical data for analytics, compliance, and potential restoration. Hard delete permanently removes the record, freeing storage space and allowing the short key to be recycled. The choice depends on requirements: regulated industries (finance, healthcare) may require retaining records for audit trails, while privacy regulations (GDPR right to erasure) may require actual deletion. A common pattern is soft delete with a grace period (30-90 days) followed by hard delete and key recycling. This allows users to recover accidentally deleted URLs while eventually reclaiming resources."},{question:"How would you handle a URL shortener migration from one database to another?",options:["Shut down and migrate everything at once","Use the strangler fig pattern: dual-write to both databases, gradually shift reads to the new database, verify consistency, then decommission the old one","Simply point the application to the new database","Migrations are unnecessary"],correctIndex:1,explanation:"The strangler fig pattern provides zero-downtime database migration. Phase 1: Set up the new database and begin dual-writing (every URL creation writes to both old and new databases). Phase 2: Backfill historical data from the old database to the new one using a batch migration job. Phase 3: Verify consistency by comparing read results from both databases (shadow reads). Phase 4: Gradually shift read traffic to the new database using feature flags (e.g., 1%  10%  50%  100%). Phase 5: Stop writing to the old database and decommission it. This approach ensures zero data loss and allows rollback at any phase. The dual-write phase requires careful handling of failures  if one write succeeds but the other fails, you need compensating logic or an eventual consistency reconciliation job."},{question:"What is the significance of the Referer header in URL shortener analytics?",options:["It specifies the URL shortener's domain","It indicates where the user was before clicking the short URL, revealing which websites or platforms are driving traffic","It contains the user's name","It specifies the redirect destination"],correctIndex:1,explanation:"The HTTP Referer (historically misspelled) header contains the URL of the page that linked to the current request. For URL shortener analytics, this reveals traffic sources  whether clicks are coming from Twitter, Facebook, email clients, direct messaging apps, or other websites. This information is invaluable for marketers tracking campaign performance across channels. However, Referer is not always available: HTTPS-to-HTTP transitions strip it (though HTTPS-to-HTTPS preserves it), some privacy-focused browsers suppress it, and Referrer-Policy headers can restrict it. Despite these limitations, Referer data typically covers 60-80% of traffic and remains one of the most valuable analytics dimensions. Modern URL shorteners supplement Referer with UTM parameters for more reliable source tracking."},{question:"How would you implement a QR code generation feature for a URL shortener?",options:["Store pre-generated QR codes for all URLs","Generate QR codes on-demand by encoding the short URL, cache the result, and optionally allow customization (colors, logo embedding)","QR codes cannot encode URLs","Use a third-party QR code service for every request"],correctIndex:1,explanation:"QR code generation is a natural complement to URL shortening  the short URL is encoded into the QR code, keeping the QR pattern simple and scannable. Implementation involves: generating QR codes on-demand using a library (like qrcode in Node.js or Python), encoding the short URL (not the long URL, for a simpler QR pattern), caching generated QR code images (since the same short URL always produces the same QR code), and optionally allowing customization (brand colors, embedded logos, rounded dots). The QR code should be served with appropriate cache headers for browser caching. For high-traffic generation, a dedicated microservice with image caching (S3 + CDN) keeps the main redirect service unaffected. This feature is increasingly important as QR codes have seen massive adoption since COVID-19."},{question:"What is the purpose of using a write-ahead log (WAL) in a URL shortener's database?",options:["To log URL redirect analytics","To ensure durability by writing changes to a sequential log before applying them to the database, preventing data loss on crashes","To compress URL data","To cache frequently accessed URLs"],correctIndex:1,explanation:"A write-ahead log (WAL) ensures that no committed data is lost even if the database server crashes mid-operation. Before any change is applied to the actual data files, it's first written to the WAL  a sequential, append-only log on disk. If the server crashes, it replays the WAL on startup to recover committed transactions. For a URL shortener, this means a successfully created short URL won't be lost due to a server crash between the creation confirmation and the data being flushed to disk. WAL also improves write performance because sequential writes to the log are much faster than random writes to data files. Both PostgreSQL and MySQL use WAL (MySQL calls it the 'redo log'), and even NoSQL databases like Cassandra use a similar commit log mechanism."},{question:"How would you implement geographic routing for a multi-region URL shortener?",options:["Route all traffic to a single region","Use GeoDNS or latency-based DNS routing to direct users to the nearest region, with health checks for failover","Use client-side routing","Route based on the URL content"],correctIndex:1,explanation:"Geographic routing directs users to the closest healthy deployment region, minimizing latency. AWS Route 53's latency-based routing measures actual latency from DNS resolvers to each region, while GeoDNS maps source IPs to geographic regions. Implementation requires: deploying the full shortener stack in multiple regions, cross-region data replication (using DynamoDB Global Tables or Cassandra multi-DC), health checks that automatically remove unhealthy regions from DNS, and a global CDN layer (CloudFront, Cloudflare) for edge caching. The DNS TTL should be low enough (e.g., 60 seconds) to enable fast failover but high enough to not add excessive DNS resolution overhead. This ensures a user in Tokyo gets served from an Asia-Pacific region (~20ms latency) rather than US-East (~200ms latency)."},{question:"What is the concept of 'backpressure' in a URL shortener's analytics pipeline?",options:["Pushing back the user's browser","A mechanism where overwhelmed downstream analytics consumers signal upstream producers to slow down, preventing system overload","Compressing analytics data","Reversing URL redirects"],correctIndex:1,explanation:"Backpressure is a flow control mechanism in stream processing systems. In a URL shortener's analytics pipeline, click events flow from the redirect service  message queue  stream processor  analytics database. If the stream processor or analytics database becomes overwhelmed, backpressure signals the message queue to slow down event delivery, which may in turn signal the redirect service to buffer events locally. Without backpressure, the analytics pipeline could crash under load, losing click data. Kafka handles this naturally through consumer lag (consumers read at their own pace, and Kafka retains events). Redis Streams and RabbitMQ provide similar mechanisms. The key principle: it's better to process analytics events slowly than to lose them, since the redirect path is independent and unaffected."},{question:"How would you implement URL shortener link rotation (cycling through multiple destination URLs)?",options:["Create separate short URLs for each destination","Store an ordered list of destination URLs with rotation rules (round-robin, weighted, time-based) in the URL mapping record","Randomly pick a destination on each redirect","URL rotation is not possible"],correctIndex:1,explanation:"Link rotation allows a single short URL to redirect to different destinations based on configurable rules. The URL mapping record stores an array of destinations with rotation strategy metadata. Round-robin rotation assigns each click to the next destination in sequence (using an atomic counter). Weighted rotation assigns different traffic percentages to each destination (similar to A/B testing). Time-based rotation changes the destination at scheduled times (e.g., redirect to morning page before noon, afternoon page after). Implementation requires atomic read-modify-write operations for round-robin counters (Redis INCR works well here). This feature is valuable for marketers running multi-variant campaigns and for load distribution across multiple landing page servers."},{question:"What are idempotency keys and how do they apply to URL shortener creation?",options:["Keys that generate identical short URLs","Client-generated unique identifiers sent with creation requests to ensure the same request processed multiple times produces only one short URL","Keys that expire after one use","Keys used for URL encryption"],correctIndex:1,explanation:"Idempotency keys solve the problem of duplicate creation in unreliable networks. When a client sends a URL creation request, it includes a unique idempotency key (typically a UUID) in the header. The server stores this key with the creation result. If the client retries the same request (due to a timeout or network error), the server recognizes the duplicate idempotency key and returns the original result without creating a new short URL. Without idempotency keys, network retries could create multiple short URLs for the same long URL, wasting key space and confusing analytics. This pattern is used by Stripe, AWS, and other API-first companies. Implementation typically uses a Redis cache with a TTL (e.g., 24 hours) to store idempotency_key  response mappings."},{question:"What is the purpose of health checks in a URL shortener's load balancer configuration?",options:["To check the health of URLs","To verify application servers are functioning correctly so the load balancer routes traffic only to healthy instances","To check database health","To monitor URL creation rates"],correctIndex:1,explanation:"Health checks are periodic probes sent by the load balancer to each application server to verify it's capable of handling requests. For a URL shortener, a health check endpoint (e.g., GET /health) should verify: the application process is running, the database connection is active, and the cache is reachable. If a server fails health checks, the load balancer stops routing traffic to it, preventing users from hitting a broken server. Health checks should be fast (<100ms) and not perform heavy operations. A deep health check (testing all dependencies) is useful but risky  if the database is temporarily slow, all servers might fail deep health checks simultaneously, causing total service outage. A common pattern is shallow checks for load balancer routing and deep checks for monitoring/alerting."},{question:"How does consistent hashing with virtual nodes improve data distribution in a sharded URL shortener?",options:["Virtual nodes are faster than real nodes","Virtual nodes create multiple points per server on the hash ring, evening out distribution and reducing hotspots when servers are added or removed","Virtual nodes provide encryption","Virtual nodes reduce storage requirements"],correctIndex:1,explanation:"Without virtual nodes, each physical server gets one point on the hash ring, which can lead to uneven data distribution  especially with a small number of servers. Virtual nodes assign multiple points (e.g., 150-256) per physical server on the ring, creating a much more uniform distribution. When a server is removed, its virtual nodes' key ranges are distributed across many other servers rather than being absorbed by a single neighbor. This prevents sudden load spikes on individual servers. For a URL shortener with billions of entries, this uniform distribution is critical  a 10% imbalance across 10 shards means one shard handles 10% more traffic, which could exceed its capacity during peak hours. Amazon's Dynamo paper popularized this technique, and it's used in Cassandra, Riak, and most modern distributed databases."},{question:"What is the difference between proactive and reactive cache invalidation in a URL shortener?",options:["There is no difference","Proactive invalidation removes cache entries before they become stale (e.g., when a URL is updated/deleted), while reactive uses TTL expiration","Proactive is always slower","Reactive requires more code"],correctIndex:1,explanation:"Proactive (or explicit) invalidation immediately removes or updates cache entries when the underlying data changes  for example, when a user updates a short URL's destination or deletes it, the cache entry is immediately invalidated. Reactive invalidation relies on TTL expiration, meaning stale data may be served until the TTL expires. For a URL shortener, both approaches are needed: TTL-based expiration for normal operation (prevents indefinite caching and handles gradual consistency), and proactive invalidation for user-initiated changes (a user updating their URL's destination expects the change to take effect immediately, not after a 5-minute TTL). The implementation uses cache-aside pattern: on URL update, delete the cache entry and let the next request repopulate it from the database. This combination is a standard practice in any caching architecture."},{question:"What is the security risk of sequential/predictable short URL keys?",options:["Sequential keys are slower to look up","Attackers can enumerate all URLs by iterating through the key space, accessing potentially sensitive destination URLs","Sequential keys use more storage","Sequential keys cause more cache misses"],correctIndex:1,explanation:"If a URL shortener uses sequential keys (1, 2, 3... converted to base62), an attacker can systematically try every possible key to discover all shortened URLs. This is a privacy and security risk because people often shorten sensitive URLs  private documents, internal company tools, personal photos, or confidential business links  assuming the short URL is effectively 'secret.' Enumeration attacks on sequential keys are trivial to automate and can expose thousands of sensitive URLs per minute. Random keys make enumeration impractical  with a 7-character base62 key space of 3.5 trillion possibilities, randomly guessing valid keys has a negligible success rate. This is one of the strongest arguments against sequential key generation and in favor of random/KGS-based approaches in system design interviews."},{question:"How should a URL shortener handle a destination URL that becomes unreachable?",options:["Immediately delete the short URL","Continue redirecting users (the destination might be temporarily down), but flag the URL for review and optionally show a warning after prolonged unavailability","Return a 404 error","Replace the destination with a cached version"],correctIndex:1,explanation:"A URL shortener's job is to redirect, not to guarantee destination availability. The destination might be temporarily down for maintenance, experiencing a brief outage, or behind a firewall that blocks the shortener's monitoring probe. Immediately deleting or blocking the redirect would break legitimate use cases. Instead, implement a monitoring system that periodically checks destination health and flags URLs that have been unreachable for extended periods (e.g., 7+ days). For flagged URLs, you could: show an interstitial warning ('This link's destination appears to be unavailable'), send a notification to the URL's creator, or mark it in analytics. The redirect itself should always attempt to work  the user might have access even if the monitoring system doesn't. This nuanced approach balances user experience with proactive monitoring."},{question:"What is the maximum QPS (Queries Per Second) a single Redis instance can typically handle for URL shortener lookups?",options:["About 1,000 QPS","About 10,000 QPS","About 100,000+ QPS","About 1,000,000+ QPS"],correctIndex:2,explanation:"A single Redis instance can typically handle 100,000 to 250,000+ simple GET operations per second, depending on hardware, network configuration, and value sizes. For URL shortener lookups (GET by short key returning a URL string of ~100-500 bytes), Redis comfortably handles 100K+ QPS per instance. This means even a small Redis cluster (3-5 nodes) can serve the redirect traffic of most URL shorteners. Redis achieves this through: single-threaded event loop (no lock contention), in-memory data storage, efficient data structures, and minimal protocol overhead. Redis 6+ introduced I/O threading for even higher throughput. For comparison, a well-tuned PostgreSQL instance might handle 10,000-50,000 simple lookups per second  an order of magnitude less. This performance difference is why Redis is the de facto caching layer for read-heavy systems."},{question:"How would you implement URL shortener access controls (private short URLs)?",options:["All short URLs must be public","Add an authentication layer where private URLs require an API key or token in a query parameter or header before redirecting","Use longer keys for private URLs","Encrypt the destination URL"],correctIndex:1,explanation:"Private short URLs add an access control layer before the redirect. Implementation options include: API key in a query parameter (sho.rt/abc123?key=secret123), bearer token in a cookie/header (requires the user to be authenticated), password-protected redirect (interstitial page asking for a password), or organization-based access (only users authenticated with the organization's SSO can access the redirect). The trade-off is user experience  adding authentication to redirects adds friction and breaks the simplicity of short URLs. For API-based access, tokens are cleaner. For browser-based access, a lightweight interstitial page with password entry works well. The short URL's metadata record needs an 'access_type' field (public/private/password-protected) and associated credentials. This feature is valuable for enterprise use cases where companies share internal links externally but want to control access."},{question:"What is the purpose of request tracing (distributed tracing) in a URL shortener system?",options:["To trace the destination URL","To track a single request's journey through all system components (load balancer  app server  cache/DB  analytics), enabling debugging and performance optimization","To trace user identity","To trace URL creation history"],correctIndex:1,explanation:"Distributed tracing assigns a unique trace ID to each incoming request and propagates it through every component the request touches. For a URL shortener redirect: the load balancer logs the trace ID, the application server logs cache hit/miss with the trace ID, the database query (if any) is tagged with the trace ID, and the analytics event includes it. When a user reports slow redirects, you can look up the trace ID and see exactly where time was spent  was it DNS resolution? TLS handshake? Cache miss? Slow database query? Tools like Jaeger, Zipkin, or AWS X-Ray provide visualization of trace data. Without distributed tracing, debugging performance issues in a multi-component system becomes guesswork. This observability capability is essential for any production system and demonstrates operational maturity in system design interviews."},{question:"What is the role of feature flags in a URL shortener deployment?",options:["To flag malicious URLs","To enable gradual rollout of new features (like new redirect logic or caching strategies) to a percentage of traffic without redeploying","To mark URLs for deletion","To flag popular URLs for caching"],correctIndex:1,explanation:"Feature flags (also called feature toggles) allow you to enable or disable specific code paths at runtime without redeploying the application. For a URL shortener, this enables: gradual rollout of a new caching strategy (test with 1% of traffic before full deployment), A/B testing different redirect logic (302 vs 307), canary deployments of new database clients, quick rollback of problematic features without redeployment, and enabling premium features for specific user tiers. Tools like LaunchDarkly, Split.io, or simple Redis-backed configurations provide feature flag infrastructure. The key benefit is risk reduction  if a new feature causes issues, you flip the flag off instantly rather than waiting for a deployment. This operational pattern is used extensively at companies like Facebook, Google, and Netflix for safe continuous deployment."}],pm=[{question:"What is the fundamental principle of event-driven architecture?",options:["Services communicate by directly calling each other's APIs","Components communicate by producing and consuming events, reacting to state changes asynchronously","All services share a single database that triggers events","Events are only used for logging and monitoring"],correctIndex:1,explanation:"Event-driven architecture (EDA) is a design pattern where components communicate through eventsnotifications that something has happened. Producers emit events without knowing who will consume them, and consumers react to events they're interested in. This creates loose coupling: producers and consumers can evolve independently, and new consumers can be added without modifying producers. Unlike direct API calls (request/response), EDA is inherently asynchronousthe producer doesn't wait for a response. This enables higher resilience, better scalability, and natural support for workflows that span multiple services. Real-world examples include order processing pipelines, IoT telemetry systems, and real-time analytics."},{question:"What is event sourcing?",options:["Sourcing events from third-party APIs","Storing the state of an entity as a sequence of immutable events rather than the current state","Using source control to track event changes","A technique for finding the source of errors in event logs"],correctIndex:1,explanation:"Event sourcing stores every state change as an immutable event in an append-only log (event store), rather than storing just the current state. To get the current state of an entity, you replay all its events from the beginning. For example, instead of storing 'account balance = $500,' you store 'deposited $1000,' 'withdrew $300,' 'deposited $100,' 'withdrew $300.' This provides a complete audit trail, enables temporal queries ('what was the balance on Tuesday?'), and allows rebuilding state from scratch. The trade-off is complexity: reading current state requires replaying events (solved by snapshots), and the event schema must be carefully managed since events are immutable. Event sourcing is the foundation of CQRS in many systems."}],fm=[{question:"What OSI layer does an L4 load balancer operate at?",options:["Layer 2 (Data Link)","Layer 4 (Transport)","Layer 7 (Application)","Layer 3 (Network)"],correctIndex:1,explanation:"An L4 load balancer operates at the Transport layer (Layer 4), making routing decisions based on TCP/UDP information such as IP addresses and port numbers. It does not inspect the actual content of packets like an L7 load balancer would. This makes L4 balancers faster since they don't need to parse HTTP headers, cookies, or other application-level data. AWS Network Load Balancer (NLB) is a real-world example of an L4 load balancer that can handle millions of requests per second with ultra-low latency."},{question:"Which load balancing algorithm distributes requests sequentially across servers in order?",options:["Least connections","IP hash","Round-robin","Weighted random"],correctIndex:2,explanation:"Round-robin distributes incoming requests sequentially across the pool of servers  first request to server 1, second to server 2, and so on, cycling back to the start. It's the simplest algorithm and works well when all servers have similar capacity and requests have similar processing costs. However, it doesn't account for current server load, so a server handling a long-running request still gets new ones. Nginx uses round-robin as its default load balancing method."},{question:"What is the main advantage of the 'least connections' algorithm over round-robin?",options:["It's faster to compute","It considers current server load","It preserves session affinity","It uses less memory"],correctIndex:1,explanation:"Least connections routes new requests to the server with the fewest active connections, effectively considering current server load. Unlike round-robin which blindly cycles through servers, least connections adapts to situations where some requests take longer than others. For example, if one server is handling several long-running database queries, new requests will be directed to less-busy servers. This makes it ideal for applications with variable request processing times, such as WebSocket connections or API endpoints with mixed query complexity."},{question:"What does SSL termination at the load balancer mean?",options:["The load balancer blocks all SSL traffic","The load balancer decrypts SSL and forwards plain HTTP to backends","The load balancer re-encrypts traffic with a different certificate","The load balancer only accepts non-SSL traffic"],correctIndex:1,explanation:"SSL termination means the load balancer handles the SSL/TLS decryption, removing the encryption overhead from backend servers. The load balancer decrypts incoming HTTPS requests and forwards plain HTTP to the backend servers over the internal network. This offloads the CPU-intensive cryptographic operations from application servers, allowing them to focus on business logic. It's widely used in production  for example, AWS ALB can terminate SSL and even handle certificate rotation via ACM, while backend EC2 instances communicate in plaintext within the VPC."},{question:"Which load balancer type can route based on URL path or HTTP headers?",options:["L3 load balancer","L4 load balancer","L7 load balancer","DNS load balancer"],correctIndex:2,explanation:"L7 (Application layer) load balancers can inspect HTTP/HTTPS content including URL paths, headers, cookies, and even request bodies to make routing decisions. This enables content-based routing  for example, sending /api/* requests to API servers and /static/* to file servers. L4 load balancers cannot do this because they only see TCP/UDP packet information without parsing the application protocol. AWS Application Load Balancer (ALB) is a classic L7 load balancer that supports path-based and host-based routing rules."},{question:"What is sticky session (session affinity) in load balancing?",options:["Sessions that cannot be terminated","Routing all requests from the same client to the same server","Sharing session data across all servers","Encrypting session data at the load balancer"],correctIndex:1,explanation:"Sticky sessions ensure that all requests from a particular client are directed to the same backend server for the duration of the session. This is typically implemented using cookies  the load balancer sets a cookie on the first response indicating which server to use. While this solves session state problems (e.g., shopping carts stored in server memory), it can lead to uneven load distribution if some users generate more traffic. Modern architectures prefer externalized session stores like Redis instead of sticky sessions, as they don't create single points of failure."},{question:"What happens when a health check fails for a backend server?",options:["The load balancer restarts the server","The load balancer stops sending traffic to that server","The load balancer sends more traffic to test recovery","The load balancer shuts down completely"],correctIndex:1,explanation:"When a health check fails, the load balancer marks that server as unhealthy and stops routing new requests to it. Existing connections may be drained gracefully depending on the configuration. The load balancer continues to periodically check the unhealthy server, and once it passes health checks again, it's returned to the active pool. For example, an ALB health check might ping /health every 30 seconds, requiring 3 consecutive failures to mark a target unhealthy and 3 successes to mark it healthy again."},{question:"IP hash load balancing is most useful when you need:",options:["Maximum throughput","Client-server affinity without cookies","Equal distribution regardless of client","Minimum latency"],correctIndex:1,explanation:"IP hash computes a hash of the client's IP address to determine which server should handle the request, ensuring the same client always reaches the same server. This provides session affinity without relying on cookies or application-layer information, making it work at L4 where cookies aren't visible. However, it can cause uneven distribution when many clients share an IP (like behind a corporate NAT), as all those clients would hit the same server. It's commonly used in scenarios where stateful connections are needed but cookie-based affinity isn't feasible, such as TCP-based protocols."},{question:"Which software is primarily known as a high-performance L4/L7 load balancer and proxy?",options:["Apache Tomcat","HAProxy","MySQL Proxy","Varnish"],correctIndex:1,explanation:"HAProxy (High Availability Proxy) is one of the most widely-used open-source load balancers, supporting both L4 (TCP) and L7 (HTTP) load balancing. It's known for extremely high performance, handling millions of concurrent connections with low resource usage. HAProxy powers major sites like GitHub, Reddit, and Stack Overflow. While Nginx also functions as a load balancer, HAProxy was purpose-built for load balancing and proxying, offering more advanced health checking, connection draining, and traffic management features out of the box."},{question:"What is the primary difference between AWS ALB and NLB?",options:["ALB is cheaper than NLB","ALB operates at L7 while NLB operates at L4","NLB supports HTTP while ALB does not","ALB is faster than NLB"],correctIndex:1,explanation:"AWS Application Load Balancer (ALB) operates at Layer 7, understanding HTTP/HTTPS and supporting features like path-based routing, host-based routing, and WebSocket support. AWS Network Load Balancer (NLB) operates at Layer 4, routing based on TCP/UDP and offering ultra-low latency with millions of requests per second. NLB is actually faster than ALB because it doesn't need to parse HTTP content. Choose ALB when you need content-based routing, and NLB when you need raw performance, static IPs, or non-HTTP protocols like gRPC or custom TCP."},{question:"What is Global Server Load Balancing (GSLB)?",options:["Load balancing within a single data center","DNS-based load balancing across geographically distributed data centers","A single load balancer handling global traffic","Round-robin within a server cluster"],correctIndex:1,explanation:"GSLB distributes traffic across multiple geographically distributed data centers, typically using DNS to direct users to the nearest or healthiest data center. When a client resolves a domain name, the GSLB-aware DNS returns the IP of the optimal data center based on factors like geographic proximity, health status, and current load. This is different from local load balancing which distributes within a single data center. Services like AWS Route 53 with latency-based routing, Cloudflare Load Balancing, and F5 BIG-IP DNS are real-world GSLB implementations."},{question:"In weighted round-robin, what does the 'weight' represent?",options:["The physical size of the server","The proportion of traffic a server should receive","The priority in case of failover","The maximum number of connections"],correctIndex:1,explanation:"In weighted round-robin, each server is assigned a weight that determines the proportion of requests it receives relative to other servers. A server with weight 3 receives three times as many requests as a server with weight 1. This is useful when servers have different capacities  a powerful machine with 16 CPU cores should handle more traffic than a smaller 4-core machine. For example, in Nginx you'd configure 'server backend1 weight=3; server backend2 weight=1;' to send 75% of traffic to backend1 and 25% to backend2."},{question:"What problem does connection draining solve during server removal?",options:["It prevents new servers from being added","It allows in-flight requests to complete before removing a server","It speeds up the removal process","It caches responses for offline servers"],correctIndex:1,explanation:"Connection draining (also called deregistration delay) ensures that when a server is being removed from the pool, existing in-flight requests are allowed to complete rather than being abruptly terminated. The load balancer stops sending new requests to the server but maintains existing connections until they finish or a timeout is reached. Without connection draining, users might see 502 errors or dropped connections during deployments. AWS ALB has a configurable deregistration delay (default 300 seconds) that defines how long to wait before forcefully closing remaining connections."},{question:"Which health check type actually makes an HTTP request to verify application health?",options:["TCP health check","ICMP ping check","HTTP health check","Port check"],correctIndex:2,explanation:"HTTP health checks send an actual HTTP request (usually GET) to a specific endpoint like /health and verify the response status code (typically expecting 200 OK). This is more thorough than TCP checks (which only verify the port is open) or ICMP pings (which only verify network reachability). An HTTP health check can detect application-level failures  for instance, a server where the web process is running but the database connection is broken would pass TCP checks but fail an HTTP health check that queries the DB. This is why production systems almost always use HTTP health checks for web applications."},{question:"What is the 'thundering herd' problem in load balancing?",options:["Too many load balancers competing for traffic","All backend servers failing simultaneously","Many clients reconnecting simultaneously after a failure, overwhelming the system","Servers processing requests too quickly"],correctIndex:2,explanation:"The thundering herd problem occurs when many clients simultaneously attempt to reconnect or retry after a failure or during a recovery event, potentially overwhelming the system. For example, if a load balancer goes down for 30 seconds, thousands of clients will retry simultaneously when it comes back, creating a massive spike. Solutions include exponential backoff with jitter on client retries, gradual ramp-up (slow start) for recovered servers, and connection rate limiting. AWS ALB's slow start feature addresses this by gradually increasing the proportion of requests sent to a newly healthy target."},{question:"What does Nginx use as its default load balancing algorithm?",options:["Least connections","IP hash","Round-robin","Random"],correctIndex:2,explanation:"Nginx uses round-robin as its default load balancing algorithm when you define an upstream block without specifying a method. Requests are distributed sequentially across the defined servers in order. You can change this by adding directives like 'least_conn;' for least connections or 'ip_hash;' for IP-based hashing inside the upstream block. Round-robin is a sensible default because it's simple, fast, and works reasonably well when servers have similar capacities. For heterogeneous server pools, you'd typically switch to weighted round-robin or least connections."},{question:"What is a reverse proxy, and how does it relate to load balancing?",options:["A proxy that clients use to access the internet anonymously","A server that sits in front of backend servers, forwarding client requests  load balancers are a type of reverse proxy","A proxy that reverses the direction of network traffic","A backup load balancer"],correctIndex:1,explanation:"A reverse proxy sits between clients and backend servers, receiving client requests and forwarding them to appropriate backends. Load balancers are essentially reverse proxies with the added intelligence of distributing requests across multiple servers. Beyond load distribution, reverse proxies provide benefits like SSL termination, caching, compression, and security (hiding backend server details). Nginx and HAProxy are commonly used as both reverse proxies and load balancers. The 'reverse' distinguishes it from a forward proxy, which sits in front of clients to access external resources."},{question:"In a Blue-Green deployment, how does the load balancer facilitate zero-downtime releases?",options:["It runs both versions simultaneously on the same servers","It switches all traffic from the old (blue) environment to the new (green) environment at once","It gradually increases traffic to the new version","It pauses all traffic during deployment"],correctIndex:1,explanation:"In Blue-Green deployment, you maintain two identical production environments. The load balancer points all traffic to the current (blue) environment while the new version is deployed to the green environment. Once the green environment is verified, the load balancer switches all traffic from blue to green instantly. If problems are found, you can immediately switch back to blue. This differs from canary deployments where traffic is gradually shifted. The load balancer is the key component that makes the instant cutover possible, and tools like AWS ALB with target group switching make this straightforward."},{question:"What is the purpose of the X-Forwarded-For HTTP header in load balancing?",options:["To specify which server should handle the request","To preserve the original client IP address when traffic passes through a proxy/load balancer","To indicate the load balancing algorithm being used","To forward authentication tokens"],correctIndex:1,explanation:"When a load balancer forwards a request to a backend server, the backend sees the load balancer's IP as the source, not the original client's IP. The X-Forwarded-For header preserves the original client IP by appending it as the request passes through each proxy. This is critical for logging, rate limiting, geolocation, and security  you need to know the real client IP, not the load balancer's. For example, a request passing through two proxies might have 'X-Forwarded-For: client-ip, proxy1-ip'. Backend applications must be configured to trust and parse this header from known load balancer IPs."},{question:"What is the 'slow start' feature in load balancing?",options:["Delaying the load balancer startup","Gradually increasing traffic to a newly added or recovered server","Slowing down request processing for better reliability","Starting health checks at a slower rate"],correctIndex:1,explanation:"Slow start gradually increases the proportion of requests sent to a newly added or recently recovered server over a configured time period, rather than immediately sending it a full share of traffic. This prevents overwhelming a cold server that may need to warm up caches, establish database connections, or JIT-compile code. Without slow start, a freshly started Java application server might receive thousands of requests before its JVM has warmed up, leading to high latency or failures. AWS ALB supports slow start mode where you can configure the ramp-up duration (e.g., 30-900 seconds)."},{question:"Which protocol does an L4 load balancer primarily use to make routing decisions?",options:["HTTP","TCP/UDP","FTP","SMTP"],correctIndex:1,explanation:"An L4 load balancer makes routing decisions based on TCP and UDP protocol information  specifically source/destination IP addresses and port numbers. It operates on network packets without understanding the application protocol carried within them. This means it can load balance any TCP or UDP-based protocol (HTTP, FTP, SMTP, custom protocols) without needing protocol-specific knowledge. The tradeoff is that it can't make smart routing decisions based on content, but it's significantly faster than L7 balancing because it doesn't need to parse application-level data. AWS NLB is a prime example of an L4 load balancer."},{question:"What is an active-passive (failover) load balancer configuration?",options:["Both load balancers handle traffic simultaneously","One load balancer handles all traffic while the other stands by as backup","One handles read traffic, the other handles write traffic","Both are passive until traffic spikes"],correctIndex:1,explanation:"In active-passive configuration, one load balancer (active) handles all incoming traffic while another (passive) monitors the active one and stands ready to take over if it fails. The passive load balancer uses heartbeat mechanisms to detect when the active one goes down. When failover occurs, the passive takes over the active's virtual IP address (VIP), making the switch transparent to clients. This is simpler than active-active but wastes the passive node's capacity during normal operation. Tools like keepalived with VRRP protocol are commonly used to implement this pattern with HAProxy."},{question:"What is an active-active load balancer configuration?",options:["Only one load balancer is active at a time","Multiple load balancers handle traffic simultaneously","Load balancers alternate between active and passive states","Load balancers are active only during peak hours"],correctIndex:1,explanation:"In active-active configuration, multiple load balancers simultaneously handle traffic, distributing the load among themselves. This provides both high availability and increased capacity compared to active-passive where one node sits idle. DNS round-robin or an upstream GSLB typically distributes traffic across the active load balancers. If one fails, the others absorb its traffic. The challenge is synchronizing state (like session persistence tables) across active instances. Active-active is preferred in high-traffic environments because it utilizes all available hardware, and services like AWS ALB are inherently active-active across multiple availability zones."},{question:"What is the primary disadvantage of DNS-based load balancing?",options:["It's too expensive","DNS TTL causes slow failover because clients cache old DNS records","It can only balance HTTP traffic","It requires special client software"],correctIndex:1,explanation:"DNS-based load balancing has a fundamental limitation: DNS responses are cached by clients, ISPs, and resolvers for the duration of the TTL (Time To Live). Even with short TTLs, many resolvers and operating systems ignore low TTL values, meaning clients may continue connecting to a failed server for minutes or hours. This makes DNS-based failover slow and unreliable compared to hardware or software load balancers that can detect failures in seconds. Additionally, DNS doesn't perform health checks, so it can resolve to unhealthy servers. Services like AWS Route 53 mitigate this with health check integration, but TTL caching remains a fundamental limitation."},{question:"What is consistent hashing used for in the context of load balancing?",options:["Encrypting traffic between load balancer and backends","Minimizing redistribution of requests when servers are added or removed","Ensuring all servers get exactly equal traffic","Hashing passwords for authentication"],correctIndex:1,explanation:"Consistent hashing maps both servers and request keys onto a hash ring, so each request is routed to the nearest server clockwise on the ring. When a server is added or removed, only the requests that map to that segment of the ring are redistributed  typically 1/N of all requests, where N is the number of servers. This is far better than traditional hash-mod-N approaches where adding a server remaps almost all requests. It's especially important for stateful or cached backends where you want the same client or request key to usually hit the same server. Memcached and many distributed caches use consistent hashing for this reason."},{question:"What does 'SSL passthrough' mean in load balancing?",options:["The load balancer generates new SSL certificates","The load balancer forwards encrypted traffic directly to backends without decryption","The load balancer converts SSL to a different encryption","The load balancer caches SSL sessions"],correctIndex:1,explanation:"SSL passthrough means the load balancer forwards the encrypted TLS traffic directly to the backend server without decrypting it. The backend server handles the SSL/TLS termination itself. This is the opposite of SSL termination where the load balancer decrypts traffic. SSL passthrough is used when end-to-end encryption is required (e.g., for compliance reasons like PCI-DSS) or when the backend needs to see the client certificate for mutual TLS authentication. The downside is the load balancer can't inspect HTTP content, so L7 features like path-based routing or header manipulation are unavailable. HAProxy supports this via 'mode tcp' configuration."},{question:"What is a Virtual IP (VIP) in the context of load balancing?",options:["An IP address only accessible to admin users","A shared IP address that floats between load balancers for high availability","A private IP address used within the data center","An IP address reserved for future use"],correctIndex:1,explanation:"A Virtual IP (VIP) is an IP address that is not tied to a specific physical server but can float between multiple load balancer instances. In an active-passive setup, the active load balancer owns the VIP, and if it fails, the passive takes over the VIP using protocols like VRRP (Virtual Router Redundancy Protocol). Clients always connect to the VIP, so failover is transparent  they don't need to know which physical load balancer is active. This is a cornerstone of high-availability configurations. Keepalived is the most common open-source tool used to manage VIPs in Linux-based load balancer setups with HAProxy or Nginx."},{question:"How does a load balancer handle WebSocket connections differently from regular HTTP?",options:["It blocks WebSocket connections","It must maintain persistent connections and use connection-aware balancing","It converts WebSocket to HTTP","It treats them exactly the same as HTTP"],correctIndex:1,explanation:"WebSocket connections are long-lived, bidirectional, and persistent  unlike typical HTTP request-response cycles that are short-lived. A load balancer must be aware that once a WebSocket connection is established (after the HTTP upgrade handshake), it should maintain that connection to the same backend server for its entire lifetime. Using round-robin per-request would break WebSocket since each message isn't a new connection. L7 load balancers like ALB and Nginx support WebSocket by recognizing the Upgrade header and maintaining connection affinity. This is why least-connections is often preferred for WebSocket workloads, as it accounts for long-held connections."},{question:"What is the 'hot spot' problem in load balancing?",options:["Servers overheating physically","One server receiving disproportionately more traffic than others","The load balancer itself becoming a bottleneck","Network switches becoming saturated"],correctIndex:1,explanation:"A hot spot occurs when one backend server receives significantly more traffic than others, often due to uneven distribution from the load balancing algorithm. This can happen with IP hash when many clients share a single IP (e.g., behind corporate NAT), with round-robin when request processing times vary greatly, or with consistent hashing when the hash space isn't evenly distributed. The hot server may become overloaded while others sit idle. Solutions include weighted balancing, virtual nodes in consistent hashing, and least-connections algorithms. Celebrity Twitter accounts causing hot spots on specific cache servers is a classic real-world example."},{question:"What is Direct Server Return (DSR) in load balancing?",options:["The server directly returns error messages to clients","Response traffic goes directly from the backend to the client, bypassing the load balancer","The server returns the request back to the load balancer","A debugging mode for load balancers"],correctIndex:1,explanation:"In Direct Server Return (DSR), the load balancer only handles incoming requests  the response traffic from the backend server goes directly back to the client, bypassing the load balancer entirely. This dramatically reduces the load on the load balancer since response data (which is typically much larger than requests) doesn't pass through it. DSR is commonly used for video streaming or large file downloads where response payloads are orders of magnitude larger than requests. The backend server must be configured to accept traffic for the VIP address, typically using a loopback interface. L4 load balancers like LVS (Linux Virtual Server) commonly implement DSR."},{question:"In canary deployment, how does the load balancer route traffic?",options:["All traffic goes to the new version immediately","A small percentage of traffic is sent to the new version, gradually increasing","Traffic alternates between old and new versions","Only test traffic goes to the new version"],correctIndex:1,explanation:"In canary deployment, the load balancer initially routes a small percentage (e.g., 1-5%) of real production traffic to the new version while the majority continues to the stable version. If metrics (error rates, latency, etc.) look good, the percentage is gradually increased until 100% reaches the new version. This is named after canaries used in coal mines to detect danger early. The load balancer implements this via weighted routing rules  for example, AWS ALB supports weighted target groups where you can assign 95% weight to the old version and 5% to the new. This provides a safer rollout compared to blue-green's all-at-once switch."},{question:"What is the purpose of a load balancer's 'idle timeout'?",options:["To shut down the load balancer when not in use","To close inactive connections after a specified period to free resources","To delay starting up after a reboot","To slow down traffic during off-peak hours"],correctIndex:1,explanation:"The idle timeout defines how long a load balancer keeps an inactive connection open before closing it. If no data is sent over a connection within this period, it's terminated to free up resources (memory, file descriptors, port numbers). This prevents resource exhaustion from abandoned connections. For example, AWS ALB has a default idle timeout of 60 seconds. For WebSocket or long-polling applications, you need to increase this timeout, while for typical REST APIs the default is usually sufficient. It's important to ensure the backend server's timeout is longer than the load balancer's to avoid the LB sending traffic to a connection the backend has already closed."},{question:"Which header does an L7 load balancer use for host-based routing?",options:["X-Forwarded-For","Host","Content-Type","Accept-Encoding"],correctIndex:1,explanation:"The Host header in HTTP requests specifies the domain name the client is trying to reach, and L7 load balancers use this for host-based routing. This allows a single load balancer to route traffic for multiple domains  for example, api.example.com to API servers and www.example.com to web servers. This is essentially virtual hosting at the load balancer level. AWS ALB supports host-based routing rules where you define conditions like 'if Host header matches api.example.com, forward to target group A.' Without the Host header, you'd need separate load balancers (or IP addresses) for each domain."},{question:"What is the advantage of using multiple availability zones with a load balancer?",options:["Lower cost","Fault tolerance  if one AZ fails, traffic is routed to healthy AZs","Faster response times for all users","Simplified configuration"],correctIndex:1,explanation:"Deploying across multiple availability zones (AZs) with a load balancer provides fault tolerance against entire data center failures. If one AZ experiences an outage (power failure, network issues), the load balancer automatically routes traffic to healthy instances in other AZs. Each AZ is an isolated data center with independent power, cooling, and networking. AWS ALB, for example, is inherently multi-AZ  it deploys nodes in each enabled AZ and performs cross-zone load balancing. This is a fundamental best practice for production workloads, as it protects against the most common cause of large-scale outages: AZ-level failures."},{question:"What is cross-zone load balancing?",options:["Load balancing between different cloud providers","Distributing traffic evenly across all targets in all enabled availability zones","Balancing traffic between development and production zones","Load balancing between different geographic regions"],correctIndex:1,explanation:"Cross-zone load balancing ensures that traffic is distributed evenly across all registered targets in all enabled availability zones, regardless of which AZ the load balancer node received the traffic. Without cross-zone balancing, each LB node only distributes to targets in its own AZ, which can cause uneven distribution if AZs have different numbers of targets. For example, if AZ-A has 2 instances and AZ-B has 8, without cross-zone balancing each AZ gets 50% of traffic, meaning AZ-A's 2 instances each handle 25% while AZ-B's 8 each handle only 6.25%. AWS ALB has cross-zone enabled by default, while NLB has it disabled by default."},{question:"What is a load balancer sandwich architecture?",options:["A load balancer placed between two firewalls","Multiple tiers of load balancers for different layers of an application","A load balancer between two identical server groups","A redundant pair of load balancers"],correctIndex:1,explanation:"A load balancer sandwich (or multi-tier load balancing) uses load balancers at multiple levels in the application architecture. For example, an external L7 ALB routes to web servers, which then connect through an internal L4 NLB to application servers, which connect through another internal NLB to database read replicas. This provides granular scaling and health checking at each tier. It's common in microservices architectures where each service layer needs independent scaling and traffic management. AWS recommends this pattern for three-tier architectures, using public ALBs for the web tier and private NLBs for internal service-to-service communication."},{question:"How does HAProxy differ from Nginx in terms of primary design philosophy?",options:["HAProxy is only L4, Nginx is only L7","HAProxy was purpose-built for proxying/load balancing; Nginx was originally a web server","Nginx is faster than HAProxy in all scenarios","HAProxy doesn't support HTTP"],correctIndex:1,explanation:"HAProxy was designed from the ground up as a high-performance TCP/HTTP load balancer and proxy, while Nginx was originally created as a web server that later gained load balancing capabilities. This means HAProxy has more sophisticated load balancing features out of the box: advanced health checking, detailed connection statistics, hitless reloads, and fine-grained traffic management. Nginx excels as a combined web server, reverse proxy, and load balancer, making it a great all-in-one solution. In practice, many architectures use both: Nginx serving static content and HAProxy handling load balancing for dynamic traffic. Both can handle millions of concurrent connections."},{question:"What is the 'least response time' load balancing algorithm?",options:["Routes to the server that started most recently","Routes to the server with the fastest response time and fewest active connections","Routes to the server closest geographically","Routes to the server with the most memory available"],correctIndex:1,explanation:"The least response time algorithm considers both the number of active connections and the server's recent response time to choose the best backend. It routes requests to the server that can respond fastest, combining the benefits of least connections with actual performance data. This is more sophisticated than pure least connections because a server with few connections might still be slow due to hardware issues or heavy background processing. Nginx Plus offers this as 'least_time' where you can optimize for header response time or full response time. It's particularly useful for heterogeneous server pools where machines have different performance characteristics."},{question:"What is preconnection (TCP connection pooling) in load balancers?",options:["Connecting to servers before any client requests arrive","Maintaining a pool of pre-established connections to backend servers","Pre-allocating IP addresses for new servers","Connecting multiple load balancers together"],correctIndex:1,explanation:"Connection pooling (or multiplexing) means the load balancer maintains a pool of pre-established, persistent TCP connections to backend servers. Instead of creating a new TCP connection (with its 3-way handshake overhead) for every client request, the load balancer reuses existing connections from the pool. This dramatically reduces latency and server load, especially under high traffic. AWS ALB uses connection multiplexing by default  it may send multiple client requests over a single backend connection. This is particularly impactful for HTTPS backends where each new connection would require a full TLS handshake, which can take 2-3 round trips."},{question:"What is the purpose of the X-Forwarded-Proto header?",options:["To specify the protocol version","To indicate the original protocol (HTTP/HTTPS) used by the client before the load balancer","To specify which backend protocol to use","To forward the prototype of the request object"],correctIndex:1,explanation:"X-Forwarded-Proto tells the backend server whether the original client connection used HTTP or HTTPS. When SSL termination occurs at the load balancer, the backend receives plain HTTP, so it can't tell if the original request was secure. This header is crucial for applications that need to enforce HTTPS redirects, generate correct absolute URLs, or set secure cookie flags. Without it, an app might generate 'http://' links even though the user accessed it via HTTPS. For example, Django uses the SECURE_PROXY_SSL_HEADER setting to check X-Forwarded-Proto, and Rails uses it to determine if a request was made over SSL."},{question:"What happens when all backend servers fail health checks?",options:["The load balancer shuts down","Behavior varies: some return 503, others fall back to sending traffic to all servers","All traffic is dropped silently","The load balancer creates new server instances"],correctIndex:1,explanation:"When all backends are unhealthy, load balancer behavior depends on configuration. Some load balancers return 503 Service Unavailable to all clients. Others implement a 'panic mode' or 'all backends down' fallback where they send traffic to all servers anyway  the reasoning being that health checks might be too strict and some servers might still be partially functional. HAProxy supports 'option allbackups' and backup server configurations for this scenario. AWS ALB returns 503 when no healthy targets exist. Nginx can be configured with 'backup' servers that only receive traffic when all primary servers are down. It's critical to plan for this scenario in your architecture."},{question:"What is a 'least bandwidth' load balancing algorithm?",options:["Routes to the server using the least network bandwidth currently","Routes to the server with the smallest bandwidth capacity","Routes to reduce overall bandwidth usage","Routes to the server with the most available bandwidth"],correctIndex:0,explanation:"The least bandwidth algorithm routes new requests to the server currently consuming the least amount of network bandwidth (measured in Mbps). This is useful for workloads with varying response sizes  for example, a file download service where some requests return 10MB files and others return 100KB files. Unlike least connections which treats all connections equally, least bandwidth accounts for the actual data transfer happening on each server. This prevents scenarios where one server is handling fewer but larger transfers while others handle many small ones. It's available in enterprise load balancers like F5 BIG-IP and Citrix ADC."},{question:"What is rate limiting at the load balancer level?",options:["Limiting the speed of the load balancer itself","Restricting the number of requests a client can make within a time period","Limiting the number of backend servers","Slowing down all traffic equally"],correctIndex:1,explanation:"Rate limiting at the load balancer restricts how many requests a particular client (identified by IP, API key, or other attributes) can make within a defined time window. This protects backend servers from abuse, DDoS attacks, and runaway clients. For example, you might limit an IP to 100 requests per minute. Requests exceeding the limit receive a 429 Too Many Requests response. Implementing rate limiting at the load balancer is more efficient than at the application level because rejected requests never reach backend servers. Nginx supports rate limiting with the 'limit_req' module, and AWS ALB integrates with AWS WAF for advanced rate limiting rules."},{question:"What is the difference between Layer 4 NAT mode and Layer 4 DR (Direct Return) mode?",options:["NAT modifies both request and response packets; DR only modifies request packets since responses go directly to clients","They are identical","NAT is faster than DR","DR requires L7 inspection"],correctIndex:0,explanation:"In NAT mode, the load balancer rewrites packet headers for both incoming requests (changing destination to the backend) and outgoing responses (changing source back to the VIP). All traffic flows through the load balancer, making it a potential bottleneck. In Direct Return (DR) mode, the load balancer only modifies incoming request packets, and the backend server sends responses directly to the client, bypassing the load balancer. DR is much more scalable because response traffic (typically 10x larger than request traffic) doesn't burden the load balancer. However, DR requires backends to be on the same L2 network and accept traffic for the VIP address. LVS (Linux Virtual Server) supports both modes."},{question:"What is a 'target group' in AWS load balancer terminology?",options:["A group of users targeted by the application","A logical grouping of backend targets (instances, IPs, or lambdas) that receive traffic from a load balancer","A group of load balancers","A security group for targets"],correctIndex:1,explanation:"A target group is an AWS concept that defines a set of backend targets (EC2 instances, IP addresses, Lambda functions, or other ALBs) along with health check settings and routing configuration. The load balancer routes requests to targets within a target group based on the listener rules. You can have multiple target groups attached to a single ALB with different routing rules  for example, one target group for /api/* and another for /web/*. Target groups also enable blue-green and canary deployments by adjusting weights between two target groups. This abstraction decouples the load balancer configuration from the specific backend instances."},{question:"How does a load balancer handle gRPC traffic?",options:["gRPC cannot be load balanced","Using L7 load balancing with HTTP/2 support","Only L4 load balancing works for gRPC","By converting gRPC to REST"],correctIndex:1,explanation:"gRPC uses HTTP/2 as its transport protocol, so a load balancer needs HTTP/2 support for effective gRPC load balancing. L7 load balancers that understand HTTP/2 can route individual gRPC requests within a single HTTP/2 connection to different backends  this is crucial because HTTP/2 multiplexes many requests over one connection, so L4 balancing would send all requests on a connection to the same backend. AWS ALB supports gRPC natively since 2020, and Envoy proxy is widely used for gRPC load balancing in service meshes. Without proper HTTP/2-aware L7 balancing, you'd lose the benefits of distributing gRPC calls across backends."},{question:"What is the 'power of two choices' load balancing algorithm?",options:["Choosing between only two backend servers","Randomly selecting two servers and routing to the one with fewer connections","Alternating between two algorithms","Using two load balancers simultaneously"],correctIndex:1,explanation:"The 'power of two choices' (P2C) algorithm randomly selects two backend servers and routes the request to whichever has fewer active connections. Despite its simplicity, it provides near-optimal load distribution  mathematically proven to reduce maximum load from O(log n / log log n) with random to O(log log n) with P2C. It's much cheaper to compute than full least-connections (which must check all servers) while providing similar benefits. Nginx Plus uses this as its 'random two least_conn' method. It's particularly effective in large clusters where checking every server's status on every request would be too expensive, and it avoids the herd behavior where multiple load balancers simultaneously route to the same 'least loaded' server."},{question:"What is a 'listener' in load balancer configuration?",options:["A monitoring tool","A process that checks for incoming connections on a specific port and protocol","A logging component","A backend server waiting for connections"],correctIndex:1,explanation:"A listener is a configuration component that defines a port and protocol that the load balancer listens on for incoming client connections. For example, you might have one listener on port 80 (HTTP) that redirects to HTTPS, and another on port 443 (HTTPS) that forwards to your target group. Each listener has rules that determine how to route requests to backend targets. AWS ALB supports multiple listeners on a single load balancer, each with its own set of routing rules. Listeners are where you configure SSL certificates, default actions, and the initial entry point for all client traffic hitting the load balancer."},{question:"Why might you choose an L4 load balancer over an L7 for a database cluster?",options:["L4 is cheaper","Databases use TCP protocols that don't need HTTP inspection, and L4 offers lower latency","L7 doesn't support TCP","L4 provides better security"],correctIndex:1,explanation:"Database protocols like MySQL (port 3306) and PostgreSQL (port 5432) use raw TCP connections, not HTTP. An L7 load balancer that inspects HTTP would be useless and add unnecessary overhead for database traffic. An L4 load balancer simply forwards TCP connections based on IP and port, which is exactly what's needed. It adds minimal latency since there's no packet inspection or protocol parsing. AWS NLB is commonly used to load balance database read replicas or to provide a single endpoint for a database cluster. The lower latency of L4 is particularly important for databases where every millisecond counts for query response times."},{question:"What is 'session persistence' and how is it different from 'sticky sessions'?",options:["They are the same concept  both ensure requests from one client go to the same backend","Session persistence stores data; sticky sessions route traffic","Session persistence is for databases; sticky sessions for web servers","Sticky sessions are permanent; session persistence is temporary"],correctIndex:0,explanation:"Session persistence and sticky sessions are two names for the same concept  ensuring that all requests from a particular client are routed to the same backend server during a session. The terms are used interchangeably across different load balancer vendors: F5 calls it 'persistence,' AWS calls it 'stickiness,' and HAProxy calls it 'stick-tables.' Implementation methods include cookie-based (load balancer sets a cookie identifying the backend), source IP-based (hash of client IP), and SSL session ID-based persistence. The choice of method depends on whether you're doing L4 or L7 balancing and whether clients support cookies."},{question:"What is the risk of having a single load balancer in your architecture?",options:["Higher cost","It becomes a single point of failure (SPOF)","Slower performance","More complex configuration"],correctIndex:1,explanation:"A single load balancer creates a single point of failure  if it goes down, all traffic stops regardless of how many healthy backend servers exist. This defeats the purpose of using a load balancer for high availability. The solution is to deploy load balancers in pairs (active-passive or active-active) with failover mechanisms. In cloud environments, managed load balancers like AWS ALB are inherently redundant across multiple availability zones, eliminating this concern. For on-premise deployments, tools like keepalived implement VRRP to manage VIP failover between HAProxy or Nginx instances, ensuring continuous availability even if one load balancer node fails."},{question:"What is Maglev hashing in the context of load balancing?",options:["A hashing algorithm for SSL certificates","Google's consistent hashing algorithm designed for software load balancers with fast lookup","A magnetic storage-based caching algorithm","A hash algorithm for compressing HTTP headers"],correctIndex:1,explanation:"Maglev hashing is a consistent hashing algorithm developed by Google for their Maglev network load balancer. It provides O(1) lookup time using a precomputed lookup table, making it significantly faster than ring-based consistent hashing which requires O(log n) lookups. Maglev hashing generates a permutation table for each backend, creating a fixed-size lookup table where each entry maps to a backend. When a backend is added or removed, minimal disruption occurs (similar to consistent hashing). It also provides excellent load uniformity across backends. Google's Maglev handles over a million requests per second per machine, and the algorithm was published in their 2016 NSDI paper."},{question:"What is the purpose of HTTP keep-alive in the context of load balancing?",options:["To keep the load balancer running","To reuse TCP connections for multiple HTTP requests, reducing connection overhead","To monitor server health","To maintain session state"],correctIndex:1,explanation:"HTTP keep-alive (persistent connections) allows multiple HTTP requests to be sent over a single TCP connection instead of opening a new connection for each request. In the context of load balancing, this reduces the overhead of TCP handshakes and TLS negotiations between clients and the load balancer, and between the load balancer and backends. However, it creates a tension with load balancing: if a client's keep-alive connection always goes to the same backend, new requests won't be distributed. L7 load balancers like ALB solve this by multiplexing  they can maintain keep-alive connections on both sides but route individual requests within those connections to different backends."},{question:"How does Envoy proxy differ from traditional load balancers like HAProxy?",options:["Envoy doesn't support load balancing","Envoy was designed as a sidecar proxy for microservices with advanced observability features","Envoy only supports L4 balancing","Envoy is faster than HAProxy in all cases"],correctIndex:1,explanation:"Envoy was designed by Lyft specifically for modern microservices architectures, functioning as a sidecar proxy deployed alongside each service instance. Unlike HAProxy which is typically deployed as a centralized load balancer, Envoy is the data plane in service meshes like Istio. Envoy provides advanced features like automatic retries, circuit breaking, zone-aware routing, and rich L7 observability with distributed tracing integration. It supports dynamic configuration via xDS APIs, allowing configuration changes without restarts. While HAProxy excels as a high-performance edge proxy, Envoy shines in service-to-service communication within distributed systems where observability and dynamic configuration are crucial."},{question:"What is a 'backend' vs 'frontend' in HAProxy terminology?",options:["Frontend is the UI, backend is the database","Frontend defines how requests are received; backend defines where they are forwarded","Frontend is external facing; backend is internal only","They are interchangeable terms"],correctIndex:1,explanation:"In HAProxy, a 'frontend' section defines how incoming connections are received  it specifies the bind address/port, protocol mode (HTTP or TCP), timeouts, and ACL rules for routing decisions. A 'backend' section defines the pool of servers that will handle forwarded requests, including the server list, load balancing algorithm, and health check configuration. A frontend can route to multiple backends based on ACL rules. For example, a frontend listening on port 443 might route /api/* to an api-backend and /static/* to a static-backend. This separation of concerns makes HAProxy configurations clean and modular."},{question:"What is circuit breaking in load balancing?",options:["Physically disconnecting network cables","Automatically stopping traffic to a failing backend to prevent cascade failures","Breaking long-running connections","A backup power system for load balancers"],correctIndex:1,explanation:"Circuit breaking is a pattern where the load balancer (or proxy) monitors error rates for each backend and automatically stops sending traffic when failures exceed a threshold, preventing cascade failures. Like an electrical circuit breaker, it 'trips open' when too many errors occur, returning immediate errors to clients instead of waiting for timeouts. After a cooldown period, it enters a 'half-open' state and allows a few test requests through. If they succeed, the circuit closes and normal traffic resumes. Envoy and Istio implement sophisticated circuit breaking with configurable thresholds for maximum connections, pending requests, and retries. This is essential in microservices to prevent one failing service from taking down the entire system."},{question:"What is the 'source' load balancing algorithm in HAProxy?",options:["Routes based on the source code of the application","Hashes the source IP to always route the same client to the same server","Selects the source server with the most resources","Routes based on the country of origin"],correctIndex:1,explanation:"The 'source' algorithm in HAProxy hashes the client's source IP address to select a backend server, ensuring the same client consistently reaches the same server. It's essentially IP hash load balancing. This provides session affinity at L4 without requiring cookies or application changes. The formula is typically hash(source_ip) mod num_servers. The main disadvantage is that if a server is added or removed, the hash changes for many clients, disrupting existing sessions. You can mitigate this with 'hash-type consistent' which uses consistent hashing to minimize remapping. It's commonly used for TCP load balancing where cookie-based affinity isn't available."},{question:"What is 'request queuing' in a load balancer?",options:["Queuing requests for later processing when all backends are busy","Sorting requests by priority","Buffering requests to batch them","Caching requests for replay"],correctIndex:0,explanation:"Request queuing occurs when the load balancer holds incoming requests in a queue when all backend servers have reached their maximum connection limits, rather than immediately rejecting them. The queue releases requests as backends become available. This smooths out traffic spikes and prevents immediate 503 errors during brief overload periods. HAProxy implements this with 'maxconn' on server definitions (limiting per-server connections) combined with the 'queue' timeout. However, queuing increases latency and can mask capacity problems  if the queue grows too long, it's better to shed load with 503 responses than to keep clients waiting. Setting appropriate queue timeouts is critical."},{question:"How does a load balancer handle HTTP/2 server push?",options:["It blocks server push entirely","It must support HTTP/2 end-to-end to proxy push frames correctly","It converts push to regular requests","Server push doesn't work with load balancers"],correctIndex:1,explanation:"HTTP/2 server push allows servers to proactively send resources to clients before they're requested. For a load balancer to correctly handle this, it must support HTTP/2 on both the client-facing side and the backend connections, properly proxying PUSH_PROMISE frames. If the load balancer terminates HTTP/2 and connects to backends via HTTP/1.1, server push won't work because HTTP/1.1 doesn't support it. AWS ALB supports HTTP/2 on the frontend but communicates with backends using HTTP/1.1, so server push doesn't work through ALB. Nginx supports HTTP/2 end-to-end (with 'grpc_pass' or HTTP/2 upstream) enabling server push passthrough."},{question:"What is the 'agent check' feature in HAProxy?",options:["A security scanning tool","An auxiliary health check where the backend server reports its own status and weight","Checking if the HAProxy agent is running","A monitoring agent installed on each server"],correctIndex:1,explanation:"HAProxy's agent check is a supplementary health check mechanism where the backend server runs a small agent (listening on a configured port) that reports its status and desired weight. The agent can respond with values like 'up', 'down', 'ready', a percentage to set weight (e.g., '75%'), or 'drain' to stop new connections. This allows application-aware load management  for example, a server under heavy CPU load can tell HAProxy to reduce its weight to 25%, and a server during maintenance can report 'drain' to stop receiving new traffic. Agent checks work alongside regular health checks, giving backend servers active participation in traffic management."},{question:"What is Anycast and how is it used with load balancing?",options:["Broadcasting to all servers simultaneously","Multiple servers share the same IP address; the network routes clients to the nearest one","Sending requests to any available server randomly","A multicast protocol for load balancers"],correctIndex:1,explanation:"Anycast is a network addressing method where the same IP address is announced by multiple servers in different geographic locations. The internet's BGP routing automatically directs clients to the nearest (in terms of network hops) server advertising that IP. This provides natural geographic load balancing without any application-level load balancer. Cloudflare uses anycast extensively  their IP addresses are announced from 300+ data centers worldwide, so a user in Tokyo reaches a Tokyo server while a user in London reaches a London server. Anycast also provides built-in DDoS resilience since attack traffic is distributed across all locations. It's commonly combined with traditional load balancing within each location."},{question:"What is 'connection multiplexing' in an L7 load balancer?",options:["Using multiple network interfaces","Sending requests from multiple clients over a single backend connection","Connecting to multiple load balancers simultaneously","Multiplying the number of available connections"],correctIndex:1,explanation:"Connection multiplexing means the L7 load balancer can take HTTP requests from many different client connections and send them over fewer backend connections. For example, 1000 client connections might be served by just 10 backend connections, with the load balancer interleaving requests. This dramatically reduces the connection overhead on backend servers, which is especially valuable for servers with high connection setup costs. AWS ALB does this by default  it maintains a small pool of connections to each backend and multiplexes client requests over them. This is one reason why backend servers see far fewer connections than expected when behind an ALB."},{question:"When would you use a TCP (L4) proxy mode in HAProxy instead of HTTP (L7)?",options:["When serving web pages","When load balancing non-HTTP protocols like databases, SMTP, or custom TCP protocols","When you need URL-based routing","When you need cookie-based persistence"],correctIndex:1,explanation:"TCP mode (mode tcp) in HAProxy is used when you need to load balance non-HTTP protocols. Databases (MySQL, PostgreSQL, Redis), mail servers (SMTP), LDAP, custom binary protocols, and any other TCP-based service requires L4 proxy mode because HAProxy wouldn't understand the application protocol for L7 inspection. TCP mode is also used for SSL passthrough where you don't want HAProxy to decrypt traffic. It's faster than HTTP mode since there's no header parsing. For example, to load balance MySQL read replicas, you'd use 'mode tcp' with 'option mysql-check' for health checking. The tradeoff is losing L7 features like path routing and header manipulation."},{question:"What is the role of a load balancer in a microservices architecture?",options:["Only external traffic management","Both external traffic routing and internal service-to-service discovery and routing","Database connection pooling only","Log aggregation"],correctIndex:1,explanation:"In microservices, load balancers serve dual roles: external-facing load balancers (edge proxies) route client traffic, while internal load balancers handle service-to-service communication. An API gateway (L7 LB) at the edge routes requests to appropriate microservices, while internal LBs or service meshes (like Envoy sidecar proxies) balance traffic between service instances. For example, an order service calling the inventory service needs load balancing across inventory instances. Kubernetes Services with kube-proxy provide built-in L4 load balancing, while Istio adds sophisticated L7 balancing. This multi-layer load balancing is essential because microservices create many more internal network hops than monolithic architectures."},{question:"What is 'graceful degradation' in the context of load balancing?",options:["Gradually shutting down the load balancer","Maintaining partial service when backends fail rather than total outage","Slowing down request processing during peak times","Downgrading to a simpler load balancing algorithm"],correctIndex:1,explanation:"Graceful degradation means the system continues to provide reduced but functional service when some components fail, rather than failing completely. In load balancing, this might mean returning cached or static content when backend servers are down, directing overflow traffic to a 'sorry' page, or serving a read-only version when write servers fail. For example, Netflix uses graceful degradation extensively  if their recommendation service is down, they show generic popular content instead of personalized recommendations. Load balancers implement this through fallback backends, custom error pages, and integration with circuit breakers that redirect to fallback services when primary backends are failing."},{question:"What is the difference between 'health check' and 'liveness probe'?",options:["They are identical concepts","Health checks are performed by load balancers; liveness probes are used by container orchestrators like Kubernetes","Health checks are manual; liveness probes are automatic","Health checks are for hardware; liveness probes are for software"],correctIndex:1,explanation:"While both verify if a service is running, health checks are performed by load balancers to decide whether to route traffic to a backend, while liveness probes are Kubernetes concepts used to determine whether to restart a container. A load balancer health check failure removes the target from the routing pool but doesn't restart it. A Kubernetes liveness probe failure causes kubelet to restart the container. They can even use different endpoints  a liveness probe might check basic process health while a health check verifies the service can handle requests. In practice, you often have both: a /livez endpoint for Kubernetes and a /readyz endpoint for the load balancer's health check."},{question:"What problem does 'least outstanding requests' solve compared to 'least connections'?",options:["It handles more protocols","It accounts for requests in the load balancer's queue, not just established connections","It's faster to compute","It uses less memory"],correctIndex:1,explanation:"Least outstanding requests considers all pending requests including those queued at the load balancer, not just established TCP connections to backends. A server might show few active connections if it's fast at accepting them but slow at processing them. Least outstanding requests tracks the total number of in-flight requests (queued + being processed) per backend. This gives a more accurate picture of actual server load. AWS ALB uses 'least outstanding requests' as an alternative to round-robin for target group routing. It's especially effective when request processing times vary significantly and you want to ensure faster servers naturally receive more traffic."},{question:"What is 'traffic mirroring' (shadowing) in load balancing?",options:["Creating backup copies of network traffic for storage","Duplicating live traffic to a test environment for testing without impacting production","Redirecting all traffic to a mirror server","Encrypting traffic twice for security"],correctIndex:1,explanation:"Traffic mirroring copies live production requests and sends them to a separate test/staging environment simultaneously, allowing you to test new service versions with real traffic patterns without affecting the production response. The client only receives the response from the primary backend; the mirror's response is discarded. This is invaluable for validating new deployments  you can compare the mirror's behavior against production to catch bugs before they impact users. Envoy, Istio, and AWS ALB all support traffic mirroring. For example, before launching a new recommendation engine, you'd mirror production traffic to it and compare results against the existing system."},{question:"What is 'retry budgets' in the context of load balancer retries?",options:["A financial budget for retry infrastructure","Limiting the total percentage of requests that can be retries to prevent retry storms","The number of times a request can be retried","A budget allocated for timeout extensions"],correctIndex:1,explanation:"A retry budget limits the ratio of retry requests to total requests (e.g., retries should not exceed 20% of original traffic). Without a budget, retries can cascade catastrophically: if a backend is overloaded and failing, retries add even more load, causing more failures and more retries  a 'retry storm.' The budget ensures retries are helpful during transient errors but are automatically throttled during systematic failures. Envoy implements retry budgets with the 'retry_budget' configuration, setting max_retries as a percentage of active requests. This is a critical safety mechanism in microservices where multiple services retrying simultaneously can amplify a minor issue into a complete outage."},{question:"How does AWS ALB handle 'slow loris' attacks?",options:["It cannot handle them","It sets idle timeouts and manages connections independently from backend servers","It blocks all slow connections","It requires a separate WAF"],correctIndex:1,explanation:"Slow loris attacks work by opening many connections and sending data very slowly, tying up server resources. AWS ALB mitigates this because it acts as a full proxy  it buffers the entire client request before forwarding it to the backend, so the backend server isn't held up by slow clients. ALB also has idle connection timeouts that close stale connections. Since ALB is a managed service with massive capacity, it can absorb the connection overhead that would overwhelm a single server. Additionally, combining ALB with AWS WAF provides rate-limiting rules that can further protect against such attacks. This proxy-based buffering is a fundamental advantage of L7 load balancers over L4."},{question:"What does 'least pending requests' mean in the context of Envoy proxy?",options:["Requests waiting to be processed at the proxy","Requests that haven't been retried yet","Requests pending cancellation","Requests pending authentication"],correctIndex:0,explanation:"In Envoy, 'least pending requests' (also called 'least request') routes new requests to the backend with the fewest outstanding (pending) requests. This includes requests currently being processed by the backend and those queued at the proxy level. It's Envoy's equivalent of least connections but operates at the request level rather than connection level  this distinction matters with HTTP/2 where multiple requests share a single connection. Envoy's implementation uses the 'power of two choices' optimization by default: it randomly picks two backends and selects the one with fewer pending requests, providing near-optimal distribution with minimal overhead. This is the recommended algorithm for most HTTP workloads in Envoy and Istio service meshes."},{question:"What is 'priority-based routing' in load balancing?",options:["Routing VIP customers first","Routing to primary backends first, falling back to secondary when primaries are unhealthy","Processing high-priority requests faster","Assigning higher network priority to certain packets"],correctIndex:1,explanation:"Priority-based routing assigns priority levels to different groups of backend servers. Traffic is first routed to the highest-priority group; only when that group becomes unhealthy (below a minimum healthy threshold) does traffic overflow to the next priority level. This is useful for preferring local servers over remote ones, or primary servers over backup servers. For example, you might assign priority 1 to servers in the same AZ and priority 2 to servers in other AZs, routing cross-AZ only when local capacity is insufficient. AWS ALB doesn't natively support this, but Envoy implements it through priority levels in clusters, and HAProxy achieves it with 'backup' server directives."},{question:"What is 'locality-aware routing' in service mesh load balancing?",options:["Routing based on language locale","Preferring backends in the same zone/region to reduce latency and cross-zone costs","Routing to locally installed software","Routing based on local time"],correctIndex:1,explanation:"Locality-aware routing preferentially routes requests to backends in the same zone, availability zone, or region as the client to minimize latency and avoid cross-zone data transfer costs. In cloud environments, cross-AZ traffic incurs charges and adds 1-2ms of latency. Istio and Envoy implement locality-aware routing by considering zone, region, and sub-zone labels. When enough healthy local endpoints exist, traffic stays local; when local capacity is insufficient, it overflows to other zones proportionally. For example, in a Kubernetes cluster spanning 3 AZs, a pod in us-east-1a preferentially communicates with other pods in us-east-1a. This can save significant costs on high-traffic internal services."},{question:"What is a 'service mesh' and how does it relate to load balancing?",options:["A physical network topology","An infrastructure layer that handles service-to-service communication including load balancing, with sidecar proxies","A type of load balancer hardware","A mesh of interconnected load balancers"],correctIndex:1,explanation:"A service mesh is a dedicated infrastructure layer for managing service-to-service communication in microservices architectures. It deploys sidecar proxies (like Envoy) alongside each service instance, handling load balancing, retries, circuit breaking, mutual TLS, and observability transparently. Instead of each service implementing its own load balancing logic, the sidecar handles it uniformly. Istio (using Envoy sidecars) is the most popular service mesh. The mesh provides consistent, centrally-managed traffic policies across all services without modifying application code. This decouples networking concerns from business logic, but adds complexity and resource overhead  each sidecar consumes CPU and memory."},{question:"What is 'outlier detection' in Envoy/Istio load balancing?",options:["Detecting unusual user behavior","Automatically ejecting backends that show abnormal error rates or latency","Finding misconfigured servers","Detecting DDoS attacks"],correctIndex:1,explanation:"Outlier detection is Envoy's mechanism for automatically identifying and ejecting misbehaving backends from the load balancing pool. It monitors metrics like consecutive errors (5xx responses) or gateway errors, and temporarily removes backends that exceed thresholds. For example, if a backend returns 5 consecutive 503 errors, it's ejected for 30 seconds. This is similar to circuit breaking but operates per-backend rather than on the overall cluster. Ejected hosts are periodically allowed back in to test if they've recovered. It's configured in Istio via DestinationRule resources. Outlier detection is critical in microservices where a single failing instance can slow down the entire system if traffic keeps being sent to it."},{question:"What is the purpose of 'retry policies' in load balancing?",options:["Retrying the load balancer configuration","Automatically retrying failed requests on different backends to handle transient failures","Retrying health checks more frequently","Retrying DNS resolution"],correctIndex:1,explanation:"Retry policies allow the load balancer to automatically retry failed requests on different backend servers, recovering from transient failures without client involvement. When a request to backend A fails with a 503, the load balancer can transparently retry it on backend B. This is especially valuable for handling single-server failures, transient network issues, and server restarts during deployments. However, retries must be configured carefully  they should only apply to idempotent requests (GET, not POST) to avoid duplicate side effects, and must include retry budgets to prevent retry storms. Envoy supports sophisticated retry policies including per-try timeouts, retry conditions (5xx, connection failure), and maximum retry attempts."},{question:"What is 'header-based routing' in an L7 load balancer?",options:["Routing based on packet headers","Routing decisions based on specific HTTP header values like API version or user agent","Adding headers to every request","Removing headers from responses"],correctIndex:1,explanation:"Header-based routing allows an L7 load balancer to examine specific HTTP headers and route requests to different backend groups based on their values. For example, routing requests with 'X-API-Version: v2' to a new API backend while 'X-API-Version: v1' goes to the legacy backend. This enables sophisticated traffic management like A/B testing (routing based on experiment headers), canary releases (routing based on beta-user headers), and multi-tenant routing. AWS ALB supports header-based routing in its listener rules, and Envoy provides extremely flexible header matching including regex, prefix, and exact matching. This is a powerful L7 capability that's impossible with L4 load balancers."},{question:"What is 'TLS re-encryption' (SSL bridging) at the load balancer?",options:["Using two different SSL certificates","Decrypting client TLS at the LB and re-encrypting with a different certificate to backends","Encrypting already encrypted traffic","Converting TLS to a newer version"],correctIndex:1,explanation:"TLS re-encryption (SSL bridging) means the load balancer terminates the client's TLS connection, inspects/routes the HTTP request at L7, and then establishes a new TLS connection to the backend server. This provides the benefits of both worlds: L7 inspection capabilities (content routing, header manipulation) AND encrypted backend communication. The backend TLS certificate is typically an internal CA certificate different from the public-facing certificate. This is required in environments where regulations mandate encryption of data in transit even within the internal network. AWS ALB supports this  you can configure HTTPS listeners with HTTPS target groups, achieving end-to-end encryption while still getting L7 features."},{question:"What is the difference between 'active' and 'passive' health checks?",options:["Active checks are faster; passive are slower","Active checks proactively send requests; passive checks monitor actual traffic for failures","Active checks run during the day; passive checks run at night","Active checks require agents; passive checks don't"],correctIndex:1,explanation:"Active health checks proactively send periodic test requests (HTTP GET, TCP connect) to backend servers to verify their health, regardless of whether real traffic is flowing. Passive health checks (also called 'real traffic' checks) monitor the responses from actual client requests to detect failures  if a backend returns too many errors, it's marked unhealthy. Active checks detect issues even on idle backends but add extra traffic. Passive checks detect issues based on real behavior but can't identify problems on backends that aren't receiving traffic. The best practice is to use both together  Envoy and Nginx Plus support combining active and passive health checking for robust failure detection."},{question:"How do load balancers handle HTTP/3 (QUIC)?",options:["They don't support it","They terminate QUIC at the edge and typically proxy to backends over HTTP/2 or HTTP/1.1","They pass QUIC through unchanged","They convert QUIC to TCP"],correctIndex:1,explanation:"HTTP/3 uses QUIC (a UDP-based transport protocol) instead of TCP, which presents challenges for load balancers. Most L7 load balancers terminate QUIC at the edge, benefiting from QUIC's faster connection establishment (0-RTT) and better handling of packet loss for end users, then proxy requests to backends over HTTP/2 or HTTP/1.1 over TCP. This is practical because the internal network typically has low latency and packet loss, making QUIC's benefits less relevant for backend connections. Cloudflare, Google Cloud, and AWS CloudFront support HTTP/3 termination. L4 load balancers face challenges because QUIC's connection IDs don't map to traditional IP:port tuples, requiring QUIC-aware load balancing."},{question:"What is the 'hash ring' approach to load balancing?",options:["Hashing requests in a circular sequence","Using consistent hashing where servers and keys are mapped to positions on a virtual ring","Arranging servers in a physical ring topology","A hardware ring buffer in the load balancer"],correctIndex:1,explanation:"The hash ring is the data structure underlying consistent hashing for load balancing. Servers are assigned positions on a circular hash space (0 to 2^32-1), and each request key is hashed to a position on the same ring. The request is routed to the nearest server clockwise on the ring. When a server is removed, only its portion of the ring is remapped to the next server; other mappings stay intact. Virtual nodes (multiple hash positions per physical server) improve uniformity. This approach is particularly valuable for caching proxies where you want the same content to consistently hit the same server to maximize cache hit rates. Nginx supports this with the 'hash' directive and 'consistent' parameter."},{question:"What is 'request hedging' in load balancing?",options:["Protecting requests with encryption","Sending the same request to multiple backends simultaneously and using the first response","Queuing requests as insurance against failure","Prioritizing certain requests over others"],correctIndex:1,explanation:"Request hedging sends redundant copies of a request to multiple backends simultaneously, returning whichever response arrives first and discarding the rest. This reduces tail latency  if one backend is slow, the duplicate on another backend may respond faster. Google uses hedging extensively to reduce p99 latency. The tradeoff is increased resource usage since you're doing N times the work for one response. A common optimization is 'delayed hedging'  send the first request normally and only hedge if no response arrives within a timeout (e.g., the p50 latency). gRPC and Envoy support hedging policies. It's most effective for read-only, idempotent operations where the extra backend load is acceptable."},{question:"What is 'load shedding' at the load balancer?",options:["Reducing the physical weight of the load balancer hardware","Deliberately dropping excess traffic to protect the system from overload","Shedding responsibility to backend servers","Reducing the number of backend servers"],correctIndex:1,explanation:"Load shedding is the practice of intentionally dropping or rejecting excess requests when the system is approaching or at capacity, preventing total system failure. It's better to reject 10% of requests cleanly with 503 responses than to accept everything and have 100% of requests fail due to overload. Load balancers implement this via connection limits, request rate limits, and queue depth limits. For example, Envoy's circuit breaker can limit maximum concurrent connections and pending requests per backend. Netflix and Google implement adaptive load shedding that adjusts thresholds based on current system health. Think of it like a nightclub with a maximum capacity  turning people away at the door keeps the experience good for everyone inside."},{question:"What is the 'proxy protocol' used by some load balancers?",options:["A protocol for communication between multiple load balancers","A protocol that prepends client connection info (IP, port) to the TCP connection for backends","A protocol for proxy server authentication","A protocol for load balancer health checks"],correctIndex:1,explanation:"The Proxy Protocol (developed by HAProxy's creator) is a simple protocol that prepends a small header to TCP connections containing the original client's IP address and port. This solves the problem of preserving client information when using L4 (TCP) load balancing, where HTTP headers like X-Forwarded-For aren't available because the load balancer doesn't inspect HTTP content. The header is added at the start of the TCP connection and contains: protocol version, source/destination IPs, and source/destination ports. AWS NLB supports Proxy Protocol v2, allowing backend applications to see the real client IP even through an L4 load balancer. The backend application must be configured to parse and strip this header."},{question:"What is an ingress controller in Kubernetes, and how does it relate to load balancing?",options:["A firewall for incoming traffic","A Kubernetes component that manages external access to services, typically implementing L7 load balancing","A controller that manages server ingress ports","A monitoring tool for incoming connections"],correctIndex:1,explanation:"A Kubernetes Ingress Controller is a component that implements the Kubernetes Ingress resource, providing L7 load balancing, SSL termination, and path/host-based routing for external traffic entering the cluster. Popular implementations include Nginx Ingress Controller, Traefik, HAProxy Ingress, and AWS ALB Ingress Controller. The Ingress resource defines routing rules (e.g., host: api.example.com, path: /v1  service-a), and the Ingress Controller translates these into actual load balancer configuration. This is different from Kubernetes Services (which provide basic L4 load balancing via kube-proxy). In production, the Ingress Controller is typically the entry point for all HTTP traffic into a Kubernetes cluster."},{question:"What is the 'random' load balancing algorithm best suited for?",options:["Production environments with strict SLAs","Simple scenarios or as a baseline; also forms the basis of 'power of two choices'","Databases requiring consistency","Financial trading systems"],correctIndex:1,explanation:"The random algorithm selects a backend server at random for each request. While seemingly primitive, it provides surprisingly good distribution due to the law of large numbers  over many requests, each server gets approximately equal traffic. It's stateless (no need to track connections or positions), making it the simplest algorithm to implement in distributed load balancers where state sharing is difficult. More importantly, random selection forms the basis of the 'power of two choices' algorithm (pick two random servers, choose the less loaded one), which is used in production by systems like Nginx Plus. Pure random is suitable for homogeneous servers with uniform request costs, but least-connections or P2C generally perform better in practice."},{question:"What is 'zone-aware routing' in AWS ALB?",options:["Routing based on DNS zones","Preferentially routing to targets in the same AZ as the LB node to reduce cross-zone traffic","Routing based on timezone","Routing between different AWS regions"],correctIndex:1,explanation:"Zone-aware routing in AWS means the ALB node in a particular availability zone preferentially routes to targets in the same AZ, reducing cross-zone data transfer (which costs money and adds latency). If the local AZ doesn't have enough healthy targets, traffic overflows to other AZs. This is related to the cross-zone load balancing setting  when cross-zone is disabled, each ALB node only routes to targets in its own AZ. For NLB, cross-zone is disabled by default to save costs, meaning zone-aware routing is the default behavior. Understanding this is important because it affects both cost optimization and latency, especially for high-traffic internal services communicating across AZs."},{question:"What are 'connection limits' on a load balancer and why are they important?",options:["Marketing limits on the number of customers","Maximum concurrent connections the LB or backends can handle, preventing resource exhaustion","Limits on physical network cables","Restrictions on who can connect"],correctIndex:1,explanation:"Connection limits define the maximum number of concurrent connections at both the load balancer level and per-backend level. Without limits, a traffic spike or DDoS attack could exhaust the load balancer's resources (memory, file descriptors) or overwhelm backend servers. HAProxy's 'maxconn' setting limits per-server connections  excess requests are queued. AWS ALB has built-in connection limits per target. When a backend hits its connection limit, the LB routes new requests to other backends or returns 503. These limits are essential for protecting both the load balancer and backends from cascade failures. The key is setting limits based on actual server capacity testing, not arbitrary numbers."},{question:"How does a load balancer support A/B testing?",options:["By running two separate load balancers","By routing a defined percentage or subset of users to different backend versions based on headers, cookies, or weights","By alternating between two server pools every day","By serving different content from the same servers"],correctIndex:1,explanation:"Load balancers enable A/B testing by splitting traffic between different backend versions based on rules. You can route based on cookies (users in experiment group A get version 1), headers (specific header values trigger routing to version 2), or weighted distribution (70% to version A, 30% to version B). AWS ALB supports weighted target groups for percentage-based splitting, and header/cookie conditions for rule-based routing. Envoy and Istio provide even more granular control through traffic splitting policies. This is crucial for data-driven product development  companies like Netflix, Google, and Amazon run thousands of simultaneous A/B tests by routing different user segments to different service versions through their load balancer configurations."},{question:"What is a 'sidecar proxy' pattern in load balancing?",options:["A backup load balancer","A proxy deployed alongside each application instance to handle its network communication","A secondary monitoring proxy","A proxy for offline processing"],correctIndex:1,explanation:"In the sidecar proxy pattern, a lightweight proxy (like Envoy) is deployed as a companion to each application instance, handling all inbound and outbound network communication. Instead of the application directly connecting to other services, it sends requests to its local sidecar, which handles load balancing, retries, circuit breaking, mutual TLS, and observability. This is the fundamental building block of service meshes like Istio. The sidecar intercepts traffic transparently using iptables rules, requiring no application code changes. The advantage is that complex networking logic is centralized in the proxy; the disadvantage is the added latency (typically <1ms) and resource overhead of running a proxy alongside every service instance."},{question:"What is the 'least time' algorithm in Nginx Plus?",options:["Routes to the most recently started server","Routes based on a combination of fewest active connections and lowest average response time","Routes to minimize total request time","Routes requests at specific time intervals"],correctIndex:1,explanation:"Nginx Plus's 'least_time' algorithm selects the backend with the best combination of fewest active connections and lowest average response time. You can configure it to optimize for either 'header' time (time to receive the first byte from the backend) or 'last_byte' (time to receive the complete response). This is more intelligent than pure least connections because it accounts for actual backend performance  a server with few connections might still be slow due to hardware issues or heavy processing. It's particularly effective for heterogeneous server pools where machines have different performance characteristics, automatically sending more traffic to faster servers without manual weight tuning."},{question:"What is 'request routing' vs 'connection routing' in load balancing?",options:["They are the same thing","Connection routing assigns a connection to a backend once; request routing can send each request within a connection to a different backend","Request routing is for HTTP; connection routing is for HTTPS","Connection routing is faster"],correctIndex:1,explanation:"Connection routing (L4) assigns an entire TCP connection to a backend server  all data on that connection goes to the same server. Request routing (L7) can inspect individual HTTP requests within a connection and route each one to potentially different backends. This distinction is critical with HTTP keep-alive and HTTP/2, where a single connection carries multiple requests. With connection routing, all requests on a keep-alive connection hit the same backend. With request routing, the load balancer can distribute individual requests from the same connection across multiple backends, achieving much better load distribution. This is why L7 load balancers generally provide better balance for HTTP workloads."},{question:"What is 'server warming' in the context of load balancing?",options:["Physically heating servers","Gradually increasing traffic to a new server so it can initialize caches and JIT compilations","Pre-installing software on servers","Running diagnostic tests before deployment"],correctIndex:1,explanation:"Server warming refers to the process of allowing a newly started or restarted server to gradually build up its operational state before receiving full production traffic. Cold JVM servers need time for JIT compilation to optimize hot paths, application caches need to be populated, database connection pools need to be established, and DNS caches need to fill. Sending full traffic to a cold server can cause high latency or failures. Load balancers support this through 'slow start' features that ramp up traffic over a configurable period. For example, ALB's slow start ramps traffic to a new target from 0% to full share over 30-900 seconds. This is especially critical for Java applications where JIT warmup can take several minutes."},{question:"What is 'upstream hashing' in Nginx?",options:["Hashing the Nginx configuration","Using a hash of a specified key (like URI or client IP) to consistently route requests to the same backend","Hashing passwords for upstream authentication","Encrypting data sent upstream"],correctIndex:1,explanation:"Nginx's 'hash' directive in upstream blocks computes a hash of a specified variable to determine which backend receives each request. For example, 'hash $request_uri consistent;' routes all requests for the same URI to the same backend, which is excellent for caching scenarios. You can hash on $remote_addr for IP-based affinity, $request_uri for cache optimization, or even custom variables. The 'consistent' parameter uses ketama consistent hashing (a hash ring approach), which minimizes remapping when servers are added or removed. Without 'consistent', adding a server remaps most requests. This is commonly used in front of caching servers where you want the same content to always hit the same cache node for maximum cache hit rates."},{question:"What is the 'keepalive' directive in Nginx upstream configuration?",options:["Enabling HTTP keep-alive for clients","Setting the maximum number of idle keep-alive connections to upstream servers per worker","Keeping upstream servers alive","A health check interval setting"],correctIndex:1,explanation:`The 'keepalive' directive in Nginx upstream blocks sets the maximum number of idle persistent connections to upstream servers that are cached per Nginx worker process. For example, 'keepalive 32;' means each worker can maintain up to 32 idle connections to the upstream group. These pre-established connections are reused for new requests, eliminating TCP handshake and TLS negotiation overhead. Without keepalive, Nginx opens a new connection for every request to the backend. This is critical for performance  establishing a new TLS connection can take 2-5 round trips. You must also set 'proxy_http_version 1.1;' and 'proxy_set_header Connection "";' in the location block for HTTP keep-alive to work with the upstream.`},{question:"What is 'horizontal scaling' and how does a load balancer enable it?",options:["Adding more CPU to existing servers","Adding more server instances and using a load balancer to distribute traffic across them","Scaling the load balancer itself","Adding more disk storage to servers"],correctIndex:1,explanation:"Horizontal scaling means adding more server instances to handle increased load, as opposed to vertical scaling which increases the resources of existing servers. The load balancer is the key enabler  it distributes incoming requests across all instances, making the group appear as a single endpoint to clients. Without a load balancer, you'd need to manually assign clients to specific servers. Horizontal scaling is preferred in modern architectures because it has no theoretical limit (just add more servers), provides fault tolerance (losing one server doesn't mean downtime), and is cost-effective (use many cheap commodity servers). Cloud auto-scaling groups work with load balancers to automatically add/remove instances based on demand."},{question:"What is the difference between 'proxy_pass' in Nginx and 'use_backend' in HAProxy?",options:["They serve completely different purposes","Both route requests to backends, but proxy_pass is a location-level directive while use_backend is a frontend-level conditional routing rule","proxy_pass is for TCP; use_backend is for HTTP","proxy_pass is deprecated"],correctIndex:1,explanation:"Both directives route traffic to backend servers, but they work within their respective architectures differently. Nginx's 'proxy_pass' is used in 'location' blocks within 'server' blocks  routing is determined by URL path matching in the server configuration. HAProxy's 'use_backend' is used in 'frontend' sections with ACL conditions  routing is determined by evaluating conditional rules against request attributes. HAProxy's ACL system is generally more flexible for complex routing logic (combining multiple conditions), while Nginx's location-based routing is more intuitive for path-based routing. For example, HAProxy can easily route based on a combination of path, header, and source IP in a single rule, which requires more configuration in Nginx."},{question:"What is 'TCP multiplexing' in the context of load balancing?",options:["Using multiple TCP ports","Combining multiple client TCP connections into fewer backend connections","Running multiple protocols over one TCP connection","Using TCP and UDP simultaneously"],correctIndex:1,explanation:"TCP multiplexing at the load balancer level means combining requests from many client connections into fewer backend connections. An L7 load balancer can accept thousands of client connections and forward their HTTP requests over a much smaller pool of persistent backend connections. This is possible because the load balancer buffers complete requests and multiplexes them over available backend connections. The benefit is massive: backend servers only need to manage a fraction of the total client connections, reducing memory usage and context switching overhead. AWS ALB performs TCP multiplexing by default. For a backend serving 100,000 clients through an ALB, it might only see 100 connections from the ALB."},{question:"What monitoring metrics are most important for a load balancer?",options:["Only request count","Request rate, error rate, latency percentiles, active connections, and backend health","Only CPU usage","Only bandwidth usage"],correctIndex:1,explanation:"Comprehensive load balancer monitoring requires several key metrics: request rate (throughput), HTTP error rates (4xx/5xx), latency percentiles (p50, p95, p99), active/new connections, backend health status, spillover count (rejected requests), and connection queue depth. Latency percentiles are particularly important  p99 latency reveals tail latency issues that averages would hide. Error rate spikes indicate backend problems. Connection metrics help with capacity planning. AWS ALB emits these as CloudWatch metrics: RequestCount, HTTPCode_Target_5XX_Count, TargetResponseTime, ActiveConnectionCount, and UnHealthyHostCount. Setting up alerts on these metrics (e.g., alert when 5xx rate exceeds 1% or p99 latency exceeds 2 seconds) is essential for maintaining reliability."},{question:"What is 'surge queue' in AWS Classic Load Balancer?",options:["A queue for surge pricing","A queue that holds requests when all backend instances are at capacity","A queue for handling traffic surges","A priority queue for important requests"],correctIndex:1,explanation:"The surge queue in AWS Classic Load Balancer (CLB) holds pending requests when all registered backend instances have reached their maximum connection capacity. The CLB can queue up to 1,024 requests. If the queue is full, additional requests receive 503 Service Unavailable errors (counted as 'SpilloverCount' metric). This was a significant limitation of CLB. AWS ALB replaced this with a more sophisticated approach  it doesn't have a fixed surge queue but uses its own connection pooling and request queuing internally with higher limits. Monitoring SpilloverCount was critical for CLB operators; a non-zero value indicated capacity problems requiring immediate attention. This was one of the reasons AWS recommended migrating from CLB to ALB."},{question:"What is a 'virtual server' in F5 BIG-IP load balancer terminology?",options:["A VM running behind the load balancer","A listener that represents the combination of IP address and port that clients connect to","A simulated test server","A server in the cloud"],correctIndex:1,explanation:"In F5 BIG-IP, a 'virtual server' is the front-end configuration object that defines the IP address and port combination where the load balancer accepts client connections. It's conceptually equivalent to HAProxy's 'frontend' or AWS ALB's 'listener.' A virtual server ties together the listening address, traffic processing profiles (SSL, HTTP compression, caching), and the pool of backend servers. For example, a virtual server at 10.0.0.1:443 with an SSL profile and HTTP profile would accept HTTPS connections and load balance them across a pool of web servers. F5 virtual servers support iRules, which are custom scripts that can perform complex traffic manipulation beyond standard load balancing."},{question:"What is the 'server_name' directive in Nginx used for in load balancing?",options:["Setting the hostname of the Nginx server","Matching the incoming Host header to select the appropriate server block for virtual hosting","Naming the backend servers","Setting DNS names for health checks"],correctIndex:1,explanation:"The 'server_name' directive in Nginx matches the Host header in incoming HTTP requests to determine which server block should process the request. This enables name-based virtual hosting  a single Nginx instance can serve multiple domains. Combined with upstream blocks, this becomes host-based load balancing: 'server_name api.example.com;' with 'proxy_pass http://api-backend;' routes API traffic to API servers, while 'server_name www.example.com;' routes to web servers. Without server_name matching, all traffic would hit the default server block regardless of the requested domain. This is equivalent to AWS ALB's host-based routing rules."},{question:"What is 'SSL session caching' and why is it important for load balancers?",options:["Caching web pages served over SSL","Storing TLS session parameters so subsequent connections skip the full handshake, reducing latency","Caching SSL certificates","Storing encrypted data"],correctIndex:1,explanation:"SSL session caching stores the negotiated TLS session parameters (session tickets or session IDs) so that when a client reconnects, it can resume the previous session instead of performing a full TLS handshake. A full handshake requires 2 round trips (TLS 1.2) of CPU-intensive key exchange. Session resumption reduces this to 1 round trip with minimal CPU usage. For a load balancer handling thousands of TLS connections per second, this dramatically reduces CPU usage and connection latency. The challenge with multiple load balancers is ensuring session data is shared  either using shared memory, memcached backends for session tickets, or sticky sessions. Nginx supports ssl_session_cache with shared memory zones."},{question:"What is 'ECMP' (Equal-Cost Multi-Path) routing and its relation to load balancing?",options:["An encryption protocol","A network-layer technique that distributes traffic across multiple equal-cost routes, enabling L3 load balancing","A load balancing algorithm for HTTP","A monitoring protocol"],correctIndex:1,explanation:"ECMP is a routing technique where network switches/routers distribute packets across multiple paths that have the same routing cost to a destination. This provides L3 (network layer) load balancing without any dedicated load balancer hardware. For example, if there are 4 equal-cost paths to a server subnet, the router distributes traffic across all 4 using a hash of the packet's 5-tuple (source/dest IP, source/dest port, protocol). ECMP is commonly used in data center leaf-spine architectures and in front of L4 load balancers to distribute traffic across multiple LB nodes. Maglev, Google's load balancer, uses ECMP with consistent hashing to ensure connection affinity even as paths change."},{question:"What happens when you configure too short a health check interval?",options:["Better reliability","Increased network overhead and potential for false positives, especially during minor GC pauses","No effect","Faster failover with no drawbacks"],correctIndex:1,explanation:"Setting health check intervals too short (e.g., 1 second with 2-failure threshold) creates several problems. First, it generates significant network overhead  checking 100 backends every second is 100 health check requests per second. Second, it increases false positives: a server experiencing a brief JVM garbage collection pause (200ms stop-the-world) or momentary CPU spike might fail consecutive checks and be incorrectly marked unhealthy. This causes unnecessary traffic shifting and potential cascading issues. AWS recommends health check intervals of 10-30 seconds with 2-3 failure thresholds for a good balance between detection speed and stability. The total detection time equals interval  failure threshold, so 10s interval  3 failures = 30 seconds to detect a real failure."},{question:"What is 'backend server weight auto-tuning'?",options:["Manually adjusting server weights daily","Automatically adjusting backend weights based on real-time performance metrics","Tuning the physical weight of servers","Automatic firmware updates"],correctIndex:1,explanation:"Backend server weight auto-tuning dynamically adjusts the load balancing weight of each backend server based on real-time performance metrics like response time, error rate, or CPU usage. Instead of static weights, the load balancer continuously adapts to actual server performance. If a server starts responding slowly (perhaps due to noisy neighbors in a cloud environment), its weight is automatically reduced. HAProxy's agent checks enable this  the backend agent reports a percentage weight based on local conditions. Envoy's adaptive load balancing and Netflix's gradient-based load balancing are more sophisticated implementations. This approach handles heterogeneous and dynamic environments better than static configuration."},{question:"What is 'traffic splitting' and how is it different from load balancing?",options:["They are identical","Load balancing distributes across identical servers; traffic splitting routes percentages to different service versions for deployment strategies","Traffic splitting is faster","Load balancing is more advanced"],correctIndex:1,explanation:"Traditional load balancing distributes traffic across identical servers for scalability and reliability. Traffic splitting directs specific percentages of traffic to different versions of a service for deployment strategies like canary releases, A/B testing, or traffic migration. For example, 95% to version 1.0 and 5% to version 2.0. While both involve distributing traffic, the intent is different  load balancing maximizes utilization and availability, while traffic splitting validates changes safely. Istio's VirtualService supports traffic splitting with exact percentages. AWS ALB implements it via weighted target groups. Traffic splitting is a deployment and experimentation tool that happens to use load balancer infrastructure."},{question:"What is 'headless service' in Kubernetes and when would you use it instead of a load balancer?",options:["A service without a UI","A service that returns all pod IPs directly via DNS instead of load balancing through a virtual IP","A service that runs without a controller","A temporary service"],correctIndex:1,explanation:"A headless Service in Kubernetes (clusterIP: None) doesn't allocate a virtual IP or use kube-proxy for load balancing. Instead, DNS resolution returns the IP addresses of all backing pods directly. This is useful when the client needs to know all endpoints and do its own load balancing or connection management  for example, databases like Cassandra or Elasticsearch that use their own cluster discovery. StatefulSets typically use headless services because each pod needs a stable, unique DNS name (pod-0.service, pod-1.service). Regular Services provide built-in L4 load balancing via kube-proxy, but headless services are necessary when the application layer needs direct control over which pod it connects to."},{question:"What is the impact of load balancer placement on SSL/TLS certificate management?",options:["No impact","Certificates only need to be managed on the load balancer (with SSL termination), simplifying certificate lifecycle management","Certificates become more complex with load balancers","Each backend needs its own unique certificate"],correctIndex:1,explanation:"With SSL termination at the load balancer, TLS certificates only need to be installed and renewed on the load balancer, not on every backend server. This dramatically simplifies certificate management  instead of updating certificates on 50 servers during renewal, you update one place. AWS ALB integrates with ACM (AWS Certificate Manager) for free, auto-renewing certificates. Without SSL termination, every backend server needs a valid certificate, and rotating them across a fleet is operationally complex. This centralization also makes it easier to enforce TLS policies (minimum version, cipher suites) consistently. The only downside is that if you need end-to-end encryption, you'll also need internal certificates on backends, though these can be self-signed from an internal CA."},{question:"What is the 'max_fails' directive in Nginx upstream configuration?",options:["Maximum number of backend servers that can fail","Number of failed requests to a backend before Nginx considers it unavailable","Maximum number of retries","Maximum number of 4xx errors allowed"],correctIndex:1,explanation:"The 'max_fails' directive in Nginx defines how many consecutive failed requests (timeouts or errors) to a backend server are needed before Nginx marks it as unavailable. Combined with 'fail_timeout' (the period during which failures are counted and the duration the server is marked down), it forms Nginx's passive health check mechanism. For example, 'server backend1 max_fails=3 fail_timeout=30s;' means after 3 failures within 30 seconds, the server is marked down for 30 seconds. After the timeout, Nginx tries the server again. This is simpler than active health checks (which require Nginx Plus) but has the drawback that real user requests are used to detect failures."},{question:"What is 'load balancer affinity' at the kernel level (IPVS)?",options:["CPU affinity for load balancer processes","Linux kernel-level load balancing using IP Virtual Server for high-performance L4 balancing","Affinity between load balancers in a cluster","Memory affinity for connection tables"],correctIndex:1,explanation:"IPVS (IP Virtual Server) is a Linux kernel module that provides L4 load balancing at the kernel level, offering much higher performance than userspace load balancers. Because IPVS operates in kernel space, it avoids the overhead of copying packets between kernel and user space. It supports multiple algorithms (round-robin, least connections, weighted, etc.) and modes (NAT, DR, IP tunneling). Kubernetes uses IPVS as an alternative to iptables for kube-proxy, providing better performance with large numbers of services. IPVS can handle millions of concurrent connections with O(1) complexity for connection scheduling, compared to iptables' O(n) chain traversal. It's the technology behind LVS (Linux Virtual Server) used by many large-scale deployments."},{question:"What is the difference between 'passive' and 'active' SSL/TLS termination?",options:["Active terminates SSL; passive just monitors","Active termination decrypts at the LB; passive termination uses SSL passthrough with traffic copying for inspection","Active is for production; passive is for testing","They are the same"],correctIndex:1,explanation:"Active SSL termination fully decrypts TLS at the load balancer, forwarding plain HTTP to backends  this is the standard SSL termination approach. Passive SSL termination (or SSL inspection/tap) copies the encrypted traffic stream for analysis (like intrusion detection) while letting the original encrypted traffic pass through to the backend. The passive approach requires access to the private key to decrypt the copy. Active termination is by far more common in load balancing. Passive inspection is primarily used in security appliances (IDS/IPS) deployed alongside load balancers. Some enterprise load balancers like F5 BIG-IP can perform both simultaneously  terminating SSL for load balancing while also sending decrypted copies to security analysis tools."},{question:"What is 'adaptive load balancing'?",options:["Adapting to new server hardware automatically","Dynamically adjusting the load balancing algorithm based on real-time system conditions and performance","A specific load balancing algorithm","Adapting to different network protocols"],correctIndex:1,explanation:"Adaptive load balancing goes beyond static algorithms by continuously monitoring backend health, response times, and resource utilization to dynamically adjust routing decisions. Rather than using a fixed algorithm like round-robin, it might shift more traffic to faster backends, reduce traffic to backends showing latency increases, and react to changing conditions in real time. Netflix's implementation uses gradient-based signals: if a backend's latency increases, it receives proportionally less traffic. Envoy's adaptive concurrency limits and AWS ALB's 'least outstanding requests' are related concepts. Adaptive load balancing is particularly valuable in cloud environments where server performance can vary due to noisy neighbors, thermal throttling, or background maintenance."},{question:"What is 'request collapsing' (coalescing) at the load balancer?",options:["Combining multiple small requests into one large request","Deduplicating identical concurrent requests and serving all clients from a single backend response","Collapsing failed requests into retries","Compressing requests to reduce size"],correctIndex:1,explanation:"Request collapsing (or coalescing) detects when multiple clients simultaneously request the same resource and sends only one request to the backend, serving all waiting clients with the same response. This is particularly valuable for cache-miss scenarios in CDNs and caching proxies  without collapsing, 1000 simultaneous requests for the same uncached resource would generate 1000 backend requests. Varnish and Nginx both implement this pattern. Nginx does it naturally for proxy_cache when proxy_cache_lock is enabled  concurrent requests for the same cache key are held while the first request populates the cache. This prevents the 'thundering herd' problem where cache misses amplify backend load."},{question:"What is the difference between ELB, ALB, and NLB in AWS?",options:["They are different names for the same service","ELB (Classic) is legacy L4/L7; ALB is modern L7; NLB is modern L4  each optimized for different use cases","ELB is for EC2, ALB is for ECS, NLB is for Lambda","They differ only in pricing"],correctIndex:1,explanation:"AWS has three load balancer types: Classic Load Balancer (CLB/ELB) is the legacy option supporting basic L4 and L7 features; Application Load Balancer (ALB) is the modern L7 load balancer with advanced features like path-based routing, WebSocket support, and Lambda targets; Network Load Balancer (NLB) is a high-performance L4 load balancer handling millions of requests per second with static IPs and TLS passthrough. AWS recommends ALB for HTTP/HTTPS workloads and NLB for TCP/UDP workloads or when you need extreme performance, static IPs, or preservation of source IP. CLB is effectively deprecated  AWS encourages migration to ALB or NLB for all new deployments."},{question:"How does a load balancer handle 'long-polling' requests?",options:["It rejects them","It must have sufficiently long idle timeouts to maintain the held connections without prematurely closing them","It converts them to WebSocket","It doesn't need any special handling"],correctIndex:1,explanation:"Long-polling involves clients sending a request that the server holds open until new data is available or a timeout occurs (typically 30-60 seconds). The load balancer must have idle timeout values higher than the long-poll timeout to avoid prematurely closing these intentionally held connections. For example, if your long-poll timeout is 60 seconds, the load balancer's idle timeout should be at least 65 seconds. Additionally, the load balancer should use least-connections rather than round-robin, since long-poll connections tie up server resources longer. AWS ALB's default idle timeout of 60 seconds may need to be increased for long-polling applications. Connection count per backend also matters more with long-polling since connections are held much longer."},{question:"What is 'consistent hashing with bounded loads'?",options:["Hashing with a maximum file size","A variant of consistent hashing that caps the maximum load on any single server to ensure even distribution","Hashing with bandwidth limits","Consistent hashing for large-scale systems only"],correctIndex:1,explanation:"Consistent hashing with bounded loads (proposed by Mirrokni et al.) adds a load-balancing constraint to consistent hashing: no server can receive more than (1 + ) times the average load. When a server reaches its bound, requests that would normally hash to it are redirected to the next available server on the ring. This solves the hot-spot problem that standard consistent hashing can suffer from (where unlucky hash distributions overload some servers) while maintaining the minimal-disruption property when servers are added or removed. Google Vimeo uses this algorithm for their load balancers. The  parameter (typically small, like 0.25) provides a tuning knob between strict load balance and hash consistency."},{question:"What is the role of 'back-end connection pooling' in load balancers?",options:["Pooling hardware resources","Maintaining reusable connections to backends to avoid per-request TCP/TLS handshake overhead","Pooling multiple backends into one","Creating backup connection paths"],correctIndex:1,explanation:"Backend connection pooling means the load balancer maintains a pool of pre-established TCP connections to each backend server, reusing them for multiple client requests rather than creating new connections each time. This eliminates the per-request overhead of TCP 3-way handshake (1 RTT) and TLS handshake (2-3 RTTs for TLS 1.2). For a load balancer handling 10,000 requests/second, this saves 10,000 handshakes per second. The pool typically has configurable minimum and maximum sizes, with idle connections kept alive using TCP keepalive packets. HAProxy's 'http-reuse' directive controls connection reuse strategies. This is one of the most impactful performance optimizations in any load balancer configuration."}],mm=[{question:"What is the primary motivation for decomposing a monolith into microservices?",options:["To reduce the total lines of code in the system","To enable independent deployment and scaling of services","To eliminate the need for a database","To ensure all teams use the same programming language"],correctIndex:1,explanation:"The primary motivation for microservices is enabling independent deployment and scaling. Each service can be deployed, updated, and scaled independently without affecting others, which accelerates release cycles and allows teams to work autonomously. Reducing total lines of code is not a goalmicroservices often increase overall code due to infrastructure overhead. Microservices still use databases (often one per service), and a key advantage is that teams can choose different programming languages (polyglot architecture)."},{question:"In Domain-Driven Design (DDD), what defines a Bounded Context?",options:["A physical server boundary where code is deployed","An explicit boundary within which a domain model is defined and applicable","The maximum number of entities allowed in a single database","A network firewall rule separating services"],correctIndex:1,explanation:"A Bounded Context in DDD is a logical boundary within which a particular domain model is consistent and meaningful. The same term (e.g., 'Order') can mean different things in different bounded contextsin the Sales context it represents a purchase intent, while in Shipping it represents a package to deliver. This is not about physical server boundaries or network rules; it's a semantic boundary that helps teams avoid model confusion. Bounded Contexts are one of the most effective tools for identifying service boundaries in microservices architectures."},{question:"Which communication pattern introduces the tightest coupling between microservices?",options:["Asynchronous messaging via a message broker","Event-driven publish/subscribe","Synchronous HTTP REST calls","Event sourcing with an append-only log"],correctIndex:2,explanation:"Synchronous HTTP REST calls create the tightest coupling because the caller must wait for the callee to respond, creating temporal couplingboth services must be available simultaneously. If the downstream service is slow or down, the caller is directly affected. Asynchronous messaging and pub/sub decouple services in time; the sender doesn't wait for a response. Event sourcing further decouples by recording state changes as events. In practice, synchronous calls are sometimes necessary (e.g., user-facing queries), but overusing them in service-to-service communication recreates the coupling problems of a monolith."},{question:"What is the Circuit Breaker pattern used for in microservices?",options:["Encrypting data in transit between services","Preventing cascading failures by failing fast when a downstream service is unhealthy","Load balancing traffic across multiple instances of a service","Compressing request payloads to reduce bandwidth"],correctIndex:1,explanation:"The Circuit Breaker pattern prevents cascading failures by monitoring calls to a downstream service and 'opening' the circuit (failing fast) when failures exceed a threshold. This stops the calling service from wasting resources on requests likely to fail, and gives the downstream service time to recover. It has three states: Closed (normal operation), Open (requests fail immediately), and Half-Open (test requests to check recovery). Libraries like Resilience4j and Netflix Hystrix implement this pattern. Without it, a single slow service can exhaust thread pools and bring down the entire system."},{question:"What is the Sidecar pattern in a microservices architecture?",options:["A secondary database replica running alongside the primary","A helper container deployed alongside the main application container to handle cross-cutting concerns","A backup service that takes over when the primary fails","A secondary API endpoint for internal-only traffic"],correctIndex:1,explanation:"The Sidecar pattern deploys a helper process or container alongside the main application container within the same pod (in Kubernetes) or host. The sidecar handles cross-cutting concerns like logging, monitoring, TLS termination, and service mesh proxying without modifying the application code. Envoy proxy in Istio is a classic exampleit intercepts all network traffic for observability and security. This pattern promotes separation of concerns and allows infrastructure teams to update sidecar functionality independently of the application. It's fundamental to service mesh architectures."},{question:"In the Saga pattern, what is a compensating transaction?",options:["A transaction that runs in parallel to speed up processing","An action that undoes the effect of a previously committed local transaction when a later step fails","A transaction that automatically retries on failure","A database rollback using ACID guarantees"],correctIndex:1,explanation:"A compensating transaction is an action that semantically reverses the effect of a previously committed step in a saga when a subsequent step fails. Unlike a database rollback, compensating transactions are application-level logicfor example, if payment was captured but shipping fails, the compensation would issue a refund. This is necessary because each service has its own database with local transactions; there's no distributed ACID transaction spanning services. Compensating transactions may not perfectly undo everything (e.g., a sent email can't be unsent), which is why saga design requires careful consideration of idempotency and ordering."},{question:"What problem does service discovery solve in a microservices environment?",options:["How to encrypt traffic between services","How clients and services locate the network addresses of dynamically changing service instances","How to split a monolith into services","How to store service source code in a repository"],correctIndex:1,explanation:"Service discovery solves the problem of locating service instances whose network addresses change dynamically due to auto-scaling, deployments, and failures. In a cloud-native environment, IP addresses are ephemeralcontainers start and stop constantly. Service discovery mechanisms (like Consul, Eureka, or Kubernetes DNS) maintain a registry of available instances so that clients can find healthy endpoints. Without service discovery, you'd need to hardcode IP addresses, which is fragile and doesn't work with elastic scaling. There are two patterns: client-side discovery (client queries registry) and server-side discovery (load balancer queries registry)."},{question:"What is the Bulkhead pattern inspired by?",options:["Electrical circuit breakers in buildings","Watertight compartments in a ship's hull that contain flooding","The human immune system's white blood cells","Traffic lanes on a highway"],correctIndex:1,explanation:"The Bulkhead pattern is named after the watertight compartments (bulkheads) in a ship's hull. If one compartment is breached, the flooding is contained and doesn't sink the entire ship. In microservices, this translates to isolating resources (thread pools, connection pools, CPU) so that a failure in one component doesn't exhaust shared resources and bring down everything. For example, you might allocate separate thread pools for calls to different downstream services, so a slow Service A can't starve requests to Service B. Netflix famously used bulkheads in their architecture to prevent cascading failures across their streaming platform."},{question:"Which type of Saga execution uses a central coordinator to manage the workflow?",options:["Choreography-based saga","Orchestration-based saga","Two-phase commit saga","Peer-to-peer saga"],correctIndex:1,explanation:"An orchestration-based saga uses a central orchestrator (saga coordinator) that tells each participant what local transaction to execute and in what order. The orchestrator maintains the saga's state and handles compensations if a step fails. In contrast, choreography-based sagas have no central coordinatoreach service listens for events and decides what to do next. Orchestration is easier to understand and debug for complex workflows but introduces a single point of coordination. Two-phase commit is a different distributed transaction protocol that blocks resources and isn't suitable for microservices. Real-world examples include order processing workflows in e-commerce systems using tools like Temporal or AWS Step Functions."},{question:"What does distributed tracing help you understand in a microservices system?",options:["The database schema of each service","The end-to-end path and latency of a request as it flows through multiple services","The number of lines of code in each service","The programming language used by each team"],correctIndex:1,explanation:"Distributed tracing tracks a request's journey across multiple services, recording timing data at each hop to create a trace. This allows engineers to visualize the complete request path, identify latency bottlenecks, and pinpoint which service is causing slowdowns. Tools like Jaeger, Zipkin, and AWS X-Ray implement the OpenTelemetry standard for this purpose. Each request gets a unique trace ID that propagates through all service calls, with each service adding a span. Without distributed tracing, debugging performance issues in a system with dozens of interconnected services would be nearly impossibleyou'd be guessing which of many services is the culprit."},{question:"In Kubernetes, what is a Pod?",options:["A virtual machine running multiple containers","The smallest deployable unit, consisting of one or more tightly coupled containers sharing network and storage","A cluster of physical servers","A DNS entry for a service"],correctIndex:1,explanation:"A Pod is the smallest deployable unit in Kubernetes, consisting of one or more containers that share the same network namespace (same IP, can communicate via localhost) and storage volumes. Pods are not virtual machinesthey're lightweight process groups. The most common pattern is a single application container per pod, but multi-container pods are used for sidecars (e.g., log collectors, service mesh proxies). Pods are ephemeral; they can be terminated and recreated at any time, which is why you use higher-level abstractions like Deployments to manage their lifecycle. Understanding pods is fundamental to running microservices on Kubernetes."},{question:"What is the primary disadvantage of a monolithic architecture as the team and codebase grow?",options:["It's impossible to write unit tests","Deployment of any change requires rebuilding and redeploying the entire application","Monoliths cannot use relational databases","Monoliths cannot handle more than 100 concurrent users"],correctIndex:1,explanation:"As a monolith grows, even a small change requires rebuilding and redeploying the entire application, which slows down release cycles and increases risk. A bug in one module can block deployment of unrelated features. This creates coordination overhead as teams growmerge conflicts, long build times, and coupled release schedules. Monoliths absolutely support unit testing, relational databases, and can handle massive traffic (many successful companies run monoliths at scale). The key issue is organizational: Conway's Law suggests that as teams scale, independent deployability becomes critical for velocity."},{question:"Which service mesh implementation uses Envoy as its data plane proxy?",options:["Netflix Zuul","Istio","Apache Kafka","RabbitMQ"],correctIndex:1,explanation:"Istio is a service mesh that uses Envoy proxy as its data plane, deploying it as a sidecar container alongside each service. Envoy handles all inbound and outbound traffic, providing features like mutual TLS, load balancing, circuit breaking, and observability without application code changes. Netflix Zuul is an API gateway, not a service mesh. Kafka and RabbitMQ are message brokers for asynchronous communication, not service meshes. Other service meshes like Linkerd use their own lightweight proxy (linkerd2-proxy), but Istio's choice of Envoy has made Envoy the de facto standard for cloud-native proxy infrastructure."},{question:"What is the 'Database per Service' pattern?",options:["All services share a single large database for consistency","Each microservice owns its private database, and other services cannot access it directly","Each service uses a different database vendor","Services store data only in memory, never on disk"],correctIndex:1,explanation:"The Database per Service pattern means each microservice has its own private data store that no other service can access directlyonly through the service's API. This ensures loose coupling: services can evolve their schemas independently, choose the most appropriate database technology, and scale their data stores independently. The trade-off is that cross-service queries become harder (no joins across service boundaries) and maintaining data consistency requires patterns like sagas or event-driven synchronization. Using different vendors is optional (polyglot persistence), not required. This pattern is considered foundational to achieving true microservice independence."},{question:"In the context of inter-service communication, what does 'temporal coupling' mean?",options:["Services must be written in the same programming language","Services must be available at the same time for communication to succeed","Services must share the same database schema","Services must be deployed in the same time zone"],correctIndex:1,explanation:"Temporal coupling means both the caller and callee must be running and available simultaneously for communication to succeed. This is inherent in synchronous communication (HTTP REST, gRPC) where the caller blocks waiting for a response. If the downstream service is down, the call fails immediately. Asynchronous messaging eliminates temporal couplingthe sender publishes a message to a broker, and the receiver processes it whenever it's available. Temporal coupling is one of the main reasons architects favor asynchronous communication for service-to-service interactions, especially for operations that don't need an immediate response."},{question:"What does a Kubernetes Deployment resource manage?",options:["Network routing rules between pods","The desired state of pod replicas, including rolling updates and rollbacks","Persistent storage volumes for databases","DNS resolution for external services"],correctIndex:1,explanation:"A Kubernetes Deployment is a higher-level abstraction that manages a ReplicaSet of pods, ensuring the desired number of identical pod replicas are running at all times. It handles rolling updates (gradually replacing old pods with new ones), rollbacks (reverting to a previous version), and self-healing (restarting failed pods). Network routing is handled by Services and Ingress resources. Persistent storage is managed by PersistentVolumes and PersistentVolumeClaims. Deployments are the standard way to run stateless microservices on Kubernetes, as they provide declarative updates and maintain availability during deployments."},{question:"What is the Strangler Fig pattern used for?",options:["Rapidly building a greenfield microservices system","Incrementally migrating a monolith to microservices by gradually replacing functionality","Monitoring service health in production","Encrypting all inter-service communication"],correctIndex:1,explanation:"The Strangler Fig pattern (named after a tropical vine that gradually envelops and replaces a tree) is a migration strategy where you incrementally replace monolith functionality with microservices. New features are built as microservices, and existing features are migrated one at a time while the monolith continues to serve unmodified functionality. A routing layer (often an API gateway) directs traffic to either the monolith or the new services. This avoids the risk of a big-bang rewrite, which historically has a high failure rate. Martin Fowler popularized this pattern, and companies like Amazon and Netflix famously used it to migrate from monoliths."},{question:"What is the key difference between choreography and orchestration in saga patterns?",options:["Choreography uses REST while orchestration uses gRPC","Choreography has no central coordinatorservices react to events; orchestration has a central coordinator directing the flow","Orchestration is always faster than choreography","Choreography can only handle two services while orchestration handles unlimited"],correctIndex:1,explanation:"In choreography, there is no central coordinatoreach service publishes events and other services subscribe and react independently, creating a decentralized flow. In orchestration, a central saga orchestrator explicitly tells each service what to do and when. Choreography is simpler for few steps but becomes hard to understand as complexity grows (the 'event spaghetti' problem). Orchestration provides a single place to see the entire workflow but adds a coordination dependency. The choice of protocol (REST vs gRPC) is orthogonal to the saga style. Both patterns can handle any number of services; the trade-off is about complexity management and coupling."},{question:"What is the primary purpose of a Service Mesh?",options:["To provide a UI dashboard for developers","To handle service-to-service communication concerns (security, observability, traffic management) transparently at the infrastructure layer","To compile microservices code into containers","To replace the need for a message broker"],correctIndex:1,explanation:"A service mesh handles cross-cutting communication concerns like mutual TLS, load balancing, circuit breaking, retries, and distributed tracing at the infrastructure layer, without requiring changes to application code. It consists of a data plane (sidecar proxies like Envoy) and a control plane (like Istio's istiod) that configures the proxies. This is valuable because these concerns would otherwise need to be implemented in every service using language-specific libraries. A service mesh doesn't replace message brokers (which handle async messaging) or compile code. It's particularly powerful in polyglot environments where services use different languages and frameworks."},{question:"Which Kubernetes resource provides a stable network endpoint for a set of pods?",options:["ConfigMap","Service","PersistentVolume","Namespace"],correctIndex:1,explanation:"A Kubernetes Service provides a stable virtual IP (ClusterIP) and DNS name that routes traffic to a set of pods selected by label selectors. Since pods are ephemeral and their IPs change when they restart, the Service abstraction gives clients a consistent endpoint. There are several types: ClusterIP (internal only), NodePort (exposes on each node's IP), LoadBalancer (provisions cloud load balancer), and ExternalName (DNS alias). ConfigMaps store configuration data, PersistentVolumes provide storage, and Namespaces provide logical isolation. Services are essential for service discovery within a Kubernetes cluster."},{question:"What is API versioning important for in microservices?",options:["It allows services to evolve their contracts without breaking existing consumers","It speeds up API response times","It reduces the number of services needed","It eliminates the need for documentation"],correctIndex:0,explanation:"API versioning allows services to evolve their APIs while maintaining backward compatibility with existing consumers. In a microservices ecosystem, services are developed and deployed independently by different teams, so breaking changes to an API can cascade and disrupt multiple consumers. Common versioning strategies include URL path versioning (/v1/users), header-based versioning, and content negotiation. Each approach has trade-offsURL versioning is most visible but clutters routes, while header-based versioning keeps URLs clean but is less discoverable. Without versioning, every API change requires coordinated deployments across all consuming services, negating a key microservices benefit."},{question:"What does the term 'polyglot persistence' mean?",options:["Using multiple programming languages in the same service","Using different database technologies for different services based on their specific needs","Persisting data in multiple geographic regions simultaneously","Translating database queries into multiple languages"],correctIndex:1,explanation:"Polyglot persistence means choosing the most appropriate database technology for each service's specific data access patterns, rather than forcing all services to use the same database. For example, a product catalog might use Elasticsearch for full-text search, a social graph might use Neo4j, a session store might use Redis, and an order service might use PostgreSQL. This is a natural fit for microservices' database-per-service pattern. The trade-off is operational complexityyour team needs expertise in multiple database technologies. However, it allows each service to optimize for its unique read/write patterns, consistency requirements, and data model."},{question:"What is a key risk of synchronous request chains (Service A  B  C  D)?",options:["It guarantees strong consistency, which is always undesirable","The overall availability is the product of individual availabilities, creating a fragile chain","It reduces network traffic to zero","It forces all services to use the same database"],correctIndex:1,explanation:"When services form synchronous chains, the system's availability becomes the product of each service's availability. If each service has 99.5% uptime, a chain of four services has ~98% uptime (0.995^4). The deeper the chain, the worse the compound availability. Additionally, latency adds upeach hop adds network time plus processing time. A timeout in the last service cascades back through the entire chain, blocking threads at every level. This is why architects try to minimize synchronous call depth, prefer asynchronous communication for non-blocking operations, and use patterns like circuit breakers and bulkheads to contain failures when synchronous calls are necessary."},{question:"In Kubernetes, what is an Ingress?",options:["A container runtime interface","An API object that manages external HTTP/HTTPS access to services, typically providing routing, TLS termination, and virtual hosting","A tool for building container images","A secrets management system"],correctIndex:1,explanation:"A Kubernetes Ingress is an API object that defines rules for routing external HTTP/HTTPS traffic to internal services. It provides features like path-based routing (/api  service-a, /web  service-b), host-based virtual hosting, TLS termination, and basic load balancing. An Ingress Controller (like NGINX Ingress Controller, Traefik, or AWS ALB Ingress Controller) implements these rules. Without Ingress, you'd need to expose each service individually via LoadBalancer services, which is expensive and unmanageable. Ingress is essentially the entry point for external traffic into your Kubernetes cluster, similar to a reverse proxy or API gateway at the cluster edge."},{question:"What is the primary benefit of asynchronous messaging between microservices?",options:["It guarantees messages are never lost","It decouples services in timethe sender doesn't wait for the receiver to process the message","It eliminates the need for serialization","It makes debugging easier than synchronous calls"],correctIndex:1,explanation:"Asynchronous messaging decouples services temporallythe sender publishes a message and continues without waiting for the receiver. This means services don't need to be available simultaneously, improving resilience and allowing independent scaling. If a consumer is down, messages queue up and are processed when it recovers. Message durability (not losing messages) depends on broker configurationit's not automatic. Serialization is still required (JSON, Avro, Protobuf). Debugging can actually be harder than synchronous calls because the flow is non-linear and distributed. Despite the debugging complexity, async messaging is preferred for most service-to-service communication because it significantly improves system resilience."},{question:"What is the 'smart endpoints, dumb pipes' principle in microservices?",options:["Use intelligent network switches and simple service logic","Put business logic in the services (endpoints) and use lightweight messaging infrastructure (pipes) for transport only","Encrypt endpoints and leave pipes unencrypted","Use smart DNS and simple HTTP"],correctIndex:1,explanation:"This principle, coined by Martin Fowler, means that business logic should reside in the microservices themselves (smart endpoints) while the communication infrastructure (pipes) should be simple and lightweightjust transporting messages without adding business logic. This contrasts with the ESB (Enterprise Service Bus) approach where the bus contained routing rules, transformations, and business logic. In microservices, you use simple protocols like HTTP/REST or lightweight message brokers (RabbitMQ, Kafka) for transport, keeping them 'dumb.' This prevents the communication layer from becoming a bottleneck and a tangled mess of routing rules, which was a common problem with SOA-era ESBs."},{question:"What is the difference between client-side and server-side service discovery?",options:["Client-side uses JavaScript, server-side uses Java","In client-side, the client queries the registry and selects an instance; in server-side, a load balancer queries the registry on behalf of the client","Client-side is for mobile apps, server-side is for web apps","Client-side uses UDP, server-side uses TCP"],correctIndex:1,explanation:"In client-side discovery, the client (calling service) directly queries the service registry to get available instances and uses a load-balancing algorithm to pick one. Netflix Eureka with Ribbon is a classic example. In server-side discovery, the client sends requests to a load balancer or router, which queries the registry and forwards the request to an appropriate instance. AWS ELB and Kubernetes Services use this approach. Client-side discovery gives the client more control over load-balancing strategy but couples it to the registry. Server-side discovery is simpler for clients but adds a network hop through the load balancer. Kubernetes DNS-based discovery is server-side and is the most common approach in cloud-native environments."},{question:"What is a common anti-pattern when designing microservice boundaries?",options:["Aligning services with business capabilities","Creating services that are too fine-grained (nano-services), leading to excessive inter-service communication","Using bounded contexts from DDD to define service boundaries","Having a single team own a single service"],correctIndex:1,explanation:"Creating nano-servicesservices that are too small and fine-grainedis a common anti-pattern. When services are too small, simple operations require many network calls, increasing latency and failure points. For example, having separate services for 'validate address,' 'format address,' and 'geocode address' creates unnecessary network overhead. A single 'Address Service' would be more appropriate. Good boundaries align with business capabilities or DDD bounded contexts, grouping related functionality that changes together. The goal is to minimize inter-service communication while maximizing independent deployability. Sam Newman's advice: start with a monolith and extract services as boundaries become clear."},{question:"What does the CAP theorem state?",options:["A system can have Consistency, Availability, and Partition tolerance simultaneously","A distributed system can provide at most two of three guarantees: Consistency, Availability, and Partition tolerance","Caching Always Performs better than database queries","Concurrent Access Patterns must be serialized"],correctIndex:1,explanation:"The CAP theorem (Brewer's theorem) states that in a distributed system experiencing a network partition, you must choose between Consistency (all nodes see the same data) and Availability (every request receives a response). Since network partitions are inevitable in distributed systems, the real choice is CP (consistent but may be unavailable during partitions, like ZooKeeper) or AP (available but may return stale data, like Cassandra). This is fundamental to microservices because the database-per-service pattern creates a distributed data system. Understanding CAP helps architects make informed trade-offs between consistency and availability for each service based on business requirements."},{question:"What is the purpose of health check endpoints in microservices?",options:["To display the service's source code","To allow orchestrators and load balancers to determine if a service instance is healthy and able to receive traffic","To encrypt service communications","To compress response payloads"],correctIndex:1,explanation:"Health check endpoints (e.g., /health or /ready) allow orchestrators like Kubernetes and load balancers to determine whether a service instance can handle traffic. Kubernetes uses liveness probes (is the process alive?) and readiness probes (is it ready to accept traffic?). If a liveness check fails, Kubernetes restarts the container; if readiness fails, it removes the pod from the service's endpoints. This enables self-healingunhealthy instances are automatically replaced. Health checks should verify critical dependencies (database connectivity, cache availability) to provide an accurate picture. Without proper health checks, traffic may be routed to broken instances, causing errors for users."},{question:"What is a 'shared nothing' architecture in the context of microservices?",options:["Services don't share any code, data stores, or infrastructure components","All services run on a single shared server","Services share a common message format","Teams don't share knowledge between them"],correctIndex:0,explanation:"Shared nothing architecture means each microservice owns all its resources independentlyits own database, its own data, its own dependencieswithout sharing these with other services. This eliminates resource contention and coupling: one service's database maintenance doesn't affect others, and schema changes are local. The trade-off is data duplication and the complexity of keeping data synchronized across services. Sharing message formats (contracts) is actually expected for interoperability. In practice, some sharing is pragmatic (shared libraries for common utilities, shared infrastructure like Kubernetes), but the data layer should remain private to each service."},{question:"Which pattern helps maintain data consistency across multiple microservices without distributed transactions?",options:["Two-phase commit (2PC)","Saga pattern","Singleton pattern","Factory pattern"],correctIndex:1,explanation:"The Saga pattern maintains data consistency across services by executing a sequence of local transactions, each within a single service's database. If any step fails, compensating transactions undo the effects of prior steps. Unlike 2PC, sagas don't lock resources across serviceseach transaction commits locally immediately. Two-phase commit technically works for distributed transactions but is impractical for microservices due to resource locking, reduced availability, and tight coupling. Singleton and Factory are OOP design patterns unrelated to distributed data consistency. Sagas are the standard approach in microservices, implemented via either choreography (events) or orchestration (coordinator)."},{question:"What is the purpose of an API Gateway in a microservices architecture?",options:["To store all service configurations","To provide a single entry point for clients, handling routing, authentication, rate limiting, and request aggregation","To compile microservices code","To replace all inter-service communication"],correctIndex:1,explanation:"An API Gateway serves as a single entry point for external clients, abstracting the complexity of the microservices topology behind a unified API. It handles cross-cutting concerns like authentication, rate limiting, SSL termination, request routing, response caching, and request/response transformation. It can also aggregate responses from multiple backend services into a single response (reducing client round trips). Without a gateway, clients would need to know the addresses of individual services and handle authentication separately for each. Popular implementations include Kong, AWS API Gateway, and Netflix Zuul/Spring Cloud Gateway."},{question:"What is the 'data consistency' challenge specific to microservices?",options:["Microservices cannot use SQL databases","With each service owning its database, maintaining consistency across services requires careful coordination since you can't use ACID transactions across service boundaries","All microservices must use eventual consistency","Data consistency is not a concern in microservices"],correctIndex:1,explanation:"When each service owns its own database, you lose the ability to use ACID transactions that span multiple services. An operation like 'place order' might involve the Order Service, Inventory Service, and Payment Serviceeach with its own database. There's no single transaction to atomically update all three. This requires patterns like sagas for managing multi-service transactions and event-driven approaches for synchronizing data. Not all microservices must use eventual consistencyindividual services can use strong consistency internally. The challenge is inter-service consistency, and the right consistency model depends on business requirements (e.g., payment needs strong guarantees, recommendation feeds can be eventually consistent)."},{question:"What does 'idempotency' mean in the context of microservices communication?",options:["A request always returns the same response time","Processing the same request multiple times produces the same result as processing it once","All services must use the same programming language","Messages are always delivered in order"],correctIndex:1,explanation:"Idempotency means that performing the same operation multiple times has the same effect as performing it once. This is critical in microservices because network failures, retries, and message broker redeliveries can cause the same request to be received multiple times. For example, a payment service must ensure that charging a customer twice for the same order doesn't actually debit their account twice. Common implementations include using idempotency keys (unique request IDs), checking if the operation was already performed before executing it, and designing operations to be naturally idempotent (like setting a value rather than incrementing). Without idempotency, retry mechanisms become dangerous."},{question:"What is the Backends for Frontends (BFF) pattern?",options:["Running backend services in the frontend browser","Creating separate backend services tailored for each type of frontend client (web, mobile, etc.)","Using the same API for all frontend platforms","A pattern where the frontend directly accesses the database"],correctIndex:1,explanation:"The BFF pattern creates dedicated backend services for each frontend platformone for the web app, one for the mobile app, one for third-party APIs, etc. Each BFF aggregates and transforms data from downstream microservices in the way that's optimal for its specific client. A mobile app might need less data and different formatting than a web dashboard. Without BFF, a general-purpose API either over-fetches for mobile (wasting bandwidth) or under-fetches for web (requiring multiple calls). Sam Newman popularized this pattern. The trade-off is maintaining multiple BFF services, but it prevents a single API from becoming a compromise that serves no client well."},{question:"What is 'contract testing' in microservices?",options:["Testing legal contracts between service teams","Verifying that a service's API meets the expectations defined by its consumers","Testing network contracts like TCP/UDP","Testing that all services use the same deployment pipeline"],correctIndex:1,explanation:"Contract testing (often called Consumer-Driven Contract Testing) verifies that a service provider's API satisfies the contracts expected by its consumers. Tools like Pact allow consumers to define their expectations (what endpoints they call, what data they need), and these contracts are verified against the provider during CI. This prevents a provider from accidentally breaking its consumers when making changes. Unlike integration tests that require running all services, contract tests run independently and are faster. This is critical in microservices where services are deployed independentlywithout contract tests, you might deploy a provider change that breaks consumers you didn't know about."},{question:"What is the recommended team structure for microservices according to Conway's Law?",options:["One large team manages all services","Teams organized around business capabilities, where each team owns one or more services end-to-end","Separate teams for frontend, backend, and database for each service","Teams organized by programming language expertise"],correctIndex:1,explanation:"Conway's Law states that organizations design systems that mirror their communication structures. For microservices, this means organizing teams around business capabilities (e.g., a Payments team, an Inventory team) where each team owns their services end-to-endfrom UI to database. Amazon's 'two-pizza teams' embody this principle. Horizontal teams (one team for all frontends, another for all backends) create handoffs and coordination overhead that slow delivery. When a team owns a service completely, they can deploy independently, make technology choices, and iterate quickly. This organizational alignment is often considered a prerequisite for successful microservices adoption."},{question:"What happens in the 'Half-Open' state of a Circuit Breaker?",options:["All requests are blocked indefinitely","A limited number of test requests are allowed through to check if the downstream service has recovered","The circuit operates normally as if fully closed","The circuit randomly accepts or rejects requests"],correctIndex:1,explanation:"In the Half-Open state, the circuit breaker allows a small number of trial requests through to the downstream service to test if it has recovered. If these test requests succeed, the circuit transitions back to Closed (normal operation). If they fail, the circuit returns to Open (fail fast). This state is crucial for automatic recoverywithout it, the circuit would either stay open forever (requiring manual intervention) or immediately flood the recovering service with traffic. The number of test requests and the timeout before entering Half-Open are configurable parameters. This three-state model (Closed  Open  Half-Open  Closed) enables self-healing without human intervention."},{question:"What is a 'correlation ID' in microservices?",options:["A database primary key shared across services","A unique identifier attached to a request that is propagated through all service calls to enable end-to-end tracing","An encryption key for inter-service communication","A version number for API contracts"],correctIndex:1,explanation:"A correlation ID (or trace ID) is a unique identifier generated at the entry point of a request (usually the API gateway) and propagated through all subsequent service calls via HTTP headers or message metadata. This allows log aggregation tools to correlate all log entries belonging to the same business operation across multiple services. Without correlation IDs, matching logs from Service A with related logs from Service B and Service C would be virtually impossible in a system processing thousands of concurrent requests. This is the foundation of distributed tracing systems like Jaeger and Zipkin, and it's essential for debugging, auditing, and performance analysis in microservices."},{question:"What is the difference between horizontal and vertical scaling?",options:["Horizontal scaling adds more machines; vertical scaling adds more resources to existing machines","Horizontal scaling is for databases; vertical scaling is for applications","Horizontal scaling is cheaper; vertical scaling is always more expensive","There is no difference; they are synonymous"],correctIndex:0,explanation:"Horizontal scaling (scaling out) adds more instances/machines to distribute the load, while vertical scaling (scaling up) increases the resources (CPU, RAM, disk) of existing machines. Microservices inherently favor horizontal scaling because each service can be independently replicated. Vertical scaling has a ceilingthere's a maximum machine sizewhile horizontal scaling is theoretically unlimited. However, horizontal scaling requires the application to be stateless or handle distributed state. Kubernetes facilitates horizontal scaling through Horizontal Pod Autoscalers (HPA) that automatically adjust replica counts based on metrics. Both strategies have their place: stateless services scale horizontally, while some databases (like traditional RDBMS) initially benefit more from vertical scaling."},{question:"What is 'eventual consistency' in distributed microservices?",options:["Data is never consistent across services","Given enough time without new updates, all replicas of the data will converge to the same state","Data is immediately consistent across all services at all times","Only the latest write is stored; all previous versions are deleted"],correctIndex:1,explanation:"Eventual consistency means that if no new updates are made, all copies of the data across services will eventually converge to the same valuebut there's a window where different services may see different values. This is common in microservices because data is replicated asynchronously via events or messages. For example, after an order is placed, the inventory service might take a few seconds to reflect the stock reduction. This trade-off is acceptable for many use cases (product catalogs, notifications, analytics) but not for others (financial transactions, inventory counts for limited stock). Understanding where eventual consistency is acceptable vs. where stronger guarantees are needed is a key architectural skill."},{question:"What tool is commonly used for container orchestration in microservices?",options:["Jenkins","Kubernetes","Git","Terraform"],correctIndex:1,explanation:"Kubernetes (K8s) is the industry-standard container orchestration platform for managing microservices. It automates deployment, scaling, self-healing, service discovery, load balancing, and rolling updates of containerized applications. Jenkins is a CI/CD tool for building and testing code, Git is version control, and Terraform is infrastructure-as-code for provisioning cloud resourcesnone of them orchestrate running containers. Kubernetes provides the runtime environment where microservices containers actually execute, managing their lifecycle, networking, and resource allocation. Alternatives like Docker Swarm and Apache Mesos exist but Kubernetes has become the dominant choice, supported by all major cloud providers."},{question:"What is the Outbox pattern used for in microservices?",options:["Storing outgoing emails in a queue","Ensuring atomic updates to a database and reliable event publishing by writing events to an outbox table in the same transaction","A logging pattern for outbound API calls","Managing outgoing network connections"],correctIndex:1,explanation:"The Outbox pattern solves the dual-write problem: when a service needs to update its database AND publish an event, but doing both atomically is impossible without distributed transactions. The solution writes both the data change and the event to the same database in a single ACID transaction (the event goes to an 'outbox' table). A separate process (like Debezium using CDC) reads the outbox table and publishes events to the message broker. This guarantees that if the data is updated, the event will eventually be published (and vice versa). Without this pattern, crashes between the database write and event publish could leave the system in an inconsistent state."},{question:"What is gRPC and why is it used in microservices?",options:["A database query language","A high-performance RPC framework using Protocol Buffers and HTTP/2 for efficient inter-service communication","A container orchestration tool","A service mesh implementation"],correctIndex:1,explanation:"gRPC is a high-performance Remote Procedure Call framework developed by Google that uses Protocol Buffers (protobuf) for serialization and HTTP/2 for transport. It offers significant advantages for inter-service communication: binary serialization is faster and smaller than JSON, HTTP/2 enables multiplexing and streaming, and strongly-typed service definitions (via .proto files) generate client/server code automatically. This makes it ideal for internal microservice-to-microservice communication where performance matters. The trade-off is that gRPC is less human-readable than REST/JSON and has limited browser support (though gRPC-Web exists). Many companies use REST for external APIs and gRPC for internal service communication."},{question:"What is the purpose of a Container Registry in microservices deployment?",options:["To register domain names for services","To store and distribute container images that services are packaged into","To register services with a service discovery mechanism","To store database connection strings"],correctIndex:1,explanation:"A Container Registry stores and distributes Docker/OCI container images. When you build a microservice, it's packaged as a container image and pushed to a registry (like Docker Hub, AWS ECR, Google GCR, or Harbor). During deployment, Kubernetes pulls the image from the registry to create container instances. This is the distribution mechanism for microservice artifactssimilar to how Maven repositories distribute JAR files, but for containers. Registries also support image versioning (tags), vulnerability scanning, access control, and image signing. Without a registry, there would be no standardized way to distribute and version microservice deployments across environments."},{question:"What is 'service decomposition' and what are common strategies for it?",options:["Breaking down a monolith's codebase by file size","Splitting a system into services based on business capabilities, subdomains, or use cases","Distributing a service across multiple geographic regions","Converting synchronous calls to asynchronous ones"],correctIndex:1,explanation:"Service decomposition is the process of dividing a system into microservices. Common strategies include: decomposing by business capability (payments, shipping, inventory), by DDD subdomain (each bounded context becomes a service), or by use case/user journey. The goal is to create services with high cohesion (related functionality together) and loose coupling (minimal dependencies between services). Anti-patterns include decomposing by technical layer (a 'data service' and 'logic service') which creates tight coupling, or making services too granular (nano-services). A practical approach is starting with a modular monolith, identifying natural seams, and extracting services incrementally as team and domain understanding matures."},{question:"What is a ConfigMap in Kubernetes?",options:["A mapping of services to their IP addresses","A Kubernetes object used to store non-confidential configuration data as key-value pairs that can be consumed by pods","A visual map of the cluster topology","A routing table for network traffic"],correctIndex:1,explanation:"A ConfigMap stores non-confidential configuration data (environment variables, configuration files, command-line arguments) as key-value pairs that pods can consume via environment variables or mounted volumes. This separates configuration from container images, following the twelve-factor app methodology. For example, you can change a service's log level or feature flags without rebuilding the imagejust update the ConfigMap and restart the pods. For sensitive data like passwords and API keys, Kubernetes provides Secrets (which are base64-encoded, not encrypted by default). ConfigMaps are essential for managing microservice configuration across different environments (dev, staging, production) without baking environment-specific values into images."},{question:"What is the 'ambassador pattern' in microservices?",options:["A diplomatic protocol for service-to-service negotiations","A proxy that handles cross-cutting concerns on behalf of a service, similar to a sidecar but specifically for outbound traffic patterns","A service that translates between different programming languages","The first service deployed in a new cluster"],correctIndex:1,explanation:"The Ambassador pattern deploys a proxy alongside a service to handle outbound communication concerns like retries, circuit breaking, logging, and routing. It's conceptually similar to the sidecar pattern but specifically focuses on acting as an 'ambassador' for the service's outbound connections. For example, an ambassador container might handle connection pooling and retries to a legacy database, allowing the application to use a simple connection. This pattern offloads cross-cutting networking concerns from the application code, making it simpler and allowing infrastructure teams to manage these concerns independently. The pattern is particularly useful when you can't modify the application code."},{question:"What is 'blue-green deployment' in microservices?",options:["Deploying services to two different cloud providers simultaneously","Running two identical production environments (blue and green), switching traffic between them for zero-downtime deployments","Color-coding services based on their team ownership","Deploying test and production environments side by side"],correctIndex:1,explanation:"Blue-green deployment maintains two identical production environments. The 'blue' environment runs the current version while the 'green' environment is deployed with the new version. Once the green environment is verified, traffic is switched from blue to green (typically via DNS or load balancer change). If issues are detected, you instantly switch back to blue. This provides zero-downtime deployments and instant rollback. The trade-off is costyou need double the infrastructure during deployment. In microservices, this can be applied per-service or for the entire system. Kubernetes supports this via service label selectors. An alternative is canary deployment, which gradually shifts traffic rather than switching all at once."},{question:"What is 'canary deployment' and how does it differ from blue-green?",options:["Deploying to a canary island data center","Gradually routing a small percentage of traffic to the new version and increasing it if metrics are healthy, unlike blue-green's all-at-once switch","Deploying a lightweight monitoring service alongside the main service","Testing in production using synthetic traffic only"],correctIndex:1,explanation:"Canary deployment rolls out a new version to a small percentage of users first (say 5%), monitoring error rates, latency, and business metrics. If the canary is healthy, traffic is gradually increased (25%, 50%, 100%). If problems are detected, only the small canary group is affected, and the deployment is rolled back. Unlike blue-green (which switches 100% of traffic at once), canary provides a gradual, metrics-driven rollout that catches issues before they affect all users. Istio and Flagger can automate canary deployments in Kubernetes by progressively shifting traffic weights between old and new deployments based on defined success criteria."},{question:"What are liveness and readiness probes in Kubernetes?",options:["Probes that test network connectivity between pods","Liveness checks if a container should be restarted; readiness checks if it should receive traffic","Probes that monitor disk space and memory usage","Probes that verify container image signatures"],correctIndex:1,explanation:"Liveness probes determine if a container is still running properly. If a liveness probe fails, Kubernetes kills the container and restarts it (assuming the restart policy allows it). This handles deadlocks, infinite loops, and corrupted state. Readiness probes determine if a container is ready to accept traffic. If readiness fails, the pod's IP is removed from Service endpointsno traffic is sent to it, but the container isn't restarted. This is important during startup (when a service is loading data or warming caches) and during temporary issues (database connection lost). There's also a startup probe for slow-starting containers. Properly configuring these probes is critical for reliable microservice operation on Kubernetes."},{question:"What is the 'retry with exponential backoff' pattern?",options:["Retrying a failed request immediately as many times as possible","Retrying failed requests with progressively increasing delays between attempts to avoid overwhelming the failing service","Sending the same request to exponentially more services","Reducing the payload size exponentially with each retry"],correctIndex:1,explanation:"Retry with exponential backoff means that when a request fails, subsequent retries wait for progressively longer intervals (e.g., 1s, 2s, 4s, 8s). This prevents overwhelming a struggling service with a flood of immediate retries, giving it time to recover. Adding random jitter (small random variation to the delay) prevents the 'thundering herd' problem where many clients retry simultaneously. Without backoff, aggressive retries can create a feedback loop that makes outages worse. Most HTTP client libraries and messaging frameworks support configurable backoff. It's typically combined with a maximum retry count and circuit breakers to avoid infinite retries. AWS SDK and gRPC libraries implement this pattern by default."},{question:"What is the difference between a Kubernetes Deployment and a StatefulSet?",options:["Deployments are for Java services; StatefulSets are for Python services","Deployments manage stateless applications with interchangeable pods; StatefulSets manage stateful applications with stable identities, persistent storage, and ordered deployment","StatefulSets are deprecated in favor of Deployments","Deployments support rolling updates; StatefulSets do not"],correctIndex:1,explanation:"Deployments are designed for stateless applications where pods are interchangeableany pod can be replaced without consequence. StatefulSets are for stateful applications (like databases, Kafka brokers, ZooKeeper) that need stable network identities (pod-0, pod-1), persistent storage that survives pod restarts, and ordered deployment/scaling. In a StatefulSet, pods are created sequentially (pod-0 before pod-1) and have predictable DNS names. Deployments create pods with random names and no ordering guarantees. For microservices, most application services use Deployments (stateless), while data infrastructure components use StatefulSets. Both support rolling updates, but StatefulSets update in reverse order."},{question:"What is 'feature flagging' and why is it important for microservices?",options:["A security mechanism to flag suspicious features","A technique to enable/disable features at runtime without redeploying, supporting progressive rollouts and A/B testing","A code review process for flagging features that need attention","A Kubernetes label used to mark services"],correctIndex:1,explanation:"Feature flags (toggles) allow teams to enable or disable features at runtime without deploying new code. This is powerful in microservices for several reasons: it enables trunk-based development (merge code early, activate features later), progressive rollouts (enable for 10% of users first), A/B testing, and instant kill switches for problematic features. Tools like LaunchDarkly, Unleash, and Flagsmith manage flags centrally. In microservices, where independent deployment is key, feature flags decouple deployment from releaseyou can deploy code anytime and activate features when ready. The trade-off is technical debt: old flags must be cleaned up, or the codebase becomes cluttered with conditional logic."},{question:"What is the 'log aggregation' pattern?",options:["Writing all logs to a single monolithic log file","Collecting logs from all service instances into a centralized system for searching, analysis, and alerting","Aggregating log entries to reduce storage costs by removing duplicates","Printing logs to the console during development only"],correctIndex:1,explanation:"Log aggregation collects logs from all microservice instances into a centralized platform for unified searching, analysis, and alerting. The ELK/EFK stack (Elasticsearch, Logstash/Fluentd, Kibana) and solutions like Datadog, Splunk, and Grafana Loki are common implementations. In microservices, logs are scattered across hundreds of container instances that are ephemeralwhen a pod restarts, its local logs are lost. Centralized log aggregation with correlation IDs enables engineers to trace a request across all services it touched. This is not about writing to a single file or deduplication, but about making distributed system behavior observable and debuggable from a single pane of glass."},{question:"What does 'infrastructure as code' mean for microservices deployments?",options:["Writing microservices in infrastructure-level languages like C","Defining and managing infrastructure (servers, networks, deployments) through code files that can be versioned and automated","Storing infrastructure passwords in source code","Running infrastructure monitoring dashboards as microservices"],correctIndex:1,explanation:"Infrastructure as Code (IaC) means defining infrastructure (Kubernetes manifests, cloud resources, networking) in declarative code files that are version-controlled, reviewed, and automatically applied. Tools like Terraform, Pulumi, Helm, and Kustomize enable this. For microservices, IaC is critical because manually configuring hundreds of services, load balancers, databases, and networks is error-prone and unreproducible. With IaC, you can recreate an entire environment from scratch, track changes through Git history, and ensure consistency across development, staging, and production. GitOps tools like ArgoCD and Flux take this further by using Git as the single source of truth for cluster state."},{question:"What is the purpose of a Namespace in Kubernetes?",options:["To define the programming language namespace for service code","To provide logical isolation and resource scoping within a cluster, allowing multiple teams or environments to share it","To create physical network partitions between nodes","To define DNS domain names for external traffic"],correctIndex:1,explanation:"Kubernetes Namespaces provide logical isolation within a cluster, allowing you to partition resources into named groups. Different teams can use separate namespaces (team-a, team-b), or you can separate environments (dev, staging) within the same cluster. Namespaces scope resource names (avoiding conflicts), allow resource quotas (limiting CPU/memory per namespace), and enable network policies (restricting cross-namespace traffic). They don't create physical network isolation by defaultpods in different namespaces can still communicate unless network policies restrict it. For microservices, namespaces help organize services by team or environment while sharing the same underlying cluster infrastructure."},{question:"What is 'observability' and how does it differ from 'monitoring'?",options:["They are the same thingdifferent names for dashboards","Monitoring checks known failure modes; observability lets you understand system behavior from its outputs, including investigating unknown unknowns","Observability is for development; monitoring is for production","Monitoring uses metrics; observability uses logs only"],correctIndex:1,explanation:"Monitoring tracks predefined metrics and alerts on known failure conditions (CPU > 90%, error rate > 5%). Observability goes furtherit enables you to understand system behavior from its external outputs (metrics, logs, traces), including diagnosing novel, unforeseen issues. The three pillars of observability are metrics (numerical measurements), logs (detailed event records), and traces (request paths across services). In microservices, monitoring alone is insufficient because the failure modes are too varied and complex to predict. Observability tools like Grafana, Jaeger, and Datadog let engineers ask arbitrary questions about system behavior and drill down from symptoms to root causes without adding new instrumentation."},{question:"What is the 'timeout pattern' and why is it critical in microservices?",options:["A pattern to limit how long developers spend on a feature","Setting maximum wait times for service calls to prevent resource exhaustion when downstream services are slow","A pattern for scheduling service shutdowns during maintenance","Limiting the time a container can run before being replaced"],correctIndex:1,explanation:"The timeout pattern sets a maximum time a caller will wait for a response from a downstream service. Without timeouts, a slow service can cause the caller to hold resources (threads, connections) indefinitely, eventually exhausting them and cascading the failure. For example, if Service A calls Service B with no timeout and B hangs, A's thread pool fills up with waiting threads, making A unresponsive. Timeouts should be carefully tunedtoo short causes false failures, too long wastes resources. Timeouts work best in combination with circuit breakers and retries. In microservices, every outbound call should have a timeout configured; the default 'infinite timeout' of most HTTP clients is dangerous in distributed systems."},{question:"What is 'event storming' in the context of microservices design?",options:["A chaos engineering technique that floods services with events","A collaborative workshop technique where domain experts and developers discover domain events, commands, and aggregates to identify service boundaries","A load testing approach using event-driven traffic","A monitoring technique for event-driven systems"],correctIndex:1,explanation:"Event Storming is a collaborative workshop method created by Alberto Brandolini where domain experts and developers use sticky notes on a wall to map out domain events (things that happen), commands (what triggers them), aggregates (entities that handle commands), and bounded contexts. This is one of the most effective techniques for discovering microservice boundaries because it focuses on business processes and domain events rather than data models. The workshop reveals natural boundaries where different teams or contexts handle different events. It produces a shared understanding of the domain that directly maps to service decomposition. It's rooted in DDD principles and typically precedes any architecture decisions."},{question:"What is a DaemonSet in Kubernetes?",options:["A set of services that run only at night (daemon hours)","A resource that ensures a copy of a pod runs on every node in the cluster, commonly used for logging agents and monitoring","A deployment strategy for stateful services","A security group for privileged containers"],correctIndex:1,explanation:"A DaemonSet ensures that a copy of a specific pod runs on every node (or a selected subset of nodes) in the cluster. When new nodes are added, the DaemonSet automatically deploys pods to them. Common use cases include log collectors (Fluentd, Filebeat), monitoring agents (Prometheus Node Exporter, Datadog Agent), network plugins (Calico, Weave), and storage daemons (GlusterFS). For microservices, DaemonSets are infrastructure-levelyou typically don't deploy your business services as DaemonSets but rather use them for the cross-cutting infrastructure that all services depend on. Unlike Deployments that create a specified number of replicas, DaemonSets tie replica count to node count."},{question:"What is 'chaos engineering' and why is it relevant to microservices?",options:["Writing code without a plan or design","Intentionally injecting failures into a system to test its resilience and identify weaknesses before they cause real outages","Using random data in unit tests","Deploying services in random order"],correctIndex:1,explanation:"Chaos engineering is the discipline of experimenting on a production (or production-like) system to build confidence in its ability to withstand turbulent conditions. Tools like Chaos Monkey (Netflix), Gremlin, and LitmusChaos inject failures such as killing pods, introducing network latency, corrupting data, and exhausting CPU. For microservices, this is particularly important because the distributed nature creates countless failure modes that are impossible to predict through testing alone. By proactively discovering weaknesses (missing timeouts, inadequate circuit breakers, poor retry logic), teams can fix them before they cause real incidents. Netflix pioneered this approach, evolving it from killing random instances to sophisticated, hypothesis-driven experiments."},{question:"What is the 'Anti-Corruption Layer' pattern in DDD?",options:["A firewall that prevents malicious data from entering the system","A translation layer that prevents one bounded context's model from leaking into and corrupting another context's model","A data validation layer that prevents database corruption","An encryption layer that prevents data tampering"],correctIndex:1,explanation:"The Anti-Corruption Layer (ACL) is a DDD pattern that acts as a translation boundary between two bounded contexts, especially when integrating with a legacy system or external service. It translates between the external model and your internal domain model, preventing foreign concepts from 'corrupting' your domain. For example, if a legacy system represents a customer differently than your new microservice, the ACL transforms data at the boundary. Without it, external models gradually leak into your codebase, making it harder to evolve independently. In microservices, ACLs are commonly implemented at service boundaries where different teams have different domain models, preserving each service's model integrity."},{question:"What is 'trunk-based development' and how does it relate to microservices?",options:["Developing all services in a single monorepo","A branching strategy where developers commit to a single main branch frequently, using feature flags to manage incomplete work","Storing code in tree-structured databases","Developing services based on a tree-structured architecture"],correctIndex:1,explanation:"Trunk-based development is a source-control branching strategy where all developers commit to a single shared branch (main/trunk) at least daily, using short-lived feature branches (if any) that merge quickly. Combined with feature flags, this enables continuous integrationcode is always in a deployable state. This complements microservices because each service needs rapid, independent deployments. Long-lived branches lead to merge hell and delayed integration, undermining the independence microservices promise. Google, Facebook, and Netflix practice trunk-based development. It requires strong CI/CD pipelines, comprehensive automated testing, and feature flags to manage unreleased functionality. Monorepo vs. polyrepo is a separate concern from branching strategy."},{question:"What is a Horizontal Pod Autoscaler (HPA) in Kubernetes?",options:["A tool that horizontally splits pods across regions","A Kubernetes resource that automatically adjusts the number of pod replicas based on observed metrics like CPU utilization or custom metrics","A manual scaling command for Kubernetes administrators","A network policy that controls horizontal traffic between pods"],correctIndex:1,explanation:"The Horizontal Pod Autoscaler automatically scales the number of pod replicas in a Deployment or StatefulSet based on observed metrics. By default, it uses CPU utilization, but it can also use memory, custom metrics (like request queue depth), or external metrics (like messages in a Kafka topic). For example, if average CPU exceeds 70%, HPA adds more replicas; if it drops below, it removes them. This is essential for microservices because traffic patterns varyan order service might spike during sales events while other services remain steady. HPA enables each service to scale independently based on its own demand, optimizing both performance and cost. It works with Cluster Autoscaler, which adds/removes nodes."},{question:"Why should microservices avoid sharing a database?",options:["Databases can only handle one service's queries","Sharing a database creates tight couplingschema changes, performance issues, and scaling decisions affect all services sharing it","Modern databases don't support multiple connections","It's a security requirement from cloud providers"],correctIndex:1,explanation:"When services share a database, they become tightly coupled in several ways: schema changes in one service can break others, a slow query from one service degrades performance for all, scaling the database means over-provisioning for all services, and teams can't independently choose the best data technology for their needs. It also creates hidden runtime couplingservices that appear independent at the code level are actually connected through shared tables. This undermines the core benefit of microservices: independent development and deployment. The migration path is gradual: start by separating schemas (separate tables/schemas per service), then move to separate database instances. Modern databases can absolutely handle multiple connections; that's not the issue."},{question:"What is the role of an 'event bus' or 'message broker' in microservices?",options:["It stores the source code for all microservices","It acts as an intermediary for asynchronous communication, decoupling producers from consumers and enabling event-driven interactions","It compiles and deploys microservices automatically","It manages database schemas across services"],correctIndex:1,explanation:"A message broker (like RabbitMQ, Apache Kafka, or AWS SNS/SQS) acts as an intermediary for asynchronous communication between services. Producers publish messages/events without knowing who will consume them, and consumers subscribe to the messages they're interested in. This decouples services in space (don't need to know each other's addresses) and time (don't need to be running simultaneously). Brokers also provide buffering (absorbing traffic spikes), guaranteed delivery (persisting messages until consumed), and fan-out (one message consumed by multiple services). In microservices, the message broker is often the backbone of the architecture, enabling event-driven patterns that are more resilient than synchronous call chains."},{question:"What is the difference between 'north-south' and 'east-west' traffic in microservices?",options:["Traffic between different geographic regions","North-south is traffic entering/leaving the system (client to services); east-west is traffic between services within the system","North-south is upload traffic; east-west is download traffic","They refer to different network protocols"],correctIndex:1,explanation:"North-south traffic flows between external clients and the system's edge (through the API gateway or load balancer). East-west traffic flows between microservices within the system. In microservices architectures, east-west traffic typically far exceeds north-south traffica single client request might trigger dozens of inter-service calls. This distinction matters for security (north-south goes through the gateway; east-west needs mutual TLS via service mesh), performance (east-west latency compounds across service chains), and monitoring (you need different tools for external vs. internal traffic). Service meshes like Istio primarily address east-west traffic concerns, while API gateways handle north-south."},{question:"What is 'domain event' in microservices?",options:["A DNS change event for a domain name","A significant occurrence within a bounded context that other contexts might need to know about","An event triggered when a new service is deployed","A scheduled cron job within a service"],correctIndex:1,explanation:"A domain event represents something meaningful that happened in the business domainlike 'OrderPlaced,' 'PaymentProcessed,' or 'InventoryReserved.' These events are first-class citizens in DDD and microservices, used to communicate state changes between bounded contexts without tight coupling. When the Order Service places an order, it publishes an 'OrderPlaced' event; the Inventory, Payment, and Notification services can independently react to this event. Domain events should be named in past tense (something that already happened) and contain the relevant data needed by consumers. They're the foundation of event-driven microservices and are distinct from infrastructure events (like deployment notifications)."},{question:"What is 'data replication' used for in microservices?",options:["Copying source code between service repositories","Maintaining copies of relevant data from other services to enable local queries and reduce runtime dependencies","Creating backup tapes for disaster recovery","Duplicating microservices in multiple programming languages"],correctIndex:1,explanation:"In microservices, data replication means maintaining a local copy of data owned by another service so you can query it without making synchronous calls. For example, the Shipping Service might maintain a local copy of customer addresses (owned by the Customer Service) by consuming address-changed events. This eliminates runtime dependency on the Customer Service for every shipment. The data is eventually consistentthere's a small delay between the original update and the replica. This pattern trades consistency for availability and autonomy. It's commonly implemented using events (CDC or domain events) and is essential for query-heavy services that need to join data from multiple domains."},{question:"What is the 'Twelve-Factor App' methodology?",options:["A methodology requiring exactly twelve microservices","A set of twelve best practices for building cloud-native applications that are portable, scalable, and suitable for continuous deployment","A twelve-step process for migrating monoliths to microservices","A security framework with twelve compliance checkpoints"],correctIndex:1,explanation:"The Twelve-Factor App methodology (by Heroku co-founder Adam Wiggins) defines twelve best practices for cloud-native applications: codebase (one repo per app), dependencies (explicitly declared), config (stored in environment), backing services (treated as attached resources), build/release/run (strict separation), processes (stateless), port binding (self-contained), concurrency (scale via processes), disposability (fast startup/shutdown), dev/prod parity, logs (event streams), and admin processes (one-off tasks). These principles align perfectly with microservices: stateless processes enable horizontal scaling, externalized config supports multiple environments, and disposability supports container orchestration. Most Kubernetes best practices derive from these principles."},{question:"What is 'mutual TLS (mTLS)' in microservices?",options:["TLS encryption used only for database connections","A security protocol where both the client and server authenticate each other using certificates, ensuring encrypted and verified communication","A backup TLS certificate used when the primary expires","A simplified TLS that skips certificate verification for performance"],correctIndex:1,explanation:"Mutual TLS (mTLS) extends standard TLS by requiring both parties to present and verify certificates. In standard TLS, only the server proves its identity; in mTLS, the client also proves its identity to the server. In microservices, mTLS ensures that service-to-service communication is both encrypted (confidentiality) and authenticated (you know who's calling). Service meshes like Istio automate mTLSthe sidecar proxies handle certificate generation, rotation, and verification without application changes. Without mTLS, any process in the network could impersonate a service and access internal APIs. Zero-trust security models require mTLS as a baseline for all east-west traffic."},{question:"What is 'rate limiting' at the microservice level used for?",options:["Limiting how fast services can write code","Protecting services from being overwhelmed by too many requests, whether from external clients or other services","Limiting the amount of data stored in databases","Controlling the rate of log file generation"],correctIndex:1,explanation:"Rate limiting restricts the number of requests a service accepts within a time window, protecting it from being overwhelmed. This is critical for several scenarios: preventing denial-of-service attacks, ensuring fair usage among clients, protecting downstream dependencies from cascading overload, and managing costs for pay-per-use resources. Rate limiting can be applied at the API gateway (for external traffic) and at individual services (for inter-service traffic). Common algorithms include token bucket, leaky bucket, and sliding window. When a limit is exceeded, the service returns HTTP 429 (Too Many Requests). Rate limiting is a form of load sheddingit's better to reject excess requests cleanly than to let the service degrade for everyone."},{question:"What is 'graceful degradation' in microservices?",options:["Slowly shutting down all services during maintenance","Continuing to provide reduced but functional service when some components fail, rather than completely failing","Gradually migrating from microservices back to a monolith","Reducing code quality over time to ship faster"],correctIndex:1,explanation:"Graceful degradation means the system continues to function in a reduced capacity when some components fail, rather than failing entirely. For example, if the Recommendation Service is down, an e-commerce site can still show products and process ordersjust without personalized recommendations. This requires designing services to handle missing dependencies: using cached data, default responses, or disabling non-essential features. Circuit breakers and fallback mechanisms are key enablers. Netflix is a master of thisif their recommendation engine fails, they show popular titles instead. Graceful degradation is a design philosophy that accepts partial failure as inevitable in distributed systems and plans for it proactively."},{question:"What is the purpose of Kubernetes Secrets?",options:["To hide services from external clients","To store and manage sensitive data like passwords, tokens, and keys, providing a more secure mechanism than ConfigMaps","To encrypt all network traffic in the cluster","To store secret algorithms used by services"],correctIndex:1,explanation:"Kubernetes Secrets store sensitive data (passwords, API keys, TLS certificates) separately from pod specifications and container images. While similar to ConfigMaps, Secrets are intended for confidential data and offer additional protections: they're stored in tmpfs (not written to disk on nodes), can be encrypted at rest in etcd, and access can be restricted via RBAC. However, by default Secrets are only base64-encoded (not encrypted), so additional measures like sealed-secrets, external-secrets-operator, or HashiCorp Vault integration are recommended for production. For microservices, Secrets are essential for managing database credentials, API keys, and TLS certificates without embedding them in source code or container images."},{question:"What is the 'shared library' vs 'shared service' debate in microservices?",options:["Whether to use open-source or proprietary libraries","Whether common functionality should be distributed as a library linked into each service or centralized as a separate microservice","Whether to share a library building or use remote offices","Whether teams should share code review feedback publicly"],correctIndex:1,explanation:"When multiple services need the same functionality (e.g., validation logic, utility functions), you can either distribute it as a shared library (NuGet, npm, Maven package) that each service includes, or centralize it as a shared microservice that others call over the network. Shared libraries are simpler (no network call) but create couplingupdating the library requires redeploying all consumers. Shared services are independently deployable but add latency and a failure point. The general guidance: use libraries for truly stable, utility-level code (logging, serialization) and services for business logic that evolves independently. Over-sharing libraries can recreate monolith-like coupling; over-creating shared services can create dependency hell."},{question:"What is 'service mesh data plane' vs 'control plane'?",options:["Data plane handles database operations; control plane handles user sessions","Data plane consists of sidecar proxies handling actual traffic; control plane configures and manages the proxies","Data plane is for testing; control plane is for production","Data plane is east-west; control plane is north-south"],correctIndex:1,explanation:"In a service mesh, the data plane consists of lightweight proxy sidecars (typically Envoy) deployed alongside each service. These proxies intercept and handle all network traffic: load balancing, retries, TLS, observability, etc. The control plane (like Istio's istiod) is the management layer that configures all data plane proxies: distributing routing rules, security policies, and telemetry configuration. Think of it like air traffic control: the data plane is the actual aircraft carrying traffic, while the control plane is the ATC system telling them where to go. This separation allows the data plane to operate even if the control plane is temporarily unavailable (using the last known configuration), improving resilience."},{question:"What is a 'monorepo' vs 'polyrepo' approach for microservices?",options:["Monorepo stores all services in one repository; polyrepo uses a separate repository per service","Monorepo is for monoliths only; polyrepo is for microservices only","Monorepo uses one programming language; polyrepo uses multiple","There is no practical difference between them"],correctIndex:0,explanation:"A monorepo stores all microservice codebases in a single repository, while a polyrepo uses a separate repository for each service. Monorepos (used by Google, Meta) simplify atomic cross-service changes, code sharing, and consistent tooling but require sophisticated build tools (Bazel, Nx) to manage scale. Polyrepos (more common in smaller organizations) give each team complete autonomy over their repo, simpler CI/CD per service, and clear ownership boundaries but make cross-service changes harder and can lead to tool/dependency fragmentation. Neither is inherently betterthe choice depends on team size, organizational structure, and tooling maturity. Many organizations use a hybrid approach."},{question:"What is the purpose of 'distributed locking' in microservices?",options:["Locking down services during deployments","Coordinating access to shared resources across multiple service instances to prevent race conditions","Encrypting data at rest in distributed databases","Restricting which services can communicate with each other"],correctIndex:1,explanation:"Distributed locking ensures that only one service instance at a time can perform a specific operation on a shared resource, preventing race conditions. For example, two instances of a Payment Service processing the same order simultaneously could charge the customer twice. Distributed locks (implemented using Redis with Redlock, ZooKeeper, or etcd) provide mutual exclusion across instances. However, distributed locks are fragilenetwork partitions, clock skew, and GC pauses can cause issues. Martin Kleppmann's critique of Redlock highlights these dangers. Whenever possible, prefer idempotent operations and optimistic concurrency control over distributed locks. When locks are necessary, use fencing tokens to prevent stale lock holders from causing harm."},{question:"What is 'service-level objective (SLO)' in microservices?",options:["The minimum number of services that must be running","A target value for a service's reliability metric, like '99.9% of requests complete in under 200ms'","The maximum number of developers assigned to a service","A standard for service naming conventions"],correctIndex:1,explanation:"A Service-Level Objective (SLO) is a target for a specific reliability metric that a service aims to meetfor example, '99.9% of requests succeed' or 'p99 latency under 200ms.' SLOs are part of the SRE framework: SLIs (indicators) measure the actual metrics, SLOs set the targets, and SLAs (agreements) are contractual obligations with consequences. In microservices, SLOs help teams make informed trade-offs between reliability and velocity. Error budgets (the acceptable amount of unreliability, e.g., 0.1%) determine how aggressively teams can deploy changes. If the error budget is exhausted, teams focus on reliability instead of new features. Google's SRE book popularized this approach for managing complex distributed systems."},{question:"What is the 'scatter-gather' pattern in microservices?",options:["A pattern for distributing databases across regions","Broadcasting a request to multiple services concurrently and aggregating their responses into a single result","A garbage collection algorithm for containers","A logging pattern that scatters log entries across multiple files"],correctIndex:1,explanation:"The scatter-gather pattern sends a request to multiple services in parallel (scatter) and then collects and aggregates their responses (gather). For example, a price comparison service might query multiple supplier services simultaneously and return the best price. Or a search service might scatter queries across multiple index shards and merge the results. This pattern reduces latency compared to sequential calls but requires handling partial failureswhat if one service times out? Strategies include returning partial results with a warning, using cached data for the missing service, or waiting with a timeout. The aggregation logic can live in an API gateway, a BFF, or a dedicated orchestration service."},{question:"What is the difference between 'orchestration' and 'choreography' for microservice integration?",options:["Orchestration uses containers; choreography uses virtual machines","Orchestration has a central controller directing the workflow; choreography has services reacting to events independently with no central control","Orchestration is synchronous; choreography is always faster","They are the same pattern with different names"],correctIndex:1,explanation:"Orchestration uses a central controller (orchestrator) that explicitly directs the sequence of service interactionsit knows the full workflow and tells each service what to do and when. Choreography is decentralizedeach service listens for events and independently decides how to react, with no single entity knowing the full picture. Orchestration is easier to understand and modify for complex workflows but creates a central point that must be maintained. Choreography is more loosely coupled and resilient but can become hard to reason about as the number of event chains grows ('event spaghetti'). Real systems often combine both: choreography between bounded contexts and orchestration within complex processes. Neither is inherently better; the choice depends on workflow complexity and coupling requirements."},{question:"What is 'consumer-driven contract testing' and why is it valuable?",options:["Testing that consumers pay for API usage","A testing approach where API consumers define their expectations, which are then verified against the provider to catch breaking changes early","Testing that all consumers use the same client library","A legal testing process for service agreements"],correctIndex:1,explanation:"Consumer-driven contract testing (CDCT) lets consumers of an API specify exactly what they expect (which endpoints, what data format, what status codes). These expectations become 'contracts' that are tested against the provider in the provider's CI pipeline. If a provider change breaks any consumer's contract, the build fails before deployment. Pact is the most popular CDCT framework. This is crucial for microservices because services are deployed independentlywithout contract tests, a provider might unknowingly break consumers discovered only in production. It's faster and more reliable than end-to-end integration tests because it runs without deploying all services. It inverts the testing relationship: providers are held accountable to their consumers' actual needs."},{question:"What is the 'database migration' challenge when splitting a monolith?",options:["Choosing which cloud provider to host databases on","Extracting each service's data from the shared database into independent stores while maintaining referential integrity and data consistency during the transition","Learning new database query languages","Converting all data to JSON format"],correctIndex:1,explanation:"When splitting a monolith, one of the hardest challenges is decomposing the shared database. Tables often have foreign keys, joins, and stored procedures that cross service boundaries. You must identify which tables belong to which service, replace cross-boundary joins with API calls or data replication, handle the transition period where both the monolith and new services need access to data, and migrate without downtime. Strategies include: starting with separate schemas in the same database, using database views to maintain backward compatibility, and employing CDC (Change Data Capture) to synchronize data during the transition. This is often the most technically challenging aspect of monolith decomposition and is why the Strangler Fig pattern recommends a gradual approach."},{question:"What is a Kubernetes Job and CronJob?",options:["Tools for hiring Kubernetes administrators","Job runs a pod to completion for batch tasks; CronJob schedules Jobs to run periodically on a cron schedule","Job monitors service health; CronJob rotates log files","Job deploys new services; CronJob rolls back failed deployments"],correctIndex:1,explanation:"A Kubernetes Job creates one or more pods and ensures they run to successful completionunlike Deployments that maintain long-running services. Jobs are ideal for batch processing, data migrations, and one-off tasks. A CronJob creates Jobs on a recurring schedule (using cron syntax), useful for periodic tasks like database backups, report generation, and cleanup operations. For microservices, Jobs handle batch processing that doesn't fit the always-running service model, and CronJobs replace traditional cron daemons in a cloud-native way. Jobs support parallelism (running multiple pods simultaneously), completion counts, and configurable retry policies for failed pods."},{question:"What is the 'death star' architecture anti-pattern?",options:["A highly centralized architecture with a single point of failure","An architecture where every service calls every other service, creating a tangled web of dependencies that's impossible to understand or manage","An architecture designed to destroy legacy systems","A top-secret internal codename for classified architectures"],correctIndex:1,explanation:"The 'death star' anti-pattern (named for the visualization that looks like the Star Wars Death Star) occurs when microservices are so interconnected that every service depends on multiple others with no clear layering or boundaries. Visualizing service dependencies produces a dense, tangled web. This typically results from poor service boundary design, excessive synchronous calls, and lack of domain-driven design. The symptoms include: a change in one service cascading unpredictably, difficulty understanding request flows, and system-wide outages from a single service failure. The solution involves introducing event-driven communication, defining clear service layers, establishing API contracts, and potentially re-evaluating service boundaries using DDD bounded contexts."},{question:"What is the role of a 'sidecar proxy' in implementing observability?",options:["It replaces the application's logging framework","It transparently captures metrics, traces, and access logs from all service traffic without modifying application code","It stores observability data in a local database","It sends alerts directly to developers' phones"],correctIndex:1,explanation:"A sidecar proxy (like Envoy in Istio) intercepts all inbound and outbound traffic for a service, automatically capturing observability data: request/response metrics (latency, status codes, throughput), distributed traces (propagating trace context between services), and access logs. This provides consistent observability across all services regardless of programming language or framework, without requiring application code changes. The sidecar emits data to observability backends (Prometheus for metrics, Jaeger for traces, Elasticsearch for logs). This is one of the most compelling features of service meshespolyglot environments get uniform observability. The application can still add custom business metrics, but infrastructure-level observability comes 'for free' from the sidecar."},{question:"What is 'semantic versioning' and how does it apply to microservice APIs?",options:["Versioning based on the meaning of code changes","A versioning scheme (MAJOR.MINOR.PATCH) where MAJOR indicates breaking changes, MINOR adds backward-compatible features, and PATCH fixes bugs","Using descriptive names instead of numbers for versions","Automatically versioning APIs based on deployment date"],correctIndex:1,explanation:"Semantic versioning (semver) uses MAJOR.MINOR.PATCH where: MAJOR increments for breaking (incompatible) changes, MINOR for backward-compatible new features, and PATCH for backward-compatible bug fixes. For microservice APIs, this communicates the impact of changes to consumers: a PATCH or MINOR update is safe to adopt without changes, but a MAJOR update requires consumer modifications. This is critical because services are independently deployedconsumers need to understand whether a provider update will break them. In practice, microservices often use simpler versioning (v1, v2) for API endpoints and semver for shared libraries. The key principle: never make breaking changes without communicating them through the version number."},{question:"What is 'load shedding' in microservices?",options:["Distributing load evenly across all service instances","Intentionally rejecting excess requests when a service is at capacity to maintain performance for accepted requests","Moving workloads from one cloud region to another","Reducing the service's codebase to improve performance"],correctIndex:1,explanation:"Load shedding is the practice of deliberately dropping excess requests when a service is at or near capacity, rather than attempting to process everything and degrading performance for all requests. For example, when a service detects it's handling more than its healthy capacity, it returns HTTP 503 (Service Unavailable) for additional requests while maintaining good latency for the requests it's processing. This is different from rate limiting (which limits per-client rates) and load balancing (which distributes requests). Load shedding is a last-resort defense that keeps the service responsive under extreme load. Google's approach prioritizes important requests (like user-facing traffic over batch jobs) during shedding."},{question:"What is 'zero-trust networking' in a microservices context?",options:["Not trusting any code that wasn't written in-house","A security model that verifies every request regardless of origin, assuming the network is always hostileno implicit trust based on network location","Using zero-configuration networking between services","A network that requires zero maintenance"],correctIndex:1,explanation:"Zero-trust networking abandons the traditional perimeter-based security model (trusted inside the firewall, untrusted outside) in favor of verifying every request regardless of where it originates. In microservices, this means: every service-to-service call is authenticated (mutual TLS), authorized (fine-grained policies checking if Service A is allowed to call Service B's specific endpoint), and encrypted. Service meshes like Istio enable zero-trust by handling mTLS and authorization policies transparently via sidecar proxies. This is essential in containerized environments where services share network infrastructure and lateral movement by attackers must be prevented. Google's BeyondCorp is a pioneering implementation of zero-trust principles."},{question:"What are 'init containers' in Kubernetes?",options:["The first containers deployed in a new cluster","Specialized containers that run and complete before the main application containers start, used for setup tasks","Containers that initialize the Kubernetes control plane","Default containers provided by Kubernetes for monitoring"],correctIndex:1,explanation:"Init containers are specialized containers in a pod that run to completion before the main application containers start. They're used for initialization tasks like: waiting for a dependent service to be ready, downloading configuration files, running database migrations, or setting up file permissions. Init containers run sequentiallyeach must succeed before the next starts, and all must complete before the main containers launch. This is useful in microservices for ensuring prerequisites are met: for example, an init container can wait for a database migration to complete or for a config service to be available. Unlike main containers, init containers don't support liveness/readiness probes since they run to completion."},{question:"What is the 'ambassador' deployment model in Kubernetes?",options:["Deploying services to a special 'ambassador' namespace","Running a proxy container in the same pod that handles routing, TLS termination, and protocol translation for the main container","Designating one pod as the spokesperson for a group of pods","A diplomatic protocol for cross-cluster communication"],correctIndex:1,explanation:"The ambassador pattern in Kubernetes deploys a proxy container within the same pod as the main application container. The ambassador handles complex outbound connectivity: routing requests to the right backend, TLS termination, protocol translation, or connection pooling. The main container communicates with the ambassador via localhost, simplifying its network logic. For example, an ambassador might handle connecting to a sharded database, routing requests to the correct shardthe application just connects to localhost:5432. This is closely related to the sidecar pattern but specifically addresses outbound communication complexity. In service mesh architectures, the Envoy sidecar serves a similar ambassador role."},{question:"What is 'graceful shutdown' and why is it important for microservices in Kubernetes?",options:["Shutting down the entire Kubernetes cluster gracefully","Allowing a service to finish processing in-flight requests before terminating, preventing data loss and request failures during deployments","A polite way to notify users that a service is going offline","Gradually reducing traffic to zero before maintenance"],correctIndex:1,explanation:"Graceful shutdown means that when a pod receives a termination signal (SIGTERM), it stops accepting new requests, finishes processing in-flight requests, cleans up resources (closes connections, flushes buffers), and then exits. In Kubernetes, this happens during rolling updates, scaling down, and node draining. Kubernetes sends SIGTERM, waits for the terminationGracePeriodSeconds (default 30s), then sends SIGKILL. Without graceful shutdown, in-flight requests are abruptly terminated, causing errors for clients. Applications must handle SIGTERM properly: stop the HTTP server from accepting new connections, wait for current requests to complete, and exit cleanly. This is especially critical for microservices with constant rolling updates."},{question:"What does 'polyglot architecture' mean?",options:["An architecture that supports multiple human languages in the UI","An architecture where different services are built using different programming languages, frameworks, and data stores based on what's optimal for each","Using a single programming language across all microservices","Translating APIs between different format standards"],correctIndex:1,explanation:"Polyglot architecture allows each microservice team to choose the best programming language, framework, and data store for their specific needs. A compute-heavy service might use Go or Rust for performance, a data science service might use Python, a web-facing service might use Node.js, and each might use the most appropriate database (PostgreSQL, MongoDB, Redis, etc.). This is a key benefit of microservices: the contract between services is the API, not the implementation. The trade-off is operational complexityyour platform team must support multiple runtimes, build pipelines, and data stores. Organizations often allow choice within guardrails (e.g., 'choose from these three approved languages')."},{question:"What is 'topology-aware routing' in Kubernetes?",options:["Routing based on the shape of the network diagram","Preferring to route traffic to service instances in the same zone or region to reduce latency and cross-zone costs","Using geographic maps to visualize service locations","Routing based on the physical topology of server racks"],correctIndex:1,explanation:"Topology-aware routing (formerly known as topology hints or service topology) in Kubernetes prefers to route traffic to endpoints that are 'closer' in the network topologytypically in the same availability zone before crossing to other zones. Cross-zone traffic in cloud environments incurs both latency (a few extra milliseconds) and cost (cloud providers charge for cross-AZ data transfer). In microservices with high inter-service call volumes, these costs add up significantly. Kubernetes topology-aware routing annotates endpoints with zone information and the kube-proxy routes traffic accordingly. This optimization is transparent to services and can reduce both latency and cloud costs substantially for east-west traffic."},{question:"What is the 'strangler fig' approach to handling the frontend during monolith decomposition?",options:["Rewriting the entire frontend from scratch","Using a reverse proxy or API gateway to route requestsnew functionality goes to microservices, legacy functionality stays in the monolith until migrated","Keeping the frontend as a monolith forever","Converting the frontend to a CLI tool"],correctIndex:1,explanation:"In the Strangler Fig pattern, a reverse proxy or API gateway sits in front of both the monolith and new microservices, routing each request to the appropriate backend. New features are built as microservices, and existing features are migrated one by one. The frontend doesn't need to know about this splitit talks to the gateway, which handles routing transparently. Over time, more routes point to microservices and fewer to the monolith, until the monolith is completely replaced (or shrunk to a manageable size). This approach minimizes risk, allows incremental validation, and avoids the dangerous big-bang rewrite that has historically led to project failures at companies like Netscape."},{question:"What is the 'saga execution coordinator' (SEC)?",options:["The Securities and Exchange Commission's role in microservices","A component in orchestration-based sagas that manages the saga's state machine, deciding what step to execute next or what to compensate","A team lead who coordinates deployment schedules","A Kubernetes controller for managing persistent volumes"],correctIndex:1,explanation:"The Saga Execution Coordinator (SEC) is the central component in orchestration-based sagas that maintains the saga's state (which steps have completed, which are pending) and determines the next action. When a step succeeds, the SEC invokes the next step; when a step fails, it triggers compensating transactions for previously completed steps in reverse order. The SEC itself must be reliableit persists its state so it can recover from crashes. Tools like Temporal, Camunda, and AWS Step Functions serve as SEC implementations. The SEC's state machine defines the entire workflow, making it easy to visualize and modify. The trade-off is that the SEC becomes a critical component that must be highly available."},{question:"What is 'request collapsing' or 'request coalescing'?",options:["Compressing multiple fields into a single field","Merging multiple identical or similar concurrent requests into a single backend call and sharing the result among all callers","Collapsing a microservice back into the monolith","Combining request and response into a single message"],correctIndex:1,explanation:"Request collapsing (or coalescing) detects multiple concurrent requests for the same data and groups them into a single backend call, sharing the result among all waiting callers. For example, if 50 threads simultaneously request user profile #123, instead of making 50 database queries, a single query is made and the result is distributed to all 50 callers. Netflix Hystrix (now Resilience4j) popularized this pattern. This is particularly effective for microservices with high concurrency and overlapping requests. It reduces load on downstream services and databases, decreasing latency and resource usage. The trade-off is that all collapsed requests share the same result, including errors, and it adds a small windowing delay to batch requests."},{question:"What is 'GitOps' in the context of microservices deployment?",options:["Using GitHub for all service repositories","An operational model where Git is the single source of truth for infrastructure and deployment configuration, with automated reconciliation","A Git branching strategy for operations teams","Using Git hooks to trigger alerts"],correctIndex:1,explanation:"GitOps uses Git as the single source of truth for both application code and infrastructure/deployment configuration. A GitOps operator (like ArgoCD or Flux) continuously monitors the Git repository and automatically reconciles the cluster state to match the desired state declared in Git. When you want to deploy a new version, you update the Kubernetes manifest in Git (via pull request), and the operator detects the change and applies it to the cluster. This provides audit trails (Git history), rollback (revert a commit), consistency (cluster always matches Git), and security (no direct kubectl access needed). For microservices with dozens of services, GitOps brings order to deployment chaos."},{question:"What is 'service-level agreement (SLA)' vs 'service-level objective (SLO)'?",options:["They are the same thing","SLA is a contractual agreement with consequences for breaching; SLO is an internal target the team aims to meet","SLA is for internal services; SLO is for external customers","SLO is stricter than SLA"],correctIndex:1,explanation:"An SLA (Service-Level Agreement) is a formal contract between a service provider and its customers that specifies performance guarantees and consequences (usually financial penalties or credits) if those guarantees are not met. An SLO (Service-Level Objective) is an internal target set by the teamit's what the team actually aims for and is typically stricter than the SLA to provide a buffer. For example, the SLA might guarantee 99.9% uptime, but the team's SLO might be 99.95%. The gap between SLO and SLA is the team's safety margin. SLIs (Service-Level Indicators) are the actual measured metrics. In microservices, each service should have SLOs that account for dependenciesif Service A depends on Service B, A's SLO can't be higher than B's."},{question:"What is a 'Network Policy' in Kubernetes?",options:["A company policy about network usage","A Kubernetes resource that defines rules controlling which pods can communicate with each other, implementing microsegmentation","A DNS configuration for external domains","A bandwidth throttling mechanism"],correctIndex:1,explanation:"Network Policies in Kubernetes are resources that specify rules for pod-to-pod communication, essentially acting as a firewall at the pod level. By default, all pods can communicate with all other pods in a Kubernetes cluster. Network Policies restrict this by defining ingress (inbound) and egress (outbound) rules based on pod labels, namespace labels, or IP blocks. For example, you might restrict a database pod to only accept traffic from the application pods that need it. This implements microsegmentationa zero-trust principle where only explicitly allowed communication is permitted. Network Policies require a CNI plugin that supports them (Calico, Cilium, Weave Net). They're essential for securing microservices by limiting blast radius if a service is compromised."},{question:"What is the 'saga log' in a saga pattern?",options:["A log file recording all API calls made during development","A persistent record of a saga's progress that enables recovery if the saga coordinator crashes during execution","The logging output of the saga framework","A changelog of saga pattern modifications"],correctIndex:1,explanation:"The saga log is a durable record of a saga instance's execution statewhich steps have completed, which are in progress, and any compensation results. If the saga coordinator crashes mid-execution, it uses the saga log to determine where to resume when it restarts. Without a saga log, a crash could leave the system in an inconsistent state with some services having committed their transactions and others not. The saga log is typically stored in the coordinator's database or a dedicated event store. This is analogous to a database transaction log that enables recovery after crashes. Tools like Temporal persist workflow state automatically, effectively managing the saga log for you."},{question:"What is 'multi-tenancy' in microservices architecture?",options:["Running services in multiple data centers","A single service instance serving multiple customers (tenants) with logical data isolation between them","Having multiple development teams work on the same service","Running multiple versions of a service simultaneously"],correctIndex:1,explanation:"Multi-tenancy means a single deployment of a microservice serves multiple customers (tenants), with each tenant's data logically isolated from others. This is more resource-efficient than deploying separate instances per tenant (single-tenancy). Implementation approaches range from shared database with tenant ID columns (simplest but least isolated), separate schemas per tenant (moderate isolation), to separate databases per tenant (strongest isolation). For microservices, multi-tenancy adds complexity: every query must be scoped to the current tenant, cross-tenant data leaks are a critical security concern, and noisy neighbor effects (one tenant's heavy usage affecting others) must be managed through resource limits and rate limiting."},{question:"What is the 'backend for frontend' pattern's main benefit over a general-purpose API?",options:["It reduces the number of microservices needed","It tailors the API to each client's specific needs, avoiding over-fetching or under-fetching of data","It eliminates the need for an API gateway","It makes all clients use the same data format"],correctIndex:1,explanation:"The BFF pattern creates a dedicated API layer for each type of client (web, mobile, third-party), each optimized for that client's specific needs. A mobile app might need smaller payloads with fewer fields, while a web dashboard might need aggregated data from multiple services. A general-purpose API serving all clients becomes a compromise that serves none optimallyit either includes too much data (mobile wastes bandwidth) or too little (web needs multiple calls). The BFF handles aggregation, transformation, and filtering specific to its client. It can also implement client-specific authentication flows. The trade-off is maintaining multiple BFF services, but the improved client experience and independent evolution usually justify the cost."},{question:"What is 'distributed saga' vs 'two-phase commit' for cross-service transactions?",options:["They are the same concept with different names","Sagas use compensating transactions and local ACID; 2PC uses a global coordinator that locks resources across all participants until all vote to commit","2PC is simpler and always preferred over sagas","Sagas require a specialized database; 2PC works with any database"],correctIndex:1,explanation:"Two-phase commit (2PC) uses a transaction coordinator that first asks all participants to prepare (vote to commit), then either commits or aborts all participants atomically. This provides strong consistency but blocks resources during the vote phase, reducing availability and throughput. Sagas break the transaction into local steps with compensating actions, each committing independently. Sagas provide better availability and performance but only guarantee eventual consistency. In microservices, 2PC is generally avoided because it creates tight coupling, blocks resources, and becomes a bottleneck. Sagas are the standard approach despite their complexity. Some databases support 2PC for sharded writes, but across independently owned microservices, sagas are the pragmatic choice."},{question:"What is the purpose of 'pod disruption budgets' (PDB) in Kubernetes?",options:["Setting financial budgets for pod resource usage","Ensuring a minimum number of pods remain available during voluntary disruptions like upgrades and node draining","Limiting the number of pods a team can create","Tracking pod creation and deletion costs"],correctIndex:1,explanation:"Pod Disruption Budgets (PDBs) define the minimum number of pods (or maximum unavailable) that must remain available during voluntary disruptions like node drains, cluster upgrades, or maintenance. For example, a PDB might specify 'at least 2 out of 3 replicas must be available at all times.' Without a PDB, a node drain could simultaneously terminate all pods of a service, causing downtime. With a PDB, Kubernetes respects the budget and waits for new pods to be ready before terminating additional ones. PDBs don't protect against involuntary disruptions (hardware failures), only voluntary ones. They're essential for microservices running on clusters with regular maintenance activities."},{question:"What is 'event-carried state transfer' in microservices?",options:["Transferring state via HTTP GET requests","Events that carry enough data for consumers to update their local state without needing to call back to the source service","Moving event processing logic between services","Using events to trigger state machine transitions"],correctIndex:1,explanation:"Event-carried state transfer is a pattern where events contain sufficient data for consumers to maintain a local replica of the source data, eliminating the need for synchronous callbacks. For example, when a Customer Service publishes a 'CustomerUpdated' event, it includes the full customer data (name, address, email), so consuming services can update their local copies without querying the Customer Service. This reduces coupling and improves resilienceconsumers don't need the source service to be available for reads. The trade-off is larger event payloads and data duplication across services. Martin Fowler described this pattern as one of four event-driven patterns. It's the foundation of data replication in event-driven microservices."},{question:"What is 'contract-first design' for microservice APIs?",options:["Writing the legal contract between teams before coding","Designing and agreeing on the API specification (e.g., OpenAPI/Swagger) before implementing the service","Implementing the service first and generating the contract from the code","Having the consumer write the provider's code"],correctIndex:1,explanation:"Contract-first design means defining the API specification (using OpenAPI/Swagger, protobuf, or AsyncAPI for events) before writing any implementation code. This allows consumer and provider teams to agree on the interface upfront, work in parallel (consumers mock the API while the provider implements it), and catch design issues early. It's the opposite of code-first, where the API spec is generated from the implementation. For microservices, contract-first is particularly valuable because it forces explicit boundary design, enables parallel development, and produces high-quality API documentation automatically. The contract becomes a shared artifact that both teams reference, reducing miscommunication and integration surprises."},{question:"What is 'Kubernetes Operator' pattern?",options:["A human operator who manages Kubernetes clusters","A custom controller that extends Kubernetes to automate the management of complex applications using domain-specific knowledge","The default controller manager in Kubernetes","A command-line tool for operating Kubernetes clusters"],correctIndex:1,explanation:"A Kubernetes Operator is a custom controller that encodes operational knowledge (human operator expertise) into software. It uses Custom Resource Definitions (CRDs) to extend the Kubernetes API with domain-specific resources and a controller that watches these resources and takes action. For example, a PostgreSQL Operator can automatically handle database provisioning, backups, failover, and scalingtasks that would otherwise require a DBA. Operators follow the Kubernetes control loop pattern: observe current state, compare to desired state, take action. For microservices, operators manage complex stateful infrastructure (databases, message brokers, monitoring systems) declaratively. The Operator Framework and Kubebuilder are popular tools for building operators."},{question:"What is 'data mesh' and how does it relate to microservices?",options:["A mesh network for database replication","A decentralized approach to data architecture where domain teams own and serve their data as products, analogous to microservices principles applied to data","A service mesh specifically for database traffic","A grid computing framework for data processing"],correctIndex:1,explanation:"Data mesh, proposed by Zhamak Dehghani, applies microservices principles (domain ownership, decentralization, self-serve platform) to analytical data. Instead of centralizing all data in a single data warehouse/lake owned by a central team, domain teams own their analytical data as 'data products' with defined contracts, SLOs, and discoverability. This addresses the bottleneck of central data teams that can't keep up with demand. The four principles are: domain-oriented data ownership, data as a product, self-serve data infrastructure platform, and federated computational governance. For microservices teams, data mesh means they're responsible not only for their operational data but also for serving their domain's analytical data to the organization."},{question:"What is 'traffic mirroring' (shadowing) in microservices?",options:["Duplicating traffic to a backup data center for disaster recovery","Sending a copy of production traffic to a new version of a service for testing without affecting the live response","Encrypting traffic to hide it from monitoring","Reflecting DDoS attack traffic back to the attacker"],correctIndex:1,explanation:"Traffic mirroring (or shadowing) copies production traffic to a new version of a service running alongside the current version. The mirrored service processes real requests but its responses are discardedonly the current version's responses are sent to clients. This allows testing a new version with real production traffic patterns and data, catching issues that synthetic tests might miss, without any risk to users. Istio's VirtualService supports traffic mirroring natively. It's particularly valuable for microservices because it validates behavior under realistic load conditions. The trade-off is resource cost (you're running two versions) and potential side effects (the mirrored version might write to databases or call other services, so safeguards are needed)."},{question:"What is the difference between 'push' and 'pull' models for service communication?",options:["Push uses TCP; pull uses UDP","In push, the producer sends data to consumers when available; in pull, consumers request data from the producer when they need it","Push is faster than pull in all scenarios","Push works within a cluster; pull works across clusters"],correctIndex:1,explanation:"In the push model, the producer actively sends data to consumers as soon as it's available (e.g., webhooks, pub/sub messaging, server-sent events). In the pull model, consumers request data when they need it (e.g., HTTP polling, Kafka consumer pulling from partitions). Push provides lower latency (consumers get data immediately) but can overwhelm slow consumers. Pull lets consumers control their consumption rate but may introduce latency. Kafka uses a pull model where consumers pull messages at their own pace, which provides excellent backpressure handling. Many systems combine both: Kafka pushes to consumer groups but consumers pull from partitions. The choice depends on latency requirements, consumer processing speed, and scalability needs."},{question:"What is the 'strangler fig' pattern's relationship with feature flags?",options:["They are unrelated concepts","Feature flags can control which implementation (monolith or microservice) handles specific requests, enabling gradual migration with instant rollback","Feature flags replace the strangler fig pattern entirely","The strangler fig pattern is a type of feature flag"],correctIndex:1,explanation:"Feature flags and the Strangler Fig pattern are complementary. During monolith decomposition, feature flags can control whether a specific request is routed to the legacy monolith or the new microservice implementation. This provides fine-grained control: you can enable the new service for 10% of users, specific user segments, or specific tenants. If the new service has issues, you instantly toggle the flag to route all traffic back to the monolith. This is much safer than hard-cutting traffic at the proxy level. Combined with canary analysis (comparing error rates and latency between old and new), feature flags turn the strangler fig migration into a measured, data-driven process with minimal risk."},{question:"What is 'domain-driven design strategic patterns' and how do they guide microservice decomposition?",options:["Patterns for choosing domain name registrars","High-level DDD patterns (bounded contexts, context maps, shared kernels, anti-corruption layers) that help identify service boundaries and inter-service relationships","Strategies for database schema design within a service","Game theory patterns for team coordination"],correctIndex:1,explanation:"DDD strategic patterns operate at the system level, guiding how bounded contexts (and thus microservices) relate to each other. Context Maps visualize these relationships using patterns like: Shared Kernel (two contexts share a subset of the domain model), Customer-Supplier (one context's output feeds another), Conformist (downstream accepts upstream's model as-is), Anti-Corruption Layer (downstream translates upstream's model), and Published Language (shared communication format). These patterns directly map to microservice integration strategiesan ACL becomes an adapter service, a shared kernel becomes a shared library, and customer-supplier relationships define API ownership. Understanding these patterns prevents ad-hoc service boundaries and helps teams design sustainable inter-service relationships."},{question:"What is the 'resource limits' concept in Kubernetes and why is it critical for microservices?",options:["Limiting the number of Kubernetes resources (pods, services) in the cluster","Setting CPU and memory bounds on containers to prevent a single service from consuming all node resources and affecting other services","Limiting how many API resources a service can create","Restricting which resources a service can access in the cloud"],correctIndex:1,explanation:"Kubernetes resource limits set maximum CPU and memory a container can use. Resource requests specify the guaranteed minimum resources for scheduling, while limits cap the maximum. Without limits, a memory-leaking service could consume all node memory, triggering the OOM killer and crashing other services on the same node. Similarly, a CPU-intensive service could starve its neighbors. For microservices sharing cluster resources, proper limits are essential: they ensure fair resource sharing, enable accurate capacity planning, and prevent noisy-neighbor problems. Setting requests too high wastes resources (pods can't be efficiently packed), while setting them too low causes throttling. Getting this right requires monitoring actual usage and iterating."},{question:"What is 'event notification' pattern vs 'event-carried state transfer'?",options:["They are identical patterns","Event notification contains minimal data (just the event type and ID, requiring callback for details); event-carried state transfer includes the full data in the event","Event notification is for errors; event-carried state transfer is for success","Event notification is synchronous; event-carried state transfer is asynchronous"],correctIndex:1,explanation:"Event notification publishes lightweight events (e.g., 'OrderCreated: orderId=123') that tell consumers something happened but don't include the full data. Consumers must call back to the source service's API to get details. Event-carried state transfer includes the full data (e.g., 'OrderCreated: {orderId: 123, items: [...], total: 99.99}') so consumers can update their local state without callbacks. Notification is simpler (smaller events, single source of truth for data) but creates runtime coupling (consumer depends on producer being available for queries). State transfer is more decoupled but creates larger events and data duplication. Martin Fowler describes these as two of four event-driven interaction patterns, each appropriate for different coupling and consistency requirements."},{question:"What is the 'ambassador' vs 'sidecar' vs 'adapter' container pattern in Kubernetes?",options:["They are three names for the same pattern","Ambassador handles outbound traffic proxying, sidecar adds functionality (logging, monitoring), adapter standardizes output formatall are multi-container pod patterns","Ambassador is for HTTP, sidecar for gRPC, adapter for WebSocket","They differ only in the container image used"],correctIndex:1,explanation:"These are three distinct multi-container pod patterns. The Sidecar extends the main container with additional functionality (log shipping, configuration reloading, service mesh proxy) without modifying the application. The Ambassador proxies outbound network connections, handling complexity like connection pooling, routing, and TLS termination for the main container. The Adapter standardizes the main container's outputfor example, converting application-specific metrics into a Prometheus-compatible format or transforming log formats. All three patterns deploy helper containers alongside the main container in the same pod, sharing network and storage. In practice, the Envoy sidecar in service meshes performs all three roles: proxying (ambassador), adding observability (sidecar), and standardizing metrics (adapter)."},{question:"What is 'distributed transaction' and why is it problematic in microservices?",options:["A transaction that takes a long time to complete","A transaction spanning multiple services/databases that's problematic because it requires all participants to be available and locks resources, reducing system availability and scalability","A transaction distributed across multiple time zones","A transaction that is processed by multiple CPUs"],correctIndex:1,explanation:"A distributed transaction spans multiple services or databases and aims to maintain ACID properties across all of them. The most common protocol is two-phase commit (2PC), where a coordinator first asks all participants to prepare, then instructs them to commit or abort. This is problematic in microservices because: it requires all services to be available simultaneously (reducing availability), it locks resources during the prepare phase (reducing throughput), it couples services at the database level, and it becomes a single point of failure if the coordinator crashes. The CAP theorem shows that strong consistency and high availability can't coexist during partitions. That's why microservices use sagas (eventual consistency with compensations) instead of distributed transactions."},{question:"What is 'shard-nothing architecture' in microservices databases?",options:["A database that never shards its data","An architecture where each database shard operates independently without sharing memory, disk, or CPU with other shards","Sharing database shards between multiple services","An architecture where sharding is handled by the application code"],correctIndex:1,explanation:"Shared-nothing architecture means each database node (shard) has its own dedicated CPU, memory, and storage, with no shared resources between nodes. This allows linear scalabilityadding more shards adds more capacity without contention. Each microservice's database can be independently sharded based on its access patterns (e.g., sharding orders by customer ID, products by category). This contrasts with shared-disk architectures (like Oracle RAC) where multiple nodes share storage. Shared-nothing is the foundation of distributed databases like Cassandra, CockroachDB, and Citus. For microservices, this aligns with the independent scaling principle: each service can scale its data tier based on its own growth, without affecting other services."},{question:"What is the role of 'Helm' in Kubernetes microservices deployment?",options:["A monitoring tool for Kubernetes","A package manager for Kubernetes that uses charts (templated manifests) to define, install, and manage application deployments","A network plugin for inter-pod communication","A security scanner for container images"],correctIndex:1,explanation:"Helm is the Kubernetes package manager that uses 'charts' (collections of templated YAML manifests) to define complete application deployments. A chart might include a Deployment, Service, ConfigMap, and Ingress, all parameterized with a values.yaml file for customization across environments. For microservices, Helm simplifies deploying complex applications with many interconnected resources: instead of managing dozens of individual YAML files, you install a chart with environment-specific values. Helm supports versioning, rollbacks, and dependency management between charts. Popular alternatives include Kustomize (overlay-based, no templating) and Jsonnet. Many organizations create a base Helm chart template that all microservices extend, ensuring consistent deployment standards."},{question:"What is 'cell-based architecture' in microservices?",options:["Organizing microservices by cellular network standards","Grouping related services into isolated cells (units of deployment and failure isolation) that can be independently deployed and scaled","Running microservices on cell phone hardware","A biology-inspired pattern where services divide like cells"],correctIndex:1,explanation:"Cell-based architecture groups related microservices into isolated cells, where each cell is a complete, independently deployable unit with its own data store, compute, and routing. Traffic is routed to a specific cell based on a partition key (like customer ID or region). If a cell fails, only its users are affectedother cells continue operating. This limits the blast radius of failures: instead of a service-wide outage affecting all users, only users in the failed cell are impacted. AWS and other hyperscalers use cell-based architectures extensively. It combines microservices principles with infrastructure isolation, providing stronger fault isolation than services alone. The trade-off is increased operational complexity and resource overhead from running multiple independent cells."}],Sh={"cap-theorem":um,databases:dm,"design-url-shortener":hm,"event-driven":pm,"load-balancers":fm,microservices:mm},xa=[{id:"cap-theorem",title:"CAP Theorem",category:"fundamentals",description:"Consistency, Availability, Partition Tolerance  pick two.",content:`In a distributed system you can only guarantee two of three properties:

**Consistency (C):** Every read receives the most recent write or an error.

**Availability (A):** Every request gets a non-error response.

**Partition Tolerance (P):** The system works despite network partitions.

Since partitions are inevitable, the real choice is CP vs AP.

**CP**  reject requests during a partition to stay consistent (e.g., banking).
**AP**  serve requests with potentially stale data (e.g., social feeds).`,tips:[`Don't just recite CAP  apply it: "I choose AP here because"`,"Mention PACELC as a more nuanced model.","Real systems aren't purely CP or AP; they make per-operation trade-offs."],relatedIds:["consistency-patterns","databases"],quizQuestions:[{question:"Which CAP property is always required in a distributed system?",options:["Consistency","Availability","Partition Tolerance","Durability"],correctIndex:2,explanation:"Network partitions are inevitable, so Partition Tolerance is always needed."},{question:"A social media feed that shows slightly stale posts during a partition is an example of?",options:["CP system","AP system","CA system","ACID system"],correctIndex:1,explanation:"It favours Availability over Consistency  classic AP."}]},{id:"acid-base",title:"ACID vs BASE",category:"fundamentals",description:"Strict transactional integrity vs eventual consistency.",content:`**ACID** (SQL databases):
- **Atomicity**  all or nothing
- **Consistency**  valid state transitions
- **Isolation**  concurrent txns don't interfere
- **Durability**  committed data survives crashes

**BASE** (NoSQL):
- **Basically Available**  always responds
- **Soft State**  state may change without input
- **Eventually Consistent**  converges over time

Choose ACID when correctness matters (payments). Choose BASE when availability & scale matter (analytics, feeds).`,tips:["Isolation levels matter  know Read Committed vs Serializable.","Many modern databases (CockroachDB, Spanner) offer ACID at scale."],relatedIds:["cap-theorem","databases"],quizQuestions:[{question:"Which ACID property ensures a transaction is all-or-nothing?",options:["Consistency","Isolation","Atomicity","Durability"],correctIndex:2,explanation:"Atomicity means the entire transaction succeeds or rolls back."}]},{id:"networking-basics",title:"Networking Basics",category:"fundamentals",description:"TCP/IP, HTTP, DNS, WebSockets  the foundation.",content:`**TCP**  reliable, ordered byte stream. 3-way handshake.
**UDP**  unreliable, fast. Good for video/gaming.
**HTTP/1.1**  request/response, keep-alive.
**HTTP/2**  multiplexed streams, header compression.
**HTTP/3**  QUIC (UDP-based), faster handshake.
**WebSockets**  full-duplex persistent connection.
**DNS**  translates domain names to IPs. Hierarchical caching.`,tips:["Know when to use WebSockets vs polling vs SSE.","DNS can be used for load balancing (round-robin DNS)."],relatedIds:["load-balancers","cdn"],quizQuestions:[{question:"Which protocol is HTTP/3 built on?",options:["TCP","UDP/QUIC","SCTP","WebSocket"],correctIndex:1,explanation:"HTTP/3 uses QUIC, which runs over UDP."}]},{id:"scalability",title:"Scalability",category:"fundamentals",description:"Vertical vs horizontal scaling, stateless design.",content:`**Vertical Scaling**  bigger machine (limited ceiling).
**Horizontal Scaling**  more machines (preferred for web).

Key principles:
- **Stateless services**  any instance can handle any request
- **Shared-nothing architecture**  nodes don't share memory/disk
- **Data partitioning**  split data across nodes
- **Replication**  copies for read throughput & fault tolerance

Amdahl's Law: speedup limited by the serial fraction of work.`,tips:["Always start with the simplest approach; scale when needed.","Horizontal scaling requires solving distributed coordination."],relatedIds:["load-balancers","databases","consistent-hashing"],quizQuestions:[{question:"Which scaling approach adds more machines?",options:["Vertical","Horizontal","Diagonal","Elastic"],correctIndex:1,explanation:"Horizontal scaling = adding more machines."}]},{id:"latency-throughput",title:"Latency & Throughput",category:"fundamentals",description:"Numbers every engineer should know.",content:`**Key latencies:**
- L1 cache: ~1 ns
- RAM: ~100 ns
- SSD random read: ~16 s
- HDD seek: ~4 ms
- Same datacenter round-trip: ~0.5 ms
- Cross-continent: ~150 ms

**Throughput** = requests/second the system handles.
**Bandwidth** = max data transfer rate.

Little's Law: L =   W (avg items = arrival rate  avg time in system).`,tips:["Memorize the orders of magnitude, not exact numbers.","Use these to do back-of-envelope calculations in interviews."],relatedIds:["caching","cdn"],quizQuestions:[{question:"Roughly how long is a cross-continent network round-trip?",options:["1 ms","15 ms","150 ms","1500 ms"],correctIndex:2,explanation:"Cross-continent RTT is roughly 100150 ms."}]},{id:"load-balancers",title:"Load Balancers",category:"building-blocks",description:"Distribute traffic across servers.",content:`**Layer 4 (Transport)**  routes based on IP/port. Fast, no content inspection.
**Layer 7 (Application)**  routes based on HTTP headers, URL, cookies. More flexible.

**Algorithms:**
- Round Robin / Weighted Round Robin
- Least Connections
- IP Hash (sticky sessions)
- Consistent Hashing

**Health checks**  periodic probes remove unhealthy backends.

Examples: AWS ALB/NLB, Nginx, HAProxy, Envoy.`,tips:["L7 is slower but enables content-based routing, SSL termination, etc.","Mention health checks  interviewers love hearing about failure handling."],relatedIds:["scalability","networking-basics","cdn"],quizQuestions:[{question:"Which LB layer can route based on HTTP headers?",options:["Layer 3","Layer 4","Layer 7","Layer 2"],correctIndex:2,explanation:"Layer 7 (Application) LBs inspect HTTP content."}]},{id:"databases",title:"Databases",category:"building-blocks",description:"SQL vs NoSQL, replication, partitioning.",content:`**SQL**  relational, ACID, schemas, joins. Great for structured data.
**NoSQL types:**
- Key-Value (Redis, DynamoDB)
- Document (MongoDB)
- Wide-Column (Cassandra, HBase)
- Graph (Neo4j)

**Replication:** Leader-follower, multi-leader, leaderless.
**Partitioning:** Range, hash, composite.
**Indexes:** B-tree (reads), LSM-tree (writes).`,tips:[`Don't say "NoSQL is faster"  it depends on the access pattern.`,"Justify your DB choice with the read/write ratio and consistency needs."],relatedIds:["acid-base","consistent-hashing","caching"],quizQuestions:[{question:"Which index structure is optimized for write-heavy workloads?",options:["B-tree","LSM-tree","Hash index","Bitmap index"],correctIndex:1,explanation:"LSM-trees batch writes to memory then flush  great for writes."}]},{id:"caching",title:"Caching",category:"building-blocks",description:"Speed up reads with in-memory data stores.",content:`**Levels:** Browser  CDN  API Gateway  Application  Database query cache.

**Strategies:**
- **Cache-Aside**  app checks cache, falls back to DB, populates cache
- **Read-Through**  cache loads from DB automatically
- **Write-Through**  write to cache + DB simultaneously
- **Write-Behind**  write to cache, async flush to DB

**Eviction:** LRU, LFU, TTL.

**Tools:** Redis, Memcached.`,tips:["Always discuss cache invalidation  it's the hard part.","Mention thundering herd: use locks or request coalescing."],relatedIds:["latency-throughput","databases","cdn"],quizQuestions:[{question:"In Cache-Aside, who is responsible for populating the cache?",options:["The cache itself","The database","The application","The CDN"],correctIndex:2,explanation:"In cache-aside, the application checks the cache and populates it on a miss."}]},{id:"message-queues",title:"Message Queues",category:"building-blocks",description:"Async communication between services.",content:`**Queue**  point-to-point, one consumer gets each message (SQS, RabbitMQ).
**Pub/Sub**  one message, many subscribers (Kafka, SNS).
**Event Streaming**  ordered, durable log (Kafka, Pulsar).

**Benefits:**
- Decouples producers & consumers
- Handles traffic spikes (buffering)
- Enables retry & dead-letter queues

**Guarantees:** At-most-once, at-least-once, exactly-once (hard!).`,tips:["Kafka for high-throughput ordered events; SQS for simple task queues.","Idempotent consumers handle at-least-once delivery safely."],relatedIds:["event-driven","microservices"],quizQuestions:[{question:"Which delivery guarantee is hardest to achieve?",options:["At-most-once","At-least-once","Exactly-once","Best-effort"],correctIndex:2,explanation:"Exactly-once requires coordination between producer, broker, and consumer."}]},{id:"cdn",title:"CDN",category:"building-blocks",description:"Content Delivery Networks  serve content from the edge.",content:`**Push CDN**  you upload content proactively.
**Pull CDN**  CDN fetches on first request, caches.

**Benefits:** Lower latency, reduced origin load, DDoS mitigation.

Edge locations worldwide. Cache static assets (images, JS, CSS) and even dynamic content (edge compute).

Examples: CloudFront, Cloudflare, Akamai, Fastly.`,tips:["Use pull CDN for most web apps; push for large static catalogs.","Mention cache invalidation strategies (versioned URLs, purge APIs)."],relatedIds:["caching","latency-throughput","load-balancers"],quizQuestions:[{question:"Which CDN type fetches content on the first request?",options:["Push CDN","Pull CDN","Hybrid CDN","Edge CDN"],correctIndex:1,explanation:"Pull CDNs lazily fetch and cache content on the first request."}]},{id:"consistent-hashing",title:"Consistent Hashing",category:"building-blocks",description:"Distribute data with minimal redistribution when nodes change.",content:`**Problem:** Simple hash(key) % N breaks when N changes  massive redistribution.

**Solution:** Hash ring. Nodes and keys mapped to a circle. Key goes to the next node clockwise.

**Virtual nodes**  each physical node gets multiple positions on the ring for better balance.

When a node joins/leaves, only K/N keys move (K=total keys, N=nodes).

Used by: DynamoDB, Cassandra, Memcached, Akamai.`,tips:["Always mention virtual nodes  without them, distribution is uneven.",'Great answer for "how would you partition this data?"'],relatedIds:["databases","scalability","caching"],quizQuestions:[{question:"What problem do virtual nodes solve?",options:["Network latency","Uneven data distribution","Cache invalidation","Leader election"],correctIndex:1,explanation:"Virtual nodes spread each physical node across the ring for balanced distribution."}]},{id:"microservices",title:"Microservices",category:"patterns",description:"Decompose into independently deployable services.",content:`**Monolith  Microservices:**
- Each service owns its data and logic
- Communicates via APIs or events
- Independently deployable and scalable

**Challenges:**
- Distributed transactions (Saga pattern)
- Service discovery
- Network latency & partial failures
- Data consistency across services

**When to use:** Large teams, different scaling needs per component, polyglot tech stacks.`,tips:["Start with a monolith, extract services when complexity demands it.","Mention the Saga pattern for cross-service transactions."],relatedIds:["message-queues","event-driven","api-gateway"],quizQuestions:[{question:"What pattern handles distributed transactions across microservices?",options:["Two-Phase Commit","Saga","CQRS","Circuit Breaker"],correctIndex:1,explanation:"Saga uses a sequence of local transactions with compensating actions."}]},{id:"event-driven",title:"Event-Driven Architecture",category:"patterns",description:"React to events instead of direct calls.",content:`**Components:**
- **Event Producer**  emits events (user clicked, order placed)
- **Event Broker**  routes events (Kafka, EventBridge)
- **Event Consumer**  reacts to events

**Patterns:**
- Event Notification  lightweight signal
- Event-Carried State Transfer  event contains full data
- Event Sourcing  store all events as the source of truth
- CQRS  separate read and write models

**Benefits:** Loose coupling, scalability, audit trail.`,tips:["Event sourcing + CQRS is powerful but complex  justify the complexity.","Idempotency is crucial in event-driven systems."],relatedIds:["message-queues","microservices","cqrs"],quizQuestions:[{question:"In Event Sourcing, what is the source of truth?",options:["Current state in DB","The event log","A snapshot","The cache"],correctIndex:1,explanation:"Event sourcing stores all state changes as an immutable event log."}]},{id:"cqrs",title:"CQRS",category:"patterns",description:"Separate read and write models for different optimization.",content:`**Command Query Responsibility Segregation:**
- **Write side**  validates and processes commands, stores events
- **Read side**  optimized projections/views for queries

The read model is eventually consistent with the write model.

**When useful:**
- Read and write workloads have very different patterns
- Need different data models for reading vs writing
- High read-to-write ratio

Often paired with Event Sourcing.`,tips:["CQRS adds complexity  only use when read/write patterns truly differ.","The read model can use a completely different database (e.g., Elasticsearch)."],relatedIds:["event-driven","databases"],quizQuestions:[{question:"What is the main benefit of CQRS?",options:["Stronger consistency","Independent optimization of reads and writes","Simpler codebase","Reduced storage"],correctIndex:1,explanation:"CQRS lets you optimize the read and write paths independently."}]},{id:"api-gateway",title:"API Gateway",category:"patterns",description:"Single entry point for all client requests.",content:`**Responsibilities:**
- Request routing to backend services
- Authentication & authorization
- Rate limiting & throttling
- Response aggregation (BFF pattern)
- SSL termination
- Request/response transformation
- Caching

**Examples:** Kong, AWS API Gateway, Zuul, Envoy.

**BFF (Backend for Frontend):** Separate gateways per client type (web, mobile, IoT).`,tips:["API Gateway can become a single point of failure  discuss redundancy.","BFF pattern is great when mobile and web need different response shapes."],relatedIds:["microservices","load-balancers","rate-limiting"],quizQuestions:[{question:"What does BFF stand for in the API Gateway context?",options:["Best Friend Forever","Backend for Frontend","Binary Format Framework","Buffered File Fetcher"],correctIndex:1,explanation:"BFF = Backend for Frontend  a gateway tailored to each client type."}]},{id:"rate-limiting",title:"Rate Limiting",category:"patterns",description:"Protect services from being overwhelmed.",content:`**Algorithms:**
- **Token Bucket**  tokens added at fixed rate; request costs a token
- **Leaky Bucket**  requests queue and drain at fixed rate
- **Fixed Window**  count requests per time window
- **Sliding Window Log**  track timestamps of recent requests
- **Sliding Window Counter**  hybrid of fixed window + log

**Where:** API Gateway, per-service, per-user, per-IP.

**Response:** HTTP 429 Too Many Requests + Retry-After header.`,tips:["Token bucket is the most commonly used  know it well.","Distributed rate limiting needs a shared store (Redis)."],relatedIds:["api-gateway","caching"],quizQuestions:[{question:"Which algorithm adds tokens at a fixed rate?",options:["Leaky Bucket","Fixed Window","Token Bucket","Sliding Log"],correctIndex:2,explanation:"Token Bucket refills tokens at a steady rate."}]},{id:"consistency-patterns",title:"Consistency Patterns",category:"patterns",description:"Strong, eventual, and causal consistency models.",content:`**Strong Consistency**  reads always return the latest write. Requires coordination (slower).

**Eventual Consistency**  reads may return stale data, but the system converges. Faster, more available.

**Causal Consistency**  operations that are causally related are seen in order. A sweet middle ground.

**Read-your-writes**  a user always sees their own updates.

**Monotonic reads**  you never see older data after seeing newer data.

Choose based on user expectations and business requirements.`,tips:["Most web apps need read-your-writes, not full strong consistency.",'Eventual consistency is fine for many use cases  quantify "how eventual".'],relatedIds:["cap-theorem","databases","cqrs"],quizQuestions:[{question:"Which consistency model guarantees you always see your own writes?",options:["Strong","Eventual","Read-your-writes","Causal"],correctIndex:2,explanation:"Read-your-writes ensures a user sees their own updates immediately."}]},{id:"design-url-shortener",title:"Design URL Shortener",category:"problems",description:"TinyURL / Bit.ly  generate short links and redirect.",content:`**Requirements:**
- Shorten a URL  7-char code
- Redirect short URL  original
- Analytics (click count)
- High read throughput (100:1 read:write)

**Key Decisions:**
- **ID Generation:** Base62 encoding of auto-increment ID, or MD5/SHA hash truncated
- **Storage:** Key-value store (DynamoDB) or SQL with index on short_code
- **Caching:** Redis for hot URLs (80/20 rule)
- **Scaling:** Stateless service + DB sharding by hash

**Back-of-envelope:** 100M URLs/month = ~40 writes/sec, 4000 reads/sec.`,tips:["Discuss collision handling if using hash truncation.","Mention custom aliases and expiration as features."],relatedIds:["databases","caching","consistent-hashing"],quizQuestions:[{question:"What encoding gives the shortest URL codes?",options:["Base16","Base36","Base62","Base64"],correctIndex:2,explanation:"Base62 (a-z, A-Z, 0-9) maximizes info per character without special chars."}]},{id:"design-chat-system",title:"Design Chat System",category:"problems",description:"WhatsApp / Slack  real-time messaging at scale.",content:`**Requirements:**
- 1:1 and group messaging
- Online/offline status
- Message history & search
- Push notifications
- Read receipts

**Architecture:**
- **WebSocket servers** for real-time delivery
- **Message queue** (Kafka) for reliable delivery
- **Chat storage:** Cassandra (write-heavy, time-series)
- **User presence:** Redis with TTL heartbeats
- **Push:** FCM/APNs for offline users

**Key challenge:** Ordering in group chats (vector clocks or server-assigned timestamps).`,tips:["Start with 1:1, then extend to groups.","Discuss how to handle the user being on multiple devices."],relatedIds:["message-queues","databases","networking-basics"],quizQuestions:[{question:"Which protocol is best for real-time chat?",options:["HTTP polling","Long polling","WebSockets","SMTP"],correctIndex:2,explanation:"WebSockets provide full-duplex persistent connections  ideal for chat."}]},{id:"design-newsfeed",title:"Design News Feed",category:"problems",description:"Facebook / Twitter feed  fanout and ranking.",content:`**Two approaches:**

**Fan-out on Write (Push):**
- When user posts, push to all followers' feeds
- Fast reads, slow writes
- Problem: celebrities with millions of followers

**Fan-out on Read (Pull):**
- Build feed at read time by querying followed users
- Slow reads, fast writes

**Hybrid:** Push for normal users, pull for celebrities.

**Ranking:** ML model scores posts by relevance, recency, engagement.

**Storage:** Feed cache in Redis (sorted set by timestamp/score).`,tips:["The hybrid approach is the expected answer.","Discuss the celebrity problem explicitly  shows depth."],relatedIds:["caching","message-queues","databases"],quizQuestions:[{question:'Which fanout model has the "celebrity problem"?',options:["Fan-out on Read","Fan-out on Write","Both equally","Neither"],correctIndex:1,explanation:"Fan-out on Write must push to millions of followers for celebrity posts."}]},{id:"design-rate-limiter",title:"Design Rate Limiter",category:"problems",description:"Build a distributed rate limiter service.",content:`**Requirements:**
- Limit requests per user/IP/API key
- Distributed (multiple servers)
- Low latency overhead
- Configurable rules

**Architecture:**
- **Rules engine**  load rules from config DB
- **Counter store**  Redis (INCR + EXPIRE, or sorted sets for sliding window)
- **Middleware**  intercepts requests before hitting the service

**Algorithm choice:** Token Bucket (flexible, allows bursts) or Sliding Window Counter (smoother).

**Race conditions:** Use Redis Lua scripts for atomic check-and-increment.`,tips:["Discuss where to place the limiter (client, server, middleware, API gateway).","Mention race conditions in distributed counters  Lua scripts solve it."],relatedIds:["rate-limiting","caching","api-gateway"],quizQuestions:[{question:"How to avoid race conditions in distributed rate limiting with Redis?",options:["Optimistic locking","Lua scripts","Mutex locks","Two-phase commit"],correctIndex:1,explanation:"Redis Lua scripts execute atomically  perfect for check-and-increment."}]}];xa.forEach(v=>{Sh[v.id]&&(v.quizQuestions=Sh[v.id])});const gm=[{from:"cap-theorem",to:"databases"},{from:"cap-theorem",to:"consistency-patterns"},{from:"acid-base",to:"databases"},{from:"networking-basics",to:"load-balancers"},{from:"networking-basics",to:"cdn"},{from:"scalability",to:"load-balancers"},{from:"scalability",to:"consistent-hashing"},{from:"latency-throughput",to:"caching"},{from:"latency-throughput",to:"cdn"},{from:"load-balancers",to:"api-gateway"},{from:"databases",to:"cqrs"},{from:"databases",to:"consistency-patterns"},{from:"caching",to:"rate-limiting"},{from:"message-queues",to:"event-driven"},{from:"message-queues",to:"microservices"},{from:"microservices",to:"design-chat-system"},{from:"event-driven",to:"design-newsfeed"},{from:"cqrs",to:"design-newsfeed"},{from:"rate-limiting",to:"design-rate-limiter"},{from:"consistency-patterns",to:"design-url-shortener"},{from:"api-gateway",to:"design-rate-limiter"},{from:"databases",to:"caching"},{from:"consistent-hashing",to:"databases"},{from:"caching",to:"design-url-shortener"},{from:"databases",to:"design-chat-system"},{from:"message-queues",to:"design-chat-system"},{from:"caching",to:"design-newsfeed"}],pl="sd-prep-progress";function ym(){try{const v=localStorage.getItem(pl);return v?JSON.parse(v):{}}catch{return{}}}function bm(v){localStorage.setItem(pl,JSON.stringify(v))}function vm(){const[v,H]=be.useState(ym);be.useEffect(()=>{bm(v)},[v]);const M=be.useCallback(P=>v[P]??"not-started",[v]),h=be.useCallback((P,K)=>{H(L=>({...L,[P]:K}))},[]),C=be.useCallback(()=>{H({}),localStorage.removeItem(pl)},[]),E=be.useCallback(()=>{const P={locked:0,"not-started":0,"in-progress":0,completed:0},K={fundamentals:{total:0,completed:0},"building-blocks":{total:0,completed:0},patterns:{total:0,completed:0},problems:{total:0,completed:0}};for(const L of xa){const y=v[L.id]??"not-started";P[y]++,K[L.category].total++,y==="completed"&&K[L.category].completed++}return{total:xa.length,byStatus:P,byCategory:K}},[v]);return{progress:v,getStatus:M,setStatus:h,resetAll:C,getStats:E}}const wm={fundamentals:{border:"border-blue-500/30",glow:"hover:shadow-blue-500/10",dot:"bg-blue-500"},"building-blocks":{border:"border-purple-500/30",glow:"hover:shadow-purple-500/10",dot:"bg-purple-500"},patterns:{border:"border-emerald-500/30",glow:"hover:shadow-emerald-500/10",dot:"bg-emerald-500"},problems:{border:"border-orange-500/30",glow:"hover:shadow-orange-500/10",dot:"bg-orange-500"}},xm={locked:"bg-slate-600","not-started":"bg-slate-500","in-progress":"bg-amber-400",completed:"bg-emerald-400"},Tm={locked:"bg-slate-800/40 opacity-50","not-started":"bg-slate-800","in-progress":"bg-slate-800",completed:"bg-slate-800"};function km({node:v,status:H,onClick:M}){const h=wm[v.category],C=H==="locked";return A.jsxs("button",{onClick:C?void 0:M,disabled:C,"data-node-id":v.id,className:`
        relative group flex items-center gap-2.5 px-4 py-3
        rounded-xl border transition-all duration-200 ease-out
        ${Tm[H]} ${h.border}
        ${C?"cursor-not-allowed":`cursor-pointer hover:scale-[1.03] hover:shadow-lg ${h.glow} hover:border-opacity-60`}
        min-w-[140px] max-w-[200px]
      `,children:[A.jsx("span",{className:`w-2.5 h-2.5 rounded-full shrink-0 ${xm[H]} ${H==="in-progress"?"animate-pulse":""}`}),A.jsx("span",{className:`text-sm font-medium text-left leading-tight ${C?"text-slate-600":"text-slate-200"}`,children:v.title}),H==="completed"&&A.jsx("svg",{className:"w-4 h-4 text-emerald-400 shrink-0 ml-auto",fill:"currentColor",viewBox:"0 0 20 20",children:A.jsx("path",{fillRule:"evenodd",d:"M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z",clipRule:"evenodd"})})]})}function Am({questions:v}){const[H,M]=be.useState(0),[h,C]=be.useState(null),[E,P]=be.useState(0),[K,L]=be.useState(!1);if(v.length===0)return null;const y=v[H],_=h!==null,I=we=>{_||(C(we),we===y.correctIndex&&P(re=>re+1))},ie=()=>{H+1>=v.length?L(!0):(M(we=>we+1),C(null))},Pe=()=>{M(0),C(null),P(0),L(!1)};return K?A.jsx("div",{className:"mt-4 p-4 bg-slate-800/60 rounded-xl border border-slate-700/50",children:A.jsxs("div",{className:"text-center",children:[A.jsxs("div",{className:"text-2xl font-bold text-emerald-400 mb-1",children:[E,"/",v.length]}),A.jsx("div",{className:"text-sm text-slate-400 mb-3",children:E===v.length?" Perfect!":E>=v.length/2?" Good job!":" Keep studying!"}),A.jsx("button",{onClick:Pe,className:"px-4 py-2 bg-slate-700 hover:bg-slate-600 text-sm text-slate-200 rounded-lg transition-colors",children:"Try Again"})]})}):A.jsxs("div",{className:"mt-4 p-4 bg-slate-800/60 rounded-xl border border-slate-700/50",children:[A.jsxs("div",{className:"flex items-center justify-between mb-3",children:[A.jsx("span",{className:"text-xs font-medium text-slate-500 uppercase tracking-wider",children:"Quiz"}),A.jsxs("span",{className:"text-xs text-slate-500",children:[H+1,"/",v.length]})]}),A.jsx("p",{className:"text-sm text-slate-200 font-medium mb-3",children:y.question}),A.jsx("div",{className:"space-y-2",children:y.options.map((we,re)=>{let De="bg-slate-700/50 border-slate-600/50 text-slate-300 hover:bg-slate-700";return _&&(re===y.correctIndex?De="bg-emerald-500/20 border-emerald-500/50 text-emerald-300":re===h?De="bg-red-500/20 border-red-500/50 text-red-300":De="bg-slate-700/30 border-slate-700/30 text-slate-500"),A.jsx("button",{onClick:()=>I(re),className:`w-full text-left px-3 py-2 rounded-lg border text-sm transition-all ${De}`,children:we},re)})}),_&&A.jsxs(A.Fragment,{children:[A.jsx("p",{className:"mt-3 text-xs text-slate-400 leading-relaxed",children:y.explanation}),A.jsx("button",{onClick:ie,className:"mt-3 px-4 py-2 bg-blue-600 hover:bg-blue-500 text-sm text-white rounded-lg transition-colors",children:H+1>=v.length?"See Results":"Next Question"})]})]})}const qh={fundamentals:{label:"Fundamentals",bg:"bg-blue-500/20 text-blue-400"},"building-blocks":{label:"Building Blocks",bg:"bg-purple-500/20 text-purple-400"},patterns:{label:"Patterns",bg:"bg-emerald-500/20 text-emerald-400"},problems:{label:"Problems",bg:"bg-orange-500/20 text-orange-400"}},Sm=[{value:"not-started",label:"Not Started",active:"bg-slate-600 text-slate-200"},{value:"in-progress",label:"In Progress",active:"bg-amber-500/20 text-amber-300 ring-1 ring-amber-500/40"},{value:"completed",label:"Completed",active:"bg-emerald-500/20 text-emerald-300 ring-1 ring-emerald-500/40"}];function qm(v){return v.split(`
`).map((H,M)=>{if(!H.trim())return A.jsx("br",{},M);const h=H.split(/(\*\*[^*]+\*\*)/g).map((C,E)=>C.startsWith("**")&&C.endsWith("**")?A.jsx("strong",{className:"text-slate-200",children:C.slice(2,-2)},E):C);return H.startsWith("- ")?A.jsx("li",{className:"ml-4 text-sm text-slate-400 leading-relaxed",children:h.slice(0)},M):A.jsx("p",{className:"text-sm text-slate-400 leading-relaxed",children:h},M)})}function Lm({node:v,status:H,onStatusChange:M,onClose:h,onNavigate:C,allNodes:E}){const P=be.useRef(null),K=v!==null;be.useEffect(()=>{const y=_=>{_.key==="Escape"&&h()};return K&&document.addEventListener("keydown",y),()=>document.removeEventListener("keydown",y)},[K,h]);const L=y=>{P.current&&!P.current.contains(y.target)&&h()};return A.jsxs(A.Fragment,{children:[A.jsx("div",{className:`fixed inset-0 bg-black/50 backdrop-blur-sm z-40 transition-opacity duration-200 ${K?"opacity-100":"opacity-0 pointer-events-none"}`,onClick:L}),A.jsx("div",{ref:P,className:`
          fixed z-50 bg-slate-900 border-l border-slate-700/50 shadow-2xl
          transition-transform duration-300 ease-out overflow-y-auto
          top-0 right-0 h-full
          w-full sm:w-[480px] md:w-[520px]
          ${K?"translate-x-0":"translate-x-full"}
        `,children:v&&A.jsxs("div",{className:"p-6 space-y-5",children:[A.jsxs("div",{className:"flex items-start justify-between gap-3",children:[A.jsxs("div",{className:"space-y-2",children:[A.jsx("span",{className:`inline-block px-2.5 py-1 rounded-md text-xs font-medium ${qh[v.category].bg}`,children:qh[v.category].label}),A.jsx("h2",{className:"text-xl font-bold text-slate-100",children:v.title}),A.jsx("p",{className:"text-sm text-slate-500",children:v.description})]}),A.jsx("button",{onClick:h,className:"p-2 rounded-lg hover:bg-slate-800 text-slate-500 hover:text-slate-300 transition-colors shrink-0",children:A.jsx("svg",{className:"w-5 h-5",fill:"none",stroke:"currentColor",viewBox:"0 0 24 24",children:A.jsx("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M6 18L18 6M6 6l12 12"})})})]}),A.jsx("div",{className:"flex gap-2",children:Sm.map(y=>A.jsx("button",{onClick:()=>M(y.value),className:`px-3 py-1.5 rounded-lg text-xs font-medium transition-all ${H===y.value?y.active:"bg-slate-800 text-slate-500 hover:text-slate-400"}`,children:y.label},y.value))}),A.jsx("div",{className:"space-y-1.5",children:qm(v.content)}),v.tips.length>0&&A.jsxs("div",{className:"space-y-2",children:[A.jsx("h3",{className:"text-xs font-semibold text-slate-500 uppercase tracking-wider",children:" Tips"}),v.tips.map((y,_)=>A.jsxs("div",{className:"flex gap-2 p-3 bg-amber-500/5 border border-amber-500/10 rounded-lg",children:[A.jsx("span",{className:"text-amber-400 shrink-0 text-sm",children:""}),A.jsx("span",{className:"text-sm text-amber-200/80 leading-relaxed",children:y})]},_))]}),v.relatedIds.length>0&&A.jsxs("div",{children:[A.jsx("h3",{className:"text-xs font-semibold text-slate-500 uppercase tracking-wider mb-2",children:"Related Topics"}),A.jsx("div",{className:"flex flex-wrap gap-2",children:v.relatedIds.map(y=>{const _=E.find(I=>I.id===y);return _?A.jsx("button",{onClick:()=>C(y),className:"px-3 py-1.5 bg-slate-800 hover:bg-slate-700 text-xs text-slate-300 rounded-lg border border-slate-700/50 transition-colors",children:_.title},y):null})})]}),v.quizQuestions.length>0&&A.jsx(Am,{questions:v.quizQuestions})]})})]})}const Rm={fundamentals:{label:"Fundamentals",color:"text-blue-400",bg:"bg-blue-500"},"building-blocks":{label:"Building Blocks",color:"text-purple-400",bg:"bg-purple-500"},patterns:{label:"Patterns",color:"text-emerald-400",bg:"bg-emerald-500"},problems:{label:"Problems",color:"text-orange-400",bg:"bg-orange-500"}};function Cm({stats:v}){const[H,M]=be.useState(!1),h=v.byStatus.completed,C=v.total>0?h/v.total*100:0;return A.jsxs("div",{className:"w-full",children:[A.jsxs("button",{onClick:()=>M(E=>!E),className:"w-full text-left group",children:[A.jsxs("div",{className:"flex items-center justify-between mb-1.5",children:[A.jsxs("span",{className:"text-sm font-medium text-slate-300",children:[h," of ",v.total," completed"]}),A.jsx("span",{className:"text-xs text-slate-500 group-hover:text-slate-400 transition-colors",children:H?"Hide":"Details"})]}),A.jsx("div",{className:"h-2.5 bg-slate-700 rounded-full overflow-hidden",children:A.jsx("div",{className:"h-full bg-gradient-to-r from-emerald-500 to-emerald-400 rounded-full transition-all duration-500 ease-out",style:{width:`${C}%`}})})]}),H&&A.jsx("div",{className:"mt-3 grid grid-cols-2 sm:grid-cols-4 gap-2",children:Object.entries(Rm).map(([E,P])=>{const K=v.byCategory[E],L=K.total>0?K.completed/K.total*100:0;return A.jsxs("div",{className:"bg-slate-800/60 rounded-lg p-2.5 border border-slate-700/50",children:[A.jsx("div",{className:`text-xs font-medium ${P.color} mb-1`,children:P.label}),A.jsxs("div",{className:"text-xs text-slate-400 mb-1.5",children:[K.completed,"/",K.total]}),A.jsx("div",{className:"h-1.5 bg-slate-700 rounded-full overflow-hidden",children:A.jsx("div",{className:`h-full ${P.bg} rounded-full transition-all duration-500`,style:{width:`${L}%`}})})]},E)})})]})}const Um=[{value:"all",label:"All",color:"bg-slate-600"},{value:"fundamentals",label:"Fundamentals",color:"bg-blue-500"},{value:"building-blocks",label:"Building Blocks",color:"bg-purple-500"},{value:"patterns",label:"Patterns",color:"bg-emerald-500"},{value:"problems",label:"Problems",color:"bg-orange-500"}],Dm=[{value:"all",label:"All"},{value:"not-started",label:"Not Started"},{value:"in-progress",label:"In Progress"},{value:"completed",label:"Completed"}];function Im({search:v,onSearchChange:H,categoryFilter:M,onCategoryChange:h,statusFilter:C,onStatusChange:E}){return A.jsxs("div",{className:"space-y-3",children:[A.jsxs("div",{className:"relative",children:[A.jsx("svg",{className:"absolute left-3 top-1/2 -translate-y-1/2 w-4 h-4 text-slate-500",fill:"none",stroke:"currentColor",viewBox:"0 0 24 24",children:A.jsx("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"})}),A.jsx("input",{type:"text",value:v,onChange:P=>H(P.target.value),placeholder:"Search topics...",className:"w-full pl-10 pr-4 py-2.5 bg-slate-800 border border-slate-700 rounded-xl text-sm text-slate-200 placeholder-slate-500 focus:outline-none focus:ring-2 focus:ring-blue-500/40 focus:border-blue-500/40 transition-all"})]}),A.jsxs("div",{className:"flex flex-wrap gap-2",children:[A.jsx("div",{className:"flex flex-wrap gap-1.5",children:Um.map(P=>A.jsx("button",{onClick:()=>h(P.value),className:`px-3 py-1.5 rounded-lg text-xs font-medium transition-all ${M===P.value?`${P.color} text-white shadow-lg`:"bg-slate-800 text-slate-400 hover:bg-slate-700 hover:text-slate-300"}`,children:P.label},P.value))}),A.jsx("div",{className:"w-px bg-slate-700 mx-1 hidden sm:block"}),A.jsx("div",{className:"flex flex-wrap gap-1.5",children:Dm.map(P=>A.jsx("button",{onClick:()=>E(P.value),className:`px-3 py-1.5 rounded-lg text-xs font-medium transition-all ${C===P.value?"bg-slate-600 text-white":"bg-slate-800 text-slate-400 hover:bg-slate-700 hover:text-slate-300"}`,children:P.label},P.value))})]})]})}const zm=[{category:"fundamentals",label:"Fundamentals",color:"text-blue-400",accent:"border-blue-500/30"},{category:"building-blocks",label:"Building Blocks",color:"text-purple-400",accent:"border-purple-500/30"},{category:"patterns",label:"Patterns",color:"text-emerald-400",accent:"border-emerald-500/30"},{category:"problems",label:"Problems",color:"text-orange-400",accent:"border-orange-500/30"}];function Pm(){const{getStatus:v,setStatus:H,getStats:M}=vm(),[h,C]=be.useState(null),[E,P]=be.useState(""),[K,L]=be.useState("all"),[y,_]=be.useState("all"),I=be.useRef(null),[ie,Pe]=be.useState({}),we=M(),re=be.useMemo(()=>new Set(xa.filter(B=>!(E&&!B.title.toLowerCase().includes(E.toLowerCase())||K!=="all"&&B.category!==K||y!=="all"&&v(B.id)!==y)).map(B=>B.id)),[E,K,y,v]),De=be.useMemo(()=>{const B={fundamentals:[],"building-blocks":[],patterns:[],problems:[]};for(const pe of xa)B[pe.category].push(pe);return B},[]);be.useEffect(()=>{const B=()=>{if(!I.current)return;const le=I.current.getBoundingClientRect(),Ee={};for(const Y of xa){const Ne=I.current.querySelector(`[data-node-id="${Y.id}"]`);if(Ne){const Qe=Ne.getBoundingClientRect();Ee[Y.id]={x:Qe.left-le.left+Qe.width/2,y:Qe.top-le.top+Qe.height/2}}}Pe(Ee)},pe=setTimeout(B,100);return window.addEventListener("resize",B),()=>{clearTimeout(pe),window.removeEventListener("resize",B)}},[re]);const je=be.useCallback(B=>{const pe=xa.find(le=>le.id===B);pe&&C(pe)},[]),St=be.useMemo(()=>gm.filter(B=>re.has(B.from)&&re.has(B.to)),[re]);return A.jsxs("div",{className:"min-h-screen bg-slate-900",children:[A.jsxs("div",{className:"max-w-7xl mx-auto px-4 sm:px-6 py-6 space-y-6",children:[A.jsxs("div",{className:"space-y-1",children:[A.jsx("h1",{className:"text-2xl sm:text-3xl font-bold text-slate-100",children:"System Design Roadmap"}),A.jsx("p",{className:"text-sm text-slate-500",children:"Master system design concepts, patterns, and problems"})]}),A.jsx(Cm,{stats:we}),A.jsx(Im,{search:E,onSearchChange:P,categoryFilter:K,onCategoryChange:L,statusFilter:y,onStatusChange:_}),A.jsxs("div",{ref:I,className:"relative",children:[A.jsxs("svg",{className:"absolute inset-0 w-full h-full pointer-events-none z-0",style:{overflow:"visible"},children:[A.jsx("defs",{children:A.jsx("marker",{id:"arrowhead",markerWidth:"8",markerHeight:"6",refX:"8",refY:"3",orient:"auto",children:A.jsx("polygon",{points:"0 0, 8 3, 0 6",fill:"#475569"})})}),St.map(B=>{const pe=ie[B.from],le=ie[B.to];return!pe||!le?null:A.jsx("line",{x1:pe.x,y1:pe.y,x2:le.x,y2:le.y,stroke:"#334155",strokeWidth:1.5,strokeDasharray:"4 4",markerEnd:"url(#arrowhead)",opacity:.5},`${B.from}-${B.to}`)})]}),A.jsx("div",{className:"space-y-6 relative z-10",children:zm.map(B=>{const pe=De[B.category].filter(le=>re.has(le.id));return pe.length===0?null:A.jsxs("div",{className:`rounded-2xl border ${B.accent} bg-slate-800/20 p-4 sm:p-5`,children:[A.jsxs("div",{className:"flex items-center gap-3 mb-4",children:[A.jsx("div",{className:`w-1 h-6 rounded-full ${B.color.replace("text-","bg-")}`}),A.jsx("h2",{className:`text-sm font-semibold uppercase tracking-wider ${B.color}`,children:B.label}),A.jsxs("span",{className:"text-xs text-slate-600",children:[De[B.category].filter(le=>v(le.id)==="completed").length,"/",De[B.category].length]})]}),A.jsx("div",{className:"flex flex-wrap gap-3",children:pe.map(le=>A.jsx(km,{node:le,status:v(le.id),onClick:()=>C(le)},le.id))})]},B.category)})})]})]}),A.jsx(Lm,{node:h,status:h?v(h.id):"not-started",onStatusChange:B=>{h&&H(h.id,B)},onClose:()=>C(null),onNavigate:je,allNodes:xa})]})}function Em(){return A.jsx("div",{className:"min-h-screen bg-slate-900 text-slate-100",children:A.jsx(Pm,{})})}cm.createRoot(document.getElementById("root")).render(A.jsx(tm.StrictMode,{children:A.jsx(Em,{})}));
