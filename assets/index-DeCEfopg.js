(function(){const N=document.createElement("link").relList;if(N&&N.supports&&N.supports("modulepreload"))return;for(const L of document.querySelectorAll('link[rel="modulepreload"]'))h(L);new MutationObserver(L=>{for(const E of L)if(E.type==="childList")for(const P of E.addedNodes)P.tagName==="LINK"&&P.rel==="modulepreload"&&h(P)}).observe(document,{childList:!0,subtree:!0});function M(L){const E={};return L.integrity&&(E.integrity=L.integrity),L.referrerPolicy&&(E.referrerPolicy=L.referrerPolicy),L.crossOrigin==="use-credentials"?E.credentials="include":L.crossOrigin==="anonymous"?E.credentials="omit":E.credentials="same-origin",E}function h(L){if(L.ep)return;L.ep=!0;const E=M(L);fetch(L.href,E)}})();function Ch(v){return v&&v.__esModule&&Object.prototype.hasOwnProperty.call(v,"default")?v.default:v}var ol={exports:{}},xi={};/**
 * @license React
 * react-jsx-runtime.production.js
 *
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var fh;function Fm(){if(fh)return xi;fh=1;var v=Symbol.for("react.transitional.element"),N=Symbol.for("react.fragment");function M(h,L,E){var P=null;if(E!==void 0&&(P=""+E),L.key!==void 0&&(P=""+L.key),"key"in L){E={};for(var K in L)K!=="key"&&(E[K]=L[K])}else E=L;return L=E.ref,{$$typeof:v,type:h,key:P,ref:L!==void 0?L:null,props:E}}return xi.Fragment=N,xi.jsx=M,xi.jsxs=M,xi}var gh;function Jm(){return gh||(gh=1,ol.exports=Fm()),ol.exports}var S=Jm(),rl={exports:{}},O={};/**
 * @license React
 * react.production.js
 *
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var yh;function $m(){if(yh)return O;yh=1;var v=Symbol.for("react.transitional.element"),N=Symbol.for("react.portal"),M=Symbol.for("react.fragment"),h=Symbol.for("react.strict_mode"),L=Symbol.for("react.profiler"),E=Symbol.for("react.consumer"),P=Symbol.for("react.context"),K=Symbol.for("react.forward_ref"),C=Symbol.for("react.suspense"),y=Symbol.for("react.memo"),Q=Symbol.for("react.lazy"),U=Symbol.for("react.activity"),ie=Symbol.iterator;function Pe(d){return d===null||typeof d!="object"?null:(d=ie&&d[ie]||d["@@iterator"],typeof d=="function"?d:null)}var we={isMounted:function(){return!1},enqueueForceUpdate:function(){},enqueueReplaceState:function(){},enqueueSetState:function(){}},re=Object.assign,De={};function je(d,T,q){this.props=d,this.context=T,this.refs=De,this.updater=q||we}je.prototype.isReactComponent={},je.prototype.setState=function(d,T){if(typeof d!="object"&&typeof d!="function"&&d!=null)throw Error("takes an object of state variables to update or a function which returns an object of state variables.");this.updater.enqueueSetState(this,d,T,"setState")},je.prototype.forceUpdate=function(d){this.updater.enqueueForceUpdate(this,d,"forceUpdate")};function At(){}At.prototype=je.prototype;function B(d,T,q){this.props=d,this.context=T,this.refs=De,this.updater=q||we}var pe=B.prototype=new At;pe.constructor=B,re(pe,je.prototype),pe.isPureReactComponent=!0;var le=Array.isArray;function Ee(){}var Y={H:null,A:null,T:null,S:null},We=Object.prototype.hasOwnProperty;function _e(d,T,q){var I=q.ref;return{$$typeof:v,type:d,key:T,ref:I!==void 0?I:null,props:q}}function Qa(d,T){return _e(d.type,T,d.props)}function qt(d){return typeof d=="object"&&d!==null&&d.$$typeof===v}function Ye(d){var T={"=":"=0",":":"=2"};return"$"+d.replace(/[=:]/g,function(q){return T[q]})}var ka=/\/+/g;function Dt(d,T){return typeof d=="object"&&d!==null&&d.key!=null?Ye(""+d.key):T.toString(36)}function xt(d){switch(d.status){case"fulfilled":return d.value;case"rejected":throw d.reason;default:switch(typeof d.status=="string"?d.then(Ee,Ee):(d.status="pending",d.then(function(T){d.status==="pending"&&(d.status="fulfilled",d.value=T)},function(T){d.status==="pending"&&(d.status="rejected",d.reason=T)})),d.status){case"fulfilled":return d.value;case"rejected":throw d.reason}}throw d}function w(d,T,q,I,j){var V=typeof d;(V==="undefined"||V==="boolean")&&(d=null);var ne=!1;if(d===null)ne=!0;else switch(V){case"bigint":case"string":case"number":ne=!0;break;case"object":switch(d.$$typeof){case v:case N:ne=!0;break;case Q:return ne=d._init,w(ne(d._payload),T,q,I,j)}}if(ne)return j=j(d),ne=I===""?"."+Dt(d,0):I,le(j)?(q="",ne!=null&&(q=ne.replace(ka,"$&/")+"/"),w(j,T,q,"",function(Rn){return Rn})):j!=null&&(qt(j)&&(j=Qa(j,q+(j.key==null||d&&d.key===j.key?"":(""+j.key).replace(ka,"$&/")+"/")+ne)),T.push(j)),1;ne=0;var Ge=I===""?".":I+":";if(le(d))for(var Te=0;Te<d.length;Te++)I=d[Te],V=Ge+Dt(I,Te),ne+=w(I,T,q,V,j);else if(Te=Pe(d),typeof Te=="function")for(d=Te.call(d),Te=0;!(I=d.next()).done;)I=I.value,V=Ge+Dt(I,Te++),ne+=w(I,T,q,V,j);else if(V==="object"){if(typeof d.then=="function")return w(xt(d),T,q,I,j);throw T=String(d),Error("Objects are not valid as a React child (found: "+(T==="[object Object]"?"object with keys {"+Object.keys(d).join(", ")+"}":T)+"). If you meant to render a collection of children, use an array instead.")}return ne}function A(d,T,q){if(d==null)return d;var I=[],j=0;return w(d,I,"","",function(V){return T.call(q,V,j++)}),I}function H(d){if(d._status===-1){var T=d._result;T=T(),T.then(function(q){(d._status===0||d._status===-1)&&(d._status=1,d._result=q)},function(q){(d._status===0||d._status===-1)&&(d._status=2,d._result=q)}),d._status===-1&&(d._status=0,d._result=T)}if(d._status===1)return d._result.default;throw d._result}var ce=typeof reportError=="function"?reportError:function(d){if(typeof window=="object"&&typeof window.ErrorEvent=="function"){var T=new window.ErrorEvent("error",{bubbles:!0,cancelable:!0,message:typeof d=="object"&&d!==null&&typeof d.message=="string"?String(d.message):String(d),error:d});if(!window.dispatchEvent(T))return}else if(typeof process=="object"&&typeof process.emit=="function"){process.emit("uncaughtException",d);return}console.error(d)},me={map:A,forEach:function(d,T,q){A(d,function(){T.apply(this,arguments)},q)},count:function(d){var T=0;return A(d,function(){T++}),T},toArray:function(d){return A(d,function(T){return T})||[]},only:function(d){if(!qt(d))throw Error("React.Children.only expected to receive a single React element child.");return d}};return O.Activity=U,O.Children=me,O.Component=je,O.Fragment=M,O.Profiler=L,O.PureComponent=B,O.StrictMode=h,O.Suspense=C,O.__CLIENT_INTERNALS_DO_NOT_USE_OR_WARN_USERS_THEY_CANNOT_UPGRADE=Y,O.__COMPILER_RUNTIME={__proto__:null,c:function(d){return Y.H.useMemoCache(d)}},O.cache=function(d){return function(){return d.apply(null,arguments)}},O.cacheSignal=function(){return null},O.cloneElement=function(d,T,q){if(d==null)throw Error("The argument must be a React element, but you passed "+d+".");var I=re({},d.props),j=d.key;if(T!=null)for(V in T.key!==void 0&&(j=""+T.key),T)!We.call(T,V)||V==="key"||V==="__self"||V==="__source"||V==="ref"&&T.ref===void 0||(I[V]=T[V]);var V=arguments.length-2;if(V===1)I.children=q;else if(1<V){for(var ne=Array(V),Ge=0;Ge<V;Ge++)ne[Ge]=arguments[Ge+2];I.children=ne}return _e(d.type,j,I)},O.createContext=function(d){return d={$$typeof:P,_currentValue:d,_currentValue2:d,_threadCount:0,Provider:null,Consumer:null},d.Provider=d,d.Consumer={$$typeof:E,_context:d},d},O.createElement=function(d,T,q){var I,j={},V=null;if(T!=null)for(I in T.key!==void 0&&(V=""+T.key),T)We.call(T,I)&&I!=="key"&&I!=="__self"&&I!=="__source"&&(j[I]=T[I]);var ne=arguments.length-2;if(ne===1)j.children=q;else if(1<ne){for(var Ge=Array(ne),Te=0;Te<ne;Te++)Ge[Te]=arguments[Te+2];j.children=Ge}if(d&&d.defaultProps)for(I in ne=d.defaultProps,ne)j[I]===void 0&&(j[I]=ne[I]);return _e(d,V,j)},O.createRef=function(){return{current:null}},O.forwardRef=function(d){return{$$typeof:K,render:d}},O.isValidElement=qt,O.lazy=function(d){return{$$typeof:Q,_payload:{_status:-1,_result:d},_init:H}},O.memo=function(d,T){return{$$typeof:y,type:d,compare:T===void 0?null:T}},O.startTransition=function(d){var T=Y.T,q={};Y.T=q;try{var I=d(),j=Y.S;j!==null&&j(q,I),typeof I=="object"&&I!==null&&typeof I.then=="function"&&I.then(Ee,ce)}catch(V){ce(V)}finally{T!==null&&q.types!==null&&(T.types=q.types),Y.T=T}},O.unstable_useCacheRefresh=function(){return Y.H.useCacheRefresh()},O.use=function(d){return Y.H.use(d)},O.useActionState=function(d,T,q){return Y.H.useActionState(d,T,q)},O.useCallback=function(d,T){return Y.H.useCallback(d,T)},O.useContext=function(d){return Y.H.useContext(d)},O.useDebugValue=function(){},O.useDeferredValue=function(d,T){return Y.H.useDeferredValue(d,T)},O.useEffect=function(d,T){return Y.H.useEffect(d,T)},O.useEffectEvent=function(d){return Y.H.useEffectEvent(d)},O.useId=function(){return Y.H.useId()},O.useImperativeHandle=function(d,T,q){return Y.H.useImperativeHandle(d,T,q)},O.useInsertionEffect=function(d,T){return Y.H.useInsertionEffect(d,T)},O.useLayoutEffect=function(d,T){return Y.H.useLayoutEffect(d,T)},O.useMemo=function(d,T){return Y.H.useMemo(d,T)},O.useOptimistic=function(d,T){return Y.H.useOptimistic(d,T)},O.useReducer=function(d,T,q){return Y.H.useReducer(d,T,q)},O.useRef=function(d){return Y.H.useRef(d)},O.useState=function(d){return Y.H.useState(d)},O.useSyncExternalStore=function(d,T,q){return Y.H.useSyncExternalStore(d,T,q)},O.useTransition=function(){return Y.H.useTransition()},O.version="19.2.4",O}var bh;function hl(){return bh||(bh=1,rl.exports=$m()),rl.exports}var be=hl();const ef=Ch(be);var ll={exports:{}},ki={},cl={exports:{}},dl={};/**
 * @license React
 * scheduler.production.js
 *
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var vh;function tf(){return vh||(vh=1,(function(v){function N(w,A){var H=w.length;w.push(A);e:for(;0<H;){var ce=H-1>>>1,me=w[ce];if(0<L(me,A))w[ce]=A,w[H]=me,H=ce;else break e}}function M(w){return w.length===0?null:w[0]}function h(w){if(w.length===0)return null;var A=w[0],H=w.pop();if(H!==A){w[0]=H;e:for(var ce=0,me=w.length,d=me>>>1;ce<d;){var T=2*(ce+1)-1,q=w[T],I=T+1,j=w[I];if(0>L(q,H))I<me&&0>L(j,q)?(w[ce]=j,w[I]=H,ce=I):(w[ce]=q,w[T]=H,ce=T);else if(I<me&&0>L(j,H))w[ce]=j,w[I]=H,ce=I;else break e}}return A}function L(w,A){var H=w.sortIndex-A.sortIndex;return H!==0?H:w.id-A.id}if(v.unstable_now=void 0,typeof performance=="object"&&typeof performance.now=="function"){var E=performance;v.unstable_now=function(){return E.now()}}else{var P=Date,K=P.now();v.unstable_now=function(){return P.now()-K}}var C=[],y=[],Q=1,U=null,ie=3,Pe=!1,we=!1,re=!1,De=!1,je=typeof setTimeout=="function"?setTimeout:null,At=typeof clearTimeout=="function"?clearTimeout:null,B=typeof setImmediate<"u"?setImmediate:null;function pe(w){for(var A=M(y);A!==null;){if(A.callback===null)h(y);else if(A.startTime<=w)h(y),A.sortIndex=A.expirationTime,N(C,A);else break;A=M(y)}}function le(w){if(re=!1,pe(w),!we)if(M(C)!==null)we=!0,Ee||(Ee=!0,Ye());else{var A=M(y);A!==null&&xt(le,A.startTime-w)}}var Ee=!1,Y=-1,We=5,_e=-1;function Qa(){return De?!0:!(v.unstable_now()-_e<We)}function qt(){if(De=!1,Ee){var w=v.unstable_now();_e=w;var A=!0;try{e:{we=!1,re&&(re=!1,At(Y),Y=-1),Pe=!0;var H=ie;try{t:{for(pe(w),U=M(C);U!==null&&!(U.expirationTime>w&&Qa());){var ce=U.callback;if(typeof ce=="function"){U.callback=null,ie=U.priorityLevel;var me=ce(U.expirationTime<=w);if(w=v.unstable_now(),typeof me=="function"){U.callback=me,pe(w),A=!0;break t}U===M(C)&&h(C),pe(w)}else h(C);U=M(C)}if(U!==null)A=!0;else{var d=M(y);d!==null&&xt(le,d.startTime-w),A=!1}}break e}finally{U=null,ie=H,Pe=!1}A=void 0}}finally{A?Ye():Ee=!1}}}var Ye;if(typeof B=="function")Ye=function(){B(qt)};else if(typeof MessageChannel<"u"){var ka=new MessageChannel,Dt=ka.port2;ka.port1.onmessage=qt,Ye=function(){Dt.postMessage(null)}}else Ye=function(){je(qt,0)};function xt(w,A){Y=je(function(){w(v.unstable_now())},A)}v.unstable_IdlePriority=5,v.unstable_ImmediatePriority=1,v.unstable_LowPriority=4,v.unstable_NormalPriority=3,v.unstable_Profiling=null,v.unstable_UserBlockingPriority=2,v.unstable_cancelCallback=function(w){w.callback=null},v.unstable_forceFrameRate=function(w){0>w||125<w?console.error("forceFrameRate takes a positive int between 0 and 125, forcing frame rates higher than 125 fps is not supported"):We=0<w?Math.floor(1e3/w):5},v.unstable_getCurrentPriorityLevel=function(){return ie},v.unstable_next=function(w){switch(ie){case 1:case 2:case 3:var A=3;break;default:A=ie}var H=ie;ie=A;try{return w()}finally{ie=H}},v.unstable_requestPaint=function(){De=!0},v.unstable_runWithPriority=function(w,A){switch(w){case 1:case 2:case 3:case 4:case 5:break;default:w=3}var H=ie;ie=w;try{return A()}finally{ie=H}},v.unstable_scheduleCallback=function(w,A,H){var ce=v.unstable_now();switch(typeof H=="object"&&H!==null?(H=H.delay,H=typeof H=="number"&&0<H?ce+H:ce):H=ce,w){case 1:var me=-1;break;case 2:me=250;break;case 5:me=1073741823;break;case 4:me=1e4;break;default:me=5e3}return me=H+me,w={id:Q++,callback:A,priorityLevel:w,startTime:H,expirationTime:me,sortIndex:-1},H>ce?(w.sortIndex=H,N(y,w),M(C)===null&&w===M(y)&&(re?(At(Y),Y=-1):re=!0,xt(le,H-ce))):(w.sortIndex=me,N(C,w),we||Pe||(we=!0,Ee||(Ee=!0,Ye()))),w},v.unstable_shouldYield=Qa,v.unstable_wrapCallback=function(w){var A=ie;return function(){var H=ie;ie=A;try{return w.apply(this,arguments)}finally{ie=H}}}})(dl)),dl}var wh;function af(){return wh||(wh=1,cl.exports=tf()),cl.exports}var ul={exports:{}},Qe={};/**
 * @license React
 * react-dom.production.js
 *
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var xh;function nf(){if(xh)return Qe;xh=1;var v=hl();function N(C){var y="https://react.dev/errors/"+C;if(1<arguments.length){y+="?args[]="+encodeURIComponent(arguments[1]);for(var Q=2;Q<arguments.length;Q++)y+="&args[]="+encodeURIComponent(arguments[Q])}return"Minified React error #"+C+"; visit "+y+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}function M(){}var h={d:{f:M,r:function(){throw Error(N(522))},D:M,C:M,L:M,m:M,X:M,S:M,M},p:0,findDOMNode:null},L=Symbol.for("react.portal");function E(C,y,Q){var U=3<arguments.length&&arguments[3]!==void 0?arguments[3]:null;return{$$typeof:L,key:U==null?null:""+U,children:C,containerInfo:y,implementation:Q}}var P=v.__CLIENT_INTERNALS_DO_NOT_USE_OR_WARN_USERS_THEY_CANNOT_UPGRADE;function K(C,y){if(C==="font")return"";if(typeof y=="string")return y==="use-credentials"?y:""}return Qe.__DOM_INTERNALS_DO_NOT_USE_OR_WARN_USERS_THEY_CANNOT_UPGRADE=h,Qe.createPortal=function(C,y){var Q=2<arguments.length&&arguments[2]!==void 0?arguments[2]:null;if(!y||y.nodeType!==1&&y.nodeType!==9&&y.nodeType!==11)throw Error(N(299));return E(C,y,null,Q)},Qe.flushSync=function(C){var y=P.T,Q=h.p;try{if(P.T=null,h.p=2,C)return C()}finally{P.T=y,h.p=Q,h.d.f()}},Qe.preconnect=function(C,y){typeof C=="string"&&(y?(y=y.crossOrigin,y=typeof y=="string"?y==="use-credentials"?y:"":void 0):y=null,h.d.C(C,y))},Qe.prefetchDNS=function(C){typeof C=="string"&&h.d.D(C)},Qe.preinit=function(C,y){if(typeof C=="string"&&y&&typeof y.as=="string"){var Q=y.as,U=K(Q,y.crossOrigin),ie=typeof y.integrity=="string"?y.integrity:void 0,Pe=typeof y.fetchPriority=="string"?y.fetchPriority:void 0;Q==="style"?h.d.S(C,typeof y.precedence=="string"?y.precedence:void 0,{crossOrigin:U,integrity:ie,fetchPriority:Pe}):Q==="script"&&h.d.X(C,{crossOrigin:U,integrity:ie,fetchPriority:Pe,nonce:typeof y.nonce=="string"?y.nonce:void 0})}},Qe.preinitModule=function(C,y){if(typeof C=="string")if(typeof y=="object"&&y!==null){if(y.as==null||y.as==="script"){var Q=K(y.as,y.crossOrigin);h.d.M(C,{crossOrigin:Q,integrity:typeof y.integrity=="string"?y.integrity:void 0,nonce:typeof y.nonce=="string"?y.nonce:void 0})}}else y==null&&h.d.M(C)},Qe.preload=function(C,y){if(typeof C=="string"&&typeof y=="object"&&y!==null&&typeof y.as=="string"){var Q=y.as,U=K(Q,y.crossOrigin);h.d.L(C,Q,{crossOrigin:U,integrity:typeof y.integrity=="string"?y.integrity:void 0,nonce:typeof y.nonce=="string"?y.nonce:void 0,type:typeof y.type=="string"?y.type:void 0,fetchPriority:typeof y.fetchPriority=="string"?y.fetchPriority:void 0,referrerPolicy:typeof y.referrerPolicy=="string"?y.referrerPolicy:void 0,imageSrcSet:typeof y.imageSrcSet=="string"?y.imageSrcSet:void 0,imageSizes:typeof y.imageSizes=="string"?y.imageSizes:void 0,media:typeof y.media=="string"?y.media:void 0})}},Qe.preloadModule=function(C,y){if(typeof C=="string")if(y){var Q=K(y.as,y.crossOrigin);h.d.m(C,{as:typeof y.as=="string"&&y.as!=="script"?y.as:void 0,crossOrigin:Q,integrity:typeof y.integrity=="string"?y.integrity:void 0})}else h.d.m(C)},Qe.requestFormReset=function(C){h.d.r(C)},Qe.unstable_batchedUpdates=function(C,y){return C(y)},Qe.useFormState=function(C,y,Q){return P.H.useFormState(C,y,Q)},Qe.useFormStatus=function(){return P.H.useHostTransitionStatus()},Qe.version="19.2.4",Qe}var kh;function sf(){if(kh)return ul.exports;kh=1;function v(){if(!(typeof __REACT_DEVTOOLS_GLOBAL_HOOK__>"u"||typeof __REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE!="function"))try{__REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE(v)}catch(N){console.error(N)}}return v(),ul.exports=nf(),ul.exports}/**
 * @license React
 * react-dom-client.production.js
 *
 * Copyright (c) Meta Platforms, Inc. and affiliates.
 *
 * This source code is licensed under the MIT license found in the
 * LICENSE file in the root directory of this source tree.
 */var Th;function of(){if(Th)return ki;Th=1;var v=af(),N=hl(),M=sf();function h(e){var t="https://react.dev/errors/"+e;if(1<arguments.length){t+="?args[]="+encodeURIComponent(arguments[1]);for(var a=2;a<arguments.length;a++)t+="&args[]="+encodeURIComponent(arguments[a])}return"Minified React error #"+e+"; visit "+t+" for the full message or use the non-minified dev environment for full errors and additional helpful warnings."}function L(e){return!(!e||e.nodeType!==1&&e.nodeType!==9&&e.nodeType!==11)}function E(e){var t=e,a=e;if(e.alternate)for(;t.return;)t=t.return;else{e=t;do t=e,(t.flags&4098)!==0&&(a=t.return),e=t.return;while(e)}return t.tag===3?a:null}function P(e){if(e.tag===13){var t=e.memoizedState;if(t===null&&(e=e.alternate,e!==null&&(t=e.memoizedState)),t!==null)return t.dehydrated}return null}function K(e){if(e.tag===31){var t=e.memoizedState;if(t===null&&(e=e.alternate,e!==null&&(t=e.memoizedState)),t!==null)return t.dehydrated}return null}function C(e){if(E(e)!==e)throw Error(h(188))}function y(e){var t=e.alternate;if(!t){if(t=E(e),t===null)throw Error(h(188));return t!==e?null:e}for(var a=e,n=t;;){var i=a.return;if(i===null)break;var s=i.alternate;if(s===null){if(n=i.return,n!==null){a=n;continue}break}if(i.child===s.child){for(s=i.child;s;){if(s===a)return C(i),e;if(s===n)return C(i),t;s=s.sibling}throw Error(h(188))}if(a.return!==n.return)a=i,n=s;else{for(var o=!1,r=i.child;r;){if(r===a){o=!0,a=i,n=s;break}if(r===n){o=!0,n=i,a=s;break}r=r.sibling}if(!o){for(r=s.child;r;){if(r===a){o=!0,a=s,n=i;break}if(r===n){o=!0,n=s,a=i;break}r=r.sibling}if(!o)throw Error(h(189))}}if(a.alternate!==n)throw Error(h(190))}if(a.tag!==3)throw Error(h(188));return a.stateNode.current===a?e:t}function Q(e){var t=e.tag;if(t===5||t===26||t===27||t===6)return e;for(e=e.child;e!==null;){if(t=Q(e),t!==null)return t;e=e.sibling}return null}var U=Object.assign,ie=Symbol.for("react.element"),Pe=Symbol.for("react.transitional.element"),we=Symbol.for("react.portal"),re=Symbol.for("react.fragment"),De=Symbol.for("react.strict_mode"),je=Symbol.for("react.profiler"),At=Symbol.for("react.consumer"),B=Symbol.for("react.context"),pe=Symbol.for("react.forward_ref"),le=Symbol.for("react.suspense"),Ee=Symbol.for("react.suspense_list"),Y=Symbol.for("react.memo"),We=Symbol.for("react.lazy"),_e=Symbol.for("react.activity"),Qa=Symbol.for("react.memo_cache_sentinel"),qt=Symbol.iterator;function Ye(e){return e===null||typeof e!="object"?null:(e=qt&&e[qt]||e["@@iterator"],typeof e=="function"?e:null)}var ka=Symbol.for("react.client.reference");function Dt(e){if(e==null)return null;if(typeof e=="function")return e.$$typeof===ka?null:e.displayName||e.name||null;if(typeof e=="string")return e;switch(e){case re:return"Fragment";case je:return"Profiler";case De:return"StrictMode";case le:return"Suspense";case Ee:return"SuspenseList";case _e:return"Activity"}if(typeof e=="object")switch(e.$$typeof){case we:return"Portal";case B:return e.displayName||"Context";case At:return(e._context.displayName||"Context")+".Consumer";case pe:var t=e.render;return e=e.displayName,e||(e=t.displayName||t.name||"",e=e!==""?"ForwardRef("+e+")":"ForwardRef"),e;case Y:return t=e.displayName||null,t!==null?t:Dt(e.type)||"Memo";case We:t=e._payload,e=e._init;try{return Dt(e(t))}catch{}}return null}var xt=Array.isArray,w=N.__CLIENT_INTERNALS_DO_NOT_USE_OR_WARN_USERS_THEY_CANNOT_UPGRADE,A=M.__DOM_INTERNALS_DO_NOT_USE_OR_WARN_USERS_THEY_CANNOT_UPGRADE,H={pending:!1,data:null,method:null,action:null},ce=[],me=-1;function d(e){return{current:e}}function T(e){0>me||(e.current=ce[me],ce[me]=null,me--)}function q(e,t){me++,ce[me]=e.current,e.current=t}var I=d(null),j=d(null),V=d(null),ne=d(null);function Ge(e,t){switch(q(V,t),q(j,e),q(I,null),t.nodeType){case 9:case 11:e=(e=t.documentElement)&&(e=e.namespaceURI)?Nu(e):0;break;default:if(e=t.tagName,t=t.namespaceURI)t=Nu(t),e=Hu(t,e);else switch(e){case"svg":e=1;break;case"math":e=2;break;default:e=0}}T(I),q(I,e)}function Te(){T(I),T(j),T(V)}function Rn(e){e.memoizedState!==null&&q(ne,e);var t=I.current,a=Hu(t,e.type);t!==a&&(q(j,e),q(I,a))}function Ti(e){j.current===e&&(T(I),T(j)),ne.current===e&&(T(ne),yi._currentValue=H)}var Os,ml;function Ta(e){if(Os===void 0)try{throw Error()}catch(a){var t=a.stack.trim().match(/\n( *(at )?)/);Os=t&&t[1]||"",ml=-1<a.stack.indexOf(`
    at`)?" (<anonymous>)":-1<a.stack.indexOf("@")?"@unknown:0:0":""}return`
`+Os+e+ml}var Qs=!1;function js(e,t){if(!e||Qs)return"";Qs=!0;var a=Error.prepareStackTrace;Error.prepareStackTrace=void 0;try{var n={DetermineComponentFrameRoot:function(){try{if(t){var k=function(){throw Error()};if(Object.defineProperty(k.prototype,"props",{set:function(){throw Error()}}),typeof Reflect=="object"&&Reflect.construct){try{Reflect.construct(k,[])}catch(g){var f=g}Reflect.construct(e,[],k)}else{try{k.call()}catch(g){f=g}e.call(k.prototype)}}else{try{throw Error()}catch(g){f=g}(k=e())&&typeof k.catch=="function"&&k.catch(function(){})}}catch(g){if(g&&f&&typeof g.stack=="string")return[g.stack,f.stack]}return[null,null]}};n.DetermineComponentFrameRoot.displayName="DetermineComponentFrameRoot";var i=Object.getOwnPropertyDescriptor(n.DetermineComponentFrameRoot,"name");i&&i.configurable&&Object.defineProperty(n.DetermineComponentFrameRoot,"name",{value:"DetermineComponentFrameRoot"});var s=n.DetermineComponentFrameRoot(),o=s[0],r=s[1];if(o&&r){var l=o.split(`
`),m=r.split(`
`);for(i=n=0;n<l.length&&!l[n].includes("DetermineComponentFrameRoot");)n++;for(;i<m.length&&!m[i].includes("DetermineComponentFrameRoot");)i++;if(n===l.length||i===m.length)for(n=l.length-1,i=m.length-1;1<=n&&0<=i&&l[n]!==m[i];)i--;for(;1<=n&&0<=i;n--,i--)if(l[n]!==m[i]){if(n!==1||i!==1)do if(n--,i--,0>i||l[n]!==m[i]){var b=`
`+l[n].replace(" at new "," at ");return e.displayName&&b.includes("<anonymous>")&&(b=b.replace("<anonymous>",e.displayName)),b}while(1<=n&&0<=i);break}}}finally{Qs=!1,Error.prepareStackTrace=a}return(a=e?e.displayName||e.name:"")?Ta(a):""}function Rh(e,t){switch(e.tag){case 26:case 27:case 5:return Ta(e.type);case 16:return Ta("Lazy");case 13:return e.child!==t&&t!==null?Ta("Suspense Fallback"):Ta("Suspense");case 19:return Ta("SuspenseList");case 0:case 15:return js(e.type,!1);case 11:return js(e.type.render,!1);case 1:return js(e.type,!0);case 31:return Ta("Activity");default:return""}}function fl(e){try{var t="",a=null;do t+=Rh(e,a),a=e,e=e.return;while(e);return t}catch(n){return`
Error generating stack: `+n.message+`
`+n.stack}}var _s=Object.prototype.hasOwnProperty,Gs=v.unstable_scheduleCallback,Ks=v.unstable_cancelCallback,Lh=v.unstable_shouldYield,Ih=v.unstable_requestPaint,tt=v.unstable_now,Dh=v.unstable_getCurrentPriorityLevel,gl=v.unstable_ImmediatePriority,yl=v.unstable_UserBlockingPriority,Si=v.unstable_NormalPriority,Uh=v.unstable_LowPriority,bl=v.unstable_IdlePriority,zh=v.log,Ph=v.unstable_setDisableYieldValue,Ln=null,at=null;function Xt(e){if(typeof zh=="function"&&Ph(e),at&&typeof at.setStrictMode=="function")try{at.setStrictMode(Ln,e)}catch{}}var nt=Math.clz32?Math.clz32:Bh,Eh=Math.log,Wh=Math.LN2;function Bh(e){return e>>>=0,e===0?32:31-(Eh(e)/Wh|0)|0}var Ai=256,qi=262144,Ci=4194304;function Sa(e){var t=e&42;if(t!==0)return t;switch(e&-e){case 1:return 1;case 2:return 2;case 4:return 4;case 8:return 8;case 16:return 16;case 32:return 32;case 64:return 64;case 128:return 128;case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:return e&261888;case 262144:case 524288:case 1048576:case 2097152:return e&3932160;case 4194304:case 8388608:case 16777216:case 33554432:return e&62914560;case 67108864:return 67108864;case 134217728:return 134217728;case 268435456:return 268435456;case 536870912:return 536870912;case 1073741824:return 0;default:return e}}function Ri(e,t,a){var n=e.pendingLanes;if(n===0)return 0;var i=0,s=e.suspendedLanes,o=e.pingedLanes;e=e.warmLanes;var r=n&134217727;return r!==0?(n=r&~s,n!==0?i=Sa(n):(o&=r,o!==0?i=Sa(o):a||(a=r&~e,a!==0&&(i=Sa(a))))):(r=n&~s,r!==0?i=Sa(r):o!==0?i=Sa(o):a||(a=n&~e,a!==0&&(i=Sa(a)))),i===0?0:t!==0&&t!==i&&(t&s)===0&&(s=i&-i,a=t&-t,s>=a||s===32&&(a&4194048)!==0)?t:i}function In(e,t){return(e.pendingLanes&~(e.suspendedLanes&~e.pingedLanes)&t)===0}function Nh(e,t){switch(e){case 1:case 2:case 4:case 8:case 64:return t+250;case 16:case 32:case 128:case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:return t+5e3;case 4194304:case 8388608:case 16777216:case 33554432:return-1;case 67108864:case 134217728:case 268435456:case 536870912:case 1073741824:return-1;default:return-1}}function vl(){var e=Ci;return Ci<<=1,(Ci&62914560)===0&&(Ci=4194304),e}function Ys(e){for(var t=[],a=0;31>a;a++)t.push(e);return t}function Dn(e,t){e.pendingLanes|=t,t!==268435456&&(e.suspendedLanes=0,e.pingedLanes=0,e.warmLanes=0)}function Hh(e,t,a,n,i,s){var o=e.pendingLanes;e.pendingLanes=a,e.suspendedLanes=0,e.pingedLanes=0,e.warmLanes=0,e.expiredLanes&=a,e.entangledLanes&=a,e.errorRecoveryDisabledLanes&=a,e.shellSuspendCounter=0;var r=e.entanglements,l=e.expirationTimes,m=e.hiddenUpdates;for(a=o&~a;0<a;){var b=31-nt(a),k=1<<b;r[b]=0,l[b]=-1;var f=m[b];if(f!==null)for(m[b]=null,b=0;b<f.length;b++){var g=f[b];g!==null&&(g.lane&=-536870913)}a&=~k}n!==0&&wl(e,n,0),s!==0&&i===0&&e.tag!==0&&(e.suspendedLanes|=s&~(o&~t))}function wl(e,t,a){e.pendingLanes|=t,e.suspendedLanes&=~t;var n=31-nt(t);e.entangledLanes|=t,e.entanglements[n]=e.entanglements[n]|1073741824|a&261930}function xl(e,t){var a=e.entangledLanes|=t;for(e=e.entanglements;a;){var n=31-nt(a),i=1<<n;i&t|e[n]&t&&(e[n]|=t),a&=~i}}function kl(e,t){var a=t&-t;return a=(a&42)!==0?1:Vs(a),(a&(e.suspendedLanes|t))!==0?0:a}function Vs(e){switch(e){case 2:e=1;break;case 8:e=4;break;case 32:e=16;break;case 256:case 512:case 1024:case 2048:case 4096:case 8192:case 16384:case 32768:case 65536:case 131072:case 262144:case 524288:case 1048576:case 2097152:case 4194304:case 8388608:case 16777216:case 33554432:e=128;break;case 268435456:e=134217728;break;default:e=0}return e}function Zs(e){return e&=-e,2<e?8<e?(e&134217727)!==0?32:268435456:8:2}function Tl(){var e=A.p;return e!==0?e:(e=window.event,e===void 0?32:lh(e.type))}function Sl(e,t){var a=A.p;try{return A.p=e,t()}finally{A.p=a}}var Ft=Math.random().toString(36).slice(2),Be="__reactFiber$"+Ft,Ve="__reactProps$"+Ft,ja="__reactContainer$"+Ft,Xs="__reactEvents$"+Ft,Mh="__reactListeners$"+Ft,Oh="__reactHandles$"+Ft,Al="__reactResources$"+Ft,Un="__reactMarker$"+Ft;function Fs(e){delete e[Be],delete e[Ve],delete e[Xs],delete e[Mh],delete e[Oh]}function _a(e){var t=e[Be];if(t)return t;for(var a=e.parentNode;a;){if(t=a[ja]||a[Be]){if(a=t.alternate,t.child!==null||a!==null&&a.child!==null)for(e=Ku(e);e!==null;){if(a=e[Be])return a;e=Ku(e)}return t}e=a,a=e.parentNode}return null}function Ga(e){if(e=e[Be]||e[ja]){var t=e.tag;if(t===5||t===6||t===13||t===31||t===26||t===27||t===3)return e}return null}function zn(e){var t=e.tag;if(t===5||t===26||t===27||t===6)return e.stateNode;throw Error(h(33))}function Ka(e){var t=e[Al];return t||(t=e[Al]={hoistableStyles:new Map,hoistableScripts:new Map}),t}function Ue(e){e[Un]=!0}var ql=new Set,Cl={};function Aa(e,t){Ya(e,t),Ya(e+"Capture",t)}function Ya(e,t){for(Cl[e]=t,e=0;e<t.length;e++)ql.add(t[e])}var Qh=RegExp("^[:A-Z_a-z\\u00C0-\\u00D6\\u00D8-\\u00F6\\u00F8-\\u02FF\\u0370-\\u037D\\u037F-\\u1FFF\\u200C-\\u200D\\u2070-\\u218F\\u2C00-\\u2FEF\\u3001-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFFD][:A-Z_a-z\\u00C0-\\u00D6\\u00D8-\\u00F6\\u00F8-\\u02FF\\u0370-\\u037D\\u037F-\\u1FFF\\u200C-\\u200D\\u2070-\\u218F\\u2C00-\\u2FEF\\u3001-\\uD7FF\\uF900-\\uFDCF\\uFDF0-\\uFFFD\\-.0-9\\u00B7\\u0300-\\u036F\\u203F-\\u2040]*$"),Rl={},Ll={};function jh(e){return _s.call(Ll,e)?!0:_s.call(Rl,e)?!1:Qh.test(e)?Ll[e]=!0:(Rl[e]=!0,!1)}function Li(e,t,a){if(jh(t))if(a===null)e.removeAttribute(t);else{switch(typeof a){case"undefined":case"function":case"symbol":e.removeAttribute(t);return;case"boolean":var n=t.toLowerCase().slice(0,5);if(n!=="data-"&&n!=="aria-"){e.removeAttribute(t);return}}e.setAttribute(t,""+a)}}function Ii(e,t,a){if(a===null)e.removeAttribute(t);else{switch(typeof a){case"undefined":case"function":case"symbol":case"boolean":e.removeAttribute(t);return}e.setAttribute(t,""+a)}}function Ut(e,t,a,n){if(n===null)e.removeAttribute(a);else{switch(typeof n){case"undefined":case"function":case"symbol":case"boolean":e.removeAttribute(a);return}e.setAttributeNS(t,a,""+n)}}function ut(e){switch(typeof e){case"bigint":case"boolean":case"number":case"string":case"undefined":return e;case"object":return e;default:return""}}function Il(e){var t=e.type;return(e=e.nodeName)&&e.toLowerCase()==="input"&&(t==="checkbox"||t==="radio")}function _h(e,t,a){var n=Object.getOwnPropertyDescriptor(e.constructor.prototype,t);if(!e.hasOwnProperty(t)&&typeof n<"u"&&typeof n.get=="function"&&typeof n.set=="function"){var i=n.get,s=n.set;return Object.defineProperty(e,t,{configurable:!0,get:function(){return i.call(this)},set:function(o){a=""+o,s.call(this,o)}}),Object.defineProperty(e,t,{enumerable:n.enumerable}),{getValue:function(){return a},setValue:function(o){a=""+o},stopTracking:function(){e._valueTracker=null,delete e[t]}}}}function Js(e){if(!e._valueTracker){var t=Il(e)?"checked":"value";e._valueTracker=_h(e,t,""+e[t])}}function Dl(e){if(!e)return!1;var t=e._valueTracker;if(!t)return!0;var a=t.getValue(),n="";return e&&(n=Il(e)?e.checked?"true":"false":e.value),e=n,e!==a?(t.setValue(e),!0):!1}function Di(e){if(e=e||(typeof document<"u"?document:void 0),typeof e>"u")return null;try{return e.activeElement||e.body}catch{return e.body}}var Gh=/[\n"\\]/g;function ht(e){return e.replace(Gh,function(t){return"\\"+t.charCodeAt(0).toString(16)+" "})}function $s(e,t,a,n,i,s,o,r){e.name="",o!=null&&typeof o!="function"&&typeof o!="symbol"&&typeof o!="boolean"?e.type=o:e.removeAttribute("type"),t!=null?o==="number"?(t===0&&e.value===""||e.value!=t)&&(e.value=""+ut(t)):e.value!==""+ut(t)&&(e.value=""+ut(t)):o!=="submit"&&o!=="reset"||e.removeAttribute("value"),t!=null?eo(e,o,ut(t)):a!=null?eo(e,o,ut(a)):n!=null&&e.removeAttribute("value"),i==null&&s!=null&&(e.defaultChecked=!!s),i!=null&&(e.checked=i&&typeof i!="function"&&typeof i!="symbol"),r!=null&&typeof r!="function"&&typeof r!="symbol"&&typeof r!="boolean"?e.name=""+ut(r):e.removeAttribute("name")}function Ul(e,t,a,n,i,s,o,r){if(s!=null&&typeof s!="function"&&typeof s!="symbol"&&typeof s!="boolean"&&(e.type=s),t!=null||a!=null){if(!(s!=="submit"&&s!=="reset"||t!=null)){Js(e);return}a=a!=null?""+ut(a):"",t=t!=null?""+ut(t):a,r||t===e.value||(e.value=t),e.defaultValue=t}n=n??i,n=typeof n!="function"&&typeof n!="symbol"&&!!n,e.checked=r?e.checked:!!n,e.defaultChecked=!!n,o!=null&&typeof o!="function"&&typeof o!="symbol"&&typeof o!="boolean"&&(e.name=o),Js(e)}function eo(e,t,a){t==="number"&&Di(e.ownerDocument)===e||e.defaultValue===""+a||(e.defaultValue=""+a)}function Va(e,t,a,n){if(e=e.options,t){t={};for(var i=0;i<a.length;i++)t["$"+a[i]]=!0;for(a=0;a<e.length;a++)i=t.hasOwnProperty("$"+e[a].value),e[a].selected!==i&&(e[a].selected=i),i&&n&&(e[a].defaultSelected=!0)}else{for(a=""+ut(a),t=null,i=0;i<e.length;i++){if(e[i].value===a){e[i].selected=!0,n&&(e[i].defaultSelected=!0);return}t!==null||e[i].disabled||(t=e[i])}t!==null&&(t.selected=!0)}}function zl(e,t,a){if(t!=null&&(t=""+ut(t),t!==e.value&&(e.value=t),a==null)){e.defaultValue!==t&&(e.defaultValue=t);return}e.defaultValue=a!=null?""+ut(a):""}function Pl(e,t,a,n){if(t==null){if(n!=null){if(a!=null)throw Error(h(92));if(xt(n)){if(1<n.length)throw Error(h(93));n=n[0]}a=n}a==null&&(a=""),t=a}a=ut(t),e.defaultValue=a,n=e.textContent,n===a&&n!==""&&n!==null&&(e.value=n),Js(e)}function Za(e,t){if(t){var a=e.firstChild;if(a&&a===e.lastChild&&a.nodeType===3){a.nodeValue=t;return}}e.textContent=t}var Kh=new Set("animationIterationCount aspectRatio borderImageOutset borderImageSlice borderImageWidth boxFlex boxFlexGroup boxOrdinalGroup columnCount columns flex flexGrow flexPositive flexShrink flexNegative flexOrder gridArea gridRow gridRowEnd gridRowSpan gridRowStart gridColumn gridColumnEnd gridColumnSpan gridColumnStart fontWeight lineClamp lineHeight opacity order orphans scale tabSize widows zIndex zoom fillOpacity floodOpacity stopOpacity strokeDasharray strokeDashoffset strokeMiterlimit strokeOpacity strokeWidth MozAnimationIterationCount MozBoxFlex MozBoxFlexGroup MozLineClamp msAnimationIterationCount msFlex msZoom msFlexGrow msFlexNegative msFlexOrder msFlexPositive msFlexShrink msGridColumn msGridColumnSpan msGridRow msGridRowSpan WebkitAnimationIterationCount WebkitBoxFlex WebKitBoxFlexGroup WebkitBoxOrdinalGroup WebkitColumnCount WebkitColumns WebkitFlex WebkitFlexGrow WebkitFlexPositive WebkitFlexShrink WebkitLineClamp".split(" "));function El(e,t,a){var n=t.indexOf("--")===0;a==null||typeof a=="boolean"||a===""?n?e.setProperty(t,""):t==="float"?e.cssFloat="":e[t]="":n?e.setProperty(t,a):typeof a!="number"||a===0||Kh.has(t)?t==="float"?e.cssFloat=a:e[t]=(""+a).trim():e[t]=a+"px"}function Wl(e,t,a){if(t!=null&&typeof t!="object")throw Error(h(62));if(e=e.style,a!=null){for(var n in a)!a.hasOwnProperty(n)||t!=null&&t.hasOwnProperty(n)||(n.indexOf("--")===0?e.setProperty(n,""):n==="float"?e.cssFloat="":e[n]="");for(var i in t)n=t[i],t.hasOwnProperty(i)&&a[i]!==n&&El(e,i,n)}else for(var s in t)t.hasOwnProperty(s)&&El(e,s,t[s])}function to(e){if(e.indexOf("-")===-1)return!1;switch(e){case"annotation-xml":case"color-profile":case"font-face":case"font-face-src":case"font-face-uri":case"font-face-format":case"font-face-name":case"missing-glyph":return!1;default:return!0}}var Yh=new Map([["acceptCharset","accept-charset"],["htmlFor","for"],["httpEquiv","http-equiv"],["crossOrigin","crossorigin"],["accentHeight","accent-height"],["alignmentBaseline","alignment-baseline"],["arabicForm","arabic-form"],["baselineShift","baseline-shift"],["capHeight","cap-height"],["clipPath","clip-path"],["clipRule","clip-rule"],["colorInterpolation","color-interpolation"],["colorInterpolationFilters","color-interpolation-filters"],["colorProfile","color-profile"],["colorRendering","color-rendering"],["dominantBaseline","dominant-baseline"],["enableBackground","enable-background"],["fillOpacity","fill-opacity"],["fillRule","fill-rule"],["floodColor","flood-color"],["floodOpacity","flood-opacity"],["fontFamily","font-family"],["fontSize","font-size"],["fontSizeAdjust","font-size-adjust"],["fontStretch","font-stretch"],["fontStyle","font-style"],["fontVariant","font-variant"],["fontWeight","font-weight"],["glyphName","glyph-name"],["glyphOrientationHorizontal","glyph-orientation-horizontal"],["glyphOrientationVertical","glyph-orientation-vertical"],["horizAdvX","horiz-adv-x"],["horizOriginX","horiz-origin-x"],["imageRendering","image-rendering"],["letterSpacing","letter-spacing"],["lightingColor","lighting-color"],["markerEnd","marker-end"],["markerMid","marker-mid"],["markerStart","marker-start"],["overlinePosition","overline-position"],["overlineThickness","overline-thickness"],["paintOrder","paint-order"],["panose-1","panose-1"],["pointerEvents","pointer-events"],["renderingIntent","rendering-intent"],["shapeRendering","shape-rendering"],["stopColor","stop-color"],["stopOpacity","stop-opacity"],["strikethroughPosition","strikethrough-position"],["strikethroughThickness","strikethrough-thickness"],["strokeDasharray","stroke-dasharray"],["strokeDashoffset","stroke-dashoffset"],["strokeLinecap","stroke-linecap"],["strokeLinejoin","stroke-linejoin"],["strokeMiterlimit","stroke-miterlimit"],["strokeOpacity","stroke-opacity"],["strokeWidth","stroke-width"],["textAnchor","text-anchor"],["textDecoration","text-decoration"],["textRendering","text-rendering"],["transformOrigin","transform-origin"],["underlinePosition","underline-position"],["underlineThickness","underline-thickness"],["unicodeBidi","unicode-bidi"],["unicodeRange","unicode-range"],["unitsPerEm","units-per-em"],["vAlphabetic","v-alphabetic"],["vHanging","v-hanging"],["vIdeographic","v-ideographic"],["vMathematical","v-mathematical"],["vectorEffect","vector-effect"],["vertAdvY","vert-adv-y"],["vertOriginX","vert-origin-x"],["vertOriginY","vert-origin-y"],["wordSpacing","word-spacing"],["writingMode","writing-mode"],["xmlnsXlink","xmlns:xlink"],["xHeight","x-height"]]),Vh=/^[\u0000-\u001F ]*j[\r\n\t]*a[\r\n\t]*v[\r\n\t]*a[\r\n\t]*s[\r\n\t]*c[\r\n\t]*r[\r\n\t]*i[\r\n\t]*p[\r\n\t]*t[\r\n\t]*:/i;function Ui(e){return Vh.test(""+e)?"javascript:throw new Error('React has blocked a javascript: URL as a security precaution.')":e}function zt(){}var ao=null;function no(e){return e=e.target||e.srcElement||window,e.correspondingUseElement&&(e=e.correspondingUseElement),e.nodeType===3?e.parentNode:e}var Xa=null,Fa=null;function Bl(e){var t=Ga(e);if(t&&(e=t.stateNode)){var a=e[Ve]||null;e:switch(e=t.stateNode,t.type){case"input":if($s(e,a.value,a.defaultValue,a.defaultValue,a.checked,a.defaultChecked,a.type,a.name),t=a.name,a.type==="radio"&&t!=null){for(a=e;a.parentNode;)a=a.parentNode;for(a=a.querySelectorAll('input[name="'+ht(""+t)+'"][type="radio"]'),t=0;t<a.length;t++){var n=a[t];if(n!==e&&n.form===e.form){var i=n[Ve]||null;if(!i)throw Error(h(90));$s(n,i.value,i.defaultValue,i.defaultValue,i.checked,i.defaultChecked,i.type,i.name)}}for(t=0;t<a.length;t++)n=a[t],n.form===e.form&&Dl(n)}break e;case"textarea":zl(e,a.value,a.defaultValue);break e;case"select":t=a.value,t!=null&&Va(e,!!a.multiple,t,!1)}}}var io=!1;function Nl(e,t,a){if(io)return e(t,a);io=!0;try{var n=e(t);return n}finally{if(io=!1,(Xa!==null||Fa!==null)&&(vs(),Xa&&(t=Xa,e=Fa,Fa=Xa=null,Bl(t),e)))for(t=0;t<e.length;t++)Bl(e[t])}}function Pn(e,t){var a=e.stateNode;if(a===null)return null;var n=a[Ve]||null;if(n===null)return null;a=n[t];e:switch(t){case"onClick":case"onClickCapture":case"onDoubleClick":case"onDoubleClickCapture":case"onMouseDown":case"onMouseDownCapture":case"onMouseMove":case"onMouseMoveCapture":case"onMouseUp":case"onMouseUpCapture":case"onMouseEnter":(n=!n.disabled)||(e=e.type,n=!(e==="button"||e==="input"||e==="select"||e==="textarea")),e=!n;break e;default:e=!1}if(e)return null;if(a&&typeof a!="function")throw Error(h(231,t,typeof a));return a}var Pt=!(typeof window>"u"||typeof window.document>"u"||typeof window.document.createElement>"u"),so=!1;if(Pt)try{var En={};Object.defineProperty(En,"passive",{get:function(){so=!0}}),window.addEventListener("test",En,En),window.removeEventListener("test",En,En)}catch{so=!1}var Jt=null,oo=null,zi=null;function Hl(){if(zi)return zi;var e,t=oo,a=t.length,n,i="value"in Jt?Jt.value:Jt.textContent,s=i.length;for(e=0;e<a&&t[e]===i[e];e++);var o=a-e;for(n=1;n<=o&&t[a-n]===i[s-n];n++);return zi=i.slice(e,1<n?1-n:void 0)}function Pi(e){var t=e.keyCode;return"charCode"in e?(e=e.charCode,e===0&&t===13&&(e=13)):e=t,e===10&&(e=13),32<=e||e===13?e:0}function Ei(){return!0}function Ml(){return!1}function Ze(e){function t(a,n,i,s,o){this._reactName=a,this._targetInst=i,this.type=n,this.nativeEvent=s,this.target=o,this.currentTarget=null;for(var r in e)e.hasOwnProperty(r)&&(a=e[r],this[r]=a?a(s):s[r]);return this.isDefaultPrevented=(s.defaultPrevented!=null?s.defaultPrevented:s.returnValue===!1)?Ei:Ml,this.isPropagationStopped=Ml,this}return U(t.prototype,{preventDefault:function(){this.defaultPrevented=!0;var a=this.nativeEvent;a&&(a.preventDefault?a.preventDefault():typeof a.returnValue!="unknown"&&(a.returnValue=!1),this.isDefaultPrevented=Ei)},stopPropagation:function(){var a=this.nativeEvent;a&&(a.stopPropagation?a.stopPropagation():typeof a.cancelBubble!="unknown"&&(a.cancelBubble=!0),this.isPropagationStopped=Ei)},persist:function(){},isPersistent:Ei}),t}var qa={eventPhase:0,bubbles:0,cancelable:0,timeStamp:function(e){return e.timeStamp||Date.now()},defaultPrevented:0,isTrusted:0},Wi=Ze(qa),Wn=U({},qa,{view:0,detail:0}),Zh=Ze(Wn),ro,lo,Bn,Bi=U({},Wn,{screenX:0,screenY:0,clientX:0,clientY:0,pageX:0,pageY:0,ctrlKey:0,shiftKey:0,altKey:0,metaKey:0,getModifierState:uo,button:0,buttons:0,relatedTarget:function(e){return e.relatedTarget===void 0?e.fromElement===e.srcElement?e.toElement:e.fromElement:e.relatedTarget},movementX:function(e){return"movementX"in e?e.movementX:(e!==Bn&&(Bn&&e.type==="mousemove"?(ro=e.screenX-Bn.screenX,lo=e.screenY-Bn.screenY):lo=ro=0,Bn=e),ro)},movementY:function(e){return"movementY"in e?e.movementY:lo}}),Ol=Ze(Bi),Xh=U({},Bi,{dataTransfer:0}),Fh=Ze(Xh),Jh=U({},Wn,{relatedTarget:0}),co=Ze(Jh),$h=U({},qa,{animationName:0,elapsedTime:0,pseudoElement:0}),ep=Ze($h),tp=U({},qa,{clipboardData:function(e){return"clipboardData"in e?e.clipboardData:window.clipboardData}}),ap=Ze(tp),np=U({},qa,{data:0}),Ql=Ze(np),ip={Esc:"Escape",Spacebar:" ",Left:"ArrowLeft",Up:"ArrowUp",Right:"ArrowRight",Down:"ArrowDown",Del:"Delete",Win:"OS",Menu:"ContextMenu",Apps:"ContextMenu",Scroll:"ScrollLock",MozPrintableKey:"Unidentified"},sp={8:"Backspace",9:"Tab",12:"Clear",13:"Enter",16:"Shift",17:"Control",18:"Alt",19:"Pause",20:"CapsLock",27:"Escape",32:" ",33:"PageUp",34:"PageDown",35:"End",36:"Home",37:"ArrowLeft",38:"ArrowUp",39:"ArrowRight",40:"ArrowDown",45:"Insert",46:"Delete",112:"F1",113:"F2",114:"F3",115:"F4",116:"F5",117:"F6",118:"F7",119:"F8",120:"F9",121:"F10",122:"F11",123:"F12",144:"NumLock",145:"ScrollLock",224:"Meta"},op={Alt:"altKey",Control:"ctrlKey",Meta:"metaKey",Shift:"shiftKey"};function rp(e){var t=this.nativeEvent;return t.getModifierState?t.getModifierState(e):(e=op[e])?!!t[e]:!1}function uo(){return rp}var lp=U({},Wn,{key:function(e){if(e.key){var t=ip[e.key]||e.key;if(t!=="Unidentified")return t}return e.type==="keypress"?(e=Pi(e),e===13?"Enter":String.fromCharCode(e)):e.type==="keydown"||e.type==="keyup"?sp[e.keyCode]||"Unidentified":""},code:0,location:0,ctrlKey:0,shiftKey:0,altKey:0,metaKey:0,repeat:0,locale:0,getModifierState:uo,charCode:function(e){return e.type==="keypress"?Pi(e):0},keyCode:function(e){return e.type==="keydown"||e.type==="keyup"?e.keyCode:0},which:function(e){return e.type==="keypress"?Pi(e):e.type==="keydown"||e.type==="keyup"?e.keyCode:0}}),cp=Ze(lp),dp=U({},Bi,{pointerId:0,width:0,height:0,pressure:0,tangentialPressure:0,tiltX:0,tiltY:0,twist:0,pointerType:0,isPrimary:0}),jl=Ze(dp),up=U({},Wn,{touches:0,targetTouches:0,changedTouches:0,altKey:0,metaKey:0,ctrlKey:0,shiftKey:0,getModifierState:uo}),hp=Ze(up),pp=U({},qa,{propertyName:0,elapsedTime:0,pseudoElement:0}),mp=Ze(pp),fp=U({},Bi,{deltaX:function(e){return"deltaX"in e?e.deltaX:"wheelDeltaX"in e?-e.wheelDeltaX:0},deltaY:function(e){return"deltaY"in e?e.deltaY:"wheelDeltaY"in e?-e.wheelDeltaY:"wheelDelta"in e?-e.wheelDelta:0},deltaZ:0,deltaMode:0}),gp=Ze(fp),yp=U({},qa,{newState:0,oldState:0}),bp=Ze(yp),vp=[9,13,27,32],ho=Pt&&"CompositionEvent"in window,Nn=null;Pt&&"documentMode"in document&&(Nn=document.documentMode);var wp=Pt&&"TextEvent"in window&&!Nn,_l=Pt&&(!ho||Nn&&8<Nn&&11>=Nn),Gl=" ",Kl=!1;function Yl(e,t){switch(e){case"keyup":return vp.indexOf(t.keyCode)!==-1;case"keydown":return t.keyCode!==229;case"keypress":case"mousedown":case"focusout":return!0;default:return!1}}function Vl(e){return e=e.detail,typeof e=="object"&&"data"in e?e.data:null}var Ja=!1;function xp(e,t){switch(e){case"compositionend":return Vl(t);case"keypress":return t.which!==32?null:(Kl=!0,Gl);case"textInput":return e=t.data,e===Gl&&Kl?null:e;default:return null}}function kp(e,t){if(Ja)return e==="compositionend"||!ho&&Yl(e,t)?(e=Hl(),zi=oo=Jt=null,Ja=!1,e):null;switch(e){case"paste":return null;case"keypress":if(!(t.ctrlKey||t.altKey||t.metaKey)||t.ctrlKey&&t.altKey){if(t.char&&1<t.char.length)return t.char;if(t.which)return String.fromCharCode(t.which)}return null;case"compositionend":return _l&&t.locale!=="ko"?null:t.data;default:return null}}var Tp={color:!0,date:!0,datetime:!0,"datetime-local":!0,email:!0,month:!0,number:!0,password:!0,range:!0,search:!0,tel:!0,text:!0,time:!0,url:!0,week:!0};function Zl(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return t==="input"?!!Tp[e.type]:t==="textarea"}function Xl(e,t,a,n){Xa?Fa?Fa.push(n):Fa=[n]:Xa=n,t=qs(t,"onChange"),0<t.length&&(a=new Wi("onChange","change",null,a,n),e.push({event:a,listeners:t}))}var Hn=null,Mn=null;function Sp(e){Uu(e,0)}function Ni(e){var t=zn(e);if(Dl(t))return e}function Fl(e,t){if(e==="change")return t}var Jl=!1;if(Pt){var po;if(Pt){var mo="oninput"in document;if(!mo){var $l=document.createElement("div");$l.setAttribute("oninput","return;"),mo=typeof $l.oninput=="function"}po=mo}else po=!1;Jl=po&&(!document.documentMode||9<document.documentMode)}function ec(){Hn&&(Hn.detachEvent("onpropertychange",tc),Mn=Hn=null)}function tc(e){if(e.propertyName==="value"&&Ni(Mn)){var t=[];Xl(t,Mn,e,no(e)),Nl(Sp,t)}}function Ap(e,t,a){e==="focusin"?(ec(),Hn=t,Mn=a,Hn.attachEvent("onpropertychange",tc)):e==="focusout"&&ec()}function qp(e){if(e==="selectionchange"||e==="keyup"||e==="keydown")return Ni(Mn)}function Cp(e,t){if(e==="click")return Ni(t)}function Rp(e,t){if(e==="input"||e==="change")return Ni(t)}function Lp(e,t){return e===t&&(e!==0||1/e===1/t)||e!==e&&t!==t}var it=typeof Object.is=="function"?Object.is:Lp;function On(e,t){if(it(e,t))return!0;if(typeof e!="object"||e===null||typeof t!="object"||t===null)return!1;var a=Object.keys(e),n=Object.keys(t);if(a.length!==n.length)return!1;for(n=0;n<a.length;n++){var i=a[n];if(!_s.call(t,i)||!it(e[i],t[i]))return!1}return!0}function ac(e){for(;e&&e.firstChild;)e=e.firstChild;return e}function nc(e,t){var a=ac(e);e=0;for(var n;a;){if(a.nodeType===3){if(n=e+a.textContent.length,e<=t&&n>=t)return{node:a,offset:t-e};e=n}e:{for(;a;){if(a.nextSibling){a=a.nextSibling;break e}a=a.parentNode}a=void 0}a=ac(a)}}function ic(e,t){return e&&t?e===t?!0:e&&e.nodeType===3?!1:t&&t.nodeType===3?ic(e,t.parentNode):"contains"in e?e.contains(t):e.compareDocumentPosition?!!(e.compareDocumentPosition(t)&16):!1:!1}function sc(e){e=e!=null&&e.ownerDocument!=null&&e.ownerDocument.defaultView!=null?e.ownerDocument.defaultView:window;for(var t=Di(e.document);t instanceof e.HTMLIFrameElement;){try{var a=typeof t.contentWindow.location.href=="string"}catch{a=!1}if(a)e=t.contentWindow;else break;t=Di(e.document)}return t}function fo(e){var t=e&&e.nodeName&&e.nodeName.toLowerCase();return t&&(t==="input"&&(e.type==="text"||e.type==="search"||e.type==="tel"||e.type==="url"||e.type==="password")||t==="textarea"||e.contentEditable==="true")}var Ip=Pt&&"documentMode"in document&&11>=document.documentMode,$a=null,go=null,Qn=null,yo=!1;function oc(e,t,a){var n=a.window===a?a.document:a.nodeType===9?a:a.ownerDocument;yo||$a==null||$a!==Di(n)||(n=$a,"selectionStart"in n&&fo(n)?n={start:n.selectionStart,end:n.selectionEnd}:(n=(n.ownerDocument&&n.ownerDocument.defaultView||window).getSelection(),n={anchorNode:n.anchorNode,anchorOffset:n.anchorOffset,focusNode:n.focusNode,focusOffset:n.focusOffset}),Qn&&On(Qn,n)||(Qn=n,n=qs(go,"onSelect"),0<n.length&&(t=new Wi("onSelect","select",null,t,a),e.push({event:t,listeners:n}),t.target=$a)))}function Ca(e,t){var a={};return a[e.toLowerCase()]=t.toLowerCase(),a["Webkit"+e]="webkit"+t,a["Moz"+e]="moz"+t,a}var en={animationend:Ca("Animation","AnimationEnd"),animationiteration:Ca("Animation","AnimationIteration"),animationstart:Ca("Animation","AnimationStart"),transitionrun:Ca("Transition","TransitionRun"),transitionstart:Ca("Transition","TransitionStart"),transitioncancel:Ca("Transition","TransitionCancel"),transitionend:Ca("Transition","TransitionEnd")},bo={},rc={};Pt&&(rc=document.createElement("div").style,"AnimationEvent"in window||(delete en.animationend.animation,delete en.animationiteration.animation,delete en.animationstart.animation),"TransitionEvent"in window||delete en.transitionend.transition);function Ra(e){if(bo[e])return bo[e];if(!en[e])return e;var t=en[e],a;for(a in t)if(t.hasOwnProperty(a)&&a in rc)return bo[e]=t[a];return e}var lc=Ra("animationend"),cc=Ra("animationiteration"),dc=Ra("animationstart"),Dp=Ra("transitionrun"),Up=Ra("transitionstart"),zp=Ra("transitioncancel"),uc=Ra("transitionend"),hc=new Map,vo="abort auxClick beforeToggle cancel canPlay canPlayThrough click close contextMenu copy cut drag dragEnd dragEnter dragExit dragLeave dragOver dragStart drop durationChange emptied encrypted ended error gotPointerCapture input invalid keyDown keyPress keyUp load loadedData loadedMetadata loadStart lostPointerCapture mouseDown mouseMove mouseOut mouseOver mouseUp paste pause play playing pointerCancel pointerDown pointerMove pointerOut pointerOver pointerUp progress rateChange reset resize seeked seeking stalled submit suspend timeUpdate touchCancel touchEnd touchStart volumeChange scroll toggle touchMove waiting wheel".split(" ");vo.push("scrollEnd");function kt(e,t){hc.set(e,t),Aa(t,[e])}var Hi=typeof reportError=="function"?reportError:function(e){if(typeof window=="object"&&typeof window.ErrorEvent=="function"){var t=new window.ErrorEvent("error",{bubbles:!0,cancelable:!0,message:typeof e=="object"&&e!==null&&typeof e.message=="string"?String(e.message):String(e),error:e});if(!window.dispatchEvent(t))return}else if(typeof process=="object"&&typeof process.emit=="function"){process.emit("uncaughtException",e);return}console.error(e)},pt=[],tn=0,wo=0;function Mi(){for(var e=tn,t=wo=tn=0;t<e;){var a=pt[t];pt[t++]=null;var n=pt[t];pt[t++]=null;var i=pt[t];pt[t++]=null;var s=pt[t];if(pt[t++]=null,n!==null&&i!==null){var o=n.pending;o===null?i.next=i:(i.next=o.next,o.next=i),n.pending=i}s!==0&&pc(a,i,s)}}function Oi(e,t,a,n){pt[tn++]=e,pt[tn++]=t,pt[tn++]=a,pt[tn++]=n,wo|=n,e.lanes|=n,e=e.alternate,e!==null&&(e.lanes|=n)}function xo(e,t,a,n){return Oi(e,t,a,n),Qi(e)}function La(e,t){return Oi(e,null,null,t),Qi(e)}function pc(e,t,a){e.lanes|=a;var n=e.alternate;n!==null&&(n.lanes|=a);for(var i=!1,s=e.return;s!==null;)s.childLanes|=a,n=s.alternate,n!==null&&(n.childLanes|=a),s.tag===22&&(e=s.stateNode,e===null||e._visibility&1||(i=!0)),e=s,s=s.return;return e.tag===3?(s=e.stateNode,i&&t!==null&&(i=31-nt(a),e=s.hiddenUpdates,n=e[i],n===null?e[i]=[t]:n.push(t),t.lane=a|536870912),s):null}function Qi(e){if(50<di)throw di=0,Ir=null,Error(h(185));for(var t=e.return;t!==null;)e=t,t=e.return;return e.tag===3?e.stateNode:null}var an={};function Pp(e,t,a,n){this.tag=e,this.key=a,this.sibling=this.child=this.return=this.stateNode=this.type=this.elementType=null,this.index=0,this.refCleanup=this.ref=null,this.pendingProps=t,this.dependencies=this.memoizedState=this.updateQueue=this.memoizedProps=null,this.mode=n,this.subtreeFlags=this.flags=0,this.deletions=null,this.childLanes=this.lanes=0,this.alternate=null}function st(e,t,a,n){return new Pp(e,t,a,n)}function ko(e){return e=e.prototype,!(!e||!e.isReactComponent)}function Et(e,t){var a=e.alternate;return a===null?(a=st(e.tag,t,e.key,e.mode),a.elementType=e.elementType,a.type=e.type,a.stateNode=e.stateNode,a.alternate=e,e.alternate=a):(a.pendingProps=t,a.type=e.type,a.flags=0,a.subtreeFlags=0,a.deletions=null),a.flags=e.flags&65011712,a.childLanes=e.childLanes,a.lanes=e.lanes,a.child=e.child,a.memoizedProps=e.memoizedProps,a.memoizedState=e.memoizedState,a.updateQueue=e.updateQueue,t=e.dependencies,a.dependencies=t===null?null:{lanes:t.lanes,firstContext:t.firstContext},a.sibling=e.sibling,a.index=e.index,a.ref=e.ref,a.refCleanup=e.refCleanup,a}function mc(e,t){e.flags&=65011714;var a=e.alternate;return a===null?(e.childLanes=0,e.lanes=t,e.child=null,e.subtreeFlags=0,e.memoizedProps=null,e.memoizedState=null,e.updateQueue=null,e.dependencies=null,e.stateNode=null):(e.childLanes=a.childLanes,e.lanes=a.lanes,e.child=a.child,e.subtreeFlags=0,e.deletions=null,e.memoizedProps=a.memoizedProps,e.memoizedState=a.memoizedState,e.updateQueue=a.updateQueue,e.type=a.type,t=a.dependencies,e.dependencies=t===null?null:{lanes:t.lanes,firstContext:t.firstContext}),e}function ji(e,t,a,n,i,s){var o=0;if(n=e,typeof e=="function")ko(e)&&(o=1);else if(typeof e=="string")o=Hm(e,a,I.current)?26:e==="html"||e==="head"||e==="body"?27:5;else e:switch(e){case _e:return e=st(31,a,t,i),e.elementType=_e,e.lanes=s,e;case re:return Ia(a.children,i,s,t);case De:o=8,i|=24;break;case je:return e=st(12,a,t,i|2),e.elementType=je,e.lanes=s,e;case le:return e=st(13,a,t,i),e.elementType=le,e.lanes=s,e;case Ee:return e=st(19,a,t,i),e.elementType=Ee,e.lanes=s,e;default:if(typeof e=="object"&&e!==null)switch(e.$$typeof){case B:o=10;break e;case At:o=9;break e;case pe:o=11;break e;case Y:o=14;break e;case We:o=16,n=null;break e}o=29,a=Error(h(130,e===null?"null":typeof e,"")),n=null}return t=st(o,a,t,i),t.elementType=e,t.type=n,t.lanes=s,t}function Ia(e,t,a,n){return e=st(7,e,n,t),e.lanes=a,e}function To(e,t,a){return e=st(6,e,null,t),e.lanes=a,e}function fc(e){var t=st(18,null,null,0);return t.stateNode=e,t}function So(e,t,a){return t=st(4,e.children!==null?e.children:[],e.key,t),t.lanes=a,t.stateNode={containerInfo:e.containerInfo,pendingChildren:null,implementation:e.implementation},t}var gc=new WeakMap;function mt(e,t){if(typeof e=="object"&&e!==null){var a=gc.get(e);return a!==void 0?a:(t={value:e,source:t,stack:fl(t)},gc.set(e,t),t)}return{value:e,source:t,stack:fl(t)}}var nn=[],sn=0,_i=null,jn=0,ft=[],gt=0,$t=null,Ct=1,Rt="";function Wt(e,t){nn[sn++]=jn,nn[sn++]=_i,_i=e,jn=t}function yc(e,t,a){ft[gt++]=Ct,ft[gt++]=Rt,ft[gt++]=$t,$t=e;var n=Ct;e=Rt;var i=32-nt(n)-1;n&=~(1<<i),a+=1;var s=32-nt(t)+i;if(30<s){var o=i-i%5;s=(n&(1<<o)-1).toString(32),n>>=o,i-=o,Ct=1<<32-nt(t)+i|a<<i|n,Rt=s+e}else Ct=1<<s|a<<i|n,Rt=e}function Ao(e){e.return!==null&&(Wt(e,1),yc(e,1,0))}function qo(e){for(;e===_i;)_i=nn[--sn],nn[sn]=null,jn=nn[--sn],nn[sn]=null;for(;e===$t;)$t=ft[--gt],ft[gt]=null,Rt=ft[--gt],ft[gt]=null,Ct=ft[--gt],ft[gt]=null}function bc(e,t){ft[gt++]=Ct,ft[gt++]=Rt,ft[gt++]=$t,Ct=t.id,Rt=t.overflow,$t=e}var Ne=null,ge=null,$=!1,ea=null,yt=!1,Co=Error(h(519));function ta(e){var t=Error(h(418,1<arguments.length&&arguments[1]!==void 0&&arguments[1]?"text":"HTML",""));throw _n(mt(t,e)),Co}function vc(e){var t=e.stateNode,a=e.type,n=e.memoizedProps;switch(t[Be]=e,t[Ve]=n,a){case"dialog":X("cancel",t),X("close",t);break;case"iframe":case"object":case"embed":X("load",t);break;case"video":case"audio":for(a=0;a<hi.length;a++)X(hi[a],t);break;case"source":X("error",t);break;case"img":case"image":case"link":X("error",t),X("load",t);break;case"details":X("toggle",t);break;case"input":X("invalid",t),Ul(t,n.value,n.defaultValue,n.checked,n.defaultChecked,n.type,n.name,!0);break;case"select":X("invalid",t);break;case"textarea":X("invalid",t),Pl(t,n.value,n.defaultValue,n.children)}a=n.children,typeof a!="string"&&typeof a!="number"&&typeof a!="bigint"||t.textContent===""+a||n.suppressHydrationWarning===!0||Wu(t.textContent,a)?(n.popover!=null&&(X("beforetoggle",t),X("toggle",t)),n.onScroll!=null&&X("scroll",t),n.onScrollEnd!=null&&X("scrollend",t),n.onClick!=null&&(t.onclick=zt),t=!0):t=!1,t||ta(e,!0)}function wc(e){for(Ne=e.return;Ne;)switch(Ne.tag){case 5:case 31:case 13:yt=!1;return;case 27:case 3:yt=!0;return;default:Ne=Ne.return}}function on(e){if(e!==Ne)return!1;if(!$)return wc(e),$=!0,!1;var t=e.tag,a;if((a=t!==3&&t!==27)&&((a=t===5)&&(a=e.type,a=!(a!=="form"&&a!=="button")||Gr(e.type,e.memoizedProps)),a=!a),a&&ge&&ta(e),wc(e),t===13){if(e=e.memoizedState,e=e!==null?e.dehydrated:null,!e)throw Error(h(317));ge=Gu(e)}else if(t===31){if(e=e.memoizedState,e=e!==null?e.dehydrated:null,!e)throw Error(h(317));ge=Gu(e)}else t===27?(t=ge,fa(e.type)?(e=Xr,Xr=null,ge=e):ge=t):ge=Ne?vt(e.stateNode.nextSibling):null;return!0}function Da(){ge=Ne=null,$=!1}function Ro(){var e=ea;return e!==null&&($e===null?$e=e:$e.push.apply($e,e),ea=null),e}function _n(e){ea===null?ea=[e]:ea.push(e)}var Lo=d(null),Ua=null,Bt=null;function aa(e,t,a){q(Lo,t._currentValue),t._currentValue=a}function Nt(e){e._currentValue=Lo.current,T(Lo)}function Io(e,t,a){for(;e!==null;){var n=e.alternate;if((e.childLanes&t)!==t?(e.childLanes|=t,n!==null&&(n.childLanes|=t)):n!==null&&(n.childLanes&t)!==t&&(n.childLanes|=t),e===a)break;e=e.return}}function Do(e,t,a,n){var i=e.child;for(i!==null&&(i.return=e);i!==null;){var s=i.dependencies;if(s!==null){var o=i.child;s=s.firstContext;e:for(;s!==null;){var r=s;s=i;for(var l=0;l<t.length;l++)if(r.context===t[l]){s.lanes|=a,r=s.alternate,r!==null&&(r.lanes|=a),Io(s.return,a,e),n||(o=null);break e}s=r.next}}else if(i.tag===18){if(o=i.return,o===null)throw Error(h(341));o.lanes|=a,s=o.alternate,s!==null&&(s.lanes|=a),Io(o,a,e),o=null}else o=i.child;if(o!==null)o.return=i;else for(o=i;o!==null;){if(o===e){o=null;break}if(i=o.sibling,i!==null){i.return=o.return,o=i;break}o=o.return}i=o}}function rn(e,t,a,n){e=null;for(var i=t,s=!1;i!==null;){if(!s){if((i.flags&524288)!==0)s=!0;else if((i.flags&262144)!==0)break}if(i.tag===10){var o=i.alternate;if(o===null)throw Error(h(387));if(o=o.memoizedProps,o!==null){var r=i.type;it(i.pendingProps.value,o.value)||(e!==null?e.push(r):e=[r])}}else if(i===ne.current){if(o=i.alternate,o===null)throw Error(h(387));o.memoizedState.memoizedState!==i.memoizedState.memoizedState&&(e!==null?e.push(yi):e=[yi])}i=i.return}e!==null&&Do(t,e,a,n),t.flags|=262144}function Gi(e){for(e=e.firstContext;e!==null;){if(!it(e.context._currentValue,e.memoizedValue))return!0;e=e.next}return!1}function za(e){Ua=e,Bt=null,e=e.dependencies,e!==null&&(e.firstContext=null)}function He(e){return xc(Ua,e)}function Ki(e,t){return Ua===null&&za(e),xc(e,t)}function xc(e,t){var a=t._currentValue;if(t={context:t,memoizedValue:a,next:null},Bt===null){if(e===null)throw Error(h(308));Bt=t,e.dependencies={lanes:0,firstContext:t},e.flags|=524288}else Bt=Bt.next=t;return a}var Ep=typeof AbortController<"u"?AbortController:function(){var e=[],t=this.signal={aborted:!1,addEventListener:function(a,n){e.push(n)}};this.abort=function(){t.aborted=!0,e.forEach(function(a){return a()})}},Wp=v.unstable_scheduleCallback,Bp=v.unstable_NormalPriority,qe={$$typeof:B,Consumer:null,Provider:null,_currentValue:null,_currentValue2:null,_threadCount:0};function Uo(){return{controller:new Ep,data:new Map,refCount:0}}function Gn(e){e.refCount--,e.refCount===0&&Wp(Bp,function(){e.controller.abort()})}var Kn=null,zo=0,ln=0,cn=null;function Np(e,t){if(Kn===null){var a=Kn=[];zo=0,ln=Wr(),cn={status:"pending",value:void 0,then:function(n){a.push(n)}}}return zo++,t.then(kc,kc),t}function kc(){if(--zo===0&&Kn!==null){cn!==null&&(cn.status="fulfilled");var e=Kn;Kn=null,ln=0,cn=null;for(var t=0;t<e.length;t++)(0,e[t])()}}function Hp(e,t){var a=[],n={status:"pending",value:null,reason:null,then:function(i){a.push(i)}};return e.then(function(){n.status="fulfilled",n.value=t;for(var i=0;i<a.length;i++)(0,a[i])(t)},function(i){for(n.status="rejected",n.reason=i,i=0;i<a.length;i++)(0,a[i])(void 0)}),n}var Tc=w.S;w.S=function(e,t){ou=tt(),typeof t=="object"&&t!==null&&typeof t.then=="function"&&Np(e,t),Tc!==null&&Tc(e,t)};var Pa=d(null);function Po(){var e=Pa.current;return e!==null?e:fe.pooledCache}function Yi(e,t){t===null?q(Pa,Pa.current):q(Pa,t.pool)}function Sc(){var e=Po();return e===null?null:{parent:qe._currentValue,pool:e}}var dn=Error(h(460)),Eo=Error(h(474)),Vi=Error(h(542)),Zi={then:function(){}};function Ac(e){return e=e.status,e==="fulfilled"||e==="rejected"}function qc(e,t,a){switch(a=e[a],a===void 0?e.push(t):a!==t&&(t.then(zt,zt),t=a),t.status){case"fulfilled":return t.value;case"rejected":throw e=t.reason,Rc(e),e;default:if(typeof t.status=="string")t.then(zt,zt);else{if(e=fe,e!==null&&100<e.shellSuspendCounter)throw Error(h(482));e=t,e.status="pending",e.then(function(n){if(t.status==="pending"){var i=t;i.status="fulfilled",i.value=n}},function(n){if(t.status==="pending"){var i=t;i.status="rejected",i.reason=n}})}switch(t.status){case"fulfilled":return t.value;case"rejected":throw e=t.reason,Rc(e),e}throw Wa=t,dn}}function Ea(e){try{var t=e._init;return t(e._payload)}catch(a){throw a!==null&&typeof a=="object"&&typeof a.then=="function"?(Wa=a,dn):a}}var Wa=null;function Cc(){if(Wa===null)throw Error(h(459));var e=Wa;return Wa=null,e}function Rc(e){if(e===dn||e===Vi)throw Error(h(483))}var un=null,Yn=0;function Xi(e){var t=Yn;return Yn+=1,un===null&&(un=[]),qc(un,e,t)}function Vn(e,t){t=t.props.ref,e.ref=t!==void 0?t:null}function Fi(e,t){throw t.$$typeof===ie?Error(h(525)):(e=Object.prototype.toString.call(t),Error(h(31,e==="[object Object]"?"object with keys {"+Object.keys(t).join(", ")+"}":e)))}function Lc(e){function t(u,c){if(e){var p=u.deletions;p===null?(u.deletions=[c],u.flags|=16):p.push(c)}}function a(u,c){if(!e)return null;for(;c!==null;)t(u,c),c=c.sibling;return null}function n(u){for(var c=new Map;u!==null;)u.key!==null?c.set(u.key,u):c.set(u.index,u),u=u.sibling;return c}function i(u,c){return u=Et(u,c),u.index=0,u.sibling=null,u}function s(u,c,p){return u.index=p,e?(p=u.alternate,p!==null?(p=p.index,p<c?(u.flags|=67108866,c):p):(u.flags|=67108866,c)):(u.flags|=1048576,c)}function o(u){return e&&u.alternate===null&&(u.flags|=67108866),u}function r(u,c,p,x){return c===null||c.tag!==6?(c=To(p,u.mode,x),c.return=u,c):(c=i(c,p),c.return=u,c)}function l(u,c,p,x){var z=p.type;return z===re?b(u,c,p.props.children,x,p.key):c!==null&&(c.elementType===z||typeof z=="object"&&z!==null&&z.$$typeof===We&&Ea(z)===c.type)?(c=i(c,p.props),Vn(c,p),c.return=u,c):(c=ji(p.type,p.key,p.props,null,u.mode,x),Vn(c,p),c.return=u,c)}function m(u,c,p,x){return c===null||c.tag!==4||c.stateNode.containerInfo!==p.containerInfo||c.stateNode.implementation!==p.implementation?(c=So(p,u.mode,x),c.return=u,c):(c=i(c,p.children||[]),c.return=u,c)}function b(u,c,p,x,z){return c===null||c.tag!==7?(c=Ia(p,u.mode,x,z),c.return=u,c):(c=i(c,p),c.return=u,c)}function k(u,c,p){if(typeof c=="string"&&c!==""||typeof c=="number"||typeof c=="bigint")return c=To(""+c,u.mode,p),c.return=u,c;if(typeof c=="object"&&c!==null){switch(c.$$typeof){case Pe:return p=ji(c.type,c.key,c.props,null,u.mode,p),Vn(p,c),p.return=u,p;case we:return c=So(c,u.mode,p),c.return=u,c;case We:return c=Ea(c),k(u,c,p)}if(xt(c)||Ye(c))return c=Ia(c,u.mode,p,null),c.return=u,c;if(typeof c.then=="function")return k(u,Xi(c),p);if(c.$$typeof===B)return k(u,Ki(u,c),p);Fi(u,c)}return null}function f(u,c,p,x){var z=c!==null?c.key:null;if(typeof p=="string"&&p!==""||typeof p=="number"||typeof p=="bigint")return z!==null?null:r(u,c,""+p,x);if(typeof p=="object"&&p!==null){switch(p.$$typeof){case Pe:return p.key===z?l(u,c,p,x):null;case we:return p.key===z?m(u,c,p,x):null;case We:return p=Ea(p),f(u,c,p,x)}if(xt(p)||Ye(p))return z!==null?null:b(u,c,p,x,null);if(typeof p.then=="function")return f(u,c,Xi(p),x);if(p.$$typeof===B)return f(u,c,Ki(u,p),x);Fi(u,p)}return null}function g(u,c,p,x,z){if(typeof x=="string"&&x!==""||typeof x=="number"||typeof x=="bigint")return u=u.get(p)||null,r(c,u,""+x,z);if(typeof x=="object"&&x!==null){switch(x.$$typeof){case Pe:return u=u.get(x.key===null?p:x.key)||null,l(c,u,x,z);case we:return u=u.get(x.key===null?p:x.key)||null,m(c,u,x,z);case We:return x=Ea(x),g(u,c,p,x,z)}if(xt(x)||Ye(x))return u=u.get(p)||null,b(c,u,x,z,null);if(typeof x.then=="function")return g(u,c,p,Xi(x),z);if(x.$$typeof===B)return g(u,c,p,Ki(c,x),z);Fi(c,x)}return null}function R(u,c,p,x){for(var z=null,ee=null,D=c,G=c=0,J=null;D!==null&&G<p.length;G++){D.index>G?(J=D,D=null):J=D.sibling;var te=f(u,D,p[G],x);if(te===null){D===null&&(D=J);break}e&&D&&te.alternate===null&&t(u,D),c=s(te,c,G),ee===null?z=te:ee.sibling=te,ee=te,D=J}if(G===p.length)return a(u,D),$&&Wt(u,G),z;if(D===null){for(;G<p.length;G++)D=k(u,p[G],x),D!==null&&(c=s(D,c,G),ee===null?z=D:ee.sibling=D,ee=D);return $&&Wt(u,G),z}for(D=n(D);G<p.length;G++)J=g(D,u,G,p[G],x),J!==null&&(e&&J.alternate!==null&&D.delete(J.key===null?G:J.key),c=s(J,c,G),ee===null?z=J:ee.sibling=J,ee=J);return e&&D.forEach(function(wa){return t(u,wa)}),$&&Wt(u,G),z}function W(u,c,p,x){if(p==null)throw Error(h(151));for(var z=null,ee=null,D=c,G=c=0,J=null,te=p.next();D!==null&&!te.done;G++,te=p.next()){D.index>G?(J=D,D=null):J=D.sibling;var wa=f(u,D,te.value,x);if(wa===null){D===null&&(D=J);break}e&&D&&wa.alternate===null&&t(u,D),c=s(wa,c,G),ee===null?z=wa:ee.sibling=wa,ee=wa,D=J}if(te.done)return a(u,D),$&&Wt(u,G),z;if(D===null){for(;!te.done;G++,te=p.next())te=k(u,te.value,x),te!==null&&(c=s(te,c,G),ee===null?z=te:ee.sibling=te,ee=te);return $&&Wt(u,G),z}for(D=n(D);!te.done;G++,te=p.next())te=g(D,u,G,te.value,x),te!==null&&(e&&te.alternate!==null&&D.delete(te.key===null?G:te.key),c=s(te,c,G),ee===null?z=te:ee.sibling=te,ee=te);return e&&D.forEach(function(Xm){return t(u,Xm)}),$&&Wt(u,G),z}function he(u,c,p,x){if(typeof p=="object"&&p!==null&&p.type===re&&p.key===null&&(p=p.props.children),typeof p=="object"&&p!==null){switch(p.$$typeof){case Pe:e:{for(var z=p.key;c!==null;){if(c.key===z){if(z=p.type,z===re){if(c.tag===7){a(u,c.sibling),x=i(c,p.props.children),x.return=u,u=x;break e}}else if(c.elementType===z||typeof z=="object"&&z!==null&&z.$$typeof===We&&Ea(z)===c.type){a(u,c.sibling),x=i(c,p.props),Vn(x,p),x.return=u,u=x;break e}a(u,c);break}else t(u,c);c=c.sibling}p.type===re?(x=Ia(p.props.children,u.mode,x,p.key),x.return=u,u=x):(x=ji(p.type,p.key,p.props,null,u.mode,x),Vn(x,p),x.return=u,u=x)}return o(u);case we:e:{for(z=p.key;c!==null;){if(c.key===z)if(c.tag===4&&c.stateNode.containerInfo===p.containerInfo&&c.stateNode.implementation===p.implementation){a(u,c.sibling),x=i(c,p.children||[]),x.return=u,u=x;break e}else{a(u,c);break}else t(u,c);c=c.sibling}x=So(p,u.mode,x),x.return=u,u=x}return o(u);case We:return p=Ea(p),he(u,c,p,x)}if(xt(p))return R(u,c,p,x);if(Ye(p)){if(z=Ye(p),typeof z!="function")throw Error(h(150));return p=z.call(p),W(u,c,p,x)}if(typeof p.then=="function")return he(u,c,Xi(p),x);if(p.$$typeof===B)return he(u,c,Ki(u,p),x);Fi(u,p)}return typeof p=="string"&&p!==""||typeof p=="number"||typeof p=="bigint"?(p=""+p,c!==null&&c.tag===6?(a(u,c.sibling),x=i(c,p),x.return=u,u=x):(a(u,c),x=To(p,u.mode,x),x.return=u,u=x),o(u)):a(u,c)}return function(u,c,p,x){try{Yn=0;var z=he(u,c,p,x);return un=null,z}catch(D){if(D===dn||D===Vi)throw D;var ee=st(29,D,null,u.mode);return ee.lanes=x,ee.return=u,ee}finally{}}}var Ba=Lc(!0),Ic=Lc(!1),na=!1;function Wo(e){e.updateQueue={baseState:e.memoizedState,firstBaseUpdate:null,lastBaseUpdate:null,shared:{pending:null,lanes:0,hiddenCallbacks:null},callbacks:null}}function Bo(e,t){e=e.updateQueue,t.updateQueue===e&&(t.updateQueue={baseState:e.baseState,firstBaseUpdate:e.firstBaseUpdate,lastBaseUpdate:e.lastBaseUpdate,shared:e.shared,callbacks:null})}function ia(e){return{lane:e,tag:0,payload:null,callback:null,next:null}}function sa(e,t,a){var n=e.updateQueue;if(n===null)return null;if(n=n.shared,(ae&2)!==0){var i=n.pending;return i===null?t.next=t:(t.next=i.next,i.next=t),n.pending=t,t=Qi(e),pc(e,null,a),t}return Oi(e,n,t,a),Qi(e)}function Zn(e,t,a){if(t=t.updateQueue,t!==null&&(t=t.shared,(a&4194048)!==0)){var n=t.lanes;n&=e.pendingLanes,a|=n,t.lanes=a,xl(e,a)}}function No(e,t){var a=e.updateQueue,n=e.alternate;if(n!==null&&(n=n.updateQueue,a===n)){var i=null,s=null;if(a=a.firstBaseUpdate,a!==null){do{var o={lane:a.lane,tag:a.tag,payload:a.payload,callback:null,next:null};s===null?i=s=o:s=s.next=o,a=a.next}while(a!==null);s===null?i=s=t:s=s.next=t}else i=s=t;a={baseState:n.baseState,firstBaseUpdate:i,lastBaseUpdate:s,shared:n.shared,callbacks:n.callbacks},e.updateQueue=a;return}e=a.lastBaseUpdate,e===null?a.firstBaseUpdate=t:e.next=t,a.lastBaseUpdate=t}var Ho=!1;function Xn(){if(Ho){var e=cn;if(e!==null)throw e}}function Fn(e,t,a,n){Ho=!1;var i=e.updateQueue;na=!1;var s=i.firstBaseUpdate,o=i.lastBaseUpdate,r=i.shared.pending;if(r!==null){i.shared.pending=null;var l=r,m=l.next;l.next=null,o===null?s=m:o.next=m,o=l;var b=e.alternate;b!==null&&(b=b.updateQueue,r=b.lastBaseUpdate,r!==o&&(r===null?b.firstBaseUpdate=m:r.next=m,b.lastBaseUpdate=l))}if(s!==null){var k=i.baseState;o=0,b=m=l=null,r=s;do{var f=r.lane&-536870913,g=f!==r.lane;if(g?(F&f)===f:(n&f)===f){f!==0&&f===ln&&(Ho=!0),b!==null&&(b=b.next={lane:0,tag:r.tag,payload:r.payload,callback:null,next:null});e:{var R=e,W=r;f=t;var he=a;switch(W.tag){case 1:if(R=W.payload,typeof R=="function"){k=R.call(he,k,f);break e}k=R;break e;case 3:R.flags=R.flags&-65537|128;case 0:if(R=W.payload,f=typeof R=="function"?R.call(he,k,f):R,f==null)break e;k=U({},k,f);break e;case 2:na=!0}}f=r.callback,f!==null&&(e.flags|=64,g&&(e.flags|=8192),g=i.callbacks,g===null?i.callbacks=[f]:g.push(f))}else g={lane:f,tag:r.tag,payload:r.payload,callback:r.callback,next:null},b===null?(m=b=g,l=k):b=b.next=g,o|=f;if(r=r.next,r===null){if(r=i.shared.pending,r===null)break;g=r,r=g.next,g.next=null,i.lastBaseUpdate=g,i.shared.pending=null}}while(!0);b===null&&(l=k),i.baseState=l,i.firstBaseUpdate=m,i.lastBaseUpdate=b,s===null&&(i.shared.lanes=0),da|=o,e.lanes=o,e.memoizedState=k}}function Dc(e,t){if(typeof e!="function")throw Error(h(191,e));e.call(t)}function Uc(e,t){var a=e.callbacks;if(a!==null)for(e.callbacks=null,e=0;e<a.length;e++)Dc(a[e],t)}var hn=d(null),Ji=d(0);function zc(e,t){e=Yt,q(Ji,e),q(hn,t),Yt=e|t.baseLanes}function Mo(){q(Ji,Yt),q(hn,hn.current)}function Oo(){Yt=Ji.current,T(hn),T(Ji)}var ot=d(null),bt=null;function oa(e){var t=e.alternate;q(Se,Se.current&1),q(ot,e),bt===null&&(t===null||hn.current!==null||t.memoizedState!==null)&&(bt=e)}function Qo(e){q(Se,Se.current),q(ot,e),bt===null&&(bt=e)}function Pc(e){e.tag===22?(q(Se,Se.current),q(ot,e),bt===null&&(bt=e)):ra()}function ra(){q(Se,Se.current),q(ot,ot.current)}function rt(e){T(ot),bt===e&&(bt=null),T(Se)}var Se=d(0);function $i(e){for(var t=e;t!==null;){if(t.tag===13){var a=t.memoizedState;if(a!==null&&(a=a.dehydrated,a===null||Vr(a)||Zr(a)))return t}else if(t.tag===19&&(t.memoizedProps.revealOrder==="forwards"||t.memoizedProps.revealOrder==="backwards"||t.memoizedProps.revealOrder==="unstable_legacy-backwards"||t.memoizedProps.revealOrder==="together")){if((t.flags&128)!==0)return t}else if(t.child!==null){t.child.return=t,t=t.child;continue}if(t===e)break;for(;t.sibling===null;){if(t.return===null||t.return===e)return null;t=t.return}t.sibling.return=t.return,t=t.sibling}return null}var Ht=0,_=null,de=null,Ce=null,es=!1,pn=!1,Na=!1,ts=0,Jn=0,mn=null,Mp=0;function xe(){throw Error(h(321))}function jo(e,t){if(t===null)return!1;for(var a=0;a<t.length&&a<e.length;a++)if(!it(e[a],t[a]))return!1;return!0}function _o(e,t,a,n,i,s){return Ht=s,_=t,t.memoizedState=null,t.updateQueue=null,t.lanes=0,w.H=e===null||e.memoizedState===null?yd:sr,Na=!1,s=a(n,i),Na=!1,pn&&(s=Wc(t,a,n,i)),Ec(e),s}function Ec(e){w.H=ti;var t=de!==null&&de.next!==null;if(Ht=0,Ce=de=_=null,es=!1,Jn=0,mn=null,t)throw Error(h(300));e===null||Re||(e=e.dependencies,e!==null&&Gi(e)&&(Re=!0))}function Wc(e,t,a,n){_=e;var i=0;do{if(pn&&(mn=null),Jn=0,pn=!1,25<=i)throw Error(h(301));if(i+=1,Ce=de=null,e.updateQueue!=null){var s=e.updateQueue;s.lastEffect=null,s.events=null,s.stores=null,s.memoCache!=null&&(s.memoCache.index=0)}w.H=bd,s=t(a,n)}while(pn);return s}function Op(){var e=w.H,t=e.useState()[0];return t=typeof t.then=="function"?$n(t):t,e=e.useState()[0],(de!==null?de.memoizedState:null)!==e&&(_.flags|=1024),t}function Go(){var e=ts!==0;return ts=0,e}function Ko(e,t,a){t.updateQueue=e.updateQueue,t.flags&=-2053,e.lanes&=~a}function Yo(e){if(es){for(e=e.memoizedState;e!==null;){var t=e.queue;t!==null&&(t.pending=null),e=e.next}es=!1}Ht=0,Ce=de=_=null,pn=!1,Jn=ts=0,mn=null}function Ke(){var e={memoizedState:null,baseState:null,baseQueue:null,queue:null,next:null};return Ce===null?_.memoizedState=Ce=e:Ce=Ce.next=e,Ce}function Ae(){if(de===null){var e=_.alternate;e=e!==null?e.memoizedState:null}else e=de.next;var t=Ce===null?_.memoizedState:Ce.next;if(t!==null)Ce=t,de=e;else{if(e===null)throw _.alternate===null?Error(h(467)):Error(h(310));de=e,e={memoizedState:de.memoizedState,baseState:de.baseState,baseQueue:de.baseQueue,queue:de.queue,next:null},Ce===null?_.memoizedState=Ce=e:Ce=Ce.next=e}return Ce}function as(){return{lastEffect:null,events:null,stores:null,memoCache:null}}function $n(e){var t=Jn;return Jn+=1,mn===null&&(mn=[]),e=qc(mn,e,t),t=_,(Ce===null?t.memoizedState:Ce.next)===null&&(t=t.alternate,w.H=t===null||t.memoizedState===null?yd:sr),e}function ns(e){if(e!==null&&typeof e=="object"){if(typeof e.then=="function")return $n(e);if(e.$$typeof===B)return He(e)}throw Error(h(438,String(e)))}function Vo(e){var t=null,a=_.updateQueue;if(a!==null&&(t=a.memoCache),t==null){var n=_.alternate;n!==null&&(n=n.updateQueue,n!==null&&(n=n.memoCache,n!=null&&(t={data:n.data.map(function(i){return i.slice()}),index:0})))}if(t==null&&(t={data:[],index:0}),a===null&&(a=as(),_.updateQueue=a),a.memoCache=t,a=t.data[t.index],a===void 0)for(a=t.data[t.index]=Array(e),n=0;n<e;n++)a[n]=Qa;return t.index++,a}function Mt(e,t){return typeof t=="function"?t(e):t}function is(e){var t=Ae();return Zo(t,de,e)}function Zo(e,t,a){var n=e.queue;if(n===null)throw Error(h(311));n.lastRenderedReducer=a;var i=e.baseQueue,s=n.pending;if(s!==null){if(i!==null){var o=i.next;i.next=s.next,s.next=o}t.baseQueue=i=s,n.pending=null}if(s=e.baseState,i===null)e.memoizedState=s;else{t=i.next;var r=o=null,l=null,m=t,b=!1;do{var k=m.lane&-536870913;if(k!==m.lane?(F&k)===k:(Ht&k)===k){var f=m.revertLane;if(f===0)l!==null&&(l=l.next={lane:0,revertLane:0,gesture:null,action:m.action,hasEagerState:m.hasEagerState,eagerState:m.eagerState,next:null}),k===ln&&(b=!0);else if((Ht&f)===f){m=m.next,f===ln&&(b=!0);continue}else k={lane:0,revertLane:m.revertLane,gesture:null,action:m.action,hasEagerState:m.hasEagerState,eagerState:m.eagerState,next:null},l===null?(r=l=k,o=s):l=l.next=k,_.lanes|=f,da|=f;k=m.action,Na&&a(s,k),s=m.hasEagerState?m.eagerState:a(s,k)}else f={lane:k,revertLane:m.revertLane,gesture:m.gesture,action:m.action,hasEagerState:m.hasEagerState,eagerState:m.eagerState,next:null},l===null?(r=l=f,o=s):l=l.next=f,_.lanes|=k,da|=k;m=m.next}while(m!==null&&m!==t);if(l===null?o=s:l.next=r,!it(s,e.memoizedState)&&(Re=!0,b&&(a=cn,a!==null)))throw a;e.memoizedState=s,e.baseState=o,e.baseQueue=l,n.lastRenderedState=s}return i===null&&(n.lanes=0),[e.memoizedState,n.dispatch]}function Xo(e){var t=Ae(),a=t.queue;if(a===null)throw Error(h(311));a.lastRenderedReducer=e;var n=a.dispatch,i=a.pending,s=t.memoizedState;if(i!==null){a.pending=null;var o=i=i.next;do s=e(s,o.action),o=o.next;while(o!==i);it(s,t.memoizedState)||(Re=!0),t.memoizedState=s,t.baseQueue===null&&(t.baseState=s),a.lastRenderedState=s}return[s,n]}function Bc(e,t,a){var n=_,i=Ae(),s=$;if(s){if(a===void 0)throw Error(h(407));a=a()}else a=t();var o=!it((de||i).memoizedState,a);if(o&&(i.memoizedState=a,Re=!0),i=i.queue,$o(Mc.bind(null,n,i,e),[e]),i.getSnapshot!==t||o||Ce!==null&&Ce.memoizedState.tag&1){if(n.flags|=2048,fn(9,{destroy:void 0},Hc.bind(null,n,i,a,t),null),fe===null)throw Error(h(349));s||(Ht&127)!==0||Nc(n,t,a)}return a}function Nc(e,t,a){e.flags|=16384,e={getSnapshot:t,value:a},t=_.updateQueue,t===null?(t=as(),_.updateQueue=t,t.stores=[e]):(a=t.stores,a===null?t.stores=[e]:a.push(e))}function Hc(e,t,a,n){t.value=a,t.getSnapshot=n,Oc(t)&&Qc(e)}function Mc(e,t,a){return a(function(){Oc(t)&&Qc(e)})}function Oc(e){var t=e.getSnapshot;e=e.value;try{var a=t();return!it(e,a)}catch{return!0}}function Qc(e){var t=La(e,2);t!==null&&et(t,e,2)}function Fo(e){var t=Ke();if(typeof e=="function"){var a=e;if(e=a(),Na){Xt(!0);try{a()}finally{Xt(!1)}}}return t.memoizedState=t.baseState=e,t.queue={pending:null,lanes:0,dispatch:null,lastRenderedReducer:Mt,lastRenderedState:e},t}function jc(e,t,a,n){return e.baseState=a,Zo(e,de,typeof n=="function"?n:Mt)}function Qp(e,t,a,n,i){if(rs(e))throw Error(h(485));if(e=t.action,e!==null){var s={payload:i,action:e,next:null,isTransition:!0,status:"pending",value:null,reason:null,listeners:[],then:function(o){s.listeners.push(o)}};w.T!==null?a(!0):s.isTransition=!1,n(s),a=t.pending,a===null?(s.next=t.pending=s,_c(t,s)):(s.next=a.next,t.pending=a.next=s)}}function _c(e,t){var a=t.action,n=t.payload,i=e.state;if(t.isTransition){var s=w.T,o={};w.T=o;try{var r=a(i,n),l=w.S;l!==null&&l(o,r),Gc(e,t,r)}catch(m){Jo(e,t,m)}finally{s!==null&&o.types!==null&&(s.types=o.types),w.T=s}}else try{s=a(i,n),Gc(e,t,s)}catch(m){Jo(e,t,m)}}function Gc(e,t,a){a!==null&&typeof a=="object"&&typeof a.then=="function"?a.then(function(n){Kc(e,t,n)},function(n){return Jo(e,t,n)}):Kc(e,t,a)}function Kc(e,t,a){t.status="fulfilled",t.value=a,Yc(t),e.state=a,t=e.pending,t!==null&&(a=t.next,a===t?e.pending=null:(a=a.next,t.next=a,_c(e,a)))}function Jo(e,t,a){var n=e.pending;if(e.pending=null,n!==null){n=n.next;do t.status="rejected",t.reason=a,Yc(t),t=t.next;while(t!==n)}e.action=null}function Yc(e){e=e.listeners;for(var t=0;t<e.length;t++)(0,e[t])()}function Vc(e,t){return t}function Zc(e,t){if($){var a=fe.formState;if(a!==null){e:{var n=_;if($){if(ge){t:{for(var i=ge,s=yt;i.nodeType!==8;){if(!s){i=null;break t}if(i=vt(i.nextSibling),i===null){i=null;break t}}s=i.data,i=s==="F!"||s==="F"?i:null}if(i){ge=vt(i.nextSibling),n=i.data==="F!";break e}}ta(n)}n=!1}n&&(t=a[0])}}return a=Ke(),a.memoizedState=a.baseState=t,n={pending:null,lanes:0,dispatch:null,lastRenderedReducer:Vc,lastRenderedState:t},a.queue=n,a=md.bind(null,_,n),n.dispatch=a,n=Fo(!1),s=ir.bind(null,_,!1,n.queue),n=Ke(),i={state:t,dispatch:null,action:e,pending:null},n.queue=i,a=Qp.bind(null,_,i,s,a),i.dispatch=a,n.memoizedState=e,[t,a,!1]}function Xc(e){var t=Ae();return Fc(t,de,e)}function Fc(e,t,a){if(t=Zo(e,t,Vc)[0],e=is(Mt)[0],typeof t=="object"&&t!==null&&typeof t.then=="function")try{var n=$n(t)}catch(o){throw o===dn?Vi:o}else n=t;t=Ae();var i=t.queue,s=i.dispatch;return a!==t.memoizedState&&(_.flags|=2048,fn(9,{destroy:void 0},jp.bind(null,i,a),null)),[n,s,e]}function jp(e,t){e.action=t}function Jc(e){var t=Ae(),a=de;if(a!==null)return Fc(t,a,e);Ae(),t=t.memoizedState,a=Ae();var n=a.queue.dispatch;return a.memoizedState=e,[t,n,!1]}function fn(e,t,a,n){return e={tag:e,create:a,deps:n,inst:t,next:null},t=_.updateQueue,t===null&&(t=as(),_.updateQueue=t),a=t.lastEffect,a===null?t.lastEffect=e.next=e:(n=a.next,a.next=e,e.next=n,t.lastEffect=e),e}function $c(){return Ae().memoizedState}function ss(e,t,a,n){var i=Ke();_.flags|=e,i.memoizedState=fn(1|t,{destroy:void 0},a,n===void 0?null:n)}function os(e,t,a,n){var i=Ae();n=n===void 0?null:n;var s=i.memoizedState.inst;de!==null&&n!==null&&jo(n,de.memoizedState.deps)?i.memoizedState=fn(t,s,a,n):(_.flags|=e,i.memoizedState=fn(1|t,s,a,n))}function ed(e,t){ss(8390656,8,e,t)}function $o(e,t){os(2048,8,e,t)}function _p(e){_.flags|=4;var t=_.updateQueue;if(t===null)t=as(),_.updateQueue=t,t.events=[e];else{var a=t.events;a===null?t.events=[e]:a.push(e)}}function td(e){var t=Ae().memoizedState;return _p({ref:t,nextImpl:e}),function(){if((ae&2)!==0)throw Error(h(440));return t.impl.apply(void 0,arguments)}}function ad(e,t){return os(4,2,e,t)}function nd(e,t){return os(4,4,e,t)}function id(e,t){if(typeof t=="function"){e=e();var a=t(e);return function(){typeof a=="function"?a():t(null)}}if(t!=null)return e=e(),t.current=e,function(){t.current=null}}function sd(e,t,a){a=a!=null?a.concat([e]):null,os(4,4,id.bind(null,t,e),a)}function er(){}function od(e,t){var a=Ae();t=t===void 0?null:t;var n=a.memoizedState;return t!==null&&jo(t,n[1])?n[0]:(a.memoizedState=[e,t],e)}function rd(e,t){var a=Ae();t=t===void 0?null:t;var n=a.memoizedState;if(t!==null&&jo(t,n[1]))return n[0];if(n=e(),Na){Xt(!0);try{e()}finally{Xt(!1)}}return a.memoizedState=[n,t],n}function tr(e,t,a){return a===void 0||(Ht&1073741824)!==0&&(F&261930)===0?e.memoizedState=t:(e.memoizedState=a,e=lu(),_.lanes|=e,da|=e,a)}function ld(e,t,a,n){return it(a,t)?a:hn.current!==null?(e=tr(e,a,n),it(e,t)||(Re=!0),e):(Ht&42)===0||(Ht&1073741824)!==0&&(F&261930)===0?(Re=!0,e.memoizedState=a):(e=lu(),_.lanes|=e,da|=e,t)}function cd(e,t,a,n,i){var s=A.p;A.p=s!==0&&8>s?s:8;var o=w.T,r={};w.T=r,ir(e,!1,t,a);try{var l=i(),m=w.S;if(m!==null&&m(r,l),l!==null&&typeof l=="object"&&typeof l.then=="function"){var b=Hp(l,n);ei(e,t,b,dt(e))}else ei(e,t,n,dt(e))}catch(k){ei(e,t,{then:function(){},status:"rejected",reason:k},dt())}finally{A.p=s,o!==null&&r.types!==null&&(o.types=r.types),w.T=o}}function Gp(){}function ar(e,t,a,n){if(e.tag!==5)throw Error(h(476));var i=dd(e).queue;cd(e,i,t,H,a===null?Gp:function(){return ud(e),a(n)})}function dd(e){var t=e.memoizedState;if(t!==null)return t;t={memoizedState:H,baseState:H,baseQueue:null,queue:{pending:null,lanes:0,dispatch:null,lastRenderedReducer:Mt,lastRenderedState:H},next:null};var a={};return t.next={memoizedState:a,baseState:a,baseQueue:null,queue:{pending:null,lanes:0,dispatch:null,lastRenderedReducer:Mt,lastRenderedState:a},next:null},e.memoizedState=t,e=e.alternate,e!==null&&(e.memoizedState=t),t}function ud(e){var t=dd(e);t.next===null&&(t=e.alternate.memoizedState),ei(e,t.next.queue,{},dt())}function nr(){return He(yi)}function hd(){return Ae().memoizedState}function pd(){return Ae().memoizedState}function Kp(e){for(var t=e.return;t!==null;){switch(t.tag){case 24:case 3:var a=dt();e=ia(a);var n=sa(t,e,a);n!==null&&(et(n,t,a),Zn(n,t,a)),t={cache:Uo()},e.payload=t;return}t=t.return}}function Yp(e,t,a){var n=dt();a={lane:n,revertLane:0,gesture:null,action:a,hasEagerState:!1,eagerState:null,next:null},rs(e)?fd(t,a):(a=xo(e,t,a,n),a!==null&&(et(a,e,n),gd(a,t,n)))}function md(e,t,a){var n=dt();ei(e,t,a,n)}function ei(e,t,a,n){var i={lane:n,revertLane:0,gesture:null,action:a,hasEagerState:!1,eagerState:null,next:null};if(rs(e))fd(t,i);else{var s=e.alternate;if(e.lanes===0&&(s===null||s.lanes===0)&&(s=t.lastRenderedReducer,s!==null))try{var o=t.lastRenderedState,r=s(o,a);if(i.hasEagerState=!0,i.eagerState=r,it(r,o))return Oi(e,t,i,0),fe===null&&Mi(),!1}catch{}finally{}if(a=xo(e,t,i,n),a!==null)return et(a,e,n),gd(a,t,n),!0}return!1}function ir(e,t,a,n){if(n={lane:2,revertLane:Wr(),gesture:null,action:n,hasEagerState:!1,eagerState:null,next:null},rs(e)){if(t)throw Error(h(479))}else t=xo(e,a,n,2),t!==null&&et(t,e,2)}function rs(e){var t=e.alternate;return e===_||t!==null&&t===_}function fd(e,t){pn=es=!0;var a=e.pending;a===null?t.next=t:(t.next=a.next,a.next=t),e.pending=t}function gd(e,t,a){if((a&4194048)!==0){var n=t.lanes;n&=e.pendingLanes,a|=n,t.lanes=a,xl(e,a)}}var ti={readContext:He,use:ns,useCallback:xe,useContext:xe,useEffect:xe,useImperativeHandle:xe,useLayoutEffect:xe,useInsertionEffect:xe,useMemo:xe,useReducer:xe,useRef:xe,useState:xe,useDebugValue:xe,useDeferredValue:xe,useTransition:xe,useSyncExternalStore:xe,useId:xe,useHostTransitionStatus:xe,useFormState:xe,useActionState:xe,useOptimistic:xe,useMemoCache:xe,useCacheRefresh:xe};ti.useEffectEvent=xe;var yd={readContext:He,use:ns,useCallback:function(e,t){return Ke().memoizedState=[e,t===void 0?null:t],e},useContext:He,useEffect:ed,useImperativeHandle:function(e,t,a){a=a!=null?a.concat([e]):null,ss(4194308,4,id.bind(null,t,e),a)},useLayoutEffect:function(e,t){return ss(4194308,4,e,t)},useInsertionEffect:function(e,t){ss(4,2,e,t)},useMemo:function(e,t){var a=Ke();t=t===void 0?null:t;var n=e();if(Na){Xt(!0);try{e()}finally{Xt(!1)}}return a.memoizedState=[n,t],n},useReducer:function(e,t,a){var n=Ke();if(a!==void 0){var i=a(t);if(Na){Xt(!0);try{a(t)}finally{Xt(!1)}}}else i=t;return n.memoizedState=n.baseState=i,e={pending:null,lanes:0,dispatch:null,lastRenderedReducer:e,lastRenderedState:i},n.queue=e,e=e.dispatch=Yp.bind(null,_,e),[n.memoizedState,e]},useRef:function(e){var t=Ke();return e={current:e},t.memoizedState=e},useState:function(e){e=Fo(e);var t=e.queue,a=md.bind(null,_,t);return t.dispatch=a,[e.memoizedState,a]},useDebugValue:er,useDeferredValue:function(e,t){var a=Ke();return tr(a,e,t)},useTransition:function(){var e=Fo(!1);return e=cd.bind(null,_,e.queue,!0,!1),Ke().memoizedState=e,[!1,e]},useSyncExternalStore:function(e,t,a){var n=_,i=Ke();if($){if(a===void 0)throw Error(h(407));a=a()}else{if(a=t(),fe===null)throw Error(h(349));(F&127)!==0||Nc(n,t,a)}i.memoizedState=a;var s={value:a,getSnapshot:t};return i.queue=s,ed(Mc.bind(null,n,s,e),[e]),n.flags|=2048,fn(9,{destroy:void 0},Hc.bind(null,n,s,a,t),null),a},useId:function(){var e=Ke(),t=fe.identifierPrefix;if($){var a=Rt,n=Ct;a=(n&~(1<<32-nt(n)-1)).toString(32)+a,t="_"+t+"R_"+a,a=ts++,0<a&&(t+="H"+a.toString(32)),t+="_"}else a=Mp++,t="_"+t+"r_"+a.toString(32)+"_";return e.memoizedState=t},useHostTransitionStatus:nr,useFormState:Zc,useActionState:Zc,useOptimistic:function(e){var t=Ke();t.memoizedState=t.baseState=e;var a={pending:null,lanes:0,dispatch:null,lastRenderedReducer:null,lastRenderedState:null};return t.queue=a,t=ir.bind(null,_,!0,a),a.dispatch=t,[e,t]},useMemoCache:Vo,useCacheRefresh:function(){return Ke().memoizedState=Kp.bind(null,_)},useEffectEvent:function(e){var t=Ke(),a={impl:e};return t.memoizedState=a,function(){if((ae&2)!==0)throw Error(h(440));return a.impl.apply(void 0,arguments)}}},sr={readContext:He,use:ns,useCallback:od,useContext:He,useEffect:$o,useImperativeHandle:sd,useInsertionEffect:ad,useLayoutEffect:nd,useMemo:rd,useReducer:is,useRef:$c,useState:function(){return is(Mt)},useDebugValue:er,useDeferredValue:function(e,t){var a=Ae();return ld(a,de.memoizedState,e,t)},useTransition:function(){var e=is(Mt)[0],t=Ae().memoizedState;return[typeof e=="boolean"?e:$n(e),t]},useSyncExternalStore:Bc,useId:hd,useHostTransitionStatus:nr,useFormState:Xc,useActionState:Xc,useOptimistic:function(e,t){var a=Ae();return jc(a,de,e,t)},useMemoCache:Vo,useCacheRefresh:pd};sr.useEffectEvent=td;var bd={readContext:He,use:ns,useCallback:od,useContext:He,useEffect:$o,useImperativeHandle:sd,useInsertionEffect:ad,useLayoutEffect:nd,useMemo:rd,useReducer:Xo,useRef:$c,useState:function(){return Xo(Mt)},useDebugValue:er,useDeferredValue:function(e,t){var a=Ae();return de===null?tr(a,e,t):ld(a,de.memoizedState,e,t)},useTransition:function(){var e=Xo(Mt)[0],t=Ae().memoizedState;return[typeof e=="boolean"?e:$n(e),t]},useSyncExternalStore:Bc,useId:hd,useHostTransitionStatus:nr,useFormState:Jc,useActionState:Jc,useOptimistic:function(e,t){var a=Ae();return de!==null?jc(a,de,e,t):(a.baseState=e,[e,a.queue.dispatch])},useMemoCache:Vo,useCacheRefresh:pd};bd.useEffectEvent=td;function or(e,t,a,n){t=e.memoizedState,a=a(n,t),a=a==null?t:U({},t,a),e.memoizedState=a,e.lanes===0&&(e.updateQueue.baseState=a)}var rr={enqueueSetState:function(e,t,a){e=e._reactInternals;var n=dt(),i=ia(n);i.payload=t,a!=null&&(i.callback=a),t=sa(e,i,n),t!==null&&(et(t,e,n),Zn(t,e,n))},enqueueReplaceState:function(e,t,a){e=e._reactInternals;var n=dt(),i=ia(n);i.tag=1,i.payload=t,a!=null&&(i.callback=a),t=sa(e,i,n),t!==null&&(et(t,e,n),Zn(t,e,n))},enqueueForceUpdate:function(e,t){e=e._reactInternals;var a=dt(),n=ia(a);n.tag=2,t!=null&&(n.callback=t),t=sa(e,n,a),t!==null&&(et(t,e,a),Zn(t,e,a))}};function vd(e,t,a,n,i,s,o){return e=e.stateNode,typeof e.shouldComponentUpdate=="function"?e.shouldComponentUpdate(n,s,o):t.prototype&&t.prototype.isPureReactComponent?!On(a,n)||!On(i,s):!0}function wd(e,t,a,n){e=t.state,typeof t.componentWillReceiveProps=="function"&&t.componentWillReceiveProps(a,n),typeof t.UNSAFE_componentWillReceiveProps=="function"&&t.UNSAFE_componentWillReceiveProps(a,n),t.state!==e&&rr.enqueueReplaceState(t,t.state,null)}function Ha(e,t){var a=t;if("ref"in t){a={};for(var n in t)n!=="ref"&&(a[n]=t[n])}if(e=e.defaultProps){a===t&&(a=U({},a));for(var i in e)a[i]===void 0&&(a[i]=e[i])}return a}function xd(e){Hi(e)}function kd(e){console.error(e)}function Td(e){Hi(e)}function ls(e,t){try{var a=e.onUncaughtError;a(t.value,{componentStack:t.stack})}catch(n){setTimeout(function(){throw n})}}function Sd(e,t,a){try{var n=e.onCaughtError;n(a.value,{componentStack:a.stack,errorBoundary:t.tag===1?t.stateNode:null})}catch(i){setTimeout(function(){throw i})}}function lr(e,t,a){return a=ia(a),a.tag=3,a.payload={element:null},a.callback=function(){ls(e,t)},a}function Ad(e){return e=ia(e),e.tag=3,e}function qd(e,t,a,n){var i=a.type.getDerivedStateFromError;if(typeof i=="function"){var s=n.value;e.payload=function(){return i(s)},e.callback=function(){Sd(t,a,n)}}var o=a.stateNode;o!==null&&typeof o.componentDidCatch=="function"&&(e.callback=function(){Sd(t,a,n),typeof i!="function"&&(ua===null?ua=new Set([this]):ua.add(this));var r=n.stack;this.componentDidCatch(n.value,{componentStack:r!==null?r:""})})}function Vp(e,t,a,n,i){if(a.flags|=32768,n!==null&&typeof n=="object"&&typeof n.then=="function"){if(t=a.alternate,t!==null&&rn(t,a,i,!0),a=ot.current,a!==null){switch(a.tag){case 31:case 13:return bt===null?ws():a.alternate===null&&ke===0&&(ke=3),a.flags&=-257,a.flags|=65536,a.lanes=i,n===Zi?a.flags|=16384:(t=a.updateQueue,t===null?a.updateQueue=new Set([n]):t.add(n),zr(e,n,i)),!1;case 22:return a.flags|=65536,n===Zi?a.flags|=16384:(t=a.updateQueue,t===null?(t={transitions:null,markerInstances:null,retryQueue:new Set([n])},a.updateQueue=t):(a=t.retryQueue,a===null?t.retryQueue=new Set([n]):a.add(n)),zr(e,n,i)),!1}throw Error(h(435,a.tag))}return zr(e,n,i),ws(),!1}if($)return t=ot.current,t!==null?((t.flags&65536)===0&&(t.flags|=256),t.flags|=65536,t.lanes=i,n!==Co&&(e=Error(h(422),{cause:n}),_n(mt(e,a)))):(n!==Co&&(t=Error(h(423),{cause:n}),_n(mt(t,a))),e=e.current.alternate,e.flags|=65536,i&=-i,e.lanes|=i,n=mt(n,a),i=lr(e.stateNode,n,i),No(e,i),ke!==4&&(ke=2)),!1;var s=Error(h(520),{cause:n});if(s=mt(s,a),ci===null?ci=[s]:ci.push(s),ke!==4&&(ke=2),t===null)return!0;n=mt(n,a),a=t;do{switch(a.tag){case 3:return a.flags|=65536,e=i&-i,a.lanes|=e,e=lr(a.stateNode,n,e),No(a,e),!1;case 1:if(t=a.type,s=a.stateNode,(a.flags&128)===0&&(typeof t.getDerivedStateFromError=="function"||s!==null&&typeof s.componentDidCatch=="function"&&(ua===null||!ua.has(s))))return a.flags|=65536,i&=-i,a.lanes|=i,i=Ad(i),qd(i,e,a,n),No(a,i),!1}a=a.return}while(a!==null);return!1}var cr=Error(h(461)),Re=!1;function Me(e,t,a,n){t.child=e===null?Ic(t,null,a,n):Ba(t,e.child,a,n)}function Cd(e,t,a,n,i){a=a.render;var s=t.ref;if("ref"in n){var o={};for(var r in n)r!=="ref"&&(o[r]=n[r])}else o=n;return za(t),n=_o(e,t,a,o,s,i),r=Go(),e!==null&&!Re?(Ko(e,t,i),Ot(e,t,i)):($&&r&&Ao(t),t.flags|=1,Me(e,t,n,i),t.child)}function Rd(e,t,a,n,i){if(e===null){var s=a.type;return typeof s=="function"&&!ko(s)&&s.defaultProps===void 0&&a.compare===null?(t.tag=15,t.type=s,Ld(e,t,s,n,i)):(e=ji(a.type,null,n,t,t.mode,i),e.ref=t.ref,e.return=t,t.child=e)}if(s=e.child,!yr(e,i)){var o=s.memoizedProps;if(a=a.compare,a=a!==null?a:On,a(o,n)&&e.ref===t.ref)return Ot(e,t,i)}return t.flags|=1,e=Et(s,n),e.ref=t.ref,e.return=t,t.child=e}function Ld(e,t,a,n,i){if(e!==null){var s=e.memoizedProps;if(On(s,n)&&e.ref===t.ref)if(Re=!1,t.pendingProps=n=s,yr(e,i))(e.flags&131072)!==0&&(Re=!0);else return t.lanes=e.lanes,Ot(e,t,i)}return dr(e,t,a,n,i)}function Id(e,t,a,n){var i=n.children,s=e!==null?e.memoizedState:null;if(e===null&&t.stateNode===null&&(t.stateNode={_visibility:1,_pendingMarkers:null,_retryCache:null,_transitions:null}),n.mode==="hidden"){if((t.flags&128)!==0){if(s=s!==null?s.baseLanes|a:a,e!==null){for(n=t.child=e.child,i=0;n!==null;)i=i|n.lanes|n.childLanes,n=n.sibling;n=i&~s}else n=0,t.child=null;return Dd(e,t,s,a,n)}if((a&536870912)!==0)t.memoizedState={baseLanes:0,cachePool:null},e!==null&&Yi(t,s!==null?s.cachePool:null),s!==null?zc(t,s):Mo(),Pc(t);else return n=t.lanes=536870912,Dd(e,t,s!==null?s.baseLanes|a:a,a,n)}else s!==null?(Yi(t,s.cachePool),zc(t,s),ra(),t.memoizedState=null):(e!==null&&Yi(t,null),Mo(),ra());return Me(e,t,i,a),t.child}function ai(e,t){return e!==null&&e.tag===22||t.stateNode!==null||(t.stateNode={_visibility:1,_pendingMarkers:null,_retryCache:null,_transitions:null}),t.sibling}function Dd(e,t,a,n,i){var s=Po();return s=s===null?null:{parent:qe._currentValue,pool:s},t.memoizedState={baseLanes:a,cachePool:s},e!==null&&Yi(t,null),Mo(),Pc(t),e!==null&&rn(e,t,n,!0),t.childLanes=i,null}function cs(e,t){return t=us({mode:t.mode,children:t.children},e.mode),t.ref=e.ref,e.child=t,t.return=e,t}function Ud(e,t,a){return Ba(t,e.child,null,a),e=cs(t,t.pendingProps),e.flags|=2,rt(t),t.memoizedState=null,e}function Zp(e,t,a){var n=t.pendingProps,i=(t.flags&128)!==0;if(t.flags&=-129,e===null){if($){if(n.mode==="hidden")return e=cs(t,n),t.lanes=536870912,ai(null,e);if(Qo(t),(e=ge)?(e=_u(e,yt),e=e!==null&&e.data==="&"?e:null,e!==null&&(t.memoizedState={dehydrated:e,treeContext:$t!==null?{id:Ct,overflow:Rt}:null,retryLane:536870912,hydrationErrors:null},a=fc(e),a.return=t,t.child=a,Ne=t,ge=null)):e=null,e===null)throw ta(t);return t.lanes=536870912,null}return cs(t,n)}var s=e.memoizedState;if(s!==null){var o=s.dehydrated;if(Qo(t),i)if(t.flags&256)t.flags&=-257,t=Ud(e,t,a);else if(t.memoizedState!==null)t.child=e.child,t.flags|=128,t=null;else throw Error(h(558));else if(Re||rn(e,t,a,!1),i=(a&e.childLanes)!==0,Re||i){if(n=fe,n!==null&&(o=kl(n,a),o!==0&&o!==s.retryLane))throw s.retryLane=o,La(e,o),et(n,e,o),cr;ws(),t=Ud(e,t,a)}else e=s.treeContext,ge=vt(o.nextSibling),Ne=t,$=!0,ea=null,yt=!1,e!==null&&bc(t,e),t=cs(t,n),t.flags|=4096;return t}return e=Et(e.child,{mode:n.mode,children:n.children}),e.ref=t.ref,t.child=e,e.return=t,e}function ds(e,t){var a=t.ref;if(a===null)e!==null&&e.ref!==null&&(t.flags|=4194816);else{if(typeof a!="function"&&typeof a!="object")throw Error(h(284));(e===null||e.ref!==a)&&(t.flags|=4194816)}}function dr(e,t,a,n,i){return za(t),a=_o(e,t,a,n,void 0,i),n=Go(),e!==null&&!Re?(Ko(e,t,i),Ot(e,t,i)):($&&n&&Ao(t),t.flags|=1,Me(e,t,a,i),t.child)}function zd(e,t,a,n,i,s){return za(t),t.updateQueue=null,a=Wc(t,n,a,i),Ec(e),n=Go(),e!==null&&!Re?(Ko(e,t,s),Ot(e,t,s)):($&&n&&Ao(t),t.flags|=1,Me(e,t,a,s),t.child)}function Pd(e,t,a,n,i){if(za(t),t.stateNode===null){var s=an,o=a.contextType;typeof o=="object"&&o!==null&&(s=He(o)),s=new a(n,s),t.memoizedState=s.state!==null&&s.state!==void 0?s.state:null,s.updater=rr,t.stateNode=s,s._reactInternals=t,s=t.stateNode,s.props=n,s.state=t.memoizedState,s.refs={},Wo(t),o=a.contextType,s.context=typeof o=="object"&&o!==null?He(o):an,s.state=t.memoizedState,o=a.getDerivedStateFromProps,typeof o=="function"&&(or(t,a,o,n),s.state=t.memoizedState),typeof a.getDerivedStateFromProps=="function"||typeof s.getSnapshotBeforeUpdate=="function"||typeof s.UNSAFE_componentWillMount!="function"&&typeof s.componentWillMount!="function"||(o=s.state,typeof s.componentWillMount=="function"&&s.componentWillMount(),typeof s.UNSAFE_componentWillMount=="function"&&s.UNSAFE_componentWillMount(),o!==s.state&&rr.enqueueReplaceState(s,s.state,null),Fn(t,n,s,i),Xn(),s.state=t.memoizedState),typeof s.componentDidMount=="function"&&(t.flags|=4194308),n=!0}else if(e===null){s=t.stateNode;var r=t.memoizedProps,l=Ha(a,r);s.props=l;var m=s.context,b=a.contextType;o=an,typeof b=="object"&&b!==null&&(o=He(b));var k=a.getDerivedStateFromProps;b=typeof k=="function"||typeof s.getSnapshotBeforeUpdate=="function",r=t.pendingProps!==r,b||typeof s.UNSAFE_componentWillReceiveProps!="function"&&typeof s.componentWillReceiveProps!="function"||(r||m!==o)&&wd(t,s,n,o),na=!1;var f=t.memoizedState;s.state=f,Fn(t,n,s,i),Xn(),m=t.memoizedState,r||f!==m||na?(typeof k=="function"&&(or(t,a,k,n),m=t.memoizedState),(l=na||vd(t,a,l,n,f,m,o))?(b||typeof s.UNSAFE_componentWillMount!="function"&&typeof s.componentWillMount!="function"||(typeof s.componentWillMount=="function"&&s.componentWillMount(),typeof s.UNSAFE_componentWillMount=="function"&&s.UNSAFE_componentWillMount()),typeof s.componentDidMount=="function"&&(t.flags|=4194308)):(typeof s.componentDidMount=="function"&&(t.flags|=4194308),t.memoizedProps=n,t.memoizedState=m),s.props=n,s.state=m,s.context=o,n=l):(typeof s.componentDidMount=="function"&&(t.flags|=4194308),n=!1)}else{s=t.stateNode,Bo(e,t),o=t.memoizedProps,b=Ha(a,o),s.props=b,k=t.pendingProps,f=s.context,m=a.contextType,l=an,typeof m=="object"&&m!==null&&(l=He(m)),r=a.getDerivedStateFromProps,(m=typeof r=="function"||typeof s.getSnapshotBeforeUpdate=="function")||typeof s.UNSAFE_componentWillReceiveProps!="function"&&typeof s.componentWillReceiveProps!="function"||(o!==k||f!==l)&&wd(t,s,n,l),na=!1,f=t.memoizedState,s.state=f,Fn(t,n,s,i),Xn();var g=t.memoizedState;o!==k||f!==g||na||e!==null&&e.dependencies!==null&&Gi(e.dependencies)?(typeof r=="function"&&(or(t,a,r,n),g=t.memoizedState),(b=na||vd(t,a,b,n,f,g,l)||e!==null&&e.dependencies!==null&&Gi(e.dependencies))?(m||typeof s.UNSAFE_componentWillUpdate!="function"&&typeof s.componentWillUpdate!="function"||(typeof s.componentWillUpdate=="function"&&s.componentWillUpdate(n,g,l),typeof s.UNSAFE_componentWillUpdate=="function"&&s.UNSAFE_componentWillUpdate(n,g,l)),typeof s.componentDidUpdate=="function"&&(t.flags|=4),typeof s.getSnapshotBeforeUpdate=="function"&&(t.flags|=1024)):(typeof s.componentDidUpdate!="function"||o===e.memoizedProps&&f===e.memoizedState||(t.flags|=4),typeof s.getSnapshotBeforeUpdate!="function"||o===e.memoizedProps&&f===e.memoizedState||(t.flags|=1024),t.memoizedProps=n,t.memoizedState=g),s.props=n,s.state=g,s.context=l,n=b):(typeof s.componentDidUpdate!="function"||o===e.memoizedProps&&f===e.memoizedState||(t.flags|=4),typeof s.getSnapshotBeforeUpdate!="function"||o===e.memoizedProps&&f===e.memoizedState||(t.flags|=1024),n=!1)}return s=n,ds(e,t),n=(t.flags&128)!==0,s||n?(s=t.stateNode,a=n&&typeof a.getDerivedStateFromError!="function"?null:s.render(),t.flags|=1,e!==null&&n?(t.child=Ba(t,e.child,null,i),t.child=Ba(t,null,a,i)):Me(e,t,a,i),t.memoizedState=s.state,e=t.child):e=Ot(e,t,i),e}function Ed(e,t,a,n){return Da(),t.flags|=256,Me(e,t,a,n),t.child}var ur={dehydrated:null,treeContext:null,retryLane:0,hydrationErrors:null};function hr(e){return{baseLanes:e,cachePool:Sc()}}function pr(e,t,a){return e=e!==null?e.childLanes&~a:0,t&&(e|=ct),e}function Wd(e,t,a){var n=t.pendingProps,i=!1,s=(t.flags&128)!==0,o;if((o=s)||(o=e!==null&&e.memoizedState===null?!1:(Se.current&2)!==0),o&&(i=!0,t.flags&=-129),o=(t.flags&32)!==0,t.flags&=-33,e===null){if($){if(i?oa(t):ra(),(e=ge)?(e=_u(e,yt),e=e!==null&&e.data!=="&"?e:null,e!==null&&(t.memoizedState={dehydrated:e,treeContext:$t!==null?{id:Ct,overflow:Rt}:null,retryLane:536870912,hydrationErrors:null},a=fc(e),a.return=t,t.child=a,Ne=t,ge=null)):e=null,e===null)throw ta(t);return Zr(e)?t.lanes=32:t.lanes=536870912,null}var r=n.children;return n=n.fallback,i?(ra(),i=t.mode,r=us({mode:"hidden",children:r},i),n=Ia(n,i,a,null),r.return=t,n.return=t,r.sibling=n,t.child=r,n=t.child,n.memoizedState=hr(a),n.childLanes=pr(e,o,a),t.memoizedState=ur,ai(null,n)):(oa(t),mr(t,r))}var l=e.memoizedState;if(l!==null&&(r=l.dehydrated,r!==null)){if(s)t.flags&256?(oa(t),t.flags&=-257,t=fr(e,t,a)):t.memoizedState!==null?(ra(),t.child=e.child,t.flags|=128,t=null):(ra(),r=n.fallback,i=t.mode,n=us({mode:"visible",children:n.children},i),r=Ia(r,i,a,null),r.flags|=2,n.return=t,r.return=t,n.sibling=r,t.child=n,Ba(t,e.child,null,a),n=t.child,n.memoizedState=hr(a),n.childLanes=pr(e,o,a),t.memoizedState=ur,t=ai(null,n));else if(oa(t),Zr(r)){if(o=r.nextSibling&&r.nextSibling.dataset,o)var m=o.dgst;o=m,n=Error(h(419)),n.stack="",n.digest=o,_n({value:n,source:null,stack:null}),t=fr(e,t,a)}else if(Re||rn(e,t,a,!1),o=(a&e.childLanes)!==0,Re||o){if(o=fe,o!==null&&(n=kl(o,a),n!==0&&n!==l.retryLane))throw l.retryLane=n,La(e,n),et(o,e,n),cr;Vr(r)||ws(),t=fr(e,t,a)}else Vr(r)?(t.flags|=192,t.child=e.child,t=null):(e=l.treeContext,ge=vt(r.nextSibling),Ne=t,$=!0,ea=null,yt=!1,e!==null&&bc(t,e),t=mr(t,n.children),t.flags|=4096);return t}return i?(ra(),r=n.fallback,i=t.mode,l=e.child,m=l.sibling,n=Et(l,{mode:"hidden",children:n.children}),n.subtreeFlags=l.subtreeFlags&65011712,m!==null?r=Et(m,r):(r=Ia(r,i,a,null),r.flags|=2),r.return=t,n.return=t,n.sibling=r,t.child=n,ai(null,n),n=t.child,r=e.child.memoizedState,r===null?r=hr(a):(i=r.cachePool,i!==null?(l=qe._currentValue,i=i.parent!==l?{parent:l,pool:l}:i):i=Sc(),r={baseLanes:r.baseLanes|a,cachePool:i}),n.memoizedState=r,n.childLanes=pr(e,o,a),t.memoizedState=ur,ai(e.child,n)):(oa(t),a=e.child,e=a.sibling,a=Et(a,{mode:"visible",children:n.children}),a.return=t,a.sibling=null,e!==null&&(o=t.deletions,o===null?(t.deletions=[e],t.flags|=16):o.push(e)),t.child=a,t.memoizedState=null,a)}function mr(e,t){return t=us({mode:"visible",children:t},e.mode),t.return=e,e.child=t}function us(e,t){return e=st(22,e,null,t),e.lanes=0,e}function fr(e,t,a){return Ba(t,e.child,null,a),e=mr(t,t.pendingProps.children),e.flags|=2,t.memoizedState=null,e}function Bd(e,t,a){e.lanes|=t;var n=e.alternate;n!==null&&(n.lanes|=t),Io(e.return,t,a)}function gr(e,t,a,n,i,s){var o=e.memoizedState;o===null?e.memoizedState={isBackwards:t,rendering:null,renderingStartTime:0,last:n,tail:a,tailMode:i,treeForkCount:s}:(o.isBackwards=t,o.rendering=null,o.renderingStartTime=0,o.last=n,o.tail=a,o.tailMode=i,o.treeForkCount=s)}function Nd(e,t,a){var n=t.pendingProps,i=n.revealOrder,s=n.tail;n=n.children;var o=Se.current,r=(o&2)!==0;if(r?(o=o&1|2,t.flags|=128):o&=1,q(Se,o),Me(e,t,n,a),n=$?jn:0,!r&&e!==null&&(e.flags&128)!==0)e:for(e=t.child;e!==null;){if(e.tag===13)e.memoizedState!==null&&Bd(e,a,t);else if(e.tag===19)Bd(e,a,t);else if(e.child!==null){e.child.return=e,e=e.child;continue}if(e===t)break e;for(;e.sibling===null;){if(e.return===null||e.return===t)break e;e=e.return}e.sibling.return=e.return,e=e.sibling}switch(i){case"forwards":for(a=t.child,i=null;a!==null;)e=a.alternate,e!==null&&$i(e)===null&&(i=a),a=a.sibling;a=i,a===null?(i=t.child,t.child=null):(i=a.sibling,a.sibling=null),gr(t,!1,i,a,s,n);break;case"backwards":case"unstable_legacy-backwards":for(a=null,i=t.child,t.child=null;i!==null;){if(e=i.alternate,e!==null&&$i(e)===null){t.child=i;break}e=i.sibling,i.sibling=a,a=i,i=e}gr(t,!0,a,null,s,n);break;case"together":gr(t,!1,null,null,void 0,n);break;default:t.memoizedState=null}return t.child}function Ot(e,t,a){if(e!==null&&(t.dependencies=e.dependencies),da|=t.lanes,(a&t.childLanes)===0)if(e!==null){if(rn(e,t,a,!1),(a&t.childLanes)===0)return null}else return null;if(e!==null&&t.child!==e.child)throw Error(h(153));if(t.child!==null){for(e=t.child,a=Et(e,e.pendingProps),t.child=a,a.return=t;e.sibling!==null;)e=e.sibling,a=a.sibling=Et(e,e.pendingProps),a.return=t;a.sibling=null}return t.child}function yr(e,t){return(e.lanes&t)!==0?!0:(e=e.dependencies,!!(e!==null&&Gi(e)))}function Xp(e,t,a){switch(t.tag){case 3:Ge(t,t.stateNode.containerInfo),aa(t,qe,e.memoizedState.cache),Da();break;case 27:case 5:Rn(t);break;case 4:Ge(t,t.stateNode.containerInfo);break;case 10:aa(t,t.type,t.memoizedProps.value);break;case 31:if(t.memoizedState!==null)return t.flags|=128,Qo(t),null;break;case 13:var n=t.memoizedState;if(n!==null)return n.dehydrated!==null?(oa(t),t.flags|=128,null):(a&t.child.childLanes)!==0?Wd(e,t,a):(oa(t),e=Ot(e,t,a),e!==null?e.sibling:null);oa(t);break;case 19:var i=(e.flags&128)!==0;if(n=(a&t.childLanes)!==0,n||(rn(e,t,a,!1),n=(a&t.childLanes)!==0),i){if(n)return Nd(e,t,a);t.flags|=128}if(i=t.memoizedState,i!==null&&(i.rendering=null,i.tail=null,i.lastEffect=null),q(Se,Se.current),n)break;return null;case 22:return t.lanes=0,Id(e,t,a,t.pendingProps);case 24:aa(t,qe,e.memoizedState.cache)}return Ot(e,t,a)}function Hd(e,t,a){if(e!==null)if(e.memoizedProps!==t.pendingProps)Re=!0;else{if(!yr(e,a)&&(t.flags&128)===0)return Re=!1,Xp(e,t,a);Re=(e.flags&131072)!==0}else Re=!1,$&&(t.flags&1048576)!==0&&yc(t,jn,t.index);switch(t.lanes=0,t.tag){case 16:e:{var n=t.pendingProps;if(e=Ea(t.elementType),t.type=e,typeof e=="function")ko(e)?(n=Ha(e,n),t.tag=1,t=Pd(null,t,e,n,a)):(t.tag=0,t=dr(null,t,e,n,a));else{if(e!=null){var i=e.$$typeof;if(i===pe){t.tag=11,t=Cd(null,t,e,n,a);break e}else if(i===Y){t.tag=14,t=Rd(null,t,e,n,a);break e}}throw t=Dt(e)||e,Error(h(306,t,""))}}return t;case 0:return dr(e,t,t.type,t.pendingProps,a);case 1:return n=t.type,i=Ha(n,t.pendingProps),Pd(e,t,n,i,a);case 3:e:{if(Ge(t,t.stateNode.containerInfo),e===null)throw Error(h(387));n=t.pendingProps;var s=t.memoizedState;i=s.element,Bo(e,t),Fn(t,n,null,a);var o=t.memoizedState;if(n=o.cache,aa(t,qe,n),n!==s.cache&&Do(t,[qe],a,!0),Xn(),n=o.element,s.isDehydrated)if(s={element:n,isDehydrated:!1,cache:o.cache},t.updateQueue.baseState=s,t.memoizedState=s,t.flags&256){t=Ed(e,t,n,a);break e}else if(n!==i){i=mt(Error(h(424)),t),_n(i),t=Ed(e,t,n,a);break e}else{switch(e=t.stateNode.containerInfo,e.nodeType){case 9:e=e.body;break;default:e=e.nodeName==="HTML"?e.ownerDocument.body:e}for(ge=vt(e.firstChild),Ne=t,$=!0,ea=null,yt=!0,a=Ic(t,null,n,a),t.child=a;a;)a.flags=a.flags&-3|4096,a=a.sibling}else{if(Da(),n===i){t=Ot(e,t,a);break e}Me(e,t,n,a)}t=t.child}return t;case 26:return ds(e,t),e===null?(a=Xu(t.type,null,t.pendingProps,null))?t.memoizedState=a:$||(a=t.type,e=t.pendingProps,n=Cs(V.current).createElement(a),n[Be]=t,n[Ve]=e,Oe(n,a,e),Ue(n),t.stateNode=n):t.memoizedState=Xu(t.type,e.memoizedProps,t.pendingProps,e.memoizedState),null;case 27:return Rn(t),e===null&&$&&(n=t.stateNode=Yu(t.type,t.pendingProps,V.current),Ne=t,yt=!0,i=ge,fa(t.type)?(Xr=i,ge=vt(n.firstChild)):ge=i),Me(e,t,t.pendingProps.children,a),ds(e,t),e===null&&(t.flags|=4194304),t.child;case 5:return e===null&&$&&((i=n=ge)&&(n=qm(n,t.type,t.pendingProps,yt),n!==null?(t.stateNode=n,Ne=t,ge=vt(n.firstChild),yt=!1,i=!0):i=!1),i||ta(t)),Rn(t),i=t.type,s=t.pendingProps,o=e!==null?e.memoizedProps:null,n=s.children,Gr(i,s)?n=null:o!==null&&Gr(i,o)&&(t.flags|=32),t.memoizedState!==null&&(i=_o(e,t,Op,null,null,a),yi._currentValue=i),ds(e,t),Me(e,t,n,a),t.child;case 6:return e===null&&$&&((e=a=ge)&&(a=Cm(a,t.pendingProps,yt),a!==null?(t.stateNode=a,Ne=t,ge=null,e=!0):e=!1),e||ta(t)),null;case 13:return Wd(e,t,a);case 4:return Ge(t,t.stateNode.containerInfo),n=t.pendingProps,e===null?t.child=Ba(t,null,n,a):Me(e,t,n,a),t.child;case 11:return Cd(e,t,t.type,t.pendingProps,a);case 7:return Me(e,t,t.pendingProps,a),t.child;case 8:return Me(e,t,t.pendingProps.children,a),t.child;case 12:return Me(e,t,t.pendingProps.children,a),t.child;case 10:return n=t.pendingProps,aa(t,t.type,n.value),Me(e,t,n.children,a),t.child;case 9:return i=t.type._context,n=t.pendingProps.children,za(t),i=He(i),n=n(i),t.flags|=1,Me(e,t,n,a),t.child;case 14:return Rd(e,t,t.type,t.pendingProps,a);case 15:return Ld(e,t,t.type,t.pendingProps,a);case 19:return Nd(e,t,a);case 31:return Zp(e,t,a);case 22:return Id(e,t,a,t.pendingProps);case 24:return za(t),n=He(qe),e===null?(i=Po(),i===null&&(i=fe,s=Uo(),i.pooledCache=s,s.refCount++,s!==null&&(i.pooledCacheLanes|=a),i=s),t.memoizedState={parent:n,cache:i},Wo(t),aa(t,qe,i)):((e.lanes&a)!==0&&(Bo(e,t),Fn(t,null,null,a),Xn()),i=e.memoizedState,s=t.memoizedState,i.parent!==n?(i={parent:n,cache:n},t.memoizedState=i,t.lanes===0&&(t.memoizedState=t.updateQueue.baseState=i),aa(t,qe,n)):(n=s.cache,aa(t,qe,n),n!==i.cache&&Do(t,[qe],a,!0))),Me(e,t,t.pendingProps.children,a),t.child;case 29:throw t.pendingProps}throw Error(h(156,t.tag))}function Qt(e){e.flags|=4}function br(e,t,a,n,i){if((t=(e.mode&32)!==0)&&(t=!1),t){if(e.flags|=16777216,(i&335544128)===i)if(e.stateNode.complete)e.flags|=8192;else if(hu())e.flags|=8192;else throw Wa=Zi,Eo}else e.flags&=-16777217}function Md(e,t){if(t.type!=="stylesheet"||(t.state.loading&4)!==0)e.flags&=-16777217;else if(e.flags|=16777216,!th(t))if(hu())e.flags|=8192;else throw Wa=Zi,Eo}function hs(e,t){t!==null&&(e.flags|=4),e.flags&16384&&(t=e.tag!==22?vl():536870912,e.lanes|=t,vn|=t)}function ni(e,t){if(!$)switch(e.tailMode){case"hidden":t=e.tail;for(var a=null;t!==null;)t.alternate!==null&&(a=t),t=t.sibling;a===null?e.tail=null:a.sibling=null;break;case"collapsed":a=e.tail;for(var n=null;a!==null;)a.alternate!==null&&(n=a),a=a.sibling;n===null?t||e.tail===null?e.tail=null:e.tail.sibling=null:n.sibling=null}}function ye(e){var t=e.alternate!==null&&e.alternate.child===e.child,a=0,n=0;if(t)for(var i=e.child;i!==null;)a|=i.lanes|i.childLanes,n|=i.subtreeFlags&65011712,n|=i.flags&65011712,i.return=e,i=i.sibling;else for(i=e.child;i!==null;)a|=i.lanes|i.childLanes,n|=i.subtreeFlags,n|=i.flags,i.return=e,i=i.sibling;return e.subtreeFlags|=n,e.childLanes=a,t}function Fp(e,t,a){var n=t.pendingProps;switch(qo(t),t.tag){case 16:case 15:case 0:case 11:case 7:case 8:case 12:case 9:case 14:return ye(t),null;case 1:return ye(t),null;case 3:return a=t.stateNode,n=null,e!==null&&(n=e.memoizedState.cache),t.memoizedState.cache!==n&&(t.flags|=2048),Nt(qe),Te(),a.pendingContext&&(a.context=a.pendingContext,a.pendingContext=null),(e===null||e.child===null)&&(on(t)?Qt(t):e===null||e.memoizedState.isDehydrated&&(t.flags&256)===0||(t.flags|=1024,Ro())),ye(t),null;case 26:var i=t.type,s=t.memoizedState;return e===null?(Qt(t),s!==null?(ye(t),Md(t,s)):(ye(t),br(t,i,null,n,a))):s?s!==e.memoizedState?(Qt(t),ye(t),Md(t,s)):(ye(t),t.flags&=-16777217):(e=e.memoizedProps,e!==n&&Qt(t),ye(t),br(t,i,e,n,a)),null;case 27:if(Ti(t),a=V.current,i=t.type,e!==null&&t.stateNode!=null)e.memoizedProps!==n&&Qt(t);else{if(!n){if(t.stateNode===null)throw Error(h(166));return ye(t),null}e=I.current,on(t)?vc(t):(e=Yu(i,n,a),t.stateNode=e,Qt(t))}return ye(t),null;case 5:if(Ti(t),i=t.type,e!==null&&t.stateNode!=null)e.memoizedProps!==n&&Qt(t);else{if(!n){if(t.stateNode===null)throw Error(h(166));return ye(t),null}if(s=I.current,on(t))vc(t);else{var o=Cs(V.current);switch(s){case 1:s=o.createElementNS("http://www.w3.org/2000/svg",i);break;case 2:s=o.createElementNS("http://www.w3.org/1998/Math/MathML",i);break;default:switch(i){case"svg":s=o.createElementNS("http://www.w3.org/2000/svg",i);break;case"math":s=o.createElementNS("http://www.w3.org/1998/Math/MathML",i);break;case"script":s=o.createElement("div"),s.innerHTML="<script><\/script>",s=s.removeChild(s.firstChild);break;case"select":s=typeof n.is=="string"?o.createElement("select",{is:n.is}):o.createElement("select"),n.multiple?s.multiple=!0:n.size&&(s.size=n.size);break;default:s=typeof n.is=="string"?o.createElement(i,{is:n.is}):o.createElement(i)}}s[Be]=t,s[Ve]=n;e:for(o=t.child;o!==null;){if(o.tag===5||o.tag===6)s.appendChild(o.stateNode);else if(o.tag!==4&&o.tag!==27&&o.child!==null){o.child.return=o,o=o.child;continue}if(o===t)break e;for(;o.sibling===null;){if(o.return===null||o.return===t)break e;o=o.return}o.sibling.return=o.return,o=o.sibling}t.stateNode=s;e:switch(Oe(s,i,n),i){case"button":case"input":case"select":case"textarea":n=!!n.autoFocus;break e;case"img":n=!0;break e;default:n=!1}n&&Qt(t)}}return ye(t),br(t,t.type,e===null?null:e.memoizedProps,t.pendingProps,a),null;case 6:if(e&&t.stateNode!=null)e.memoizedProps!==n&&Qt(t);else{if(typeof n!="string"&&t.stateNode===null)throw Error(h(166));if(e=V.current,on(t)){if(e=t.stateNode,a=t.memoizedProps,n=null,i=Ne,i!==null)switch(i.tag){case 27:case 5:n=i.memoizedProps}e[Be]=t,e=!!(e.nodeValue===a||n!==null&&n.suppressHydrationWarning===!0||Wu(e.nodeValue,a)),e||ta(t,!0)}else e=Cs(e).createTextNode(n),e[Be]=t,t.stateNode=e}return ye(t),null;case 31:if(a=t.memoizedState,e===null||e.memoizedState!==null){if(n=on(t),a!==null){if(e===null){if(!n)throw Error(h(318));if(e=t.memoizedState,e=e!==null?e.dehydrated:null,!e)throw Error(h(557));e[Be]=t}else Da(),(t.flags&128)===0&&(t.memoizedState=null),t.flags|=4;ye(t),e=!1}else a=Ro(),e!==null&&e.memoizedState!==null&&(e.memoizedState.hydrationErrors=a),e=!0;if(!e)return t.flags&256?(rt(t),t):(rt(t),null);if((t.flags&128)!==0)throw Error(h(558))}return ye(t),null;case 13:if(n=t.memoizedState,e===null||e.memoizedState!==null&&e.memoizedState.dehydrated!==null){if(i=on(t),n!==null&&n.dehydrated!==null){if(e===null){if(!i)throw Error(h(318));if(i=t.memoizedState,i=i!==null?i.dehydrated:null,!i)throw Error(h(317));i[Be]=t}else Da(),(t.flags&128)===0&&(t.memoizedState=null),t.flags|=4;ye(t),i=!1}else i=Ro(),e!==null&&e.memoizedState!==null&&(e.memoizedState.hydrationErrors=i),i=!0;if(!i)return t.flags&256?(rt(t),t):(rt(t),null)}return rt(t),(t.flags&128)!==0?(t.lanes=a,t):(a=n!==null,e=e!==null&&e.memoizedState!==null,a&&(n=t.child,i=null,n.alternate!==null&&n.alternate.memoizedState!==null&&n.alternate.memoizedState.cachePool!==null&&(i=n.alternate.memoizedState.cachePool.pool),s=null,n.memoizedState!==null&&n.memoizedState.cachePool!==null&&(s=n.memoizedState.cachePool.pool),s!==i&&(n.flags|=2048)),a!==e&&a&&(t.child.flags|=8192),hs(t,t.updateQueue),ye(t),null);case 4:return Te(),e===null&&Mr(t.stateNode.containerInfo),ye(t),null;case 10:return Nt(t.type),ye(t),null;case 19:if(T(Se),n=t.memoizedState,n===null)return ye(t),null;if(i=(t.flags&128)!==0,s=n.rendering,s===null)if(i)ni(n,!1);else{if(ke!==0||e!==null&&(e.flags&128)!==0)for(e=t.child;e!==null;){if(s=$i(e),s!==null){for(t.flags|=128,ni(n,!1),e=s.updateQueue,t.updateQueue=e,hs(t,e),t.subtreeFlags=0,e=a,a=t.child;a!==null;)mc(a,e),a=a.sibling;return q(Se,Se.current&1|2),$&&Wt(t,n.treeForkCount),t.child}e=e.sibling}n.tail!==null&&tt()>ys&&(t.flags|=128,i=!0,ni(n,!1),t.lanes=4194304)}else{if(!i)if(e=$i(s),e!==null){if(t.flags|=128,i=!0,e=e.updateQueue,t.updateQueue=e,hs(t,e),ni(n,!0),n.tail===null&&n.tailMode==="hidden"&&!s.alternate&&!$)return ye(t),null}else 2*tt()-n.renderingStartTime>ys&&a!==536870912&&(t.flags|=128,i=!0,ni(n,!1),t.lanes=4194304);n.isBackwards?(s.sibling=t.child,t.child=s):(e=n.last,e!==null?e.sibling=s:t.child=s,n.last=s)}return n.tail!==null?(e=n.tail,n.rendering=e,n.tail=e.sibling,n.renderingStartTime=tt(),e.sibling=null,a=Se.current,q(Se,i?a&1|2:a&1),$&&Wt(t,n.treeForkCount),e):(ye(t),null);case 22:case 23:return rt(t),Oo(),n=t.memoizedState!==null,e!==null?e.memoizedState!==null!==n&&(t.flags|=8192):n&&(t.flags|=8192),n?(a&536870912)!==0&&(t.flags&128)===0&&(ye(t),t.subtreeFlags&6&&(t.flags|=8192)):ye(t),a=t.updateQueue,a!==null&&hs(t,a.retryQueue),a=null,e!==null&&e.memoizedState!==null&&e.memoizedState.cachePool!==null&&(a=e.memoizedState.cachePool.pool),n=null,t.memoizedState!==null&&t.memoizedState.cachePool!==null&&(n=t.memoizedState.cachePool.pool),n!==a&&(t.flags|=2048),e!==null&&T(Pa),null;case 24:return a=null,e!==null&&(a=e.memoizedState.cache),t.memoizedState.cache!==a&&(t.flags|=2048),Nt(qe),ye(t),null;case 25:return null;case 30:return null}throw Error(h(156,t.tag))}function Jp(e,t){switch(qo(t),t.tag){case 1:return e=t.flags,e&65536?(t.flags=e&-65537|128,t):null;case 3:return Nt(qe),Te(),e=t.flags,(e&65536)!==0&&(e&128)===0?(t.flags=e&-65537|128,t):null;case 26:case 27:case 5:return Ti(t),null;case 31:if(t.memoizedState!==null){if(rt(t),t.alternate===null)throw Error(h(340));Da()}return e=t.flags,e&65536?(t.flags=e&-65537|128,t):null;case 13:if(rt(t),e=t.memoizedState,e!==null&&e.dehydrated!==null){if(t.alternate===null)throw Error(h(340));Da()}return e=t.flags,e&65536?(t.flags=e&-65537|128,t):null;case 19:return T(Se),null;case 4:return Te(),null;case 10:return Nt(t.type),null;case 22:case 23:return rt(t),Oo(),e!==null&&T(Pa),e=t.flags,e&65536?(t.flags=e&-65537|128,t):null;case 24:return Nt(qe),null;case 25:return null;default:return null}}function Od(e,t){switch(qo(t),t.tag){case 3:Nt(qe),Te();break;case 26:case 27:case 5:Ti(t);break;case 4:Te();break;case 31:t.memoizedState!==null&&rt(t);break;case 13:rt(t);break;case 19:T(Se);break;case 10:Nt(t.type);break;case 22:case 23:rt(t),Oo(),e!==null&&T(Pa);break;case 24:Nt(qe)}}function ii(e,t){try{var a=t.updateQueue,n=a!==null?a.lastEffect:null;if(n!==null){var i=n.next;a=i;do{if((a.tag&e)===e){n=void 0;var s=a.create,o=a.inst;n=s(),o.destroy=n}a=a.next}while(a!==i)}}catch(r){oe(t,t.return,r)}}function la(e,t,a){try{var n=t.updateQueue,i=n!==null?n.lastEffect:null;if(i!==null){var s=i.next;n=s;do{if((n.tag&e)===e){var o=n.inst,r=o.destroy;if(r!==void 0){o.destroy=void 0,i=t;var l=a,m=r;try{m()}catch(b){oe(i,l,b)}}}n=n.next}while(n!==s)}}catch(b){oe(t,t.return,b)}}function Qd(e){var t=e.updateQueue;if(t!==null){var a=e.stateNode;try{Uc(t,a)}catch(n){oe(e,e.return,n)}}}function jd(e,t,a){a.props=Ha(e.type,e.memoizedProps),a.state=e.memoizedState;try{a.componentWillUnmount()}catch(n){oe(e,t,n)}}function si(e,t){try{var a=e.ref;if(a!==null){switch(e.tag){case 26:case 27:case 5:var n=e.stateNode;break;case 30:n=e.stateNode;break;default:n=e.stateNode}typeof a=="function"?e.refCleanup=a(n):a.current=n}}catch(i){oe(e,t,i)}}function Lt(e,t){var a=e.ref,n=e.refCleanup;if(a!==null)if(typeof n=="function")try{n()}catch(i){oe(e,t,i)}finally{e.refCleanup=null,e=e.alternate,e!=null&&(e.refCleanup=null)}else if(typeof a=="function")try{a(null)}catch(i){oe(e,t,i)}else a.current=null}function _d(e){var t=e.type,a=e.memoizedProps,n=e.stateNode;try{e:switch(t){case"button":case"input":case"select":case"textarea":a.autoFocus&&n.focus();break e;case"img":a.src?n.src=a.src:a.srcSet&&(n.srcset=a.srcSet)}}catch(i){oe(e,e.return,i)}}function vr(e,t,a){try{var n=e.stateNode;wm(n,e.type,a,t),n[Ve]=t}catch(i){oe(e,e.return,i)}}function Gd(e){return e.tag===5||e.tag===3||e.tag===26||e.tag===27&&fa(e.type)||e.tag===4}function wr(e){e:for(;;){for(;e.sibling===null;){if(e.return===null||Gd(e.return))return null;e=e.return}for(e.sibling.return=e.return,e=e.sibling;e.tag!==5&&e.tag!==6&&e.tag!==18;){if(e.tag===27&&fa(e.type)||e.flags&2||e.child===null||e.tag===4)continue e;e.child.return=e,e=e.child}if(!(e.flags&2))return e.stateNode}}function xr(e,t,a){var n=e.tag;if(n===5||n===6)e=e.stateNode,t?(a.nodeType===9?a.body:a.nodeName==="HTML"?a.ownerDocument.body:a).insertBefore(e,t):(t=a.nodeType===9?a.body:a.nodeName==="HTML"?a.ownerDocument.body:a,t.appendChild(e),a=a._reactRootContainer,a!=null||t.onclick!==null||(t.onclick=zt));else if(n!==4&&(n===27&&fa(e.type)&&(a=e.stateNode,t=null),e=e.child,e!==null))for(xr(e,t,a),e=e.sibling;e!==null;)xr(e,t,a),e=e.sibling}function ps(e,t,a){var n=e.tag;if(n===5||n===6)e=e.stateNode,t?a.insertBefore(e,t):a.appendChild(e);else if(n!==4&&(n===27&&fa(e.type)&&(a=e.stateNode),e=e.child,e!==null))for(ps(e,t,a),e=e.sibling;e!==null;)ps(e,t,a),e=e.sibling}function Kd(e){var t=e.stateNode,a=e.memoizedProps;try{for(var n=e.type,i=t.attributes;i.length;)t.removeAttributeNode(i[0]);Oe(t,n,a),t[Be]=e,t[Ve]=a}catch(s){oe(e,e.return,s)}}var jt=!1,Le=!1,kr=!1,Yd=typeof WeakSet=="function"?WeakSet:Set,ze=null;function $p(e,t){if(e=e.containerInfo,jr=Ps,e=sc(e),fo(e)){if("selectionStart"in e)var a={start:e.selectionStart,end:e.selectionEnd};else e:{a=(a=e.ownerDocument)&&a.defaultView||window;var n=a.getSelection&&a.getSelection();if(n&&n.rangeCount!==0){a=n.anchorNode;var i=n.anchorOffset,s=n.focusNode;n=n.focusOffset;try{a.nodeType,s.nodeType}catch{a=null;break e}var o=0,r=-1,l=-1,m=0,b=0,k=e,f=null;t:for(;;){for(var g;k!==a||i!==0&&k.nodeType!==3||(r=o+i),k!==s||n!==0&&k.nodeType!==3||(l=o+n),k.nodeType===3&&(o+=k.nodeValue.length),(g=k.firstChild)!==null;)f=k,k=g;for(;;){if(k===e)break t;if(f===a&&++m===i&&(r=o),f===s&&++b===n&&(l=o),(g=k.nextSibling)!==null)break;k=f,f=k.parentNode}k=g}a=r===-1||l===-1?null:{start:r,end:l}}else a=null}a=a||{start:0,end:0}}else a=null;for(_r={focusedElem:e,selectionRange:a},Ps=!1,ze=t;ze!==null;)if(t=ze,e=t.child,(t.subtreeFlags&1028)!==0&&e!==null)e.return=t,ze=e;else for(;ze!==null;){switch(t=ze,s=t.alternate,e=t.flags,t.tag){case 0:if((e&4)!==0&&(e=t.updateQueue,e=e!==null?e.events:null,e!==null))for(a=0;a<e.length;a++)i=e[a],i.ref.impl=i.nextImpl;break;case 11:case 15:break;case 1:if((e&1024)!==0&&s!==null){e=void 0,a=t,i=s.memoizedProps,s=s.memoizedState,n=a.stateNode;try{var R=Ha(a.type,i);e=n.getSnapshotBeforeUpdate(R,s),n.__reactInternalSnapshotBeforeUpdate=e}catch(W){oe(a,a.return,W)}}break;case 3:if((e&1024)!==0){if(e=t.stateNode.containerInfo,a=e.nodeType,a===9)Yr(e);else if(a===1)switch(e.nodeName){case"HEAD":case"HTML":case"BODY":Yr(e);break;default:e.textContent=""}}break;case 5:case 26:case 27:case 6:case 4:case 17:break;default:if((e&1024)!==0)throw Error(h(163))}if(e=t.sibling,e!==null){e.return=t.return,ze=e;break}ze=t.return}}function Vd(e,t,a){var n=a.flags;switch(a.tag){case 0:case 11:case 15:Gt(e,a),n&4&&ii(5,a);break;case 1:if(Gt(e,a),n&4)if(e=a.stateNode,t===null)try{e.componentDidMount()}catch(o){oe(a,a.return,o)}else{var i=Ha(a.type,t.memoizedProps);t=t.memoizedState;try{e.componentDidUpdate(i,t,e.__reactInternalSnapshotBeforeUpdate)}catch(o){oe(a,a.return,o)}}n&64&&Qd(a),n&512&&si(a,a.return);break;case 3:if(Gt(e,a),n&64&&(e=a.updateQueue,e!==null)){if(t=null,a.child!==null)switch(a.child.tag){case 27:case 5:t=a.child.stateNode;break;case 1:t=a.child.stateNode}try{Uc(e,t)}catch(o){oe(a,a.return,o)}}break;case 27:t===null&&n&4&&Kd(a);case 26:case 5:Gt(e,a),t===null&&n&4&&_d(a),n&512&&si(a,a.return);break;case 12:Gt(e,a);break;case 31:Gt(e,a),n&4&&Fd(e,a);break;case 13:Gt(e,a),n&4&&Jd(e,a),n&64&&(e=a.memoizedState,e!==null&&(e=e.dehydrated,e!==null&&(a=lm.bind(null,a),Rm(e,a))));break;case 22:if(n=a.memoizedState!==null||jt,!n){t=t!==null&&t.memoizedState!==null||Le,i=jt;var s=Le;jt=n,(Le=t)&&!s?Kt(e,a,(a.subtreeFlags&8772)!==0):Gt(e,a),jt=i,Le=s}break;case 30:break;default:Gt(e,a)}}function Zd(e){var t=e.alternate;t!==null&&(e.alternate=null,Zd(t)),e.child=null,e.deletions=null,e.sibling=null,e.tag===5&&(t=e.stateNode,t!==null&&Fs(t)),e.stateNode=null,e.return=null,e.dependencies=null,e.memoizedProps=null,e.memoizedState=null,e.pendingProps=null,e.stateNode=null,e.updateQueue=null}var ve=null,Xe=!1;function _t(e,t,a){for(a=a.child;a!==null;)Xd(e,t,a),a=a.sibling}function Xd(e,t,a){if(at&&typeof at.onCommitFiberUnmount=="function")try{at.onCommitFiberUnmount(Ln,a)}catch{}switch(a.tag){case 26:Le||Lt(a,t),_t(e,t,a),a.memoizedState?a.memoizedState.count--:a.stateNode&&(a=a.stateNode,a.parentNode.removeChild(a));break;case 27:Le||Lt(a,t);var n=ve,i=Xe;fa(a.type)&&(ve=a.stateNode,Xe=!1),_t(e,t,a),mi(a.stateNode),ve=n,Xe=i;break;case 5:Le||Lt(a,t);case 6:if(n=ve,i=Xe,ve=null,_t(e,t,a),ve=n,Xe=i,ve!==null)if(Xe)try{(ve.nodeType===9?ve.body:ve.nodeName==="HTML"?ve.ownerDocument.body:ve).removeChild(a.stateNode)}catch(s){oe(a,t,s)}else try{ve.removeChild(a.stateNode)}catch(s){oe(a,t,s)}break;case 18:ve!==null&&(Xe?(e=ve,Qu(e.nodeType===9?e.body:e.nodeName==="HTML"?e.ownerDocument.body:e,a.stateNode),Cn(e)):Qu(ve,a.stateNode));break;case 4:n=ve,i=Xe,ve=a.stateNode.containerInfo,Xe=!0,_t(e,t,a),ve=n,Xe=i;break;case 0:case 11:case 14:case 15:la(2,a,t),Le||la(4,a,t),_t(e,t,a);break;case 1:Le||(Lt(a,t),n=a.stateNode,typeof n.componentWillUnmount=="function"&&jd(a,t,n)),_t(e,t,a);break;case 21:_t(e,t,a);break;case 22:Le=(n=Le)||a.memoizedState!==null,_t(e,t,a),Le=n;break;default:_t(e,t,a)}}function Fd(e,t){if(t.memoizedState===null&&(e=t.alternate,e!==null&&(e=e.memoizedState,e!==null))){e=e.dehydrated;try{Cn(e)}catch(a){oe(t,t.return,a)}}}function Jd(e,t){if(t.memoizedState===null&&(e=t.alternate,e!==null&&(e=e.memoizedState,e!==null&&(e=e.dehydrated,e!==null))))try{Cn(e)}catch(a){oe(t,t.return,a)}}function em(e){switch(e.tag){case 31:case 13:case 19:var t=e.stateNode;return t===null&&(t=e.stateNode=new Yd),t;case 22:return e=e.stateNode,t=e._retryCache,t===null&&(t=e._retryCache=new Yd),t;default:throw Error(h(435,e.tag))}}function ms(e,t){var a=em(e);t.forEach(function(n){if(!a.has(n)){a.add(n);var i=cm.bind(null,e,n);n.then(i,i)}})}function Fe(e,t){var a=t.deletions;if(a!==null)for(var n=0;n<a.length;n++){var i=a[n],s=e,o=t,r=o;e:for(;r!==null;){switch(r.tag){case 27:if(fa(r.type)){ve=r.stateNode,Xe=!1;break e}break;case 5:ve=r.stateNode,Xe=!1;break e;case 3:case 4:ve=r.stateNode.containerInfo,Xe=!0;break e}r=r.return}if(ve===null)throw Error(h(160));Xd(s,o,i),ve=null,Xe=!1,s=i.alternate,s!==null&&(s.return=null),i.return=null}if(t.subtreeFlags&13886)for(t=t.child;t!==null;)$d(t,e),t=t.sibling}var Tt=null;function $d(e,t){var a=e.alternate,n=e.flags;switch(e.tag){case 0:case 11:case 14:case 15:Fe(t,e),Je(e),n&4&&(la(3,e,e.return),ii(3,e),la(5,e,e.return));break;case 1:Fe(t,e),Je(e),n&512&&(Le||a===null||Lt(a,a.return)),n&64&&jt&&(e=e.updateQueue,e!==null&&(n=e.callbacks,n!==null&&(a=e.shared.hiddenCallbacks,e.shared.hiddenCallbacks=a===null?n:a.concat(n))));break;case 26:var i=Tt;if(Fe(t,e),Je(e),n&512&&(Le||a===null||Lt(a,a.return)),n&4){var s=a!==null?a.memoizedState:null;if(n=e.memoizedState,a===null)if(n===null)if(e.stateNode===null){e:{n=e.type,a=e.memoizedProps,i=i.ownerDocument||i;t:switch(n){case"title":s=i.getElementsByTagName("title")[0],(!s||s[Un]||s[Be]||s.namespaceURI==="http://www.w3.org/2000/svg"||s.hasAttribute("itemprop"))&&(s=i.createElement(n),i.head.insertBefore(s,i.querySelector("head > title"))),Oe(s,n,a),s[Be]=e,Ue(s),n=s;break e;case"link":var o=$u("link","href",i).get(n+(a.href||""));if(o){for(var r=0;r<o.length;r++)if(s=o[r],s.getAttribute("href")===(a.href==null||a.href===""?null:a.href)&&s.getAttribute("rel")===(a.rel==null?null:a.rel)&&s.getAttribute("title")===(a.title==null?null:a.title)&&s.getAttribute("crossorigin")===(a.crossOrigin==null?null:a.crossOrigin)){o.splice(r,1);break t}}s=i.createElement(n),Oe(s,n,a),i.head.appendChild(s);break;case"meta":if(o=$u("meta","content",i).get(n+(a.content||""))){for(r=0;r<o.length;r++)if(s=o[r],s.getAttribute("content")===(a.content==null?null:""+a.content)&&s.getAttribute("name")===(a.name==null?null:a.name)&&s.getAttribute("property")===(a.property==null?null:a.property)&&s.getAttribute("http-equiv")===(a.httpEquiv==null?null:a.httpEquiv)&&s.getAttribute("charset")===(a.charSet==null?null:a.charSet)){o.splice(r,1);break t}}s=i.createElement(n),Oe(s,n,a),i.head.appendChild(s);break;default:throw Error(h(468,n))}s[Be]=e,Ue(s),n=s}e.stateNode=n}else eh(i,e.type,e.stateNode);else e.stateNode=Ju(i,n,e.memoizedProps);else s!==n?(s===null?a.stateNode!==null&&(a=a.stateNode,a.parentNode.removeChild(a)):s.count--,n===null?eh(i,e.type,e.stateNode):Ju(i,n,e.memoizedProps)):n===null&&e.stateNode!==null&&vr(e,e.memoizedProps,a.memoizedProps)}break;case 27:Fe(t,e),Je(e),n&512&&(Le||a===null||Lt(a,a.return)),a!==null&&n&4&&vr(e,e.memoizedProps,a.memoizedProps);break;case 5:if(Fe(t,e),Je(e),n&512&&(Le||a===null||Lt(a,a.return)),e.flags&32){i=e.stateNode;try{Za(i,"")}catch(R){oe(e,e.return,R)}}n&4&&e.stateNode!=null&&(i=e.memoizedProps,vr(e,i,a!==null?a.memoizedProps:i)),n&1024&&(kr=!0);break;case 6:if(Fe(t,e),Je(e),n&4){if(e.stateNode===null)throw Error(h(162));n=e.memoizedProps,a=e.stateNode;try{a.nodeValue=n}catch(R){oe(e,e.return,R)}}break;case 3:if(Is=null,i=Tt,Tt=Rs(t.containerInfo),Fe(t,e),Tt=i,Je(e),n&4&&a!==null&&a.memoizedState.isDehydrated)try{Cn(t.containerInfo)}catch(R){oe(e,e.return,R)}kr&&(kr=!1,eu(e));break;case 4:n=Tt,Tt=Rs(e.stateNode.containerInfo),Fe(t,e),Je(e),Tt=n;break;case 12:Fe(t,e),Je(e);break;case 31:Fe(t,e),Je(e),n&4&&(n=e.updateQueue,n!==null&&(e.updateQueue=null,ms(e,n)));break;case 13:Fe(t,e),Je(e),e.child.flags&8192&&e.memoizedState!==null!=(a!==null&&a.memoizedState!==null)&&(gs=tt()),n&4&&(n=e.updateQueue,n!==null&&(e.updateQueue=null,ms(e,n)));break;case 22:i=e.memoizedState!==null;var l=a!==null&&a.memoizedState!==null,m=jt,b=Le;if(jt=m||i,Le=b||l,Fe(t,e),Le=b,jt=m,Je(e),n&8192)e:for(t=e.stateNode,t._visibility=i?t._visibility&-2:t._visibility|1,i&&(a===null||l||jt||Le||Ma(e)),a=null,t=e;;){if(t.tag===5||t.tag===26){if(a===null){l=a=t;try{if(s=l.stateNode,i)o=s.style,typeof o.setProperty=="function"?o.setProperty("display","none","important"):o.display="none";else{r=l.stateNode;var k=l.memoizedProps.style,f=k!=null&&k.hasOwnProperty("display")?k.display:null;r.style.display=f==null||typeof f=="boolean"?"":(""+f).trim()}}catch(R){oe(l,l.return,R)}}}else if(t.tag===6){if(a===null){l=t;try{l.stateNode.nodeValue=i?"":l.memoizedProps}catch(R){oe(l,l.return,R)}}}else if(t.tag===18){if(a===null){l=t;try{var g=l.stateNode;i?ju(g,!0):ju(l.stateNode,!1)}catch(R){oe(l,l.return,R)}}}else if((t.tag!==22&&t.tag!==23||t.memoizedState===null||t===e)&&t.child!==null){t.child.return=t,t=t.child;continue}if(t===e)break e;for(;t.sibling===null;){if(t.return===null||t.return===e)break e;a===t&&(a=null),t=t.return}a===t&&(a=null),t.sibling.return=t.return,t=t.sibling}n&4&&(n=e.updateQueue,n!==null&&(a=n.retryQueue,a!==null&&(n.retryQueue=null,ms(e,a))));break;case 19:Fe(t,e),Je(e),n&4&&(n=e.updateQueue,n!==null&&(e.updateQueue=null,ms(e,n)));break;case 30:break;case 21:break;default:Fe(t,e),Je(e)}}function Je(e){var t=e.flags;if(t&2){try{for(var a,n=e.return;n!==null;){if(Gd(n)){a=n;break}n=n.return}if(a==null)throw Error(h(160));switch(a.tag){case 27:var i=a.stateNode,s=wr(e);ps(e,s,i);break;case 5:var o=a.stateNode;a.flags&32&&(Za(o,""),a.flags&=-33);var r=wr(e);ps(e,r,o);break;case 3:case 4:var l=a.stateNode.containerInfo,m=wr(e);xr(e,m,l);break;default:throw Error(h(161))}}catch(b){oe(e,e.return,b)}e.flags&=-3}t&4096&&(e.flags&=-4097)}function eu(e){if(e.subtreeFlags&1024)for(e=e.child;e!==null;){var t=e;eu(t),t.tag===5&&t.flags&1024&&t.stateNode.reset(),e=e.sibling}}function Gt(e,t){if(t.subtreeFlags&8772)for(t=t.child;t!==null;)Vd(e,t.alternate,t),t=t.sibling}function Ma(e){for(e=e.child;e!==null;){var t=e;switch(t.tag){case 0:case 11:case 14:case 15:la(4,t,t.return),Ma(t);break;case 1:Lt(t,t.return);var a=t.stateNode;typeof a.componentWillUnmount=="function"&&jd(t,t.return,a),Ma(t);break;case 27:mi(t.stateNode);case 26:case 5:Lt(t,t.return),Ma(t);break;case 22:t.memoizedState===null&&Ma(t);break;case 30:Ma(t);break;default:Ma(t)}e=e.sibling}}function Kt(e,t,a){for(a=a&&(t.subtreeFlags&8772)!==0,t=t.child;t!==null;){var n=t.alternate,i=e,s=t,o=s.flags;switch(s.tag){case 0:case 11:case 15:Kt(i,s,a),ii(4,s);break;case 1:if(Kt(i,s,a),n=s,i=n.stateNode,typeof i.componentDidMount=="function")try{i.componentDidMount()}catch(m){oe(n,n.return,m)}if(n=s,i=n.updateQueue,i!==null){var r=n.stateNode;try{var l=i.shared.hiddenCallbacks;if(l!==null)for(i.shared.hiddenCallbacks=null,i=0;i<l.length;i++)Dc(l[i],r)}catch(m){oe(n,n.return,m)}}a&&o&64&&Qd(s),si(s,s.return);break;case 27:Kd(s);case 26:case 5:Kt(i,s,a),a&&n===null&&o&4&&_d(s),si(s,s.return);break;case 12:Kt(i,s,a);break;case 31:Kt(i,s,a),a&&o&4&&Fd(i,s);break;case 13:Kt(i,s,a),a&&o&4&&Jd(i,s);break;case 22:s.memoizedState===null&&Kt(i,s,a),si(s,s.return);break;case 30:break;default:Kt(i,s,a)}t=t.sibling}}function Tr(e,t){var a=null;e!==null&&e.memoizedState!==null&&e.memoizedState.cachePool!==null&&(a=e.memoizedState.cachePool.pool),e=null,t.memoizedState!==null&&t.memoizedState.cachePool!==null&&(e=t.memoizedState.cachePool.pool),e!==a&&(e!=null&&e.refCount++,a!=null&&Gn(a))}function Sr(e,t){e=null,t.alternate!==null&&(e=t.alternate.memoizedState.cache),t=t.memoizedState.cache,t!==e&&(t.refCount++,e!=null&&Gn(e))}function St(e,t,a,n){if(t.subtreeFlags&10256)for(t=t.child;t!==null;)tu(e,t,a,n),t=t.sibling}function tu(e,t,a,n){var i=t.flags;switch(t.tag){case 0:case 11:case 15:St(e,t,a,n),i&2048&&ii(9,t);break;case 1:St(e,t,a,n);break;case 3:St(e,t,a,n),i&2048&&(e=null,t.alternate!==null&&(e=t.alternate.memoizedState.cache),t=t.memoizedState.cache,t!==e&&(t.refCount++,e!=null&&Gn(e)));break;case 12:if(i&2048){St(e,t,a,n),e=t.stateNode;try{var s=t.memoizedProps,o=s.id,r=s.onPostCommit;typeof r=="function"&&r(o,t.alternate===null?"mount":"update",e.passiveEffectDuration,-0)}catch(l){oe(t,t.return,l)}}else St(e,t,a,n);break;case 31:St(e,t,a,n);break;case 13:St(e,t,a,n);break;case 23:break;case 22:s=t.stateNode,o=t.alternate,t.memoizedState!==null?s._visibility&2?St(e,t,a,n):oi(e,t):s._visibility&2?St(e,t,a,n):(s._visibility|=2,gn(e,t,a,n,(t.subtreeFlags&10256)!==0||!1)),i&2048&&Tr(o,t);break;case 24:St(e,t,a,n),i&2048&&Sr(t.alternate,t);break;default:St(e,t,a,n)}}function gn(e,t,a,n,i){for(i=i&&((t.subtreeFlags&10256)!==0||!1),t=t.child;t!==null;){var s=e,o=t,r=a,l=n,m=o.flags;switch(o.tag){case 0:case 11:case 15:gn(s,o,r,l,i),ii(8,o);break;case 23:break;case 22:var b=o.stateNode;o.memoizedState!==null?b._visibility&2?gn(s,o,r,l,i):oi(s,o):(b._visibility|=2,gn(s,o,r,l,i)),i&&m&2048&&Tr(o.alternate,o);break;case 24:gn(s,o,r,l,i),i&&m&2048&&Sr(o.alternate,o);break;default:gn(s,o,r,l,i)}t=t.sibling}}function oi(e,t){if(t.subtreeFlags&10256)for(t=t.child;t!==null;){var a=e,n=t,i=n.flags;switch(n.tag){case 22:oi(a,n),i&2048&&Tr(n.alternate,n);break;case 24:oi(a,n),i&2048&&Sr(n.alternate,n);break;default:oi(a,n)}t=t.sibling}}var ri=8192;function yn(e,t,a){if(e.subtreeFlags&ri)for(e=e.child;e!==null;)au(e,t,a),e=e.sibling}function au(e,t,a){switch(e.tag){case 26:yn(e,t,a),e.flags&ri&&e.memoizedState!==null&&Mm(a,Tt,e.memoizedState,e.memoizedProps);break;case 5:yn(e,t,a);break;case 3:case 4:var n=Tt;Tt=Rs(e.stateNode.containerInfo),yn(e,t,a),Tt=n;break;case 22:e.memoizedState===null&&(n=e.alternate,n!==null&&n.memoizedState!==null?(n=ri,ri=16777216,yn(e,t,a),ri=n):yn(e,t,a));break;default:yn(e,t,a)}}function nu(e){var t=e.alternate;if(t!==null&&(e=t.child,e!==null)){t.child=null;do t=e.sibling,e.sibling=null,e=t;while(e!==null)}}function li(e){var t=e.deletions;if((e.flags&16)!==0){if(t!==null)for(var a=0;a<t.length;a++){var n=t[a];ze=n,su(n,e)}nu(e)}if(e.subtreeFlags&10256)for(e=e.child;e!==null;)iu(e),e=e.sibling}function iu(e){switch(e.tag){case 0:case 11:case 15:li(e),e.flags&2048&&la(9,e,e.return);break;case 3:li(e);break;case 12:li(e);break;case 22:var t=e.stateNode;e.memoizedState!==null&&t._visibility&2&&(e.return===null||e.return.tag!==13)?(t._visibility&=-3,fs(e)):li(e);break;default:li(e)}}function fs(e){var t=e.deletions;if((e.flags&16)!==0){if(t!==null)for(var a=0;a<t.length;a++){var n=t[a];ze=n,su(n,e)}nu(e)}for(e=e.child;e!==null;){switch(t=e,t.tag){case 0:case 11:case 15:la(8,t,t.return),fs(t);break;case 22:a=t.stateNode,a._visibility&2&&(a._visibility&=-3,fs(t));break;default:fs(t)}e=e.sibling}}function su(e,t){for(;ze!==null;){var a=ze;switch(a.tag){case 0:case 11:case 15:la(8,a,t);break;case 23:case 22:if(a.memoizedState!==null&&a.memoizedState.cachePool!==null){var n=a.memoizedState.cachePool.pool;n!=null&&n.refCount++}break;case 24:Gn(a.memoizedState.cache)}if(n=a.child,n!==null)n.return=a,ze=n;else e:for(a=e;ze!==null;){n=ze;var i=n.sibling,s=n.return;if(Zd(n),n===a){ze=null;break e}if(i!==null){i.return=s,ze=i;break e}ze=s}}}var tm={getCacheForType:function(e){var t=He(qe),a=t.data.get(e);return a===void 0&&(a=e(),t.data.set(e,a)),a},cacheSignal:function(){return He(qe).controller.signal}},am=typeof WeakMap=="function"?WeakMap:Map,ae=0,fe=null,Z=null,F=0,se=0,lt=null,ca=!1,bn=!1,Ar=!1,Yt=0,ke=0,da=0,Oa=0,qr=0,ct=0,vn=0,ci=null,$e=null,Cr=!1,gs=0,ou=0,ys=1/0,bs=null,ua=null,Ie=0,ha=null,wn=null,Vt=0,Rr=0,Lr=null,ru=null,di=0,Ir=null;function dt(){return(ae&2)!==0&&F!==0?F&-F:w.T!==null?Wr():Tl()}function lu(){if(ct===0)if((F&536870912)===0||$){var e=qi;qi<<=1,(qi&3932160)===0&&(qi=262144),ct=e}else ct=536870912;return e=ot.current,e!==null&&(e.flags|=32),ct}function et(e,t,a){(e===fe&&(se===2||se===9)||e.cancelPendingCommit!==null)&&(xn(e,0),pa(e,F,ct,!1)),Dn(e,a),((ae&2)===0||e!==fe)&&(e===fe&&((ae&2)===0&&(Oa|=a),ke===4&&pa(e,F,ct,!1)),It(e))}function cu(e,t,a){if((ae&6)!==0)throw Error(h(327));var n=!a&&(t&127)===0&&(t&e.expiredLanes)===0||In(e,t),i=n?sm(e,t):Ur(e,t,!0),s=n;do{if(i===0){bn&&!n&&pa(e,t,0,!1);break}else{if(a=e.current.alternate,s&&!nm(a)){i=Ur(e,t,!1),s=!1;continue}if(i===2){if(s=t,e.errorRecoveryDisabledLanes&s)var o=0;else o=e.pendingLanes&-536870913,o=o!==0?o:o&536870912?536870912:0;if(o!==0){t=o;e:{var r=e;i=ci;var l=r.current.memoizedState.isDehydrated;if(l&&(xn(r,o).flags|=256),o=Ur(r,o,!1),o!==2){if(Ar&&!l){r.errorRecoveryDisabledLanes|=s,Oa|=s,i=4;break e}s=$e,$e=i,s!==null&&($e===null?$e=s:$e.push.apply($e,s))}i=o}if(s=!1,i!==2)continue}}if(i===1){xn(e,0),pa(e,t,0,!0);break}e:{switch(n=e,s=i,s){case 0:case 1:throw Error(h(345));case 4:if((t&4194048)!==t)break;case 6:pa(n,t,ct,!ca);break e;case 2:$e=null;break;case 3:case 5:break;default:throw Error(h(329))}if((t&62914560)===t&&(i=gs+300-tt(),10<i)){if(pa(n,t,ct,!ca),Ri(n,0,!0)!==0)break e;Vt=t,n.timeoutHandle=Mu(du.bind(null,n,a,$e,bs,Cr,t,ct,Oa,vn,ca,s,"Throttled",-0,0),i);break e}du(n,a,$e,bs,Cr,t,ct,Oa,vn,ca,s,null,-0,0)}}break}while(!0);It(e)}function du(e,t,a,n,i,s,o,r,l,m,b,k,f,g){if(e.timeoutHandle=-1,k=t.subtreeFlags,k&8192||(k&16785408)===16785408){k={stylesheets:null,count:0,imgCount:0,imgBytes:0,suspenseyImages:[],waitingForImages:!0,waitingForViewTransition:!1,unsuspend:zt},au(t,s,k);var R=(s&62914560)===s?gs-tt():(s&4194048)===s?ou-tt():0;if(R=Om(k,R),R!==null){Vt=s,e.cancelPendingCommit=R(bu.bind(null,e,t,s,a,n,i,o,r,l,b,k,null,f,g)),pa(e,s,o,!m);return}}bu(e,t,s,a,n,i,o,r,l)}function nm(e){for(var t=e;;){var a=t.tag;if((a===0||a===11||a===15)&&t.flags&16384&&(a=t.updateQueue,a!==null&&(a=a.stores,a!==null)))for(var n=0;n<a.length;n++){var i=a[n],s=i.getSnapshot;i=i.value;try{if(!it(s(),i))return!1}catch{return!1}}if(a=t.child,t.subtreeFlags&16384&&a!==null)a.return=t,t=a;else{if(t===e)break;for(;t.sibling===null;){if(t.return===null||t.return===e)return!0;t=t.return}t.sibling.return=t.return,t=t.sibling}}return!0}function pa(e,t,a,n){t&=~qr,t&=~Oa,e.suspendedLanes|=t,e.pingedLanes&=~t,n&&(e.warmLanes|=t),n=e.expirationTimes;for(var i=t;0<i;){var s=31-nt(i),o=1<<s;n[s]=-1,i&=~o}a!==0&&wl(e,a,t)}function vs(){return(ae&6)===0?(ui(0),!1):!0}function Dr(){if(Z!==null){if(se===0)var e=Z.return;else e=Z,Bt=Ua=null,Yo(e),un=null,Yn=0,e=Z;for(;e!==null;)Od(e.alternate,e),e=e.return;Z=null}}function xn(e,t){var a=e.timeoutHandle;a!==-1&&(e.timeoutHandle=-1,Tm(a)),a=e.cancelPendingCommit,a!==null&&(e.cancelPendingCommit=null,a()),Vt=0,Dr(),fe=e,Z=a=Et(e.current,null),F=t,se=0,lt=null,ca=!1,bn=In(e,t),Ar=!1,vn=ct=qr=Oa=da=ke=0,$e=ci=null,Cr=!1,(t&8)!==0&&(t|=t&32);var n=e.entangledLanes;if(n!==0)for(e=e.entanglements,n&=t;0<n;){var i=31-nt(n),s=1<<i;t|=e[i],n&=~s}return Yt=t,Mi(),a}function uu(e,t){_=null,w.H=ti,t===dn||t===Vi?(t=Cc(),se=3):t===Eo?(t=Cc(),se=4):se=t===cr?8:t!==null&&typeof t=="object"&&typeof t.then=="function"?6:1,lt=t,Z===null&&(ke=1,ls(e,mt(t,e.current)))}function hu(){var e=ot.current;return e===null?!0:(F&4194048)===F?bt===null:(F&62914560)===F||(F&536870912)!==0?e===bt:!1}function pu(){var e=w.H;return w.H=ti,e===null?ti:e}function mu(){var e=w.A;return w.A=tm,e}function ws(){ke=4,ca||(F&4194048)!==F&&ot.current!==null||(bn=!0),(da&134217727)===0&&(Oa&134217727)===0||fe===null||pa(fe,F,ct,!1)}function Ur(e,t,a){var n=ae;ae|=2;var i=pu(),s=mu();(fe!==e||F!==t)&&(bs=null,xn(e,t)),t=!1;var o=ke;e:do try{if(se!==0&&Z!==null){var r=Z,l=lt;switch(se){case 8:Dr(),o=6;break e;case 3:case 2:case 9:case 6:ot.current===null&&(t=!0);var m=se;if(se=0,lt=null,kn(e,r,l,m),a&&bn){o=0;break e}break;default:m=se,se=0,lt=null,kn(e,r,l,m)}}im(),o=ke;break}catch(b){uu(e,b)}while(!0);return t&&e.shellSuspendCounter++,Bt=Ua=null,ae=n,w.H=i,w.A=s,Z===null&&(fe=null,F=0,Mi()),o}function im(){for(;Z!==null;)fu(Z)}function sm(e,t){var a=ae;ae|=2;var n=pu(),i=mu();fe!==e||F!==t?(bs=null,ys=tt()+500,xn(e,t)):bn=In(e,t);e:do try{if(se!==0&&Z!==null){t=Z;var s=lt;t:switch(se){case 1:se=0,lt=null,kn(e,t,s,1);break;case 2:case 9:if(Ac(s)){se=0,lt=null,gu(t);break}t=function(){se!==2&&se!==9||fe!==e||(se=7),It(e)},s.then(t,t);break e;case 3:se=7;break e;case 4:se=5;break e;case 7:Ac(s)?(se=0,lt=null,gu(t)):(se=0,lt=null,kn(e,t,s,7));break;case 5:var o=null;switch(Z.tag){case 26:o=Z.memoizedState;case 5:case 27:var r=Z;if(o?th(o):r.stateNode.complete){se=0,lt=null;var l=r.sibling;if(l!==null)Z=l;else{var m=r.return;m!==null?(Z=m,xs(m)):Z=null}break t}}se=0,lt=null,kn(e,t,s,5);break;case 6:se=0,lt=null,kn(e,t,s,6);break;case 8:Dr(),ke=6;break e;default:throw Error(h(462))}}om();break}catch(b){uu(e,b)}while(!0);return Bt=Ua=null,w.H=n,w.A=i,ae=a,Z!==null?0:(fe=null,F=0,Mi(),ke)}function om(){for(;Z!==null&&!Lh();)fu(Z)}function fu(e){var t=Hd(e.alternate,e,Yt);e.memoizedProps=e.pendingProps,t===null?xs(e):Z=t}function gu(e){var t=e,a=t.alternate;switch(t.tag){case 15:case 0:t=zd(a,t,t.pendingProps,t.type,void 0,F);break;case 11:t=zd(a,t,t.pendingProps,t.type.render,t.ref,F);break;case 5:Yo(t);default:Od(a,t),t=Z=mc(t,Yt),t=Hd(a,t,Yt)}e.memoizedProps=e.pendingProps,t===null?xs(e):Z=t}function kn(e,t,a,n){Bt=Ua=null,Yo(t),un=null,Yn=0;var i=t.return;try{if(Vp(e,i,t,a,F)){ke=1,ls(e,mt(a,e.current)),Z=null;return}}catch(s){if(i!==null)throw Z=i,s;ke=1,ls(e,mt(a,e.current)),Z=null;return}t.flags&32768?($||n===1?e=!0:bn||(F&536870912)!==0?e=!1:(ca=e=!0,(n===2||n===9||n===3||n===6)&&(n=ot.current,n!==null&&n.tag===13&&(n.flags|=16384))),yu(t,e)):xs(t)}function xs(e){var t=e;do{if((t.flags&32768)!==0){yu(t,ca);return}e=t.return;var a=Fp(t.alternate,t,Yt);if(a!==null){Z=a;return}if(t=t.sibling,t!==null){Z=t;return}Z=t=e}while(t!==null);ke===0&&(ke=5)}function yu(e,t){do{var a=Jp(e.alternate,e);if(a!==null){a.flags&=32767,Z=a;return}if(a=e.return,a!==null&&(a.flags|=32768,a.subtreeFlags=0,a.deletions=null),!t&&(e=e.sibling,e!==null)){Z=e;return}Z=e=a}while(e!==null);ke=6,Z=null}function bu(e,t,a,n,i,s,o,r,l){e.cancelPendingCommit=null;do ks();while(Ie!==0);if((ae&6)!==0)throw Error(h(327));if(t!==null){if(t===e.current)throw Error(h(177));if(s=t.lanes|t.childLanes,s|=wo,Hh(e,a,s,o,r,l),e===fe&&(Z=fe=null,F=0),wn=t,ha=e,Vt=a,Rr=s,Lr=i,ru=n,(t.subtreeFlags&10256)!==0||(t.flags&10256)!==0?(e.callbackNode=null,e.callbackPriority=0,dm(Si,function(){return Tu(),null})):(e.callbackNode=null,e.callbackPriority=0),n=(t.flags&13878)!==0,(t.subtreeFlags&13878)!==0||n){n=w.T,w.T=null,i=A.p,A.p=2,o=ae,ae|=4;try{$p(e,t,a)}finally{ae=o,A.p=i,w.T=n}}Ie=1,vu(),wu(),xu()}}function vu(){if(Ie===1){Ie=0;var e=ha,t=wn,a=(t.flags&13878)!==0;if((t.subtreeFlags&13878)!==0||a){a=w.T,w.T=null;var n=A.p;A.p=2;var i=ae;ae|=4;try{$d(t,e);var s=_r,o=sc(e.containerInfo),r=s.focusedElem,l=s.selectionRange;if(o!==r&&r&&r.ownerDocument&&ic(r.ownerDocument.documentElement,r)){if(l!==null&&fo(r)){var m=l.start,b=l.end;if(b===void 0&&(b=m),"selectionStart"in r)r.selectionStart=m,r.selectionEnd=Math.min(b,r.value.length);else{var k=r.ownerDocument||document,f=k&&k.defaultView||window;if(f.getSelection){var g=f.getSelection(),R=r.textContent.length,W=Math.min(l.start,R),he=l.end===void 0?W:Math.min(l.end,R);!g.extend&&W>he&&(o=he,he=W,W=o);var u=nc(r,W),c=nc(r,he);if(u&&c&&(g.rangeCount!==1||g.anchorNode!==u.node||g.anchorOffset!==u.offset||g.focusNode!==c.node||g.focusOffset!==c.offset)){var p=k.createRange();p.setStart(u.node,u.offset),g.removeAllRanges(),W>he?(g.addRange(p),g.extend(c.node,c.offset)):(p.setEnd(c.node,c.offset),g.addRange(p))}}}}for(k=[],g=r;g=g.parentNode;)g.nodeType===1&&k.push({element:g,left:g.scrollLeft,top:g.scrollTop});for(typeof r.focus=="function"&&r.focus(),r=0;r<k.length;r++){var x=k[r];x.element.scrollLeft=x.left,x.element.scrollTop=x.top}}Ps=!!jr,_r=jr=null}finally{ae=i,A.p=n,w.T=a}}e.current=t,Ie=2}}function wu(){if(Ie===2){Ie=0;var e=ha,t=wn,a=(t.flags&8772)!==0;if((t.subtreeFlags&8772)!==0||a){a=w.T,w.T=null;var n=A.p;A.p=2;var i=ae;ae|=4;try{Vd(e,t.alternate,t)}finally{ae=i,A.p=n,w.T=a}}Ie=3}}function xu(){if(Ie===4||Ie===3){Ie=0,Ih();var e=ha,t=wn,a=Vt,n=ru;(t.subtreeFlags&10256)!==0||(t.flags&10256)!==0?Ie=5:(Ie=0,wn=ha=null,ku(e,e.pendingLanes));var i=e.pendingLanes;if(i===0&&(ua=null),Zs(a),t=t.stateNode,at&&typeof at.onCommitFiberRoot=="function")try{at.onCommitFiberRoot(Ln,t,void 0,(t.current.flags&128)===128)}catch{}if(n!==null){t=w.T,i=A.p,A.p=2,w.T=null;try{for(var s=e.onRecoverableError,o=0;o<n.length;o++){var r=n[o];s(r.value,{componentStack:r.stack})}}finally{w.T=t,A.p=i}}(Vt&3)!==0&&ks(),It(e),i=e.pendingLanes,(a&261930)!==0&&(i&42)!==0?e===Ir?di++:(di=0,Ir=e):di=0,ui(0)}}function ku(e,t){(e.pooledCacheLanes&=t)===0&&(t=e.pooledCache,t!=null&&(e.pooledCache=null,Gn(t)))}function ks(){return vu(),wu(),xu(),Tu()}function Tu(){if(Ie!==5)return!1;var e=ha,t=Rr;Rr=0;var a=Zs(Vt),n=w.T,i=A.p;try{A.p=32>a?32:a,w.T=null,a=Lr,Lr=null;var s=ha,o=Vt;if(Ie=0,wn=ha=null,Vt=0,(ae&6)!==0)throw Error(h(331));var r=ae;if(ae|=4,iu(s.current),tu(s,s.current,o,a),ae=r,ui(0,!1),at&&typeof at.onPostCommitFiberRoot=="function")try{at.onPostCommitFiberRoot(Ln,s)}catch{}return!0}finally{A.p=i,w.T=n,ku(e,t)}}function Su(e,t,a){t=mt(a,t),t=lr(e.stateNode,t,2),e=sa(e,t,2),e!==null&&(Dn(e,2),It(e))}function oe(e,t,a){if(e.tag===3)Su(e,e,a);else for(;t!==null;){if(t.tag===3){Su(t,e,a);break}else if(t.tag===1){var n=t.stateNode;if(typeof t.type.getDerivedStateFromError=="function"||typeof n.componentDidCatch=="function"&&(ua===null||!ua.has(n))){e=mt(a,e),a=Ad(2),n=sa(t,a,2),n!==null&&(qd(a,n,t,e),Dn(n,2),It(n));break}}t=t.return}}function zr(e,t,a){var n=e.pingCache;if(n===null){n=e.pingCache=new am;var i=new Set;n.set(t,i)}else i=n.get(t),i===void 0&&(i=new Set,n.set(t,i));i.has(a)||(Ar=!0,i.add(a),e=rm.bind(null,e,t,a),t.then(e,e))}function rm(e,t,a){var n=e.pingCache;n!==null&&n.delete(t),e.pingedLanes|=e.suspendedLanes&a,e.warmLanes&=~a,fe===e&&(F&a)===a&&(ke===4||ke===3&&(F&62914560)===F&&300>tt()-gs?(ae&2)===0&&xn(e,0):qr|=a,vn===F&&(vn=0)),It(e)}function Au(e,t){t===0&&(t=vl()),e=La(e,t),e!==null&&(Dn(e,t),It(e))}function lm(e){var t=e.memoizedState,a=0;t!==null&&(a=t.retryLane),Au(e,a)}function cm(e,t){var a=0;switch(e.tag){case 31:case 13:var n=e.stateNode,i=e.memoizedState;i!==null&&(a=i.retryLane);break;case 19:n=e.stateNode;break;case 22:n=e.stateNode._retryCache;break;default:throw Error(h(314))}n!==null&&n.delete(t),Au(e,a)}function dm(e,t){return Gs(e,t)}var Ts=null,Tn=null,Pr=!1,Ss=!1,Er=!1,ma=0;function It(e){e!==Tn&&e.next===null&&(Tn===null?Ts=Tn=e:Tn=Tn.next=e),Ss=!0,Pr||(Pr=!0,hm())}function ui(e,t){if(!Er&&Ss){Er=!0;do for(var a=!1,n=Ts;n!==null;){if(e!==0){var i=n.pendingLanes;if(i===0)var s=0;else{var o=n.suspendedLanes,r=n.pingedLanes;s=(1<<31-nt(42|e)+1)-1,s&=i&~(o&~r),s=s&201326741?s&201326741|1:s?s|2:0}s!==0&&(a=!0,Lu(n,s))}else s=F,s=Ri(n,n===fe?s:0,n.cancelPendingCommit!==null||n.timeoutHandle!==-1),(s&3)===0||In(n,s)||(a=!0,Lu(n,s));n=n.next}while(a);Er=!1}}function um(){qu()}function qu(){Ss=Pr=!1;var e=0;ma!==0&&km()&&(e=ma);for(var t=tt(),a=null,n=Ts;n!==null;){var i=n.next,s=Cu(n,t);s===0?(n.next=null,a===null?Ts=i:a.next=i,i===null&&(Tn=a)):(a=n,(e!==0||(s&3)!==0)&&(Ss=!0)),n=i}Ie!==0&&Ie!==5||ui(e),ma!==0&&(ma=0)}function Cu(e,t){for(var a=e.suspendedLanes,n=e.pingedLanes,i=e.expirationTimes,s=e.pendingLanes&-62914561;0<s;){var o=31-nt(s),r=1<<o,l=i[o];l===-1?((r&a)===0||(r&n)!==0)&&(i[o]=Nh(r,t)):l<=t&&(e.expiredLanes|=r),s&=~r}if(t=fe,a=F,a=Ri(e,e===t?a:0,e.cancelPendingCommit!==null||e.timeoutHandle!==-1),n=e.callbackNode,a===0||e===t&&(se===2||se===9)||e.cancelPendingCommit!==null)return n!==null&&n!==null&&Ks(n),e.callbackNode=null,e.callbackPriority=0;if((a&3)===0||In(e,a)){if(t=a&-a,t===e.callbackPriority)return t;switch(n!==null&&Ks(n),Zs(a)){case 2:case 8:a=yl;break;case 32:a=Si;break;case 268435456:a=bl;break;default:a=Si}return n=Ru.bind(null,e),a=Gs(a,n),e.callbackPriority=t,e.callbackNode=a,t}return n!==null&&n!==null&&Ks(n),e.callbackPriority=2,e.callbackNode=null,2}function Ru(e,t){if(Ie!==0&&Ie!==5)return e.callbackNode=null,e.callbackPriority=0,null;var a=e.callbackNode;if(ks()&&e.callbackNode!==a)return null;var n=F;return n=Ri(e,e===fe?n:0,e.cancelPendingCommit!==null||e.timeoutHandle!==-1),n===0?null:(cu(e,n,t),Cu(e,tt()),e.callbackNode!=null&&e.callbackNode===a?Ru.bind(null,e):null)}function Lu(e,t){if(ks())return null;cu(e,t,!0)}function hm(){Sm(function(){(ae&6)!==0?Gs(gl,um):qu()})}function Wr(){if(ma===0){var e=ln;e===0&&(e=Ai,Ai<<=1,(Ai&261888)===0&&(Ai=256)),ma=e}return ma}function Iu(e){return e==null||typeof e=="symbol"||typeof e=="boolean"?null:typeof e=="function"?e:Ui(""+e)}function Du(e,t){var a=t.ownerDocument.createElement("input");return a.name=t.name,a.value=t.value,e.id&&a.setAttribute("form",e.id),t.parentNode.insertBefore(a,t),e=new FormData(e),a.parentNode.removeChild(a),e}function pm(e,t,a,n,i){if(t==="submit"&&a&&a.stateNode===i){var s=Iu((i[Ve]||null).action),o=n.submitter;o&&(t=(t=o[Ve]||null)?Iu(t.formAction):o.getAttribute("formAction"),t!==null&&(s=t,o=null));var r=new Wi("action","action",null,n,i);e.push({event:r,listeners:[{instance:null,listener:function(){if(n.defaultPrevented){if(ma!==0){var l=o?Du(i,o):new FormData(i);ar(a,{pending:!0,data:l,method:i.method,action:s},null,l)}}else typeof s=="function"&&(r.preventDefault(),l=o?Du(i,o):new FormData(i),ar(a,{pending:!0,data:l,method:i.method,action:s},s,l))},currentTarget:i}]})}}for(var Br=0;Br<vo.length;Br++){var Nr=vo[Br],mm=Nr.toLowerCase(),fm=Nr[0].toUpperCase()+Nr.slice(1);kt(mm,"on"+fm)}kt(lc,"onAnimationEnd"),kt(cc,"onAnimationIteration"),kt(dc,"onAnimationStart"),kt("dblclick","onDoubleClick"),kt("focusin","onFocus"),kt("focusout","onBlur"),kt(Dp,"onTransitionRun"),kt(Up,"onTransitionStart"),kt(zp,"onTransitionCancel"),kt(uc,"onTransitionEnd"),Ya("onMouseEnter",["mouseout","mouseover"]),Ya("onMouseLeave",["mouseout","mouseover"]),Ya("onPointerEnter",["pointerout","pointerover"]),Ya("onPointerLeave",["pointerout","pointerover"]),Aa("onChange","change click focusin focusout input keydown keyup selectionchange".split(" ")),Aa("onSelect","focusout contextmenu dragend focusin keydown keyup mousedown mouseup selectionchange".split(" ")),Aa("onBeforeInput",["compositionend","keypress","textInput","paste"]),Aa("onCompositionEnd","compositionend focusout keydown keypress keyup mousedown".split(" ")),Aa("onCompositionStart","compositionstart focusout keydown keypress keyup mousedown".split(" ")),Aa("onCompositionUpdate","compositionupdate focusout keydown keypress keyup mousedown".split(" "));var hi="abort canplay canplaythrough durationchange emptied encrypted ended error loadeddata loadedmetadata loadstart pause play playing progress ratechange resize seeked seeking stalled suspend timeupdate volumechange waiting".split(" "),gm=new Set("beforetoggle cancel close invalid load scroll scrollend toggle".split(" ").concat(hi));function Uu(e,t){t=(t&4)!==0;for(var a=0;a<e.length;a++){var n=e[a],i=n.event;n=n.listeners;e:{var s=void 0;if(t)for(var o=n.length-1;0<=o;o--){var r=n[o],l=r.instance,m=r.currentTarget;if(r=r.listener,l!==s&&i.isPropagationStopped())break e;s=r,i.currentTarget=m;try{s(i)}catch(b){Hi(b)}i.currentTarget=null,s=l}else for(o=0;o<n.length;o++){if(r=n[o],l=r.instance,m=r.currentTarget,r=r.listener,l!==s&&i.isPropagationStopped())break e;s=r,i.currentTarget=m;try{s(i)}catch(b){Hi(b)}i.currentTarget=null,s=l}}}}function X(e,t){var a=t[Xs];a===void 0&&(a=t[Xs]=new Set);var n=e+"__bubble";a.has(n)||(zu(t,e,2,!1),a.add(n))}function Hr(e,t,a){var n=0;t&&(n|=4),zu(a,e,n,t)}var As="_reactListening"+Math.random().toString(36).slice(2);function Mr(e){if(!e[As]){e[As]=!0,ql.forEach(function(a){a!=="selectionchange"&&(gm.has(a)||Hr(a,!1,e),Hr(a,!0,e))});var t=e.nodeType===9?e:e.ownerDocument;t===null||t[As]||(t[As]=!0,Hr("selectionchange",!1,t))}}function zu(e,t,a,n){switch(lh(t)){case 2:var i=_m;break;case 8:i=Gm;break;default:i=tl}a=i.bind(null,t,a,e),i=void 0,!so||t!=="touchstart"&&t!=="touchmove"&&t!=="wheel"||(i=!0),n?i!==void 0?e.addEventListener(t,a,{capture:!0,passive:i}):e.addEventListener(t,a,!0):i!==void 0?e.addEventListener(t,a,{passive:i}):e.addEventListener(t,a,!1)}function Or(e,t,a,n,i){var s=n;if((t&1)===0&&(t&2)===0&&n!==null)e:for(;;){if(n===null)return;var o=n.tag;if(o===3||o===4){var r=n.stateNode.containerInfo;if(r===i)break;if(o===4)for(o=n.return;o!==null;){var l=o.tag;if((l===3||l===4)&&o.stateNode.containerInfo===i)return;o=o.return}for(;r!==null;){if(o=_a(r),o===null)return;if(l=o.tag,l===5||l===6||l===26||l===27){n=s=o;continue e}r=r.parentNode}}n=n.return}Nl(function(){var m=s,b=no(a),k=[];e:{var f=hc.get(e);if(f!==void 0){var g=Wi,R=e;switch(e){case"keypress":if(Pi(a)===0)break e;case"keydown":case"keyup":g=cp;break;case"focusin":R="focus",g=co;break;case"focusout":R="blur",g=co;break;case"beforeblur":case"afterblur":g=co;break;case"click":if(a.button===2)break e;case"auxclick":case"dblclick":case"mousedown":case"mousemove":case"mouseup":case"mouseout":case"mouseover":case"contextmenu":g=Ol;break;case"drag":case"dragend":case"dragenter":case"dragexit":case"dragleave":case"dragover":case"dragstart":case"drop":g=Fh;break;case"touchcancel":case"touchend":case"touchmove":case"touchstart":g=hp;break;case lc:case cc:case dc:g=ep;break;case uc:g=mp;break;case"scroll":case"scrollend":g=Zh;break;case"wheel":g=gp;break;case"copy":case"cut":case"paste":g=ap;break;case"gotpointercapture":case"lostpointercapture":case"pointercancel":case"pointerdown":case"pointermove":case"pointerout":case"pointerover":case"pointerup":g=jl;break;case"toggle":case"beforetoggle":g=bp}var W=(t&4)!==0,he=!W&&(e==="scroll"||e==="scrollend"),u=W?f!==null?f+"Capture":null:f;W=[];for(var c=m,p;c!==null;){var x=c;if(p=x.stateNode,x=x.tag,x!==5&&x!==26&&x!==27||p===null||u===null||(x=Pn(c,u),x!=null&&W.push(pi(c,x,p))),he)break;c=c.return}0<W.length&&(f=new g(f,R,null,a,b),k.push({event:f,listeners:W}))}}if((t&7)===0){e:{if(f=e==="mouseover"||e==="pointerover",g=e==="mouseout"||e==="pointerout",f&&a!==ao&&(R=a.relatedTarget||a.fromElement)&&(_a(R)||R[ja]))break e;if((g||f)&&(f=b.window===b?b:(f=b.ownerDocument)?f.defaultView||f.parentWindow:window,g?(R=a.relatedTarget||a.toElement,g=m,R=R?_a(R):null,R!==null&&(he=E(R),W=R.tag,R!==he||W!==5&&W!==27&&W!==6)&&(R=null)):(g=null,R=m),g!==R)){if(W=Ol,x="onMouseLeave",u="onMouseEnter",c="mouse",(e==="pointerout"||e==="pointerover")&&(W=jl,x="onPointerLeave",u="onPointerEnter",c="pointer"),he=g==null?f:zn(g),p=R==null?f:zn(R),f=new W(x,c+"leave",g,a,b),f.target=he,f.relatedTarget=p,x=null,_a(b)===m&&(W=new W(u,c+"enter",R,a,b),W.target=p,W.relatedTarget=he,x=W),he=x,g&&R)t:{for(W=ym,u=g,c=R,p=0,x=u;x;x=W(x))p++;x=0;for(var z=c;z;z=W(z))x++;for(;0<p-x;)u=W(u),p--;for(;0<x-p;)c=W(c),x--;for(;p--;){if(u===c||c!==null&&u===c.alternate){W=u;break t}u=W(u),c=W(c)}W=null}else W=null;g!==null&&Pu(k,f,g,W,!1),R!==null&&he!==null&&Pu(k,he,R,W,!0)}}e:{if(f=m?zn(m):window,g=f.nodeName&&f.nodeName.toLowerCase(),g==="select"||g==="input"&&f.type==="file")var ee=Fl;else if(Zl(f))if(Jl)ee=Rp;else{ee=qp;var D=Ap}else g=f.nodeName,!g||g.toLowerCase()!=="input"||f.type!=="checkbox"&&f.type!=="radio"?m&&to(m.elementType)&&(ee=Fl):ee=Cp;if(ee&&(ee=ee(e,m))){Xl(k,ee,a,b);break e}D&&D(e,f,m),e==="focusout"&&m&&f.type==="number"&&m.memoizedProps.value!=null&&eo(f,"number",f.value)}switch(D=m?zn(m):window,e){case"focusin":(Zl(D)||D.contentEditable==="true")&&($a=D,go=m,Qn=null);break;case"focusout":Qn=go=$a=null;break;case"mousedown":yo=!0;break;case"contextmenu":case"mouseup":case"dragend":yo=!1,oc(k,a,b);break;case"selectionchange":if(Ip)break;case"keydown":case"keyup":oc(k,a,b)}var G;if(ho)e:{switch(e){case"compositionstart":var J="onCompositionStart";break e;case"compositionend":J="onCompositionEnd";break e;case"compositionupdate":J="onCompositionUpdate";break e}J=void 0}else Ja?Yl(e,a)&&(J="onCompositionEnd"):e==="keydown"&&a.keyCode===229&&(J="onCompositionStart");J&&(_l&&a.locale!=="ko"&&(Ja||J!=="onCompositionStart"?J==="onCompositionEnd"&&Ja&&(G=Hl()):(Jt=b,oo="value"in Jt?Jt.value:Jt.textContent,Ja=!0)),D=qs(m,J),0<D.length&&(J=new Ql(J,e,null,a,b),k.push({event:J,listeners:D}),G?J.data=G:(G=Vl(a),G!==null&&(J.data=G)))),(G=wp?xp(e,a):kp(e,a))&&(J=qs(m,"onBeforeInput"),0<J.length&&(D=new Ql("onBeforeInput","beforeinput",null,a,b),k.push({event:D,listeners:J}),D.data=G)),pm(k,e,m,a,b)}Uu(k,t)})}function pi(e,t,a){return{instance:e,listener:t,currentTarget:a}}function qs(e,t){for(var a=t+"Capture",n=[];e!==null;){var i=e,s=i.stateNode;if(i=i.tag,i!==5&&i!==26&&i!==27||s===null||(i=Pn(e,a),i!=null&&n.unshift(pi(e,i,s)),i=Pn(e,t),i!=null&&n.push(pi(e,i,s))),e.tag===3)return n;e=e.return}return[]}function ym(e){if(e===null)return null;do e=e.return;while(e&&e.tag!==5&&e.tag!==27);return e||null}function Pu(e,t,a,n,i){for(var s=t._reactName,o=[];a!==null&&a!==n;){var r=a,l=r.alternate,m=r.stateNode;if(r=r.tag,l!==null&&l===n)break;r!==5&&r!==26&&r!==27||m===null||(l=m,i?(m=Pn(a,s),m!=null&&o.unshift(pi(a,m,l))):i||(m=Pn(a,s),m!=null&&o.push(pi(a,m,l)))),a=a.return}o.length!==0&&e.push({event:t,listeners:o})}var bm=/\r\n?/g,vm=/\u0000|\uFFFD/g;function Eu(e){return(typeof e=="string"?e:""+e).replace(bm,`
`).replace(vm,"")}function Wu(e,t){return t=Eu(t),Eu(e)===t}function ue(e,t,a,n,i,s){switch(a){case"children":typeof n=="string"?t==="body"||t==="textarea"&&n===""||Za(e,n):(typeof n=="number"||typeof n=="bigint")&&t!=="body"&&Za(e,""+n);break;case"className":Ii(e,"class",n);break;case"tabIndex":Ii(e,"tabindex",n);break;case"dir":case"role":case"viewBox":case"width":case"height":Ii(e,a,n);break;case"style":Wl(e,n,s);break;case"data":if(t!=="object"){Ii(e,"data",n);break}case"src":case"href":if(n===""&&(t!=="a"||a!=="href")){e.removeAttribute(a);break}if(n==null||typeof n=="function"||typeof n=="symbol"||typeof n=="boolean"){e.removeAttribute(a);break}n=Ui(""+n),e.setAttribute(a,n);break;case"action":case"formAction":if(typeof n=="function"){e.setAttribute(a,"javascript:throw new Error('A React form was unexpectedly submitted. If you called form.submit() manually, consider using form.requestSubmit() instead. If you\\'re trying to use event.stopPropagation() in a submit event handler, consider also calling event.preventDefault().')");break}else typeof s=="function"&&(a==="formAction"?(t!=="input"&&ue(e,t,"name",i.name,i,null),ue(e,t,"formEncType",i.formEncType,i,null),ue(e,t,"formMethod",i.formMethod,i,null),ue(e,t,"formTarget",i.formTarget,i,null)):(ue(e,t,"encType",i.encType,i,null),ue(e,t,"method",i.method,i,null),ue(e,t,"target",i.target,i,null)));if(n==null||typeof n=="symbol"||typeof n=="boolean"){e.removeAttribute(a);break}n=Ui(""+n),e.setAttribute(a,n);break;case"onClick":n!=null&&(e.onclick=zt);break;case"onScroll":n!=null&&X("scroll",e);break;case"onScrollEnd":n!=null&&X("scrollend",e);break;case"dangerouslySetInnerHTML":if(n!=null){if(typeof n!="object"||!("__html"in n))throw Error(h(61));if(a=n.__html,a!=null){if(i.children!=null)throw Error(h(60));e.innerHTML=a}}break;case"multiple":e.multiple=n&&typeof n!="function"&&typeof n!="symbol";break;case"muted":e.muted=n&&typeof n!="function"&&typeof n!="symbol";break;case"suppressContentEditableWarning":case"suppressHydrationWarning":case"defaultValue":case"defaultChecked":case"innerHTML":case"ref":break;case"autoFocus":break;case"xlinkHref":if(n==null||typeof n=="function"||typeof n=="boolean"||typeof n=="symbol"){e.removeAttribute("xlink:href");break}a=Ui(""+n),e.setAttributeNS("http://www.w3.org/1999/xlink","xlink:href",a);break;case"contentEditable":case"spellCheck":case"draggable":case"value":case"autoReverse":case"externalResourcesRequired":case"focusable":case"preserveAlpha":n!=null&&typeof n!="function"&&typeof n!="symbol"?e.setAttribute(a,""+n):e.removeAttribute(a);break;case"inert":case"allowFullScreen":case"async":case"autoPlay":case"controls":case"default":case"defer":case"disabled":case"disablePictureInPicture":case"disableRemotePlayback":case"formNoValidate":case"hidden":case"loop":case"noModule":case"noValidate":case"open":case"playsInline":case"readOnly":case"required":case"reversed":case"scoped":case"seamless":case"itemScope":n&&typeof n!="function"&&typeof n!="symbol"?e.setAttribute(a,""):e.removeAttribute(a);break;case"capture":case"download":n===!0?e.setAttribute(a,""):n!==!1&&n!=null&&typeof n!="function"&&typeof n!="symbol"?e.setAttribute(a,n):e.removeAttribute(a);break;case"cols":case"rows":case"size":case"span":n!=null&&typeof n!="function"&&typeof n!="symbol"&&!isNaN(n)&&1<=n?e.setAttribute(a,n):e.removeAttribute(a);break;case"rowSpan":case"start":n==null||typeof n=="function"||typeof n=="symbol"||isNaN(n)?e.removeAttribute(a):e.setAttribute(a,n);break;case"popover":X("beforetoggle",e),X("toggle",e),Li(e,"popover",n);break;case"xlinkActuate":Ut(e,"http://www.w3.org/1999/xlink","xlink:actuate",n);break;case"xlinkArcrole":Ut(e,"http://www.w3.org/1999/xlink","xlink:arcrole",n);break;case"xlinkRole":Ut(e,"http://www.w3.org/1999/xlink","xlink:role",n);break;case"xlinkShow":Ut(e,"http://www.w3.org/1999/xlink","xlink:show",n);break;case"xlinkTitle":Ut(e,"http://www.w3.org/1999/xlink","xlink:title",n);break;case"xlinkType":Ut(e,"http://www.w3.org/1999/xlink","xlink:type",n);break;case"xmlBase":Ut(e,"http://www.w3.org/XML/1998/namespace","xml:base",n);break;case"xmlLang":Ut(e,"http://www.w3.org/XML/1998/namespace","xml:lang",n);break;case"xmlSpace":Ut(e,"http://www.w3.org/XML/1998/namespace","xml:space",n);break;case"is":Li(e,"is",n);break;case"innerText":case"textContent":break;default:(!(2<a.length)||a[0]!=="o"&&a[0]!=="O"||a[1]!=="n"&&a[1]!=="N")&&(a=Yh.get(a)||a,Li(e,a,n))}}function Qr(e,t,a,n,i,s){switch(a){case"style":Wl(e,n,s);break;case"dangerouslySetInnerHTML":if(n!=null){if(typeof n!="object"||!("__html"in n))throw Error(h(61));if(a=n.__html,a!=null){if(i.children!=null)throw Error(h(60));e.innerHTML=a}}break;case"children":typeof n=="string"?Za(e,n):(typeof n=="number"||typeof n=="bigint")&&Za(e,""+n);break;case"onScroll":n!=null&&X("scroll",e);break;case"onScrollEnd":n!=null&&X("scrollend",e);break;case"onClick":n!=null&&(e.onclick=zt);break;case"suppressContentEditableWarning":case"suppressHydrationWarning":case"innerHTML":case"ref":break;case"innerText":case"textContent":break;default:if(!Cl.hasOwnProperty(a))e:{if(a[0]==="o"&&a[1]==="n"&&(i=a.endsWith("Capture"),t=a.slice(2,i?a.length-7:void 0),s=e[Ve]||null,s=s!=null?s[a]:null,typeof s=="function"&&e.removeEventListener(t,s,i),typeof n=="function")){typeof s!="function"&&s!==null&&(a in e?e[a]=null:e.hasAttribute(a)&&e.removeAttribute(a)),e.addEventListener(t,n,i);break e}a in e?e[a]=n:n===!0?e.setAttribute(a,""):Li(e,a,n)}}}function Oe(e,t,a){switch(t){case"div":case"span":case"svg":case"path":case"a":case"g":case"p":case"li":break;case"img":X("error",e),X("load",e);var n=!1,i=!1,s;for(s in a)if(a.hasOwnProperty(s)){var o=a[s];if(o!=null)switch(s){case"src":n=!0;break;case"srcSet":i=!0;break;case"children":case"dangerouslySetInnerHTML":throw Error(h(137,t));default:ue(e,t,s,o,a,null)}}i&&ue(e,t,"srcSet",a.srcSet,a,null),n&&ue(e,t,"src",a.src,a,null);return;case"input":X("invalid",e);var r=s=o=i=null,l=null,m=null;for(n in a)if(a.hasOwnProperty(n)){var b=a[n];if(b!=null)switch(n){case"name":i=b;break;case"type":o=b;break;case"checked":l=b;break;case"defaultChecked":m=b;break;case"value":s=b;break;case"defaultValue":r=b;break;case"children":case"dangerouslySetInnerHTML":if(b!=null)throw Error(h(137,t));break;default:ue(e,t,n,b,a,null)}}Ul(e,s,r,l,m,o,i,!1);return;case"select":X("invalid",e),n=o=s=null;for(i in a)if(a.hasOwnProperty(i)&&(r=a[i],r!=null))switch(i){case"value":s=r;break;case"defaultValue":o=r;break;case"multiple":n=r;default:ue(e,t,i,r,a,null)}t=s,a=o,e.multiple=!!n,t!=null?Va(e,!!n,t,!1):a!=null&&Va(e,!!n,a,!0);return;case"textarea":X("invalid",e),s=i=n=null;for(o in a)if(a.hasOwnProperty(o)&&(r=a[o],r!=null))switch(o){case"value":n=r;break;case"defaultValue":i=r;break;case"children":s=r;break;case"dangerouslySetInnerHTML":if(r!=null)throw Error(h(91));break;default:ue(e,t,o,r,a,null)}Pl(e,n,i,s);return;case"option":for(l in a)if(a.hasOwnProperty(l)&&(n=a[l],n!=null))switch(l){case"selected":e.selected=n&&typeof n!="function"&&typeof n!="symbol";break;default:ue(e,t,l,n,a,null)}return;case"dialog":X("beforetoggle",e),X("toggle",e),X("cancel",e),X("close",e);break;case"iframe":case"object":X("load",e);break;case"video":case"audio":for(n=0;n<hi.length;n++)X(hi[n],e);break;case"image":X("error",e),X("load",e);break;case"details":X("toggle",e);break;case"embed":case"source":case"link":X("error",e),X("load",e);case"area":case"base":case"br":case"col":case"hr":case"keygen":case"meta":case"param":case"track":case"wbr":case"menuitem":for(m in a)if(a.hasOwnProperty(m)&&(n=a[m],n!=null))switch(m){case"children":case"dangerouslySetInnerHTML":throw Error(h(137,t));default:ue(e,t,m,n,a,null)}return;default:if(to(t)){for(b in a)a.hasOwnProperty(b)&&(n=a[b],n!==void 0&&Qr(e,t,b,n,a,void 0));return}}for(r in a)a.hasOwnProperty(r)&&(n=a[r],n!=null&&ue(e,t,r,n,a,null))}function wm(e,t,a,n){switch(t){case"div":case"span":case"svg":case"path":case"a":case"g":case"p":case"li":break;case"input":var i=null,s=null,o=null,r=null,l=null,m=null,b=null;for(g in a){var k=a[g];if(a.hasOwnProperty(g)&&k!=null)switch(g){case"checked":break;case"value":break;case"defaultValue":l=k;default:n.hasOwnProperty(g)||ue(e,t,g,null,n,k)}}for(var f in n){var g=n[f];if(k=a[f],n.hasOwnProperty(f)&&(g!=null||k!=null))switch(f){case"type":s=g;break;case"name":i=g;break;case"checked":m=g;break;case"defaultChecked":b=g;break;case"value":o=g;break;case"defaultValue":r=g;break;case"children":case"dangerouslySetInnerHTML":if(g!=null)throw Error(h(137,t));break;default:g!==k&&ue(e,t,f,g,n,k)}}$s(e,o,r,l,m,b,s,i);return;case"select":g=o=r=f=null;for(s in a)if(l=a[s],a.hasOwnProperty(s)&&l!=null)switch(s){case"value":break;case"multiple":g=l;default:n.hasOwnProperty(s)||ue(e,t,s,null,n,l)}for(i in n)if(s=n[i],l=a[i],n.hasOwnProperty(i)&&(s!=null||l!=null))switch(i){case"value":f=s;break;case"defaultValue":r=s;break;case"multiple":o=s;default:s!==l&&ue(e,t,i,s,n,l)}t=r,a=o,n=g,f!=null?Va(e,!!a,f,!1):!!n!=!!a&&(t!=null?Va(e,!!a,t,!0):Va(e,!!a,a?[]:"",!1));return;case"textarea":g=f=null;for(r in a)if(i=a[r],a.hasOwnProperty(r)&&i!=null&&!n.hasOwnProperty(r))switch(r){case"value":break;case"children":break;default:ue(e,t,r,null,n,i)}for(o in n)if(i=n[o],s=a[o],n.hasOwnProperty(o)&&(i!=null||s!=null))switch(o){case"value":f=i;break;case"defaultValue":g=i;break;case"children":break;case"dangerouslySetInnerHTML":if(i!=null)throw Error(h(91));break;default:i!==s&&ue(e,t,o,i,n,s)}zl(e,f,g);return;case"option":for(var R in a)if(f=a[R],a.hasOwnProperty(R)&&f!=null&&!n.hasOwnProperty(R))switch(R){case"selected":e.selected=!1;break;default:ue(e,t,R,null,n,f)}for(l in n)if(f=n[l],g=a[l],n.hasOwnProperty(l)&&f!==g&&(f!=null||g!=null))switch(l){case"selected":e.selected=f&&typeof f!="function"&&typeof f!="symbol";break;default:ue(e,t,l,f,n,g)}return;case"img":case"link":case"area":case"base":case"br":case"col":case"embed":case"hr":case"keygen":case"meta":case"param":case"source":case"track":case"wbr":case"menuitem":for(var W in a)f=a[W],a.hasOwnProperty(W)&&f!=null&&!n.hasOwnProperty(W)&&ue(e,t,W,null,n,f);for(m in n)if(f=n[m],g=a[m],n.hasOwnProperty(m)&&f!==g&&(f!=null||g!=null))switch(m){case"children":case"dangerouslySetInnerHTML":if(f!=null)throw Error(h(137,t));break;default:ue(e,t,m,f,n,g)}return;default:if(to(t)){for(var he in a)f=a[he],a.hasOwnProperty(he)&&f!==void 0&&!n.hasOwnProperty(he)&&Qr(e,t,he,void 0,n,f);for(b in n)f=n[b],g=a[b],!n.hasOwnProperty(b)||f===g||f===void 0&&g===void 0||Qr(e,t,b,f,n,g);return}}for(var u in a)f=a[u],a.hasOwnProperty(u)&&f!=null&&!n.hasOwnProperty(u)&&ue(e,t,u,null,n,f);for(k in n)f=n[k],g=a[k],!n.hasOwnProperty(k)||f===g||f==null&&g==null||ue(e,t,k,f,n,g)}function Bu(e){switch(e){case"css":case"script":case"font":case"img":case"image":case"input":case"link":return!0;default:return!1}}function xm(){if(typeof performance.getEntriesByType=="function"){for(var e=0,t=0,a=performance.getEntriesByType("resource"),n=0;n<a.length;n++){var i=a[n],s=i.transferSize,o=i.initiatorType,r=i.duration;if(s&&r&&Bu(o)){for(o=0,r=i.responseEnd,n+=1;n<a.length;n++){var l=a[n],m=l.startTime;if(m>r)break;var b=l.transferSize,k=l.initiatorType;b&&Bu(k)&&(l=l.responseEnd,o+=b*(l<r?1:(r-m)/(l-m)))}if(--n,t+=8*(s+o)/(i.duration/1e3),e++,10<e)break}}if(0<e)return t/e/1e6}return navigator.connection&&(e=navigator.connection.downlink,typeof e=="number")?e:5}var jr=null,_r=null;function Cs(e){return e.nodeType===9?e:e.ownerDocument}function Nu(e){switch(e){case"http://www.w3.org/2000/svg":return 1;case"http://www.w3.org/1998/Math/MathML":return 2;default:return 0}}function Hu(e,t){if(e===0)switch(t){case"svg":return 1;case"math":return 2;default:return 0}return e===1&&t==="foreignObject"?0:e}function Gr(e,t){return e==="textarea"||e==="noscript"||typeof t.children=="string"||typeof t.children=="number"||typeof t.children=="bigint"||typeof t.dangerouslySetInnerHTML=="object"&&t.dangerouslySetInnerHTML!==null&&t.dangerouslySetInnerHTML.__html!=null}var Kr=null;function km(){var e=window.event;return e&&e.type==="popstate"?e===Kr?!1:(Kr=e,!0):(Kr=null,!1)}var Mu=typeof setTimeout=="function"?setTimeout:void 0,Tm=typeof clearTimeout=="function"?clearTimeout:void 0,Ou=typeof Promise=="function"?Promise:void 0,Sm=typeof queueMicrotask=="function"?queueMicrotask:typeof Ou<"u"?function(e){return Ou.resolve(null).then(e).catch(Am)}:Mu;function Am(e){setTimeout(function(){throw e})}function fa(e){return e==="head"}function Qu(e,t){var a=t,n=0;do{var i=a.nextSibling;if(e.removeChild(a),i&&i.nodeType===8)if(a=i.data,a==="/$"||a==="/&"){if(n===0){e.removeChild(i),Cn(t);return}n--}else if(a==="$"||a==="$?"||a==="$~"||a==="$!"||a==="&")n++;else if(a==="html")mi(e.ownerDocument.documentElement);else if(a==="head"){a=e.ownerDocument.head,mi(a);for(var s=a.firstChild;s;){var o=s.nextSibling,r=s.nodeName;s[Un]||r==="SCRIPT"||r==="STYLE"||r==="LINK"&&s.rel.toLowerCase()==="stylesheet"||a.removeChild(s),s=o}}else a==="body"&&mi(e.ownerDocument.body);a=i}while(a);Cn(t)}function ju(e,t){var a=e;e=0;do{var n=a.nextSibling;if(a.nodeType===1?t?(a._stashedDisplay=a.style.display,a.style.display="none"):(a.style.display=a._stashedDisplay||"",a.getAttribute("style")===""&&a.removeAttribute("style")):a.nodeType===3&&(t?(a._stashedText=a.nodeValue,a.nodeValue=""):a.nodeValue=a._stashedText||""),n&&n.nodeType===8)if(a=n.data,a==="/$"){if(e===0)break;e--}else a!=="$"&&a!=="$?"&&a!=="$~"&&a!=="$!"||e++;a=n}while(a)}function Yr(e){var t=e.firstChild;for(t&&t.nodeType===10&&(t=t.nextSibling);t;){var a=t;switch(t=t.nextSibling,a.nodeName){case"HTML":case"HEAD":case"BODY":Yr(a),Fs(a);continue;case"SCRIPT":case"STYLE":continue;case"LINK":if(a.rel.toLowerCase()==="stylesheet")continue}e.removeChild(a)}}function qm(e,t,a,n){for(;e.nodeType===1;){var i=a;if(e.nodeName.toLowerCase()!==t.toLowerCase()){if(!n&&(e.nodeName!=="INPUT"||e.type!=="hidden"))break}else if(n){if(!e[Un])switch(t){case"meta":if(!e.hasAttribute("itemprop"))break;return e;case"link":if(s=e.getAttribute("rel"),s==="stylesheet"&&e.hasAttribute("data-precedence"))break;if(s!==i.rel||e.getAttribute("href")!==(i.href==null||i.href===""?null:i.href)||e.getAttribute("crossorigin")!==(i.crossOrigin==null?null:i.crossOrigin)||e.getAttribute("title")!==(i.title==null?null:i.title))break;return e;case"style":if(e.hasAttribute("data-precedence"))break;return e;case"script":if(s=e.getAttribute("src"),(s!==(i.src==null?null:i.src)||e.getAttribute("type")!==(i.type==null?null:i.type)||e.getAttribute("crossorigin")!==(i.crossOrigin==null?null:i.crossOrigin))&&s&&e.hasAttribute("async")&&!e.hasAttribute("itemprop"))break;return e;default:return e}}else if(t==="input"&&e.type==="hidden"){var s=i.name==null?null:""+i.name;if(i.type==="hidden"&&e.getAttribute("name")===s)return e}else return e;if(e=vt(e.nextSibling),e===null)break}return null}function Cm(e,t,a){if(t==="")return null;for(;e.nodeType!==3;)if((e.nodeType!==1||e.nodeName!=="INPUT"||e.type!=="hidden")&&!a||(e=vt(e.nextSibling),e===null))return null;return e}function _u(e,t){for(;e.nodeType!==8;)if((e.nodeType!==1||e.nodeName!=="INPUT"||e.type!=="hidden")&&!t||(e=vt(e.nextSibling),e===null))return null;return e}function Vr(e){return e.data==="$?"||e.data==="$~"}function Zr(e){return e.data==="$!"||e.data==="$?"&&e.ownerDocument.readyState!=="loading"}function Rm(e,t){var a=e.ownerDocument;if(e.data==="$~")e._reactRetry=t;else if(e.data!=="$?"||a.readyState!=="loading")t();else{var n=function(){t(),a.removeEventListener("DOMContentLoaded",n)};a.addEventListener("DOMContentLoaded",n),e._reactRetry=n}}function vt(e){for(;e!=null;e=e.nextSibling){var t=e.nodeType;if(t===1||t===3)break;if(t===8){if(t=e.data,t==="$"||t==="$!"||t==="$?"||t==="$~"||t==="&"||t==="F!"||t==="F")break;if(t==="/$"||t==="/&")return null}}return e}var Xr=null;function Gu(e){e=e.nextSibling;for(var t=0;e;){if(e.nodeType===8){var a=e.data;if(a==="/$"||a==="/&"){if(t===0)return vt(e.nextSibling);t--}else a!=="$"&&a!=="$!"&&a!=="$?"&&a!=="$~"&&a!=="&"||t++}e=e.nextSibling}return null}function Ku(e){e=e.previousSibling;for(var t=0;e;){if(e.nodeType===8){var a=e.data;if(a==="$"||a==="$!"||a==="$?"||a==="$~"||a==="&"){if(t===0)return e;t--}else a!=="/$"&&a!=="/&"||t++}e=e.previousSibling}return null}function Yu(e,t,a){switch(t=Cs(a),e){case"html":if(e=t.documentElement,!e)throw Error(h(452));return e;case"head":if(e=t.head,!e)throw Error(h(453));return e;case"body":if(e=t.body,!e)throw Error(h(454));return e;default:throw Error(h(451))}}function mi(e){for(var t=e.attributes;t.length;)e.removeAttributeNode(t[0]);Fs(e)}var wt=new Map,Vu=new Set;function Rs(e){return typeof e.getRootNode=="function"?e.getRootNode():e.nodeType===9?e:e.ownerDocument}var Zt=A.d;A.d={f:Lm,r:Im,D:Dm,C:Um,L:zm,m:Pm,X:Wm,S:Em,M:Bm};function Lm(){var e=Zt.f(),t=vs();return e||t}function Im(e){var t=Ga(e);t!==null&&t.tag===5&&t.type==="form"?ud(t):Zt.r(e)}var Sn=typeof document>"u"?null:document;function Zu(e,t,a){var n=Sn;if(n&&typeof t=="string"&&t){var i=ht(t);i='link[rel="'+e+'"][href="'+i+'"]',typeof a=="string"&&(i+='[crossorigin="'+a+'"]'),Vu.has(i)||(Vu.add(i),e={rel:e,crossOrigin:a,href:t},n.querySelector(i)===null&&(t=n.createElement("link"),Oe(t,"link",e),Ue(t),n.head.appendChild(t)))}}function Dm(e){Zt.D(e),Zu("dns-prefetch",e,null)}function Um(e,t){Zt.C(e,t),Zu("preconnect",e,t)}function zm(e,t,a){Zt.L(e,t,a);var n=Sn;if(n&&e&&t){var i='link[rel="preload"][as="'+ht(t)+'"]';t==="image"&&a&&a.imageSrcSet?(i+='[imagesrcset="'+ht(a.imageSrcSet)+'"]',typeof a.imageSizes=="string"&&(i+='[imagesizes="'+ht(a.imageSizes)+'"]')):i+='[href="'+ht(e)+'"]';var s=i;switch(t){case"style":s=An(e);break;case"script":s=qn(e)}wt.has(s)||(e=U({rel:"preload",href:t==="image"&&a&&a.imageSrcSet?void 0:e,as:t},a),wt.set(s,e),n.querySelector(i)!==null||t==="style"&&n.querySelector(fi(s))||t==="script"&&n.querySelector(gi(s))||(t=n.createElement("link"),Oe(t,"link",e),Ue(t),n.head.appendChild(t)))}}function Pm(e,t){Zt.m(e,t);var a=Sn;if(a&&e){var n=t&&typeof t.as=="string"?t.as:"script",i='link[rel="modulepreload"][as="'+ht(n)+'"][href="'+ht(e)+'"]',s=i;switch(n){case"audioworklet":case"paintworklet":case"serviceworker":case"sharedworker":case"worker":case"script":s=qn(e)}if(!wt.has(s)&&(e=U({rel:"modulepreload",href:e},t),wt.set(s,e),a.querySelector(i)===null)){switch(n){case"audioworklet":case"paintworklet":case"serviceworker":case"sharedworker":case"worker":case"script":if(a.querySelector(gi(s)))return}n=a.createElement("link"),Oe(n,"link",e),Ue(n),a.head.appendChild(n)}}}function Em(e,t,a){Zt.S(e,t,a);var n=Sn;if(n&&e){var i=Ka(n).hoistableStyles,s=An(e);t=t||"default";var o=i.get(s);if(!o){var r={loading:0,preload:null};if(o=n.querySelector(fi(s)))r.loading=5;else{e=U({rel:"stylesheet",href:e,"data-precedence":t},a),(a=wt.get(s))&&Fr(e,a);var l=o=n.createElement("link");Ue(l),Oe(l,"link",e),l._p=new Promise(function(m,b){l.onload=m,l.onerror=b}),l.addEventListener("load",function(){r.loading|=1}),l.addEventListener("error",function(){r.loading|=2}),r.loading|=4,Ls(o,t,n)}o={type:"stylesheet",instance:o,count:1,state:r},i.set(s,o)}}}function Wm(e,t){Zt.X(e,t);var a=Sn;if(a&&e){var n=Ka(a).hoistableScripts,i=qn(e),s=n.get(i);s||(s=a.querySelector(gi(i)),s||(e=U({src:e,async:!0},t),(t=wt.get(i))&&Jr(e,t),s=a.createElement("script"),Ue(s),Oe(s,"link",e),a.head.appendChild(s)),s={type:"script",instance:s,count:1,state:null},n.set(i,s))}}function Bm(e,t){Zt.M(e,t);var a=Sn;if(a&&e){var n=Ka(a).hoistableScripts,i=qn(e),s=n.get(i);s||(s=a.querySelector(gi(i)),s||(e=U({src:e,async:!0,type:"module"},t),(t=wt.get(i))&&Jr(e,t),s=a.createElement("script"),Ue(s),Oe(s,"link",e),a.head.appendChild(s)),s={type:"script",instance:s,count:1,state:null},n.set(i,s))}}function Xu(e,t,a,n){var i=(i=V.current)?Rs(i):null;if(!i)throw Error(h(446));switch(e){case"meta":case"title":return null;case"style":return typeof a.precedence=="string"&&typeof a.href=="string"?(t=An(a.href),a=Ka(i).hoistableStyles,n=a.get(t),n||(n={type:"style",instance:null,count:0,state:null},a.set(t,n)),n):{type:"void",instance:null,count:0,state:null};case"link":if(a.rel==="stylesheet"&&typeof a.href=="string"&&typeof a.precedence=="string"){e=An(a.href);var s=Ka(i).hoistableStyles,o=s.get(e);if(o||(i=i.ownerDocument||i,o={type:"stylesheet",instance:null,count:0,state:{loading:0,preload:null}},s.set(e,o),(s=i.querySelector(fi(e)))&&!s._p&&(o.instance=s,o.state.loading=5),wt.has(e)||(a={rel:"preload",as:"style",href:a.href,crossOrigin:a.crossOrigin,integrity:a.integrity,media:a.media,hrefLang:a.hrefLang,referrerPolicy:a.referrerPolicy},wt.set(e,a),s||Nm(i,e,a,o.state))),t&&n===null)throw Error(h(528,""));return o}if(t&&n!==null)throw Error(h(529,""));return null;case"script":return t=a.async,a=a.src,typeof a=="string"&&t&&typeof t!="function"&&typeof t!="symbol"?(t=qn(a),a=Ka(i).hoistableScripts,n=a.get(t),n||(n={type:"script",instance:null,count:0,state:null},a.set(t,n)),n):{type:"void",instance:null,count:0,state:null};default:throw Error(h(444,e))}}function An(e){return'href="'+ht(e)+'"'}function fi(e){return'link[rel="stylesheet"]['+e+"]"}function Fu(e){return U({},e,{"data-precedence":e.precedence,precedence:null})}function Nm(e,t,a,n){e.querySelector('link[rel="preload"][as="style"]['+t+"]")?n.loading=1:(t=e.createElement("link"),n.preload=t,t.addEventListener("load",function(){return n.loading|=1}),t.addEventListener("error",function(){return n.loading|=2}),Oe(t,"link",a),Ue(t),e.head.appendChild(t))}function qn(e){return'[src="'+ht(e)+'"]'}function gi(e){return"script[async]"+e}function Ju(e,t,a){if(t.count++,t.instance===null)switch(t.type){case"style":var n=e.querySelector('style[data-href~="'+ht(a.href)+'"]');if(n)return t.instance=n,Ue(n),n;var i=U({},a,{"data-href":a.href,"data-precedence":a.precedence,href:null,precedence:null});return n=(e.ownerDocument||e).createElement("style"),Ue(n),Oe(n,"style",i),Ls(n,a.precedence,e),t.instance=n;case"stylesheet":i=An(a.href);var s=e.querySelector(fi(i));if(s)return t.state.loading|=4,t.instance=s,Ue(s),s;n=Fu(a),(i=wt.get(i))&&Fr(n,i),s=(e.ownerDocument||e).createElement("link"),Ue(s);var o=s;return o._p=new Promise(function(r,l){o.onload=r,o.onerror=l}),Oe(s,"link",n),t.state.loading|=4,Ls(s,a.precedence,e),t.instance=s;case"script":return s=qn(a.src),(i=e.querySelector(gi(s)))?(t.instance=i,Ue(i),i):(n=a,(i=wt.get(s))&&(n=U({},a),Jr(n,i)),e=e.ownerDocument||e,i=e.createElement("script"),Ue(i),Oe(i,"link",n),e.head.appendChild(i),t.instance=i);case"void":return null;default:throw Error(h(443,t.type))}else t.type==="stylesheet"&&(t.state.loading&4)===0&&(n=t.instance,t.state.loading|=4,Ls(n,a.precedence,e));return t.instance}function Ls(e,t,a){for(var n=a.querySelectorAll('link[rel="stylesheet"][data-precedence],style[data-precedence]'),i=n.length?n[n.length-1]:null,s=i,o=0;o<n.length;o++){var r=n[o];if(r.dataset.precedence===t)s=r;else if(s!==i)break}s?s.parentNode.insertBefore(e,s.nextSibling):(t=a.nodeType===9?a.head:a,t.insertBefore(e,t.firstChild))}function Fr(e,t){e.crossOrigin==null&&(e.crossOrigin=t.crossOrigin),e.referrerPolicy==null&&(e.referrerPolicy=t.referrerPolicy),e.title==null&&(e.title=t.title)}function Jr(e,t){e.crossOrigin==null&&(e.crossOrigin=t.crossOrigin),e.referrerPolicy==null&&(e.referrerPolicy=t.referrerPolicy),e.integrity==null&&(e.integrity=t.integrity)}var Is=null;function $u(e,t,a){if(Is===null){var n=new Map,i=Is=new Map;i.set(a,n)}else i=Is,n=i.get(a),n||(n=new Map,i.set(a,n));if(n.has(e))return n;for(n.set(e,null),a=a.getElementsByTagName(e),i=0;i<a.length;i++){var s=a[i];if(!(s[Un]||s[Be]||e==="link"&&s.getAttribute("rel")==="stylesheet")&&s.namespaceURI!=="http://www.w3.org/2000/svg"){var o=s.getAttribute(t)||"";o=e+o;var r=n.get(o);r?r.push(s):n.set(o,[s])}}return n}function eh(e,t,a){e=e.ownerDocument||e,e.head.insertBefore(a,t==="title"?e.querySelector("head > title"):null)}function Hm(e,t,a){if(a===1||t.itemProp!=null)return!1;switch(e){case"meta":case"title":return!0;case"style":if(typeof t.precedence!="string"||typeof t.href!="string"||t.href==="")break;return!0;case"link":if(typeof t.rel!="string"||typeof t.href!="string"||t.href===""||t.onLoad||t.onError)break;switch(t.rel){case"stylesheet":return e=t.disabled,typeof t.precedence=="string"&&e==null;default:return!0}case"script":if(t.async&&typeof t.async!="function"&&typeof t.async!="symbol"&&!t.onLoad&&!t.onError&&t.src&&typeof t.src=="string")return!0}return!1}function th(e){return!(e.type==="stylesheet"&&(e.state.loading&3)===0)}function Mm(e,t,a,n){if(a.type==="stylesheet"&&(typeof n.media!="string"||matchMedia(n.media).matches!==!1)&&(a.state.loading&4)===0){if(a.instance===null){var i=An(n.href),s=t.querySelector(fi(i));if(s){t=s._p,t!==null&&typeof t=="object"&&typeof t.then=="function"&&(e.count++,e=Ds.bind(e),t.then(e,e)),a.state.loading|=4,a.instance=s,Ue(s);return}s=t.ownerDocument||t,n=Fu(n),(i=wt.get(i))&&Fr(n,i),s=s.createElement("link"),Ue(s);var o=s;o._p=new Promise(function(r,l){o.onload=r,o.onerror=l}),Oe(s,"link",n),a.instance=s}e.stylesheets===null&&(e.stylesheets=new Map),e.stylesheets.set(a,t),(t=a.state.preload)&&(a.state.loading&3)===0&&(e.count++,a=Ds.bind(e),t.addEventListener("load",a),t.addEventListener("error",a))}}var $r=0;function Om(e,t){return e.stylesheets&&e.count===0&&zs(e,e.stylesheets),0<e.count||0<e.imgCount?function(a){var n=setTimeout(function(){if(e.stylesheets&&zs(e,e.stylesheets),e.unsuspend){var s=e.unsuspend;e.unsuspend=null,s()}},6e4+t);0<e.imgBytes&&$r===0&&($r=62500*xm());var i=setTimeout(function(){if(e.waitingForImages=!1,e.count===0&&(e.stylesheets&&zs(e,e.stylesheets),e.unsuspend)){var s=e.unsuspend;e.unsuspend=null,s()}},(e.imgBytes>$r?50:800)+t);return e.unsuspend=a,function(){e.unsuspend=null,clearTimeout(n),clearTimeout(i)}}:null}function Ds(){if(this.count--,this.count===0&&(this.imgCount===0||!this.waitingForImages)){if(this.stylesheets)zs(this,this.stylesheets);else if(this.unsuspend){var e=this.unsuspend;this.unsuspend=null,e()}}}var Us=null;function zs(e,t){e.stylesheets=null,e.unsuspend!==null&&(e.count++,Us=new Map,t.forEach(Qm,e),Us=null,Ds.call(e))}function Qm(e,t){if(!(t.state.loading&4)){var a=Us.get(e);if(a)var n=a.get(null);else{a=new Map,Us.set(e,a);for(var i=e.querySelectorAll("link[data-precedence],style[data-precedence]"),s=0;s<i.length;s++){var o=i[s];(o.nodeName==="LINK"||o.getAttribute("media")!=="not all")&&(a.set(o.dataset.precedence,o),n=o)}n&&a.set(null,n)}i=t.instance,o=i.getAttribute("data-precedence"),s=a.get(o)||n,s===n&&a.set(null,i),a.set(o,i),this.count++,n=Ds.bind(this),i.addEventListener("load",n),i.addEventListener("error",n),s?s.parentNode.insertBefore(i,s.nextSibling):(e=e.nodeType===9?e.head:e,e.insertBefore(i,e.firstChild)),t.state.loading|=4}}var yi={$$typeof:B,Provider:null,Consumer:null,_currentValue:H,_currentValue2:H,_threadCount:0};function jm(e,t,a,n,i,s,o,r,l){this.tag=1,this.containerInfo=e,this.pingCache=this.current=this.pendingChildren=null,this.timeoutHandle=-1,this.callbackNode=this.next=this.pendingContext=this.context=this.cancelPendingCommit=null,this.callbackPriority=0,this.expirationTimes=Ys(-1),this.entangledLanes=this.shellSuspendCounter=this.errorRecoveryDisabledLanes=this.expiredLanes=this.warmLanes=this.pingedLanes=this.suspendedLanes=this.pendingLanes=0,this.entanglements=Ys(0),this.hiddenUpdates=Ys(null),this.identifierPrefix=n,this.onUncaughtError=i,this.onCaughtError=s,this.onRecoverableError=o,this.pooledCache=null,this.pooledCacheLanes=0,this.formState=l,this.incompleteTransitions=new Map}function ah(e,t,a,n,i,s,o,r,l,m,b,k){return e=new jm(e,t,a,o,l,m,b,k,r),t=1,s===!0&&(t|=24),s=st(3,null,null,t),e.current=s,s.stateNode=e,t=Uo(),t.refCount++,e.pooledCache=t,t.refCount++,s.memoizedState={element:n,isDehydrated:a,cache:t},Wo(s),e}function nh(e){return e?(e=an,e):an}function ih(e,t,a,n,i,s){i=nh(i),n.context===null?n.context=i:n.pendingContext=i,n=ia(t),n.payload={element:a},s=s===void 0?null:s,s!==null&&(n.callback=s),a=sa(e,n,t),a!==null&&(et(a,e,t),Zn(a,e,t))}function sh(e,t){if(e=e.memoizedState,e!==null&&e.dehydrated!==null){var a=e.retryLane;e.retryLane=a!==0&&a<t?a:t}}function el(e,t){sh(e,t),(e=e.alternate)&&sh(e,t)}function oh(e){if(e.tag===13||e.tag===31){var t=La(e,67108864);t!==null&&et(t,e,67108864),el(e,67108864)}}function rh(e){if(e.tag===13||e.tag===31){var t=dt();t=Vs(t);var a=La(e,t);a!==null&&et(a,e,t),el(e,t)}}var Ps=!0;function _m(e,t,a,n){var i=w.T;w.T=null;var s=A.p;try{A.p=2,tl(e,t,a,n)}finally{A.p=s,w.T=i}}function Gm(e,t,a,n){var i=w.T;w.T=null;var s=A.p;try{A.p=8,tl(e,t,a,n)}finally{A.p=s,w.T=i}}function tl(e,t,a,n){if(Ps){var i=al(n);if(i===null)Or(e,t,n,Es,a),ch(e,n);else if(Ym(i,e,t,a,n))n.stopPropagation();else if(ch(e,n),t&4&&-1<Km.indexOf(e)){for(;i!==null;){var s=Ga(i);if(s!==null)switch(s.tag){case 3:if(s=s.stateNode,s.current.memoizedState.isDehydrated){var o=Sa(s.pendingLanes);if(o!==0){var r=s;for(r.pendingLanes|=2,r.entangledLanes|=2;o;){var l=1<<31-nt(o);r.entanglements[1]|=l,o&=~l}It(s),(ae&6)===0&&(ys=tt()+500,ui(0))}}break;case 31:case 13:r=La(s,2),r!==null&&et(r,s,2),vs(),el(s,2)}if(s=al(n),s===null&&Or(e,t,n,Es,a),s===i)break;i=s}i!==null&&n.stopPropagation()}else Or(e,t,n,null,a)}}function al(e){return e=no(e),nl(e)}var Es=null;function nl(e){if(Es=null,e=_a(e),e!==null){var t=E(e);if(t===null)e=null;else{var a=t.tag;if(a===13){if(e=P(t),e!==null)return e;e=null}else if(a===31){if(e=K(t),e!==null)return e;e=null}else if(a===3){if(t.stateNode.current.memoizedState.isDehydrated)return t.tag===3?t.stateNode.containerInfo:null;e=null}else t!==e&&(e=null)}}return Es=e,null}function lh(e){switch(e){case"beforetoggle":case"cancel":case"click":case"close":case"contextmenu":case"copy":case"cut":case"auxclick":case"dblclick":case"dragend":case"dragstart":case"drop":case"focusin":case"focusout":case"input":case"invalid":case"keydown":case"keypress":case"keyup":case"mousedown":case"mouseup":case"paste":case"pause":case"play":case"pointercancel":case"pointerdown":case"pointerup":case"ratechange":case"reset":case"resize":case"seeked":case"submit":case"toggle":case"touchcancel":case"touchend":case"touchstart":case"volumechange":case"change":case"selectionchange":case"textInput":case"compositionstart":case"compositionend":case"compositionupdate":case"beforeblur":case"afterblur":case"beforeinput":case"blur":case"fullscreenchange":case"focus":case"hashchange":case"popstate":case"select":case"selectstart":return 2;case"drag":case"dragenter":case"dragexit":case"dragleave":case"dragover":case"mousemove":case"mouseout":case"mouseover":case"pointermove":case"pointerout":case"pointerover":case"scroll":case"touchmove":case"wheel":case"mouseenter":case"mouseleave":case"pointerenter":case"pointerleave":return 8;case"message":switch(Dh()){case gl:return 2;case yl:return 8;case Si:case Uh:return 32;case bl:return 268435456;default:return 32}default:return 32}}var il=!1,ga=null,ya=null,ba=null,bi=new Map,vi=new Map,va=[],Km="mousedown mouseup touchcancel touchend touchstart auxclick dblclick pointercancel pointerdown pointerup dragend dragstart drop compositionend compositionstart keydown keypress keyup input textInput copy cut paste click change contextmenu reset".split(" ");function ch(e,t){switch(e){case"focusin":case"focusout":ga=null;break;case"dragenter":case"dragleave":ya=null;break;case"mouseover":case"mouseout":ba=null;break;case"pointerover":case"pointerout":bi.delete(t.pointerId);break;case"gotpointercapture":case"lostpointercapture":vi.delete(t.pointerId)}}function wi(e,t,a,n,i,s){return e===null||e.nativeEvent!==s?(e={blockedOn:t,domEventName:a,eventSystemFlags:n,nativeEvent:s,targetContainers:[i]},t!==null&&(t=Ga(t),t!==null&&oh(t)),e):(e.eventSystemFlags|=n,t=e.targetContainers,i!==null&&t.indexOf(i)===-1&&t.push(i),e)}function Ym(e,t,a,n,i){switch(t){case"focusin":return ga=wi(ga,e,t,a,n,i),!0;case"dragenter":return ya=wi(ya,e,t,a,n,i),!0;case"mouseover":return ba=wi(ba,e,t,a,n,i),!0;case"pointerover":var s=i.pointerId;return bi.set(s,wi(bi.get(s)||null,e,t,a,n,i)),!0;case"gotpointercapture":return s=i.pointerId,vi.set(s,wi(vi.get(s)||null,e,t,a,n,i)),!0}return!1}function dh(e){var t=_a(e.target);if(t!==null){var a=E(t);if(a!==null){if(t=a.tag,t===13){if(t=P(a),t!==null){e.blockedOn=t,Sl(e.priority,function(){rh(a)});return}}else if(t===31){if(t=K(a),t!==null){e.blockedOn=t,Sl(e.priority,function(){rh(a)});return}}else if(t===3&&a.stateNode.current.memoizedState.isDehydrated){e.blockedOn=a.tag===3?a.stateNode.containerInfo:null;return}}}e.blockedOn=null}function Ws(e){if(e.blockedOn!==null)return!1;for(var t=e.targetContainers;0<t.length;){var a=al(e.nativeEvent);if(a===null){a=e.nativeEvent;var n=new a.constructor(a.type,a);ao=n,a.target.dispatchEvent(n),ao=null}else return t=Ga(a),t!==null&&oh(t),e.blockedOn=a,!1;t.shift()}return!0}function uh(e,t,a){Ws(e)&&a.delete(t)}function Vm(){il=!1,ga!==null&&Ws(ga)&&(ga=null),ya!==null&&Ws(ya)&&(ya=null),ba!==null&&Ws(ba)&&(ba=null),bi.forEach(uh),vi.forEach(uh)}function Bs(e,t){e.blockedOn===t&&(e.blockedOn=null,il||(il=!0,v.unstable_scheduleCallback(v.unstable_NormalPriority,Vm)))}var Ns=null;function hh(e){Ns!==e&&(Ns=e,v.unstable_scheduleCallback(v.unstable_NormalPriority,function(){Ns===e&&(Ns=null);for(var t=0;t<e.length;t+=3){var a=e[t],n=e[t+1],i=e[t+2];if(typeof n!="function"){if(nl(n||a)===null)continue;break}var s=Ga(a);s!==null&&(e.splice(t,3),t-=3,ar(s,{pending:!0,data:i,method:a.method,action:n},n,i))}}))}function Cn(e){function t(l){return Bs(l,e)}ga!==null&&Bs(ga,e),ya!==null&&Bs(ya,e),ba!==null&&Bs(ba,e),bi.forEach(t),vi.forEach(t);for(var a=0;a<va.length;a++){var n=va[a];n.blockedOn===e&&(n.blockedOn=null)}for(;0<va.length&&(a=va[0],a.blockedOn===null);)dh(a),a.blockedOn===null&&va.shift();if(a=(e.ownerDocument||e).$$reactFormReplay,a!=null)for(n=0;n<a.length;n+=3){var i=a[n],s=a[n+1],o=i[Ve]||null;if(typeof s=="function")o||hh(a);else if(o){var r=null;if(s&&s.hasAttribute("formAction")){if(i=s,o=s[Ve]||null)r=o.formAction;else if(nl(i)!==null)continue}else r=o.action;typeof r=="function"?a[n+1]=r:(a.splice(n,3),n-=3),hh(a)}}}function ph(){function e(s){s.canIntercept&&s.info==="react-transition"&&s.intercept({handler:function(){return new Promise(function(o){return i=o})},focusReset:"manual",scroll:"manual"})}function t(){i!==null&&(i(),i=null),n||setTimeout(a,20)}function a(){if(!n&&!navigation.transition){var s=navigation.currentEntry;s&&s.url!=null&&navigation.navigate(s.url,{state:s.getState(),info:"react-transition",history:"replace"})}}if(typeof navigation=="object"){var n=!1,i=null;return navigation.addEventListener("navigate",e),navigation.addEventListener("navigatesuccess",t),navigation.addEventListener("navigateerror",t),setTimeout(a,100),function(){n=!0,navigation.removeEventListener("navigate",e),navigation.removeEventListener("navigatesuccess",t),navigation.removeEventListener("navigateerror",t),i!==null&&(i(),i=null)}}}function sl(e){this._internalRoot=e}Hs.prototype.render=sl.prototype.render=function(e){var t=this._internalRoot;if(t===null)throw Error(h(409));var a=t.current,n=dt();ih(a,n,e,t,null,null)},Hs.prototype.unmount=sl.prototype.unmount=function(){var e=this._internalRoot;if(e!==null){this._internalRoot=null;var t=e.containerInfo;ih(e.current,2,null,e,null,null),vs(),t[ja]=null}};function Hs(e){this._internalRoot=e}Hs.prototype.unstable_scheduleHydration=function(e){if(e){var t=Tl();e={blockedOn:null,target:e,priority:t};for(var a=0;a<va.length&&t!==0&&t<va[a].priority;a++);va.splice(a,0,e),a===0&&dh(e)}};var mh=N.version;if(mh!=="19.2.4")throw Error(h(527,mh,"19.2.4"));A.findDOMNode=function(e){var t=e._reactInternals;if(t===void 0)throw typeof e.render=="function"?Error(h(188)):(e=Object.keys(e).join(","),Error(h(268,e)));return e=y(t),e=e!==null?Q(e):null,e=e===null?null:e.stateNode,e};var Zm={bundleType:0,version:"19.2.4",rendererPackageName:"react-dom",currentDispatcherRef:w,reconcilerVersion:"19.2.4"};if(typeof __REACT_DEVTOOLS_GLOBAL_HOOK__<"u"){var Ms=__REACT_DEVTOOLS_GLOBAL_HOOK__;if(!Ms.isDisabled&&Ms.supportsFiber)try{Ln=Ms.inject(Zm),at=Ms}catch{}}return ki.createRoot=function(e,t){if(!L(e))throw Error(h(299));var a=!1,n="",i=xd,s=kd,o=Td;return t!=null&&(t.unstable_strictMode===!0&&(a=!0),t.identifierPrefix!==void 0&&(n=t.identifierPrefix),t.onUncaughtError!==void 0&&(i=t.onUncaughtError),t.onCaughtError!==void 0&&(s=t.onCaughtError),t.onRecoverableError!==void 0&&(o=t.onRecoverableError)),t=ah(e,1,!1,null,null,a,n,null,i,s,o,ph),e[ja]=t.current,Mr(e),new sl(t)},ki.hydrateRoot=function(e,t,a){if(!L(e))throw Error(h(299));var n=!1,i="",s=xd,o=kd,r=Td,l=null;return a!=null&&(a.unstable_strictMode===!0&&(n=!0),a.identifierPrefix!==void 0&&(i=a.identifierPrefix),a.onUncaughtError!==void 0&&(s=a.onUncaughtError),a.onCaughtError!==void 0&&(o=a.onCaughtError),a.onRecoverableError!==void 0&&(r=a.onRecoverableError),a.formState!==void 0&&(l=a.formState)),t=ah(e,1,!0,t,a??null,n,i,l,s,o,r,ph),t.context=nh(null),a=t.current,n=dt(),n=Vs(n),i=ia(n),i.callback=null,sa(a,i,n),a=n,t.current.lanes=a,Dn(t,a),It(t),e[ja]=t.current,Mr(e),new Hs(t)},ki.version="19.2.4",ki}var Sh;function rf(){if(Sh)return ll.exports;Sh=1;function v(){if(!(typeof __REACT_DEVTOOLS_GLOBAL_HOOK__>"u"||typeof __REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE!="function"))try{__REACT_DEVTOOLS_GLOBAL_HOOK__.checkDCE(v)}catch(N){console.error(N)}}return v(),ll.exports=of(),ll.exports}var lf=rf();const cf=Ch(lf),df=[{question:"What are the three guarantees described by the CAP theorem?",options:["Consistency, Availability, Partition Tolerance","Concurrency, Atomicity, Performance","Caching, Authentication, Persistence","Consensus, Agreement, Propagation"],correctIndex:0,explanation:"The CAP theorem, proposed by Eric Brewer in 2000, states that a distributed system can only guarantee two out of three properties: Consistency (every read receives the most recent write), Availability (every request receives a non-error response), and Partition Tolerance (the system continues to operate despite network partitions). The other options mix unrelated distributed systems concepts. This theorem is foundational because it forces architects to make explicit trade-offs when designing systems that span multiple nodes."},{question:"In the context of CAP theorem, what does 'Consistency' mean?",options:["All nodes see the same data at the same time (linearizability)","Data is never lost even if a node crashes","The system always returns a successful response","Transactions follow ACID properties"],correctIndex:0,explanation:"CAP Consistency means linearizability  every read returns the value of the most recent completed write, as if there's a single copy of the data. This is different from ACID consistency, which refers to database invariants being maintained. Durability (option B) is about persistence, availability (option C) is a separate CAP property, and ACID (option D) is a transaction model concept. In practice, achieving CAP consistency requires coordination between nodes, which adds latency  this is why many systems choose eventual consistency instead."},{question:"What does 'Partition Tolerance' mean in CAP theorem?",options:["The system can split data across multiple partitions","The system continues to function despite network communication failures between nodes","The system tolerates disk partitioning failures","The system can handle partitioned database tables"],correctIndex:1,explanation:"Partition tolerance means the system continues to operate even when network messages between nodes are lost or delayed  essentially, when the network 'partitions' into groups that can't communicate. This is about network failures, not data partitioning (sharding) or disk failures. In real distributed systems, network partitions are inevitable (cables get cut, switches fail, cloud availability zones lose connectivity), so partition tolerance is not really optional  you must handle it. This is why the real CAP choice is between CP and AP during a partition event."},{question:"Why is a 'CA' system (Consistency + Availability, no Partition Tolerance) considered impractical in distributed systems?",options:["Because network partitions are unavoidable in distributed systems","Because consistency and availability are mutually exclusive","Because CA systems require infinite bandwidth","Because no database supports both consistency and availability"],correctIndex:0,explanation:"Network partitions are a fact of life in distributed systems  switches fail, cables get cut, packets get dropped. Since you can't prevent partitions, you must tolerate them, making P mandatory. A CA system would only work on a single node or a network that never fails, which doesn't exist in practice. This is why the real CAP trade-off is between CP (sacrifice availability during partitions) and AP (sacrifice consistency during partitions). A traditional single-node PostgreSQL instance is technically CA, but the moment you distribute it, you must choose CP or AP."},{question:"Which of the following is a CP (Consistency + Partition Tolerance) system?",options:["Amazon DynamoDB (default config)","Apache Cassandra (default config)","Apache HBase","CouchDB"],correctIndex:2,explanation:"HBase is a CP system  during a network partition, it will refuse to serve requests from partitioned regions rather than risk returning stale data. It relies on ZooKeeper for coordination and will make a region unavailable if it can't confirm consistency. DynamoDB (default) and Cassandra (default) are AP systems  they prioritize availability and accept eventual consistency. CouchDB is also AP, designed for eventual consistency with multi-master replication. The key insight: CP systems choose correctness over responsiveness when the network misbehaves."},{question:"Which of the following is an AP (Availability + Partition Tolerance) system?",options:["Google Spanner","Apache ZooKeeper","Amazon DynamoDB (default configuration)","etcd"],correctIndex:2,explanation:"DynamoDB in its default configuration is an AP system  it uses eventual consistency to ensure that reads always return a response, even during network partitions, though the data might be slightly stale. Google Spanner is CP (it uses TrueTime and Paxos for strong consistency). ZooKeeper and etcd are both CP systems used for coordination  they use consensus protocols (ZAB and Raft respectively) and will become unavailable if they lose quorum. The trade-off DynamoDB makes is accepting occasional stale reads in exchange for always being responsive."},{question:"What happens in a CP system during a network partition?",options:["It returns stale data to maintain availability","It becomes unavailable rather than risk inconsistent data","It automatically resolves the partition","It switches to eventual consistency mode"],correctIndex:1,explanation:"A CP system prioritizes consistency over availability. During a network partition, nodes that can't confirm they have the latest data will refuse to serve requests  returning errors or timing out rather than potentially serving stale or inconsistent data. This is the behavior you see in systems like HBase, ZooKeeper, and etcd. For example, if a ZooKeeper ensemble loses quorum (majority of nodes can't communicate), the remaining nodes stop accepting writes. This is critical for use cases like distributed locks or configuration management where stale data could cause catastrophic errors."},{question:"What happens in an AP system during a network partition?",options:["It stops accepting writes until the partition heals","All nodes shut down to prevent data corruption","It continues serving requests but may return stale or divergent data","It automatically promotes a new leader"],correctIndex:2,explanation:"An AP system continues serving both reads and writes during a partition, accepting that different nodes might have divergent data. For example, Cassandra will accept writes on both sides of a partition, and when the partition heals, it uses techniques like last-write-wins or vector clocks to reconcile conflicts. This is ideal for systems where availability is more important than immediate consistency  like a shopping cart (Amazon's original Dynamo paper) where it's better to accept a potentially stale cart than to show the user an error page. The reconciliation after partition healing is where the complexity lives."},{question:"Who originally proposed the CAP theorem?",options:["Leslie Lamport","Eric Brewer","Martin Fowler","Jeff Dean"],correctIndex:1,explanation:"Eric Brewer proposed the CAP theorem as a conjecture at the ACM Symposium on Principles of Distributed Computing (PODC) in 2000. It was later formally proved by Seth Gilbert and Nancy Lynch in 2002. Leslie Lamport is famous for Paxos, Lamport clocks, and LaTeX. Martin Fowler is known for software architecture patterns and refactoring. Jeff Dean is the legendary Google engineer behind MapReduce, BigTable, and TensorFlow. Brewer's insight fundamentally changed how engineers reason about distributed system design trade-offs."},{question:"DynamoDB offers both eventually consistent and strongly consistent reads. When using strongly consistent reads, what CAP trade-off is being made?",options:["It becomes a CA system","It trades some availability for consistency (moves toward CP)","It gains partition tolerance it didn't have before","No trade-off  strong consistency is free in DynamoDB"],correctIndex:1,explanation:"When you request strongly consistent reads in DynamoDB, you're moving along the CAP spectrum toward CP. The read must go to the leader node of the partition, and if that leader is unreachable due to a network partition, the read will fail  sacrificing availability for consistency. With eventually consistent reads (default), DynamoDB can serve from any replica, giving you availability even during partitions but potentially stale data. This demonstrates that CAP isn't a binary choice but a spectrum  systems can offer different consistency levels for different operations. Strong consistent reads in DynamoDB also have higher latency and lower throughput."},{question:"What is the PACELC theorem?",options:["An alternative to CAP that ignores partition tolerance","An extension of CAP: during Partition choose A or C; Else (no partition) choose Latency or Consistency","A theorem about parallel computing efficiency","A consensus protocol for distributed databases"],correctIndex:1,explanation:"PACELC extends CAP by addressing what happens when there is NO partition (the normal case). It says: if there's a Partition, choose Availability or Consistency (PAC); Else, choose Latency or Consistency (ELC). This is important because CAP only describes behavior during partitions, but most of the time the network is fine. Even without partitions, there's still a trade-off: strong consistency requires coordination (adding latency), while relaxed consistency can be faster. For example, DynamoDB is PA/EL (available during partitions, low latency normally), while Spanner is PC/EC (consistent during partitions, consistent normally but with higher latency)."},{question:"In PACELC, what does a PA/EL system prioritize?",options:["Consistency during partitions, latency during normal operation","Availability during partitions, low latency during normal operation","Consistency always, regardless of partition status","Availability during partitions, consistency during normal operation"],correctIndex:1,explanation:"A PA/EL system chooses Availability during Partitions and Low Latency when there's no partition (Else). This describes systems like Cassandra and DynamoDB (default)  they always prioritize responsiveness, whether or not there's a partition. During partitions they serve potentially stale data (PA), and during normal operation they use eventual consistency to minimize latency (EL). Contrast this with a PC/EC system like Google Spanner, which always prioritizes consistency at the cost of higher latency. The PACELC model is more nuanced than CAP because it captures behavior during the 99.99% of time when there's no partition."},{question:"Which consistency model guarantees that once a write is confirmed, all subsequent reads will see that write?",options:["Eventual consistency","Strong consistency (linearizability)","Causal consistency","Read-your-writes consistency"],correctIndex:1,explanation:"Strong consistency (linearizability) guarantees that once a write completes, ALL subsequent reads from ANY client will see that write  as if there's a single copy of the data updated atomically. Eventual consistency only guarantees that reads will EVENTUALLY see the write, but there's no bound on when. Causal consistency preserves causally related operation order but allows concurrent operations to be seen in different orders. Read-your-writes only guarantees that the client that performed the write sees it  other clients might still see stale data. Linearizability is the strongest guarantee but the most expensive in terms of latency and availability."},{question:"A social media platform needs to display a user's post count. The count doesn't need to be perfectly accurate in real-time. Which CAP trade-off is most appropriate?",options:["CP  count must always be exactly correct","AP  eventual consistency is acceptable for display counts","CA  the count should never be unavailable or wrong","None  this doesn't involve CAP trade-offs"],correctIndex:1,explanation:"For a non-critical metric like a post count display, AP with eventual consistency is the right trade-off. Users won't notice if their post count is off by one for a few seconds, but they WILL notice if the page fails to load (unavailability). Facebook and Twitter use this approach  counters like likes, followers, and post counts are eventually consistent and may temporarily show different values to different users. This is a classic example of choosing the right consistency level based on business requirements rather than defaulting to the strongest guarantee. Overengineering consistency for non-critical data wastes resources and hurts availability."},{question:"A banking system processes fund transfers between accounts. Which CAP trade-off is most appropriate?",options:["AP  the system must always be available for transfers","CP  consistency is critical to prevent double-spending","CA  both consistency and availability are mandatory","AP with conflict resolution after partitions heal"],correctIndex:1,explanation:"For financial transactions like fund transfers, consistency is non-negotiable  you cannot risk double-spending or lost money due to stale reads. A CP approach ensures that during a network partition, the system will refuse the transfer rather than potentially process it incorrectly. This is why traditional banking systems use strong consistency and would rather show 'service temporarily unavailable' than process an inconsistent transaction. Banks like HSBC and JPMorgan use CP databases for core transaction processing. The cost of an inconsistency (regulatory fines, lost money) far outweighs the cost of brief unavailability."},{question:"What is a 'split-brain' scenario in distributed systems?",options:["When a database table is split across multiple shards","When network partition causes two groups of nodes to independently believe they are the active cluster","When a query execution plan is split into parallel sub-plans","When a leader node's CPU is overloaded and splits tasks"],correctIndex:1,explanation:"Split-brain occurs during a network partition when two (or more) groups of nodes can't communicate and each group independently believes it's the legitimate active cluster. Both sides may accept writes, leading to conflicting data. For example, in a two-node cluster, if the network between them fails, both nodes might promote themselves to leader and accept writes  creating divergent state that's extremely difficult to reconcile. This is why consensus protocols like Raft and Paxos require a majority quorum  only the side with the majority can continue operating, preventing split-brain. This is a critical failure mode that CP systems are designed to prevent."},{question:"How does a quorum-based system prevent split-brain during a network partition?",options:["By requiring all nodes to agree on every operation","By requiring a majority of nodes to agree before proceeding","By using timestamps to order conflicting operations","By shutting down all nodes when a partition is detected"],correctIndex:1,explanation:"Quorum-based systems require a majority (more than half) of nodes to agree before an operation can proceed. Since a majority can only exist on one side of any partition, this prevents split-brain  only one partition can form a quorum and accept operations. For example, in a 5-node cluster, you need 3 nodes to agree. If the network splits into groups of 3 and 2, only the group of 3 can form a quorum and continue operating. The group of 2 becomes unavailable. This is fundamentally why consensus protocols like Raft, Paxos, and ZAB use odd numbers of nodes (3, 5, 7)  it maximizes the chance that one side can achieve quorum."},{question:"Cassandra uses tunable consistency with settings like ONE, QUORUM, and ALL. If you set both read and write consistency to QUORUM, what behavior do you get?",options:["Eventual consistency only","Strong consistency for operations where R + W > N","The same as setting consistency to ALL","No consistency guarantees at all"],correctIndex:1,explanation:"When R (read replicas) + W (write replicas) > N (total replicas), you get strong consistency because there must be at least one node that participated in both the latest write and the current read. With QUORUM on both reads and writes in a replication factor of 3, R=2 and W=2, so R+W=4 > N=3  guaranteeing overlap. Setting both to ALL would give even stronger guarantees but at the cost of availability (any single node failure blocks the operation). Setting just one to ONE would break the overlap guarantee. This tunable consistency is what makes Cassandra flexible  you can choose per-query whether you need strong or eventual consistency."},{question:"What is 'eventual consistency'?",options:["Data is consistent after every write operation","If no new updates are made, all replicas will eventually converge to the same value","Data becomes consistent only at scheduled intervals","Consistency is guaranteed within a fixed time window"],correctIndex:1,explanation:"Eventual consistency means that if no new writes occur, all replicas will eventually converge to the same value  but there's no guarantee on how long this takes. It's not about scheduled intervals or fixed windows; it's a convergence guarantee without a time bound. In practice, convergence usually happens within milliseconds to seconds, but during partitions it could take longer. DNS is a classic example  when you update a DNS record, it propagates eventually but different resolvers may see the old value for varying durations (TTL-based). Amazon's Dynamo paper popularized this model, arguing that many applications can tolerate brief inconsistency in exchange for better availability and performance."},{question:"What consistency model does DNS use?",options:["Strong consistency","Eventual consistency with TTL-based propagation","Causal consistency","Sequential consistency"],correctIndex:1,explanation:"DNS uses eventual consistency with TTL (Time-To-Live) based propagation. When you update a DNS record, the change doesn't instantly propagate to all DNS resolvers worldwide. Each resolver caches the record for its TTL duration and only fetches the new value when the TTL expires. This means different clients might resolve the same domain to different IPs for a period. This is by design  DNS prioritizes availability and performance (billions of queries per day) over immediate consistency. The TTL mechanism gives a rough upper bound on propagation time, but in practice, some resolvers may not respect TTLs. This is why DNS migrations often involve lowering TTLs beforehand."},{question:"Google Spanner achieves external consistency (strongest form of consistency) in a distributed system. What technology makes this possible?",options:["Blockchain-based verification","TrueTime API with GPS and atomic clocks","Eventual consistency with automatic conflict resolution","Single-leader replication with synchronous followers"],correctIndex:1,explanation:"Google Spanner uses TrueTime, an API that provides globally consistent timestamps using GPS receivers and atomic clocks in every Google datacenter. TrueTime gives a bounded uncertainty interval for the current time, and Spanner waits out this uncertainty before committing transactions  ensuring that transaction timestamps reflect their real-time order. This allows Spanner to achieve external consistency (even stronger than linearizability) across globally distributed nodes. No other production system has replicated this approach because it requires Google's custom hardware infrastructure. This is why Spanner is often called the 'impossibility-defying' database  it achieves properties that the CAP theorem suggests shouldn't coexist."},{question:"What is a vector clock, and which CAP concern does it address?",options:["A physical timestamp mechanism that ensures strong consistency","A logical clock that tracks causality to help resolve conflicts in AP systems","A scheduling algorithm for distributed task execution","A technique for synchronizing wall clocks across data centers"],correctIndex:1,explanation:"A vector clock is a logical clock mechanism where each node maintains a vector of counters (one per node). When events happen, counters are incremented and exchanged, allowing the system to determine causal ordering  whether event A happened before event B, or if they're concurrent (and thus conflicting). This is crucial in AP systems where concurrent writes during partitions create conflicts that need resolution. Amazon's original Dynamo used vector clocks to detect conflicting versions and present them to the application for resolution (e.g., merging shopping carts). Vector clocks don't prevent inconsistency  they help detect and resolve it after the fact."},{question:"What conflict resolution strategy does 'last-write-wins' (LWW) use?",options:["It merges all conflicting writes into a single combined value","It keeps the write with the highest timestamp and discards others","It asks the user to manually resolve the conflict","It rejects all writes that conflict"],correctIndex:1,explanation:"Last-write-wins (LWW) resolves conflicts by simply keeping the write with the highest timestamp and discarding all others. This is used by Cassandra by default and is simple to implement, but it can silently lose data  if two clients write different values concurrently, one write is silently dropped. For example, if client A writes 'blue' and client B writes 'red' at nearly the same time, only the one with the later timestamp survives. This is acceptable for some use cases (like session data or caches) but dangerous for others (like shopping carts, where you'd lose items). The simplicity of LWW comes at the cost of potential data loss, which is why some systems like Riak offer alternative strategies like merge functions."},{question:"A system uses Raft consensus protocol. What CAP classification does it fall under?",options:["AP  Raft ensures all nodes can serve requests","CP  Raft requires a quorum and sacrifices availability during partitions","CA  Raft prevents partitions from occurring","None  consensus protocols are unrelated to CAP"],correctIndex:1,explanation:"Raft is a CP consensus protocol. It requires a majority quorum to elect a leader and commit log entries. During a network partition, the minority side cannot form a quorum and becomes unavailable  sacrificing availability for consistency. The majority side can still operate with full consistency guarantees. Systems built on Raft (like etcd, CockroachDB, TiKV) inherit this CP characteristic. Raft was designed by Diego Ongaro as an understandable alternative to Paxos, and it explicitly prioritizes safety (consistency) over liveness (availability). This makes it ideal for coordination services, configuration stores, and metadata management where correctness is paramount."},{question:"What is the difference between 'consistency' in CAP theorem and 'consistency' in ACID?",options:["They are exactly the same concept","CAP consistency is about linearizability across nodes; ACID consistency is about maintaining database invariants","ACID consistency is stronger than CAP consistency","CAP consistency only applies to NoSQL databases"],correctIndex:1,explanation:"These are fundamentally different concepts that unfortunately share a name. CAP consistency (linearizability) means all nodes see the same data at the same time  it's about replica agreement in a distributed system. ACID consistency means a transaction moves the database from one valid state to another, maintaining all defined rules (constraints, cascades, triggers). You could have a single-node database with perfect ACID consistency but no CAP consistency concerns (since there's only one node). Conversely, you could have a distributed system with CAP consistency but no ACID transactions. This naming collision causes endless confusion in system design discussions  always clarify which 'consistency' you mean."},{question:"ZooKeeper is classified as a CP system. In what scenario would ZooKeeper become unavailable?",options:["When any single node fails","When the leader node fails (before a new leader is elected)","When a minority of nodes are partitioned from the majority","Both B and C"],correctIndex:3,explanation:"ZooKeeper becomes unavailable in two scenarios: (1) When the leader fails, there's a brief unavailability window while a new leader is elected via the ZAB protocol (typically a few seconds). (2) When nodes are partitioned such that some are in the minority  those minority nodes cannot serve consistent reads or accept writes because they can't confirm they have the latest state. The majority partition will elect a new leader and continue operating. A single node failure in a 3+ node ensemble is tolerable as long as quorum is maintained. This is the CP trade-off in action  ZooKeeper would rather be unavailable than serve potentially inconsistent data, which is exactly what you want from a coordination service."},{question:"What is 'read-your-writes' consistency?",options:["A guarantee that all clients see writes in the order they were issued","A guarantee that a client will always see its own previous writes","A guarantee that reads are always served from the node that processed the write","A guarantee that all writes are immediately visible to all readers"],correctIndex:1,explanation:"Read-your-writes consistency guarantees that if a client writes a value, subsequent reads BY THAT SAME CLIENT will see the write (or a more recent value). Other clients might still see stale data. This is weaker than strong consistency but stronger than eventual consistency, and it's crucial for user experience  imagine posting a tweet and then not seeing it on your own timeline. Many systems implement this by routing a user's reads to the same replica that handled their writes (sticky sessions), or by tracking the latest write timestamp and ensuring reads go to replicas that are at least that up-to-date. Facebook uses a version of this for their social graph."},{question:"What is 'monotonic reads' consistency?",options:["Reads always return the latest value","Once a client reads a value, subsequent reads will never return an older value","All reads are served in the order they were issued","Reads always return values in alphabetical order"],correctIndex:1,explanation:"Monotonic reads guarantees that if a client reads a value at time T, any subsequent read by that client will return that value or a newer one  never an older value. Without this guarantee, a user might see a post, refresh the page, and the post disappears (because they hit a less-up-to-date replica), then see it again on the next refresh. This 'time travel' effect is confusing and breaks user expectations. Monotonic reads is typically implemented by ensuring a client always reads from the same replica (or one that's at least as up-to-date). It's a session-level guarantee that's weaker than strong consistency but prevents the most jarring user experience issues."},{question:"A system uses eventual consistency with a conflict resolution policy. During a network partition, two clients update the same record simultaneously on different sides of the partition. What is this called?",options:["A race condition","A write-write conflict (conflicting writes / siblings)","A deadlock","A dirty read"],correctIndex:1,explanation:"When two clients update the same data on different sides of a network partition, it creates a write-write conflict (also called 'siblings' in Riak terminology). Both writes succeed locally because the nodes can't coordinate across the partition. When the partition heals, the system must reconcile these conflicting versions. This is different from a race condition (which is a timing-dependent bug), a deadlock (which is about resource locking), or a dirty read (which is about reading uncommitted data). AP systems must have a strategy for handling these conflicts  LWW, vector clocks, CRDTs, or application-level resolution. The choice of conflict resolution strategy is one of the most important design decisions in AP systems."},{question:"What are CRDTs, and how do they relate to CAP theorem?",options:["Conflict-free Replicated Data Types  data structures that can be merged without conflicts, enabling AP systems","Consistent Replication Data Transfers  a protocol for CP systems","Concurrent Read Data Transactions  a transaction isolation mechanism","Checkpoint Recovery Data Techniques  a durability mechanism"],correctIndex:0,explanation:"CRDTs (Conflict-free Replicated Data Types) are data structures mathematically designed to always converge when replicas are merged, without requiring coordination. They enable AP systems to handle concurrent updates without conflicts  the merge operation is commutative, associative, and idempotent. Examples include G-Counters (grow-only counters), OR-Sets (observed-remove sets), and LWW-Registers. Redis Enterprise uses CRDTs for active-active geo-replication, and collaborative editors like Figma use CRDT-like structures. They're powerful because they give you the availability benefits of AP systems while eliminating the need for explicit conflict resolution  the math guarantees convergence."},{question:"What is 'tunable consistency' as implemented in Apache Cassandra?",options:["The ability to change the consistency model of the cluster at runtime","The ability to specify per-query how many replicas must respond for reads and writes","A feature that automatically adjusts consistency based on network conditions","The ability to set different consistency levels for different tables"],correctIndex:1,explanation:"Cassandra's tunable consistency allows you to specify per-query how many replicas must acknowledge a read or write. With consistency level ONE, only one replica needs to respond (fast but weakly consistent). With QUORUM, a majority must respond. With ALL, every replica must respond (slowest but strongest). The key insight is that when Read replicas + Write replicas > Total replicas (R + W > N), you get strong consistency because at least one node overlaps. This lets you make different trade-offs for different queries  strong consistency for critical operations like payments, eventual consistency for non-critical ones like analytics. No other database popularized this concept as effectively as Cassandra."},{question:"What is the significance of the formula R + W > N in quorum-based systems?",options:["It calculates the minimum number of nodes needed","It ensures at least one node has the latest write when reading, providing strong consistency","It determines the maximum throughput of the system","It calculates the replication factor needed for fault tolerance"],correctIndex:1,explanation:"When R (read quorum) + W (write quorum) > N (replication factor), there's guaranteed overlap  at least one node that handled the write will also be queried during the read. This node has the latest value, so the system can return consistent data. For example, with N=3, R=2, W=2: any 2 nodes you read from must include at least 1 of the 2 nodes that acknowledged the write (since 2+2=4 > 3). If R+W  N, there's no guaranteed overlap, and reads might miss the latest write. This formula is the mathematical foundation of quorum-based consistency in systems like Cassandra, DynamoDB, and Riak. It lets you trade off consistency for latency by adjusting R and W."},{question:"In a Cassandra cluster with replication factor 3, what is the minimum consistency level for BOTH reads and writes to guarantee strong consistency?",options:["ONE for both","TWO for reads, ONE for writes","QUORUM for both","ALL for both"],correctIndex:2,explanation:"With replication factor N=3, QUORUM means 2 nodes (3/2 + 1 = 2). If both reads and writes use QUORUM, then R + W = 2 + 2 = 4 > 3 = N, satisfying the strong consistency condition. ALL for both would also work but is unnecessarily restrictive  a single node failure would block all operations. ONE for both gives R + W = 2, which is less than N=3, so there's no overlap guarantee. TWO reads + ONE write gives R + W = 3 = N, which is borderline  it works only if there are no concurrent writes. QUORUM for both is the standard strong consistency configuration in Cassandra that also tolerates one node failure."},{question:"What is 'hinted handoff' in distributed databases like Cassandra and DynamoDB?",options:["A technique where a node stores writes intended for an unavailable node and forwards them when it recovers","A method for transferring leadership during leader election","A way to redirect client requests to the nearest data center","A protocol for negotiating consistency levels between client and server"],correctIndex:0,explanation:"Hinted handoff is a technique used in AP systems where, if a target replica is down, another node temporarily stores the write as a 'hint.' When the unavailable node comes back online, the hint is replayed, bringing it up to date. This improves availability (writes don't fail just because one replica is down) and helps achieve eventual consistency faster after node recovery. However, hints are stored temporarily and have limits  if a node is down for too long, hints may expire, requiring anti-entropy repair mechanisms (like Merkle trees in Cassandra) to fully synchronize. Hinted handoff is a pragmatic trade-off: it sacrifices some consistency guarantees for better availability and faster convergence."},{question:"What is 'anti-entropy' in the context of distributed systems?",options:["A mechanism to prevent data corruption from hardware failures","A background process that compares and synchronizes data between replicas to ensure they converge","A technique to prevent entropy-based encryption attacks","A method for reducing the randomness in load balancer routing"],correctIndex:1,explanation:"Anti-entropy is a background process where nodes periodically compare their data and synchronize any differences. In Cassandra, this is called 'repair' and uses Merkle trees  hash trees that efficiently identify which data ranges differ between replicas. Dynamo-style systems use anti-entropy to handle cases where hinted handoff isn't sufficient (e.g., a node was down too long and hints expired). The term comes from information theory  entropy represents disorder, so anti-entropy reduces disorder (data inconsistency) between replicas. It's like periodically reconciling two copies of a spreadsheet to ensure they match. This is a key mechanism for achieving eventual consistency in AP systems."},{question:"What is a 'sloppy quorum' as described in the Amazon Dynamo paper?",options:["A quorum that includes nodes outside the preference list when some preferred nodes are unavailable","A quorum that requires fewer nodes than a strict majority","A quorum that allows reads from any node without coordination","A quorum that uses approximate rather than exact counts"],correctIndex:0,explanation:"In a sloppy quorum, if the designated replica nodes for a key are unavailable, the system writes to other 'healthy' nodes that aren't normally responsible for that key. These temporary holders use hinted handoff to forward the data back when the proper nodes recover. This maximizes availability because writes can always succeed as long as any W nodes in the cluster are reachable  not necessarily the 'correct' W nodes. The trade-off is that this breaks the R + W > N strong consistency guarantee since the read and write sets may not overlap. Sloppy quorums are a key innovation in the Dynamo paper that enables always-writable behavior, making DynamoDB an AP system."},{question:"What is 'causal consistency' and where does it fall in the consistency spectrum?",options:["It's stronger than strong consistency  it guarantees global ordering of all operations","It's between strong and eventual consistency  it preserves the order of causally related operations","It's the weakest form of consistency  weaker than eventual","It's identical to sequential consistency"],correctIndex:1,explanation:"Causal consistency guarantees that operations that are causally related (one depends on or is influenced by another) are seen in the correct order by all nodes. Operations that are NOT causally related (concurrent) can be seen in any order. This is stronger than eventual consistency (which provides no ordering guarantees) but weaker than strong consistency (which orders ALL operations). For example, if Alice posts a message and Bob replies, causal consistency ensures everyone sees Alice's message before Bob's reply. But two unrelated posts by different users might appear in different orders on different nodes. MongoDB offers causal consistency through causal sessions, and it's often a sweet spot  providing meaningful guarantees without the performance cost of linearizability."},{question:"Which of the following is NOT a valid way to handle conflicts in an AP system after a partition heals?",options:["Last-write-wins using timestamps","Application-level conflict resolution","Using CRDTs for automatic merge","Rolling back all writes from both sides of the partition"],correctIndex:3,explanation:"Rolling back ALL writes from both partition sides would mean losing all data written during the partition  this defeats the purpose of an AP system, which accepts writes during partitions to maintain availability. The whole point of AP is that writes succeed during partitions and are reconciled afterwards. LWW (option A) picks one write and discards others  simple but lossy. Application-level resolution (option B) lets the app decide how to merge (e.g., Amazon's shopping cart merges items from both sides). CRDTs (option C) mathematically guarantee conflict-free merging. The challenge of AP systems is designing good conflict resolution  it's the price you pay for availability during partitions."},{question:"Netflix uses Cassandra as its primary data store. What CAP trade-off does this reflect about Netflix's priorities?",options:["Netflix prioritizes consistency  users must always see the exact same catalog","Netflix prioritizes availability  it's better to show a slightly stale catalog than show an error","Netflix doesn't need partition tolerance because it runs on a single server","Netflix uses Cassandra in CP mode for all operations"],correctIndex:1,explanation:"Netflix chose Cassandra (an AP system) because for their use case, availability is paramount. If a user opens Netflix, it's far better to show a slightly outdated 'continue watching' list than to show an error page. A few seconds of stale data in a recommendation or viewing history is invisible to the user, but an error page means a lost customer. Netflix operates across multiple AWS regions, where network partitions between regions are not uncommon. Cassandra's AP design with tunable consistency lets Netflix serve requests from any region regardless of partition status. This is a textbook example of choosing your CAP trade-off based on business impact rather than technical purity."},{question:"What is the CAP classification of Redis Sentinel?",options:["AP  Redis always serves reads even during failover","CP  Redis Sentinel ensures consistency through leader election","Neither  Redis Sentinel doesn't fit neatly into CAP","CA  Redis prevents partitions through fast networking"],correctIndex:2,explanation:"Redis Sentinel is actually difficult to classify cleanly in CAP terms. During normal operation, it's single-leader (CP-ish), but during failover it can lose acknowledged writes (not fully consistent). It uses asynchronous replication by default, so writes acknowledged by the leader may be lost if the leader fails before replicating. During a partition, the old leader might still accept writes (split-brain), though Sentinel tries to mitigate this with 'min-replicas-to-write' configuration. This illustrates an important point: many real-world systems don't fit neatly into CAP categories. CAP is a simplified model  real systems exist on a spectrum and may behave differently depending on configuration and failure modes."},{question:"What does 'linearizability' mean, and how is it different from 'serializability'?",options:["They are the same thing  both ensure operations appear to happen in order","Linearizability is about single-object real-time ordering; serializability is about multi-object transaction ordering","Linearizability applies only to reads; serializability applies only to writes","Linearizability is weaker than serializability"],correctIndex:1,explanation:"Linearizability and serializability are both correctness conditions but for different contexts. Linearizability ensures that operations on a SINGLE object appear to take effect instantaneously at some point between their invocation and response, respecting real-time ordering. Serializability ensures that the execution of MULTIPLE concurrent transactions is equivalent to some serial (sequential) execution of those transactions, but doesn't require the serial order to respect real-time ordering. Strict serializability (or 'linearizable + serializable') combines both  Google Spanner offers this. The confusion between these terms is one of the most common misunderstandings in distributed systems. Linearizability is a recency guarantee; serializability is a transaction isolation guarantee."},{question:"In a 5-node Raft cluster, what is the maximum number of node failures the system can tolerate while remaining available?",options:["1 node","2 nodes","3 nodes","4 nodes"],correctIndex:1,explanation:"A Raft cluster requires a majority quorum to operate. In a 5-node cluster, the quorum size is 3 (5/2 + 1). This means the system can tolerate 2 node failures and still have 3 nodes available to form a quorum. With 3 failures, only 2 nodes remain, which cannot form a majority of 5  the cluster becomes unavailable. This is the fundamental CP trade-off: more nodes improve fault tolerance but increase the coordination overhead. A 5-node cluster is the sweet spot for many production systems  it tolerates 2 failures (including during rolling upgrades) while keeping quorum size manageable. etcd, used by Kubernetes, typically runs as a 3 or 5-node cluster for this reason."},{question:"What is 'session consistency'?",options:["A guarantee that all sessions across all clients see the same data","A guarantee that within a single client session, the client sees a consistent view of the data","A guarantee that sessions are never lost during server failures","A guarantee that all database sessions use the same consistency level"],correctIndex:1,explanation:"Session consistency provides guarantees within the scope of a single client session. It typically combines read-your-writes, monotonic reads, and monotonic writes  ensuring that within a session, a client sees a progressively more up-to-date view of the data, never going backwards. Different sessions from different clients may see different data states. This is implemented by tracking the session's read/write timestamps and ensuring subsequent operations go to replicas that are at least as up-to-date. Azure Cosmos DB explicitly offers session consistency as one of its five consistency levels. It's a practical middle ground  strong enough for good UX, weak enough for reasonable performance in geo-distributed systems."},{question:"What is the 'FLP impossibility result' and how does it relate to CAP?",options:["It proves that distributed consensus is impossible in a purely asynchronous system with even one faulty node","It proves that CAP is wrong and all three properties can be achieved","It proves that network partitions can always be prevented","It proves that eventual consistency always converges within a bounded time"],correctIndex:0,explanation:"The FLP (Fischer, Lynch, Paterson) impossibility result from 1985 proves that in a purely asynchronous distributed system (no timeouts, no clock bounds), deterministic consensus is impossible if even one process can crash. This complements CAP  while CAP says you can't have all three properties during partitions, FLP says you can't even solve consensus reliably in asynchronous networks. In practice, systems work around FLP by using partial synchrony assumptions (timeouts), randomization, or failure detectors. Raft and Paxos, for example, assume partial synchrony  they're guaranteed safe always, but only make progress when the network is behaving. FLP and CAP together form the theoretical bedrock explaining why distributed systems are fundamentally hard."},{question:"You're designing a global e-commerce inventory system. The business requires that items are never oversold. Which approach is most appropriate?",options:["AP with eventual consistency  reconcile oversells later with customer apologies","CP with strong consistency on inventory decrements, accepting brief unavailability during partitions","AP with optimistic concurrency and CRDTs for inventory counts","Eventual consistency with last-write-wins for all inventory operations"],correctIndex:1,explanation:"When business rules demand 'never oversold,' you need CP with strong consistency for inventory decrements. An oversell means shipping product you don't have or canceling orders  both are costly in money and reputation. CP ensures that during a network partition, the system refuses to process sales rather than risk selling the same item twice. This is exactly the approach that high-value e-commerce uses for limited inventory items. Options A and D risk overselling during partitions. Option C with CRDTs doesn't work for inventory because a counter CRDT can't enforce 'must be  0'  that's a constraint requiring coordination. For high-volume, non-limited items, you might accept some overselling, but the question specifies 'never oversold.'"},{question:"Cosmos DB offers five consistency levels. From strongest to weakest, what is the correct order?",options:["Strong, Session, Bounded Staleness, Consistent Prefix, Eventual","Strong, Bounded Staleness, Session, Consistent Prefix, Eventual","Eventual, Consistent Prefix, Session, Bounded Staleness, Strong","Strong, Bounded Staleness, Consistent Prefix, Session, Eventual"],correctIndex:1,explanation:"Azure Cosmos DB offers five consistency levels from strongest to weakest: Strong  Bounded Staleness  Session  Consistent Prefix  Eventual. Strong provides linearizability. Bounded Staleness guarantees reads lag behind writes by at most K versions or T seconds. Session guarantees consistency within a client session (read-your-writes, monotonic reads). Consistent Prefix guarantees that reads never see out-of-order writes. Eventual provides no ordering guarantees but maximum performance. This five-level spectrum is Cosmos DB's answer to the false binary of 'strong vs eventual'  real applications need different consistency levels for different operations. Most Cosmos DB users use Session consistency as their default, which provides a good balance of correctness and performance."},{question:"What is 'bounded staleness' consistency?",options:["Reads return data that is at most K versions or T time units behind the latest write","Data is never stale  it's always the latest version","Staleness is bounded by the network latency between data centers","Reads are guaranteed fresh within one second of the write"],correctIndex:0,explanation:"Bounded staleness provides a quantitative guarantee on how stale data can be  reads will see data that is at most K versions old or T time units behind the latest write. This is stronger than eventual consistency (which provides no staleness bound) but weaker than strong consistency (which requires zero staleness). It's useful when you can tolerate some lag but need to guarantee it's limited. For example, Cosmos DB's bounded staleness with T=5 seconds means a read is guaranteed to see data no more than 5 seconds old. This is valuable for regulatory or business requirements that need a concrete SLA on data freshness without paying the full cost of strong consistency."},{question:"In the context of distributed databases, what is a 'read repair'?",options:["Fixing corrupted data during a read by checking parity bits","When a coordinator detects stale data on some replicas during a read and triggers an update to those replicas","Repairing broken read connections to the database","A scheduled job that re-reads all data to verify integrity"],correctIndex:1,explanation:"Read repair is an opportunistic consistency mechanism in Dynamo-style databases. When a coordinator sends a read request to multiple replicas and detects that some replicas have stale data (older versions), it sends the latest version back to the stale replicas to bring them up to date. This happens 'for free' during the read path  no extra coordination needed. Cassandra uses read repair extensively. The coordinator compares digests (hashes) from replicas, and if they differ, it fetches full data from all replicas, determines the latest version, and pushes it to the out-of-date ones. It's an elegant mechanism that gradually improves consistency through normal read traffic, though it only repairs data that's actually being read  rarely accessed data may remain inconsistent until anti-entropy repair runs."},{question:"What is a 'Merkle tree' and how is it used for anti-entropy in distributed systems?",options:["A B-tree variant used for indexing in distributed databases","A hash tree where each leaf is a hash of a data block, and parent nodes are hashes of their children, used to efficiently detect data differences between replicas","A tree structure for routing queries in distributed hash tables","A binary tree used for sorting data during map-reduce operations"],correctIndex:1,explanation:"A Merkle tree is a binary tree of hashes where each leaf node is the hash of a data block, and each parent node is the hash of its children's hashes. To compare data between two replicas, you only need to compare root hashes. If they differ, you traverse down the tree to find exactly which data blocks differ  a process that requires O(log n) comparisons instead of comparing all data. Cassandra uses Merkle trees during 'nodetool repair' to efficiently identify and synchronize data differences between replicas. This was described in the original Dynamo paper and is crucial for making anti-entropy practical at scale  without Merkle trees, comparing terabytes of data between replicas would be prohibitively expensive."},{question:"What is the 'consensus number' concept, and why does it matter for distributed systems?",options:["The minimum number of nodes needed to reach consensus","The maximum number of concurrent processes for which an object can solve consensus in a wait-free manner","The number of rounds needed for Paxos to converge","The percentage of nodes that must agree in a quorum"],correctIndex:1,explanation:"The consensus number (from Herlihy's 1991 paper) defines the maximum number of concurrent processes for which a synchronization primitive can solve wait-free consensus. Read-write registers have consensus number 1, test-and-set has consensus number 2, and compare-and-swap (CAS) has consensus number infinity (can solve consensus for any number of processes). This matters because it establishes a hierarchy of synchronization primitives  you can't build stronger primitives from weaker ones. This is why CAS is the foundation of most lock-free data structures and why modern CPUs include CAS instructions. It's a fundamental impossibility result that guides the design of concurrent and distributed algorithms."},{question:"What is the Paxos consensus algorithm, and which CAP property does it sacrifice?",options:["A consensus protocol that sacrifices consistency for availability","A consensus protocol that sacrifices availability for consistency (CP)","A consensus protocol that achieves all three CAP properties","A consensus protocol that only works without network partitions (CA)"],correctIndex:1,explanation:"Paxos, invented by Leslie Lamport, is a CP consensus protocol. It ensures that a group of nodes can agree on a single value even if some nodes fail or messages are lost. During a network partition, Paxos requires a majority quorum to make progress  the minority side becomes unavailable. Paxos guarantees safety (consistency) always but only guarantees liveness (availability) when a majority of nodes can communicate. It's notoriously difficult to understand and implement correctly  Lamport's original paper used a Greek parliament metaphor that confused many readers. Google's Chubby lock service uses Paxos internally, and many modern systems use Raft instead because it's easier to understand while providing equivalent guarantees."},{question:"What is 'consistent hashing' and how does it relate to partition tolerance?",options:["A hashing algorithm that guarantees consistent hash values across different programming languages","A technique for distributing data across nodes where adding/removing nodes only redistributes a minimal amount of data","A method for ensuring hash-based partitions are always consistent","A consensus algorithm for hash-partitioned databases"],correctIndex:1,explanation:"Consistent hashing maps both data and nodes onto a conceptual ring (hash space). Each node is responsible for the data between it and its predecessor on the ring. When a node joins or leaves, only the data in its immediate neighborhood needs to be redistributed  not the entire dataset. This is critical for partition tolerance because it minimizes data movement during topology changes (node failures, scaling). DynamoDB, Cassandra, and many CDNs use consistent hashing. Without it, adding a node to a traditional hash-based system (hash(key) % N) would require redistributing nearly all data because N changes. Consistent hashing makes the system resilient to node changes, which is a form of handling partitions gracefully."},{question:"What is a 'gossip protocol' and which CAP-related property does it support?",options:["A protocol for achieving strong consistency through message passing","A protocol for leader election in CP systems","A protocol where nodes periodically exchange state with random peers, supporting eventual consistency (AP)","A protocol for encrypting inter-node communication"],correctIndex:2,explanation:"A gossip protocol (also called epidemic protocol) is where each node periodically selects a random peer and exchanges state information. Over time, information propagates to all nodes  like how rumors spread in a social network. This supports AP systems because it doesn't require coordination or quorum  nodes spread information opportunistically, and the system eventually converges. Cassandra uses gossip for cluster membership and failure detection. Gossip protocols are inherently eventually consistent  there's no guarantee on when all nodes will have the same information, but they will converge. They're highly scalable (O(log N) rounds to propagate to all N nodes) and robust to node failures, making them ideal for large-scale distributed systems."},{question:"What does the 'C' in PACELC's 'ELC' portion represent differently from the 'C' in 'PAC'?",options:["They represent the same thing  linearizable consistency","ELC's C represents the consistency-latency trade-off during normal operation, while PAC's C is about consistency during partitions","ELC's C means caching, PAC's C means consistency","They are unrelated properties that happen to share a letter"],correctIndex:1,explanation:"While both use 'C' for consistency, the trade-off context differs. In PAC (during a Partition), choosing C means the system becomes unavailable to maintain consistency  it's a binary choice of serving or not serving. In ELC (Else, no partition), choosing C means accepting higher latency to ensure consistency through coordination between nodes  it's a continuous trade-off between response time and consistency strength. For example, even without partitions, a system like Spanner adds latency for TrueTime uncertainty waits to ensure external consistency (PC/EC). DynamoDB normally returns fast but potentially stale data (PA/EL). The PACELC model captures that consistency costs are different during partitions (availability cost) versus normal operation (latency cost)."},{question:"MongoDB uses a single-leader replication model. During a leader failover, what CAP behavior does MongoDB exhibit?",options:["It continues serving reads and writes from secondaries (AP behavior)","It becomes briefly unavailable for writes until a new primary is elected (CP behavior)","It maintains full availability and consistency throughout the failover","It switches to eventual consistency for all operations"],correctIndex:1,explanation:"During a MongoDB primary (leader) failover, writes are unavailable for 10-30 seconds while the replica set elects a new primary via Raft-like consensus. Reads can continue from secondaries but may be stale. This makes MongoDB exhibit CP behavior during failovers  it sacrifices write availability to maintain consistency (only one primary can accept writes at a time, preventing split-brain). The election uses a majority vote among replica set members, ensuring only one primary exists. This is a common pattern in single-leader systems: the failover window is a brief availability sacrifice. MongoDB applications must handle 'not master' errors during this window, which is why drivers have built-in retry logic."},{question:"What is a 'witness replica' or 'arbiter' in a consensus system?",options:["A replica that stores full data and serves client requests","A lightweight node that participates in voting/quorum but doesn't store full data","A node that monitors system health but doesn't participate in consensus","A backup node that only activates when the leader fails"],correctIndex:1,explanation:"A witness (or arbiter in MongoDB terminology) is a node that participates in quorum voting for leader election and consensus but doesn't store a full copy of the data. Its purpose is to provide an odd number of voters to prevent tied elections, without the cost of storing and replicating all the data. For example, if you have two data-bearing nodes in different data centers, adding a lightweight witness in a third location gives you 3 voters for quorum. MongoDB uses arbiters in replica sets, and CockroachDB supports witness replicas. This is cost-effective: you get better fault tolerance for consensus without tripling your storage costs, though you still need at least two full replicas for data durability."},{question:"What is the difference between 'safety' and 'liveness' in distributed systems, and how does it relate to CAP?",options:["Safety means data is encrypted; liveness means the system is running","Safety means nothing bad happens (e.g., inconsistency); liveness means something good eventually happens (e.g., requests complete)","Safety is about hardware; liveness is about software","They are the same thing expressed differently"],correctIndex:1,explanation:"Safety properties guarantee that 'nothing bad happens'  for example, consistency means you never read incorrect data, and agreement means nodes never decide different values. Liveness properties guarantee that 'something good eventually happens'  for example, availability means requests eventually get responses, and termination means consensus eventually completes. In CAP terms, consistency is a safety property and availability is a liveness property. The FLP impossibility result shows you can't guarantee both in asynchronous systems. Raft and Paxos always maintain safety (consistency) but may sacrifice liveness (availability) during partitions. This safety/liveness framework is fundamental to reasoning about correctness in distributed systems."},{question:"What is a 'network partition' at the infrastructure level? Give a realistic example.",options:["A deliberate splitting of data into shards for scalability","A failure where some nodes can communicate with each other but not with other groups of nodes","A DNS failure that prevents name resolution","A firewall rule that blocks all external traffic"],correctIndex:1,explanation:"A network partition is when the network breaks into two or more groups that can communicate within their group but not across groups. Real examples: a switch failure between two racks in a data center, a severed fiber optic cable between two AWS availability zones, or a misconfigured firewall rule. The critical characteristic is that nodes on both sides are healthy and running  they just can't reach each other. This is different from a total network failure (where nothing works) because both sides continue operating independently, potentially accepting writes and diverging in state. GitHub experienced a famous 24-second network partition between their primary MySQL and replica in 2018 that caused widespread data inconsistency and a multi-hour outage."},{question:"CockroachDB claims to be both consistent and available. How does it achieve this, and what's the catch?",options:["It violates the CAP theorem through a novel algorithm","It's CP but optimizes for availability during normal operation, becoming unavailable only during actual partitions","It achieves CA by running in a single data center","It uses eventual consistency with very fast convergence that appears consistent"],correctIndex:1,explanation:"CockroachDB is a CP system that uses Raft consensus for strong consistency. The 'catch' is that CAP only applies during network partitions  which are rare. During normal operation (the vast majority of time), CockroachDB provides both consistency AND high availability through multi-replica Raft groups, automatic failover, and optimized consensus. During an actual partition, it will sacrifice availability (minority partition becomes unavailable) to maintain consistency. This highlights a key misconception about CAP: it's about behavior during partitions, not during normal operation. CockroachDB, Spanner, and YugabyteDB all take this approach  be CP during partitions but design for high availability during the 99.99% of time when the network is healthy."},{question:"What is 'quorum intersection' and why is it essential for consistency?",options:["The physical location where quorum servers are co-located","The guarantee that any two quorums share at least one common member, ensuring data overlap","The time when two quorum votes happen simultaneously","The intersection of read and write timeout settings"],correctIndex:1,explanation:"Quorum intersection means that any two quorums (sets of nodes that can independently make decisions) must share at least one node in common. This is what guarantees consistency in quorum-based systems  the shared node(s) have participated in both the write quorum and the read quorum, so they can provide the latest data. With majority quorums in a set of N nodes, any two majorities must overlap by at least one node (since two groups each larger than N/2 must share members). If quorums could be disjoint, the system could have two independent writes without any node knowing about both, breaking consistency. This is the mathematical foundation of why R + W > N works."},{question:"What is the 'Two Generals' Problem and how does it relate to distributed consensus?",options:["A problem about optimizing military strategy with two leaders","A thought experiment proving that reaching agreement over an unreliable communication channel is impossible","A technique for dual-leader replication in databases","An algorithm for achieving consensus with exactly two nodes"],correctIndex:1,explanation:"The Two Generals' Problem illustrates that two parties communicating over an unreliable channel can never be certain they've reached agreement. General A sends 'attack at dawn' to General B, but the messenger might be captured. Even if B confirms, A can't be sure the confirmation arrived. This creates an infinite regress of confirmations. It was the first computer communication problem proved to be unsolvable (1975). It demonstrates why distributed consensus is fundamentally hard  messages can be lost, and you can never be 100% certain the other party received your message. Real systems work around this with timeouts, retries, and probabilistic guarantees (like TCP's three-way handshake), but the fundamental impossibility remains."},{question:"In a geo-distributed system spanning US-East and EU-West regions, a network partition occurs between the regions. Under CP semantics, what happens to requests in each region?",options:["Both regions continue serving all requests normally","Both regions become completely unavailable","The region with the majority of replicas continues; the other becomes unavailable for writes","Both regions switch to eventual consistency automatically"],correctIndex:2,explanation:"Under CP semantics, the region that can form a quorum (majority of replicas) continues operating normally. The region in the minority becomes unavailable for writes and potentially for consistent reads. For example, if you have 3 replicas (2 in US-East, 1 in EU-West) and the inter-region link goes down, US-East has 2 out of 3 nodes (quorum) and continues serving requests. EU-West has only 1 node and cannot form a quorum  it will refuse writes and return errors. This is why replica placement is a critical design decision in geo-distributed systems. Google Spanner, for instance, carefully places replicas to ensure quorum can be maintained even if one region is isolated."},{question:"What is 'lease-based consistency' and which systems use it?",options:["A consistency model where nodes 'lease' data from a central server for a fixed duration","A technique where a leader holds a time-bounded lease, ensuring only one leader exists and reads can be served locally during the lease","A subscription model for database access","A method for renting cloud database instances"],correctIndex:1,explanation:"Lease-based consistency uses time-bounded leases to ensure that at most one node acts as leader for a given data partition. The leader can serve consistent reads locally (without quorum) during its lease period, because no other node can become leader until the lease expires. If the leader fails, the system waits for the lease to expire before electing a new one  preventing split-brain. Google's Chubby lock service uses leases extensively, and Raft implementations often use leader leases to optimize read performance. The key trade-off is that after a leader failure, there's a mandatory wait period (lease duration) before a new leader can be elected, which is a bounded unavailability window. This is a practical optimization that reduces read latency in CP systems."},{question:"You're designing a collaborative document editor (like Google Docs). Which consistency model and conflict resolution approach would you use?",options:["Strong consistency with pessimistic locking  lock the document for each editor","Eventual consistency with OT (Operational Transformation) or CRDTs for real-time conflict-free merging","CP with single-writer at a time  queue edits and apply sequentially","Last-write-wins  whoever saves last, their version is kept"],correctIndex:1,explanation:"Collaborative editors like Google Docs use eventual consistency with conflict resolution algorithms  either OT (Operational Transformation, which Google Docs uses) or CRDTs (which Figma and some newer editors use). Multiple users edit simultaneously on their local copies, and the system automatically merges changes in a way that converges to the same document. Strong consistency with locks would make the experience terrible  only one person could type at a time. Single-writer would queue users. LWW would lose entire edits. OT transforms operations based on concurrent changes (if user A inserts at position 5 and user B deletes at position 3, A's position is adjusted to 4). CRDTs offer a mathematically cleaner approach where merge is always safe. Both provide AP-style availability with automatic conflict resolution."},{question:"What is the role of a 'coordinator node' in Dynamo-style distributed databases?",options:["A permanent leader that handles all reads and writes for the cluster","A node that receives a client request and coordinates the operation across the relevant replicas","A node dedicated to monitoring cluster health","A node that handles schema changes and DDL operations"],correctIndex:1,explanation:"In Dynamo-style systems (Cassandra, DynamoDB, Riak), any node can act as a coordinator for a given request. When a client sends a request, the receiving node becomes the coordinator for that operation. It determines which replica nodes hold the data (using consistent hashing), forwards the request to those replicas, collects responses, and returns the result to the client based on the configured consistency level. This is different from single-leader systems where one node is permanently the leader. The coordinator role is per-request, enabling any node to serve any request  this is key to the AP nature of these systems. If one node is down, clients can connect to any other node. This leaderless architecture eliminates the single point of failure inherent in leader-based systems."},{question:"What is 'Jepsen testing' and what does it validate about distributed systems?",options:["A performance benchmarking tool for measuring throughput","A framework that tests distributed systems under network partitions and other faults to verify consistency and safety claims","A unit testing framework for distributed applications","A security testing tool for finding vulnerabilities in databases"],correctIndex:1,explanation:"Jepsen, created by Kyle Kingsbury (Aphyr), is a testing framework that subjects distributed databases to network partitions, clock skew, process crashes, and other faults while checking whether the system maintains its claimed consistency guarantees. It has found critical bugs in nearly every distributed database tested  including MongoDB, Cassandra, CockroachDB, and Elasticsearch. Jepsen uses a formal model to verify properties like linearizability and serializability against actual observed histories. Its findings have been so impactful that 'passing Jepsen testing' has become a credibility milestone for distributed database vendors. It directly validates CAP-related claims  does the system actually maintain consistency during partitions as it claims?"},{question:"In Amazon's original Dynamo paper, what was the 'shopping cart' problem that motivated the AP design?",options:["Shopping carts needed ACID transactions for payment processing","Shopping carts needed to always be available for adds/removes, even during partitions, with conflicts merged by union","Shopping carts needed strong consistency to prevent duplicate items","Shopping carts needed to support millions of concurrent users with CP guarantees"],correctIndex:1,explanation:"The Dynamo paper was motivated by Amazon's need for an always-available shopping cart. The insight was that it's better to accept an add-to-cart request during a partition (even if it creates a conflict) than to reject it  a rejected add means a lost sale. If conflicts arise (e.g., two versions of the cart after a partition), the resolution is to merge by union (keep all items from both versions). The worst case is a deleted item reappearing, which is far less costly than a customer being unable to add items. This was a groundbreaking business-driven architecture decision: the cost of unavailability (lost revenue) was quantified and found to be much higher than the cost of occasional inconsistency (a reappearing cart item). This thinking spawned an entire generation of AP databases."},{question:"What is 'external consistency' as defined by Google Spanner?",options:["Consistency between the database and external APIs","Consistency that respects real-world time ordering  if transaction T1 commits before T2 starts, T1's commit timestamp is earlier","Consistency enforced by an external validation service","The ability to query consistent data from outside the cluster"],correctIndex:1,explanation:"External consistency (or strict serializability) is the strongest consistency guarantee  it means that if transaction T1 commits before transaction T2 starts in real (wall-clock) time, then T1's timestamp will be less than T2's timestamp in the database. This is stronger than linearizability because it applies to transactions, not just single operations. Google Spanner achieves this using TrueTime  GPS and atomic clocks that give bounded clock uncertainty. Spanner waits out the uncertainty interval before committing, ensuring timestamps reflect real-time order. This means Spanner's globally distributed database behaves as if it were a single machine processing transactions sequentially in real-time order  a remarkable achievement that requires specialized hardware (atomic clocks in every datacenter)."},{question:"A system uses asynchronous replication from a primary to secondaries. During a primary failure, acknowledged but unreplicated writes are lost. What CAP implication does this have?",options:["The system is strongly consistent (CP)","The system is NOT strongly consistent  it may lose acknowledged writes, breaking consistency guarantees","This has no CAP implications  data loss is a durability concern, not consistency","The system achieves perfect availability (AP)"],correctIndex:1,explanation:"If the system acknowledges a write to the client but the write hasn't been replicated when the primary fails, the client believes the write succeeded but the data is gone  this violates the consistency guarantee (linearizability requires that once a write is acknowledged, all subsequent reads see it). This is a common issue with asynchronous replication in systems like Redis Sentinel, MongoDB (default), and PostgreSQL streaming replication. The system appears consistent during normal operation but reveals its inconsistency during failures. To achieve true CP behavior, you need synchronous replication or consensus-based replication (like Raft), where writes are only acknowledged after a majority of replicas have them. The CAP classification of a system should be evaluated based on its behavior during failures, not just during normal operation."},{question:"What does it mean for a system to provide 'monotonic writes' consistency?",options:["All writes must be larger than the previous write value","Writes from the same client are applied in the order they were issued","Writes are applied in alphabetical order across all clients","Each write must complete before the next write can begin"],correctIndex:1,explanation:"Monotonic writes guarantees that writes from the same process are serialized  if a client writes A then writes B, all replicas will apply A before B. Without this guarantee, a replica might see B before A due to message reordering, leading to inconsistent state. For example, if you update a user's email (write A) then send a notification to the new email (write B), monotonic writes ensures no replica processes the notification before the email update. This is weaker than strong consistency (it doesn't order writes across different clients) but prevents a specific class of anomalies. Session-based systems often provide this guarantee by routing all writes from a session through the same replica or by including sequence numbers."},{question:"What is the relationship between consensus and state machine replication (SMR)?",options:["They are completely unrelated concepts","Consensus is used to agree on the order of commands, which SMR applies to replicas to keep them in sync","SMR replaces the need for consensus","Consensus handles reads, SMR handles writes"],correctIndex:1,explanation:"State Machine Replication (SMR) is the technique of replicating a deterministic state machine across multiple nodes. If all replicas start in the same state and apply the same commands in the same order, they'll end up in the same state. Consensus protocols (Paxos, Raft) provide the mechanism to agree on the order of commands  this is the hard part in a distributed system. Raft explicitly implements SMR: the leader appends commands to a replicated log, consensus ensures all nodes agree on the log order, and each node applies the log to its state machine. This is the foundation of nearly all CP distributed systems  etcd, ZooKeeper, CockroachDB, and TiKV all use consensus-based SMR. The key insight is that ordering + determinism = consistency."},{question:"What is 'chain replication' and how does it differ from quorum-based replication in terms of CAP?",options:["It's the same as quorum replication but with a different name","Writes go to the head of a chain and propagate to the tail; reads are served from the tail  providing strong consistency with different availability trade-offs","Data is replicated in a blockchain-like immutable chain","It uses hash chains for data integrity verification"],correctIndex:1,explanation:"In chain replication, nodes are arranged in a chain. Writes enter at the head and propagate sequentially to each node until reaching the tail, which sends the acknowledgment. Reads are served only from the tail, which has the latest committed data. This provides strong consistency (the tail always has all committed writes) with high read throughput (reads only hit one node). The trade-off versus quorum replication: chain replication has higher write latency (must traverse the entire chain) and is more sensitive to individual node failures (any node failure breaks the chain until reconfigured). HDFS NameNode and Microsoft's Azure Storage use variants of chain replication. It's CP like quorum systems but with different performance characteristics  better read throughput but worse write latency and failure handling."},{question:"A distributed system uses 'leader leases' where the leader can serve reads locally without consulting followers. What happens if the leader's clock is significantly ahead of other nodes?",options:["No impact  clock skew doesn't affect leader leases","The leader might serve stale reads after its lease has actually expired (from other nodes' perspective) but it still thinks it's valid","The system automatically corrects for clock skew","Other nodes will also advance their clocks to match"],correctIndex:1,explanation:"If the leader's clock is fast, its lease will expire (by its own clock) before other nodes think it should. This is actually the safe direction  the leader stops serving before others could elect a new one. The dangerous case is if the leader's clock is SLOW  it thinks its lease is still valid when other nodes have already started a new election. In this case, two nodes might both think they're the leader, causing split-brain and inconsistent reads. This is why Google's TrueTime is so valuable  it provides bounded clock uncertainty, and Spanner waits out the uncertainty before relying on timestamps. Without hardware clock synchronization, leader leases depend on the assumption that clock drift is bounded, which NTP tries to provide but can't guarantee."},{question:"What is a 'tombstone' in eventually consistent systems, and why is it needed?",options:["A marker indicating a record has been logically deleted, preventing its resurrection during replica synchronization","A backup copy of deleted data for recovery purposes","A log entry marking the end of a database transaction","A timestamp indicating when a node was last seen alive"],correctIndex:0,explanation:"In eventually consistent systems, you can't simply delete a record because during synchronization, another replica that still has the record would re-introduce it (thinking the deleting replica just missed the write). Tombstones solve this by replacing the deleted record with a special marker that says 'this was intentionally deleted.' During sync, when a node sees a tombstone, it knows to delete its copy rather than propagate the live version. Cassandra uses tombstones extensively, and they have a configurable retention period (gc_grace_seconds, default 10 days). Tombstones can accumulate and cause performance issues if not cleaned up  a common operational challenge in Cassandra. This is a non-obvious consequence of choosing AP/eventual consistency: even deletes are more complex."},{question:"What is the 'harvest and yield' framework proposed by Eric Brewer as an alternative way to think about CAP?",options:["A farming analogy for database scaling","Yield is the probability of completing a request; harvest is the fraction of complete data in the response  systems can trade between them","A method for calculating database throughput","A framework for deciding when to scale up vs scale out"],correctIndex:1,explanation:"Brewer proposed 'harvest and yield' as a more nuanced way to think about CAP trade-offs. Yield is the probability that a request completes (related to availability). Harvest is the completeness of the answer  the fraction of total data reflected in the response. Instead of a binary available/unavailable choice, systems can degrade gracefully: return partial results (lower harvest) while maintaining high yield. For example, a search engine might return results from 99 out of 100 index shards if one is down  high yield (the request completes) with slightly reduced harvest (missing some results). Google Search does exactly this. This framework is more practical than binary CAP thinking because it allows for partial degradation rather than complete failure."},{question:"What is the 'CAP theorem proof' by Gilbert and Lynch (2002) based on?",options:["An empirical study of distributed database failures","A formal proof showing that in an asynchronous network, it's impossible to implement a read/write data object that guarantees availability and atomic consistency in all executions including partitions","A mathematical proof based on information theory","A simulation of network partition scenarios"],correctIndex:1,explanation:"Gilbert and Lynch formally proved Brewer's conjecture by constructing an asynchronous network model and showing that no algorithm can implement an atomic (linearizable) read/write register that is both available (always responds) and consistent (linearizable) in all executions where messages can be lost (partitions). The proof is by contradiction: they show that if a partition occurs and both sides must respond (availability), at least one side must return a stale value (violating consistency), or one side must not respond (violating availability). The formal proof is important because it moved CAP from a conjecture ('I think this is true') to a theorem ('this is mathematically proven'). It also precisely defined the terms, reducing the ambiguity in Brewer's original talk."},{question:"etcd is used by Kubernetes for cluster state. Why is a CP system appropriate for this use case?",options:["Because Kubernetes doesn't need high availability","Because cluster state (pod scheduling, service discovery) must be consistent to prevent conflicting assignments","Because etcd is the fastest available key-value store","Because Kubernetes only runs in a single data center"],correctIndex:1,explanation:"Kubernetes uses etcd for storing all cluster state  pod assignments, service definitions, ConfigMaps, Secrets, etc. This data must be strongly consistent because inconsistency could cause catastrophic problems: two nodes might both think they own the same pod, or a scheduler might assign work to a node that's been removed. It's better for the control plane to be briefly unavailable (CP) than to make conflicting scheduling decisions. Kubernetes can tolerate brief control plane unavailability because running workloads continue operating even if the control plane is down  it's the control plane decisions that need consistency. This is a textbook CP use case: metadata and coordination data where correctness trumps availability."},{question:"What is a 'phi accrual failure detector' used in systems like Cassandra?",options:["A mechanism that gives a continuous suspicion level (phi value) for each node rather than a binary alive/dead determination","A cryptographic method for detecting data tampering","A load balancing algorithm that detects hot spots","A technique for detecting network partitions using packet analysis"],correctIndex:0,explanation:"The phi accrual failure detector outputs a continuous suspicion level (phi) rather than a binary alive/dead verdict. Phi represents the likelihood that a node has failed, based on the statistical distribution of heartbeat inter-arrival times. A higher phi means more suspicion. Applications can choose their own phi threshold based on their tolerance for false positives vs. detection speed. This is more nuanced than simple timeout-based detection: a node that usually responds in 10ms but hasn't responded in 100ms gets a high phi, while in a system where responses normally take 500ms, 100ms of silence barely registers. Cassandra uses this to adapt to varying network conditions and avoid falsely marking healthy-but-slow nodes as down, which would cause unnecessary data movement."},{question:"What are the implications of the CAP theorem for microservices architectures?",options:["Each microservice must choose the same CAP trade-off for consistency across the system","Different microservices can make different CAP trade-offs based on their specific requirements","CAP theorem doesn't apply to microservices, only to databases","Microservices avoid CAP issues by using REST APIs"],correctIndex:1,explanation:"One of the key benefits of microservices is that each service can make independent CAP trade-offs based on its domain requirements. A payment service might use a CP database (PostgreSQL with synchronous replication) because financial consistency is critical. A product catalog service might use an AP database (Cassandra) because availability is more important than immediate consistency for product listings. A session store might use Redis with eventual consistency because session data is ephemeral. This per-service optimization is impossible in a monolith with a single database. However, this creates complexity at service boundaries  when a workflow spans multiple services with different consistency models, you need patterns like sagas, eventual consistency, or compensation transactions to maintain overall system correctness."},{question:"What is the 'saga pattern' and how does it relate to consistency across AP and CP services?",options:["A Norse mythology-inspired naming convention for database tables","A pattern for managing distributed transactions across services through a sequence of local transactions with compensating actions for rollback","A protocol for achieving strong consistency in AP systems","A logging pattern for tracking data changes across services"],correctIndex:1,explanation:"The saga pattern manages distributed transactions by breaking them into a sequence of local transactions, each in its own service. If one step fails, compensating transactions undo the previous steps. For example, an order saga: (1) Reserve inventory  (2) Charge payment  (3) Ship order. If payment fails, a compensating transaction releases the reserved inventory. Sagas provide eventual consistency across services without requiring distributed locking (2PC), which would create tight coupling and reduce availability. This is essential in microservices where different services may use AP or CP databases. The trade-off: sagas are eventually consistent (there's a window where the system is in a partially completed state), and compensating logic can be complex. Orchestrated sagas use a central coordinator; choreographed sagas use events."},{question:"What is 'strict quorum' vs 'sloppy quorum' in Dynamo-style systems?",options:["Strict quorum uses exactly N nodes; sloppy quorum uses fewer than N nodes","Strict quorum only counts designated replica nodes; sloppy quorum can count any healthy node toward the quorum","Strict quorum requires all nodes to respond; sloppy quorum requires only a majority","They are the same thing with different names"],correctIndex:1,explanation:"A strict quorum only counts the designated replica nodes (the N nodes responsible for a key based on consistent hashing) toward the R or W threshold. If some designated nodes are down, the operation fails. A sloppy quorum allows the system to count any healthy node toward the quorum  if a designated node is down, the write goes to another node (with a hint for later handoff). Sloppy quorums increase availability (writes succeed even if designated nodes are down) but break the R + W > N consistency guarantee because reads and writes might go to entirely different sets of nodes. DynamoDB uses sloppy quorums to achieve its always-writable design. Cassandra uses strict quorums by default. The choice between them is fundamentally a CP vs AP trade-off."},{question:"How does the 'Raft' consensus protocol handle a leader failure?",options:["A human operator manually promotes a new leader","Followers detect leader absence via heartbeat timeout, start an election with randomized timeouts to prevent split votes, and the first candidate to win majority votes becomes leader","The node with the most data automatically becomes leader","All nodes stop processing until the old leader recovers"],correctIndex:1,explanation:"In Raft, the leader periodically sends heartbeats to followers. If a follower doesn't receive a heartbeat within its election timeout (randomized to prevent simultaneous elections), it increments its term, transitions to 'candidate' state, votes for itself, and requests votes from other nodes. A candidate wins if it receives votes from a majority. The randomized timeout is crucial  without it, multiple nodes would start elections simultaneously, splitting votes and delaying leader election. The new leader must have all committed log entries (candidates with incomplete logs are rejected). This entire process typically completes in milliseconds, making the unavailability window very brief. Raft's clear separation of leader election, log replication, and safety makes it much easier to understand and implement correctly compared to Paxos."},{question:"What is 'multi-leader replication' (also called 'multi-master'), and what CAP classification does it typically have?",options:["CP  multiple leaders ensure stronger consistency","AP  multiple leaders can accept writes independently, tolerating partitions at the cost of potential conflicts","CA  multiple leaders prevent both partitions and unavailability","It doesn't have a CAP classification"],correctIndex:1,explanation:"Multi-leader replication allows multiple nodes to accept writes independently and asynchronously replicate changes to each other. This is an AP design because during a network partition, leaders on both sides continue accepting writes  maintaining availability at the cost of consistency (conflicting writes may need resolution later). CouchDB, MySQL Group Replication (multi-primary mode), and DynamoDB global tables use multi-leader replication. The main use case is geo-distributed systems where users write to their nearest leader for low latency. The downside is write conflicts when different leaders modify the same data  requiring conflict resolution strategies (LWW, merge functions, CRDTs). Multi-leader is essentially choosing availability and low latency over consistency."},{question:"Google Bigtable is often described as a CP system. What mechanism provides its consistency guarantee?",options:["Multi-leader replication with conflict resolution","Single-leader design where each tablet is served by exactly one tablet server, using Chubby for leader election","Paxos consensus across all tablet servers","Eventual consistency with automatic reconciliation"],correctIndex:1,explanation:"Bigtable achieves strong consistency through a simple but effective mechanism: each tablet (data partition) is assigned to exactly one tablet server at a time. All reads and writes for that tablet go to its assigned server, eliminating the possibility of conflicting writes. If the tablet server fails, Chubby (Google's distributed lock service, which uses Paxos internally) detects the failure and reassigns the tablet to another server. During reassignment, the tablet is briefly unavailable  the CP trade-off. This single-server-per-tablet design makes consistency trivial (no concurrent writers) at the cost of availability during failures. HBase, the open-source Bigtable clone, uses ZooKeeper instead of Chubby for the same purpose."},{question:"What is 'conflict-free' about CRDTs, and what types of CRDTs exist?",options:["CRDTs prevent conflicts by using locks  there are read-CRDTs and write-CRDTs","CRDTs are mathematically designed so concurrent updates can always be merged without conflicts  the two types are state-based (CvRDT) and operation-based (CmRDT)","CRDTs avoid conflicts by only allowing append operations  there are log-CRDTs and queue-CRDTs","CRDTs resolve conflicts using timestamps  there are LWW-CRDTs and MWW-CRDTs"],correctIndex:1,explanation:"CRDTs achieve conflict-freedom through mathematical properties: the merge function is commutative (order doesn't matter), associative (grouping doesn't matter), and idempotent (merging the same state twice has no effect). State-based CRDTs (CvRDTs - Convergent) transmit their full state, and any two states can be merged. Operation-based CRDTs (CmRDTs - Commutative) transmit operations, which must be commutative. Examples include G-Counter (grow-only counter), PN-Counter (increment/decrement), G-Set (grow-only set), OR-Set (observed-remove set), and LWW-Register. The key insight is that by constraining the data structure's operations, you can guarantee convergence without coordination  no locks, no consensus, no conflicts. This makes CRDTs ideal for AP systems and collaborative applications."},{question:"What is the practical impact of CAP theorem on system design decisions in a real-world company?",options:["Teams must choose one database technology for the entire company","It forces engineers to have explicit conversations about consistency vs availability trade-offs for each use case and choose appropriate technologies","It means distributed systems are impossible to build correctly","It only matters for database vendors, not application developers"],correctIndex:1,explanation:"CAP's greatest practical impact is forcing explicit trade-off discussions. When designing a feature, engineers must ask: 'What happens during a network partition? Is it worse to show stale data or to show an error?' This leads to polyglot persistence  using different databases for different use cases based on their CAP properties. A company might use PostgreSQL (CP) for billing, Cassandra (AP) for user activity feeds, Redis (typically CP with caveats) for caching, and Elasticsearch (AP-ish) for search. CAP also influences API design (should this endpoint return stale data or error?), SLA definitions, and incident response procedures. The theorem's real value isn't the mathematical proof  it's the vocabulary and framework it gives teams to reason about and communicate distributed system trade-offs."},{question:"What happens to a Cassandra cluster if ALL nodes for a particular partition key go down?",options:["Other nodes automatically take over the partition","The data is served from a cache layer","Read and write requests for that partition key fail, regardless of consistency level","Cassandra reconstructs the data from parity information"],correctIndex:2,explanation:"Even in AP systems like Cassandra, if ALL replicas for a key are down, requests for that key will fail. AP guarantees availability only if at least one replica is reachable. With a replication factor of 3, losing all 3 replicas for a token range makes that data unavailable. This is why Cassandra recommends replication factor of 3 across multiple racks/data centers  it makes simultaneous failure of all replicas extremely unlikely. Cassandra doesn't use parity (like RAID) or automatic data reconstruction. It also doesn't dynamically reassign token ranges to other nodes (unlike some systems with virtual nodes that support range takeover). The data is simply unavailable until at least one replica recovers. This illustrates that 'available' in CAP means 'every non-failing node responds,' not 'the system handles any number of failures.'"},{question:"What is the difference between 'synchronous' and 'asynchronous' replication in terms of CAP?",options:["Synchronous replication is always AP; asynchronous is always CP","Synchronous replication provides stronger consistency (CP-leaning) by waiting for replicas to confirm before acknowledging; asynchronous is faster but may lose data (AP-leaning)","There is no difference  both provide the same CAP guarantees","Synchronous replication doesn't involve the network; asynchronous does"],correctIndex:1,explanation:"Synchronous replication waits for one or more replicas to confirm the write before acknowledging it to the client. This provides stronger durability and consistency (the data exists on multiple nodes before the client considers the write complete) but increases latency and reduces availability (if a replica is slow or down, the write is delayed or fails). Asynchronous replication acknowledges the write immediately after the primary stores it, then replicates in the background. This is faster and more available but risks data loss during primary failure. PostgreSQL offers both: synchronous_commit=on waits for replica confirmation, synchronous_commit=off doesn't. The choice is a direct CAP/PACELC trade-off  synchronous replication is CP/EC behavior, asynchronous is AP/EL behavior."},{question:"What is the 'impossibility of exactly-once delivery' and how does it relate to CAP?",options:["Messages can always be delivered exactly once with the right protocol","In a distributed system with possible failures, you can only guarantee at-most-once or at-least-once delivery, not exactly-once  making idempotency crucial for AP systems","Exactly-once delivery is trivially achievable with TCP","This impossibility only applies to email systems"],correctIndex:1,explanation:"In a distributed system where messages can be lost or duplicated, you can guarantee at-most-once delivery (send once, don't retry  might be lost) or at-least-once delivery (retry until acknowledged  might be duplicated), but not exactly-once delivery at the network level. This is related to CAP because AP systems that retry writes for availability may create duplicates. The practical solution is 'effectively exactly-once' through idempotent operations  designing operations so that applying them multiple times has the same effect as applying them once. For example, 'set balance to $100' is idempotent, but 'add $10 to balance' is not. Kafka achieves 'exactly-once semantics' through idempotent producers and transactional consumers  it's still at-least-once at the network level, but deduplication makes it appear exactly-once to the application."},{question:"What is the impact of CAP theorem on global database deployments spanning multiple continents?",options:["Global deployments are impossible due to CAP constraints","Cross-continent latency (~100-300ms) amplifies the consistency-latency trade-off: CP means high write latency for coordination; AP means fast writes but potential conflicts","CAP doesn't apply to global deployments","All global databases must use eventual consistency"],correctIndex:1,explanation:"In global deployments, the physics of light speed creates ~100-300ms round-trip latency between continents. For CP systems, every write requires cross-continent coordination (consensus), adding this latency to every write. Spanner accepts this cost for external consistency. For AP systems, writes can complete locally and replicate asynchronously, giving low latency but risking conflicts. The PACELC model is especially relevant here: even without partitions (which are more common across continents), the latency-consistency trade-off is severe. This is why many global systems use a hybrid approach: CP for critical data (like user identity), AP for non-critical data (like activity feeds), and careful data placement to keep most operations within a single region."},{question:"What is 'data sovereignty' and how does it intersect with CAP theorem decisions?",options:["A marketing term for data encryption","Legal requirements that data must reside in specific geographic regions, constraining replica placement and affecting CAP trade-offs","The right of a database to refuse queries it doesn't want to answer","A technique for preventing data replication"],correctIndex:1,explanation:"Data sovereignty laws (GDPR in Europe, LGPD in Brazil, etc.) require that certain data physically resides within specific geographic boundaries. This constrains where you can place replicas, directly affecting CAP trade-offs. If EU user data can only be in EU data centers, you can't have replicas in US-East for faster quorum  limiting your options for fault tolerance and latency. A partition between EU data centers has no US-based backup. Some systems (like CockroachDB and Spanner) support geo-partitioning to pin data to specific regions while maintaining global replication for other data. This intersection of legal requirements and distributed systems theory is increasingly important as more countries enact data localization laws, forcing architects to make CAP trade-offs within geographic constraints."},{question:"What is the 'Dynamo' model and how has it influenced modern databases?",options:["A relational database model from Oracle","A design philosophy from Amazon's Dynamo paper: leaderless replication, consistent hashing, sloppy quorums, vector clocks, hinted handoff, and eventual consistency","A specific AWS database product","A consensus-based replication model"],correctIndex:1,explanation:"Amazon's 2007 Dynamo paper introduced a revolutionary AP database architecture featuring: leaderless replication (any node can handle any request), consistent hashing (for data distribution), sloppy quorums (for high availability), vector clocks (for conflict detection), hinted handoff (for handling unavailable nodes), and gossip protocols (for membership and failure detection). This design prioritized availability and scalability over strong consistency. Its influence is massive: Cassandra (Facebook, based on Dynamo + Bigtable), Riak (Basho), Voldemort (LinkedIn), and DynamoDB (Amazon's managed version) all descend from this model. The paper's key insight  that many applications can tolerate eventual consistency and benefit enormously from the resulting availability  fundamentally changed distributed database design."},{question:"What is 'convergent conflict resolution' and why is it important?",options:["Resolving conflicts by choosing the value that converges to zero","A conflict resolution strategy where all replicas are guaranteed to reach the same final state regardless of the order they receive updates","A method for resolving network routing conflicts","An algorithm for merging sorted data from multiple sources"],correctIndex:1,explanation:"Convergent conflict resolution ensures that no matter what order replicas receive and process conflicting updates, they all arrive at the same final state. This is critical in AP systems where different replicas may receive updates in different orders. LWW (Last-Write-Wins) is convergent because all replicas will keep the same write (the one with the highest timestamp). CRDTs are convergent by mathematical design. Non-convergent resolution (like 'first-write-wins' in an uncoordinated system) could leave replicas permanently diverged. Convergence is the minimum viable correctness property for AP systems  without it, replicas might never agree, and clients would get different answers depending on which replica they hit, with no path to consistency even after partitions heal."},{question:"What is the 'write-ahead log' (WAL) pattern and which CAP-related property does it primarily support?",options:["Partition tolerance  it helps nodes recover from partitions","Availability  it keeps the system available during failures","It primarily supports durability (related to the 'D' in ACID), not directly a CAP property, but is essential for recovery in both CP and AP systems","Consistency  it ensures all nodes have the same data"],correctIndex:2,explanation:"Write-ahead logging (WAL) writes changes to a sequential log on disk before applying them to the main data structure. This ensures durability  if a node crashes mid-operation, it can replay the log on startup to recover its state. WAL is orthogonal to CAP (which is about distributed consistency/availability trade-offs) and is instead about single-node durability and recovery. However, WAL is essential infrastructure for both CP and AP systems: Raft and Paxos use replicated logs (WALs across nodes), and single-node databases use WAL for crash recovery. PostgreSQL, MySQL, etcd, and Cassandra all use WAL. Without WAL, a crash could leave data in a corrupt, partially-written state. It's one of the most fundamental patterns in database engineering."},{question:"How do service meshes like Istio relate to CAP theorem concerns?",options:["Service meshes eliminate CAP trade-offs by providing perfect networking","Service meshes provide the infrastructure layer (retries, timeouts, circuit breakers) that implements the chosen CAP trade-offs at the application level","Service meshes only handle security, not consistency","Service meshes enforce strong consistency across all services"],correctIndex:1,explanation:"Service meshes don't eliminate CAP trade-offs  they provide the infrastructure to implement and manage them. Circuit breakers implement availability trade-offs (fail fast vs retry). Retry policies affect consistency (retrying a non-idempotent operation could cause duplicates). Timeouts determine when a service considers a downstream service 'partitioned.' Traffic routing can direct reads to nearby replicas (AP-style) or to the primary (CP-style). For example, Istio's retry and timeout configuration directly affects whether your system behaves more CP (strict timeouts, no retries, fail-closed) or AP (generous retries, graceful degradation). The service mesh makes these policies configurable and observable, but the fundamental CAP trade-off decisions still rest with the architect."},{question:"What is 'optimistic replication' and which CAP property does it favor?",options:["Replication that assumes the best case and uses strong consistency","Replication that allows replicas to diverge temporarily and reconcile later, favoring availability (AP)","Replication that optimizes for speed by skipping error checking","Replication that only works when network conditions are optimal"],correctIndex:1,explanation:"Optimistic replication (also called lazy replication) allows replicas to accept updates independently without coordinating with other replicas first, on the 'optimistic' assumption that conflicts are rare. Conflicting updates are detected and resolved after the fact. This favors availability (AP) because nodes can always accept writes, even during partitions. Git is a great analogy  you commit locally (optimistic, no coordination) and merge/resolve conflicts later when you push/pull. Pessimistic replication (like synchronous replication) coordinates before accepting writes, favoring consistency (CP). Most AP databases use optimistic replication: Cassandra, DynamoDB, CouchDB. The trade-off is that conflict resolution can be complex and application-specific, but the availability and latency benefits are significant for many use cases."},{question:"In CAP terms, what is the difference between a 'partition' and a 'node failure'?",options:["They are the same thing  both mean a node is unreachable","A partition means groups of nodes can't communicate with EACH OTHER but are individually healthy; a node failure means a node is actually down","A partition only affects data; a node failure affects processing","A partition is permanent; a node failure is temporary"],correctIndex:1,explanation:"A network partition means the communication link between groups of nodes is broken, but the nodes themselves are healthy and running. Both sides might continue accepting client requests independently. A node failure means a specific node has actually crashed or become unresponsive. The distinction matters because partitions can cause split-brain (both sides independently serving requests) while node failures typically don't (the failed node isn't doing anything). In practice, it's often impossible to distinguish between the two from the perspective of a remote node  if you can't reach a node, is it down or is the network broken? This ambiguity is why timeout-based failure detection is inherently imperfect, and why quorum systems are designed to handle both scenarios safely."},{question:"What is 'active-active' vs 'active-passive' replication, and how do they map to CAP?",options:["Active-active is CP; active-passive is AP","Active-active (multi-leader, AP-leaning) has all replicas serving writes; active-passive (single-leader, CP-leaning) has one primary and standby replicas","They're the same thing with different marketing names","Active-active requires consensus; active-passive doesn't"],correctIndex:1,explanation:"Active-active replication has multiple nodes accepting writes simultaneously (multi-leader), which is AP-leaning because during a partition, any active node can accept writes independently. However, this creates conflict resolution challenges. Active-passive has one primary accepting writes and one or more standby replicas that take over if the primary fails. This is CP-leaning because there's always one source of truth. During a partition, if you can't reach the primary, writes are unavailable (CP behavior). Active-active is used for geo-distributed systems where latency matters (users write to their nearest leader)  Redis Enterprise, CouchDB, and DynamoDB Global Tables support this. Active-passive is common in traditional RDBMS setups  PostgreSQL streaming replication, MySQL replication."},{question:"Why did Amazon build DynamoDB as an AP system rather than a CP system?",options:["Because CP systems hadn't been invented yet in 2007","Because Amazon's SLA research showed that even 100ms of added latency reduced sales by 1%, making availability the top priority over consistency","Because CP systems can't scale to Amazon's size","Because AP systems are always faster than CP systems"],correctIndex:1,explanation:"Amazon's research quantified the business impact of latency and unavailability on e-commerce revenue. Even small increases in page load time (100ms) measurably reduced sales. An unavailable shopping cart or product page is a guaranteed lost sale. In contrast, a slightly stale product recommendation or shopping cart is barely noticeable to users. This business analysis drove the architectural decision: always be available and responsive, even at the cost of occasional inconsistency. The Dynamo paper explicitly states that Amazon's services needed an 'always-on' experience. This is a masterclass in letting business requirements drive technical trade-offs  the CAP choice wasn't made in a vacuum but based on quantified revenue impact. It demonstrates that system design decisions should start with 'what does the business need?' not 'what's the theoretically correct approach?'"},{question:"What is a 'read quorum' and 'write quorum' in the context of Dynamo-style systems?",options:["The number of nodes that must be consulted during read and write operations respectively","The minimum disk space required for reads and writes","The CPU quota allocated for read and write operations","The maximum number of concurrent reads and writes allowed"],correctIndex:0,explanation:"In Dynamo-style systems, R (read quorum) is the number of replicas that must respond to a read request, and W (write quorum) is the number of replicas that must acknowledge a write. With N replicas, common configurations are: R=1, W=N (fast reads, slow writes); R=N, W=1 (slow reads, fast writes); R=W=QUORUM (balanced). The fundamental trade-off is captured by R + W > N for strong consistency. Lower R means faster reads but potentially stale data. Lower W means faster writes but less durable. Cassandra exposes this as consistency levels (ONE, QUORUM, ALL) per query. This tunability is powerful because different access patterns within the same application can use different quorum settings  a product search might use R=1 for speed while a checkout might use R=QUORUM for correctness."}],uf=[{question:"What does the 'A' in ACID stand for?",options:["Availability","Atomicity","Asynchrony","Authorization"],correctIndex:1,explanation:"Atomicity means that a transaction is treated as a single, indivisible unit of work  either all operations succeed or none do. If any part of the transaction fails, the entire transaction is rolled back to its previous state. This ensures that the database never ends up in a partially completed state. For example, in a bank transfer, both the debit and credit must succeed together or neither should happen."},{question:"Which isolation level allows dirty reads?",options:["Serializable","Read Committed","Read Uncommitted","Repeatable Read"],correctIndex:2,explanation:"Read Uncommitted is the lowest isolation level and allows transactions to read data that has been modified by other transactions but not yet committed. This means a transaction can see 'dirty' data that might later be rolled back, leading to inconsistent reads. While this level offers the highest concurrency and performance, it sacrifices data consistency. Higher isolation levels like Read Committed prevent this by only allowing reads of committed data."},{question:"What is a phantom read?",options:["Reading data that was rolled back","A new row appearing in a repeated range query within the same transaction","Reading stale data from cache","A deadlock between two transactions"],correctIndex:1,explanation:"A phantom read occurs when a transaction re-executes a range query and finds new rows that were inserted by another committed transaction since the first execution. Unlike non-repeatable reads (where existing rows change), phantoms involve entirely new rows appearing. For example, if you query all orders above $100, another transaction inserts a qualifying order, and your re-query now returns an extra row. Serializable isolation or gap locking prevents phantom reads."},{question:"Which ACID property ensures that once a transaction is committed, its changes survive system failures?",options:["Atomicity","Consistency","Isolation","Durability"],correctIndex:3,explanation:"Durability guarantees that once a transaction has been committed, its effects are permanently recorded in the database, even in the event of power loss, crashes, or other system failures. This is typically achieved through write-ahead logging (WAL), where changes are written to a persistent log before being applied. Many databases also use techniques like fsync to ensure data reaches non-volatile storage. Without durability, committed transactions could be lost during recovery."},{question:"What is the purpose of Write-Ahead Logging (WAL)?",options:["To speed up read queries","To ensure changes are logged to disk before being applied to the database","To replicate data across nodes","To compress database files"],correctIndex:1,explanation:"Write-Ahead Logging ensures that all modifications are written to a sequential log file on disk before the actual database pages are updated. This provides both atomicity and durability: if the system crashes, the log can be replayed to redo committed transactions or undo uncommitted ones. WAL is highly efficient because sequential writes to the log are much faster than random writes to data pages. PostgreSQL, SQLite, and many other databases rely heavily on WAL for crash recovery."},{question:"In the BASE model, what does 'S' stand for?",options:["Synchronous","Soft state","Strong consistency","Stateless"],correctIndex:1,explanation:"BASE stands for Basically Available, Soft state, Eventually consistent. The 'S' represents Soft state, meaning the system's state may change over time even without new input, due to the eventual consistency model. Data might be temporarily inconsistent across replicas as updates propagate asynchronously. This is a fundamental trade-off in distributed systems where availability is prioritized over immediate consistency. BASE is often contrasted with ACID as the approach favored by NoSQL databases."},{question:"What is the Two-Phase Commit (2PC) protocol used for?",options:["Optimizing query execution plans","Coordinating distributed transactions across multiple nodes","Compressing data for storage","Load balancing read requests"],correctIndex:1,explanation:"Two-Phase Commit is a distributed algorithm that ensures all participating nodes in a distributed transaction either commit or abort together. In the prepare phase, the coordinator asks all participants if they can commit, and each responds with a vote. In the commit phase, if all voted yes, the coordinator tells everyone to commit; otherwise, all abort. While 2PC guarantees atomicity across nodes, it has drawbacks including blocking if the coordinator fails. This is why alternatives like Saga patterns are sometimes preferred."},{question:"Which isolation level prevents non-repeatable reads but allows phantom reads?",options:["Read Uncommitted","Read Committed","Repeatable Read","Serializable"],correctIndex:2,explanation:"Repeatable Read ensures that if a transaction reads a row, subsequent reads of the same row within that transaction will return the same data, even if other transactions modify it. However, it does not prevent phantom reads  new rows inserted by other transactions can still appear in range queries. In MySQL's InnoDB, Repeatable Read actually does prevent phantoms through next-key locking, but the SQL standard definition allows them. Serializable is the only standard level that prevents all anomalies."},{question:"What happens during the 'prepare' phase of 2PC?",options:["The coordinator commits the transaction","Each participant votes whether it can commit the transaction","Data is replicated to backup nodes","The transaction log is compressed"],correctIndex:1,explanation:"During the prepare phase, the coordinator sends a prepare request to all participants, asking them to guarantee they can commit the transaction. Each participant performs all necessary work (acquiring locks, writing to log) and responds with either 'yes' (ready to commit) or 'no' (cannot commit). Once a participant votes yes, it must be able to commit even after a crash, which means the decision is durable in its log. Only after receiving all votes does the coordinator proceed to the commit or abort phase."},{question:"What is a dirty read?",options:["Reading data from a crashed node","Reading uncommitted data from another transaction that might be rolled back","Reading data from a stale cache","Reading corrupted data from disk"],correctIndex:1,explanation:"A dirty read occurs when a transaction reads data that has been written by another transaction that has not yet committed. If the writing transaction later rolls back, the reading transaction will have used data that never officially existed. This can lead to serious data integrity issues, such as making business decisions based on phantom values. Only the Read Uncommitted isolation level permits dirty reads; all higher levels prevent them."},{question:"Which ACID property ensures that a transaction brings the database from one valid state to another?",options:["Atomicity","Consistency","Isolation","Durability"],correctIndex:1,explanation:"Consistency in ACID means that a transaction must transition the database from one valid state to another valid state, maintaining all defined rules, constraints, cascades, and triggers. If a transaction would violate any integrity constraint (like a foreign key or unique constraint), the entire transaction is rolled back. This is different from consistency in the CAP theorem, which refers to all nodes seeing the same data. Database consistency relies on both the database engine enforcing constraints and application-level logic being correct."},{question:"What is the main disadvantage of the Serializable isolation level?",options:["It allows dirty reads","It cannot handle concurrent transactions efficiently, causing significant performance overhead","It doesn't support rollbacks","It requires eventual consistency"],correctIndex:1,explanation:"Serializable is the strictest isolation level, ensuring that concurrent transactions produce results identical to some serial execution order. This prevents all anomalies (dirty reads, non-repeatable reads, and phantom reads) but comes at a significant performance cost. The database must use extensive locking, predicate locks, or serializable snapshot isolation (SSI), which can dramatically reduce throughput and increase latency. Many applications choose lower isolation levels and handle edge cases in application logic for better performance."},{question:"In BASE, what does 'Eventually Consistent' mean?",options:["Data is always immediately consistent","Data will become consistent across all nodes given enough time without new updates","Data is never consistent","Consistency is checked only during reads"],correctIndex:1,explanation:"Eventually consistent means that if no new updates are made to a piece of data, all replicas will eventually converge to the same value. There is no guarantee about how long this convergence takes, but the system will reach consistency given sufficient time. This model is widely used in distributed systems like DynamoDB and Cassandra where high availability is prioritized. Applications using eventually consistent stores must be designed to tolerate temporary inconsistencies between replicas."},{question:"What is a non-repeatable read?",options:["A read that returns different results when re-executed within the same transaction because another transaction modified and committed the data","A read that always returns null","A read that takes too long to execute","A read from an uncommitted transaction"],correctIndex:0,explanation:"A non-repeatable read occurs when a transaction reads the same row twice and gets different values because another transaction modified and committed that row between the two reads. Unlike dirty reads, the data read was actually committed  it just changed between reads. For example, reading an account balance of $100, then re-reading it as $50 after another transaction's withdrawal committed. Repeatable Read and Serializable isolation levels prevent this anomaly."},{question:"How does optimistic concurrency control differ from pessimistic locking?",options:["Optimistic control assumes conflicts are rare and checks at commit time, while pessimistic locking acquires locks upfront","Optimistic control is slower than pessimistic locking","Pessimistic locking never causes deadlocks","They are the same thing with different names"],correctIndex:0,explanation:"Optimistic concurrency control (OCC) allows transactions to proceed without acquiring locks, assuming conflicts are rare. At commit time, the system validates whether any conflicts occurred and aborts the transaction if they did. Pessimistic locking, on the other hand, acquires locks on data before accessing it, preventing other transactions from modifying it. OCC works well with low-contention workloads and read-heavy scenarios, while pessimistic locking is better when conflicts are frequent and retries would be costly."},{question:"What is the role of a coordinator in 2PC?",options:["To execute all queries directly","To manage the commit/abort decision by collecting votes from all participants","To replicate data to backup servers","To optimize query execution plans"],correctIndex:1,explanation:"The coordinator in Two-Phase Commit orchestrates the entire distributed transaction process. It initiates the prepare phase by asking all participants to vote, collects their responses, and then makes the final commit or abort decision based on unanimous agreement. The coordinator must durably log its decision before sending the final command, because if it crashes, participants need to know the outcome during recovery. This central role makes the coordinator a single point of failure, which is one of 2PC's main weaknesses."},{question:"What does the CAP theorem state?",options:["A distributed system can achieve consistency, availability, and partition tolerance simultaneously","A distributed system can only guarantee two out of three: consistency, availability, and partition tolerance","A distributed system must always choose availability over consistency","A distributed system must use ACID transactions"],correctIndex:1,explanation:"The CAP theorem, formulated by Eric Brewer, states that in the presence of a network partition, a distributed system must choose between consistency (all nodes see the same data) and availability (every request receives a response). Since network partitions are inevitable in distributed systems, the real choice is between CP (consistent but may be unavailable during partitions) and AP (available but may return stale data). In practice, most systems make nuanced trade-offs rather than being strictly CP or AP."},{question:"What is a write-ahead log (WAL) checkpoint?",options:["A point where the WAL file is deleted","A point where all dirty pages are flushed to disk and the recovery start position is advanced","A backup of the entire database","A lock on the WAL file"],correctIndex:1,explanation:"A WAL checkpoint is an operation where the database flushes all dirty (modified) pages from memory to the actual data files on disk and records a new recovery start position. After a checkpoint, the database only needs to replay WAL entries after that point during crash recovery, significantly reducing recovery time. Without checkpoints, the WAL would grow indefinitely and recovery would take longer as the entire log would need to be replayed. Checkpoints balance between write performance and recovery speed."},{question:"Which of the following is NOT an ACID property?",options:["Atomicity","Consistency","Availability","Durability"],correctIndex:2,explanation:"Availability is not part of ACID  it belongs to the CAP theorem and BASE model. The four ACID properties are Atomicity (all-or-nothing transactions), Consistency (valid state transitions), Isolation (concurrent transactions don't interfere), and Durability (committed data survives failures). Availability refers to a system's ability to respond to every request, which is a distributed systems concern rather than a transaction property."},{question:"What problem does the Saga pattern solve that 2PC cannot handle well?",options:["Query optimization across databases","Long-running distributed transactions where holding locks is impractical","Data compression","Index creation on large tables"],correctIndex:1,explanation:"The Saga pattern addresses long-running distributed transactions by breaking them into a sequence of local transactions, each with a compensating action for rollback. Unlike 2PC, which holds locks across all participants until the final commit, Sagas release resources after each local transaction completes. If a step fails, compensating transactions undo the previously completed steps. This makes Sagas more suitable for microservices architectures where transactions might span minutes or hours and locking resources that long would be impractical."},{question:"In MVCC (Multi-Version Concurrency Control), how are reads handled?",options:["Reads always block writes","Reads access a snapshot of the data at a point in time, without blocking writers","Reads require exclusive locks","Reads are always served from cache"],correctIndex:1,explanation:"MVCC maintains multiple versions of each data item, allowing read operations to access a consistent snapshot without acquiring locks or blocking concurrent writers. Each transaction sees a snapshot of the database as it existed at the transaction's start time (or statement's start time, depending on isolation level). Writers create new versions rather than overwriting existing data, so readers and writers don't conflict. PostgreSQL, Oracle, and MySQL InnoDB all use MVCC to achieve high concurrency while maintaining consistency."},{question:"What is the 'blocking problem' in 2PC?",options:["Transactions block all reads","If the coordinator fails after participants voted yes, participants must wait indefinitely for the coordinator to recover","Network latency blocks transaction processing","Database indexes block inserts"],correctIndex:1,explanation:"The blocking problem is 2PC's most significant weakness. After a participant votes 'yes' in the prepare phase, it cannot unilaterally decide to commit or abort  it must wait for the coordinator's decision. If the coordinator crashes before sending the final decision, all participants that voted yes are stuck holding locks and cannot proceed. This can block the entire system until the coordinator recovers. Three-Phase Commit (3PC) was designed to address this, though it adds complexity and is rarely used in practice."},{question:"What is the difference between pessimistic and optimistic locking in database transactions?",options:["Pessimistic locking is always faster","Optimistic locking checks for conflicts at read time","Pessimistic locking prevents access upfront; optimistic locking allows access and validates at commit time","There is no difference"],correctIndex:2,explanation:"Pessimistic locking acquires locks before accessing data, preventing other transactions from reading or modifying it until the lock is released. Optimistic locking allows multiple transactions to proceed concurrently without locks and only checks for conflicts when a transaction tries to commit, typically using version numbers or timestamps. If a conflict is detected, the transaction is aborted and must retry. Optimistic locking shines in low-contention environments, while pessimistic locking is preferred when conflicts are frequent."},{question:"What does 'Basically Available' mean in the BASE model?",options:["The system is always 100% available","The system guarantees availability most of the time, possibly with degraded functionality during failures","The system requires manual restart after failures","Availability is not a concern"],correctIndex:1,explanation:"Basically Available means the system appears to work most of the time and will respond to requests even during partial failures, though the response might be stale or approximate. Unlike strict availability guarantees, the system may degrade gracefully  for example, serving cached data when the primary is unreachable or showing partial results. This approach prioritizes keeping the system operational over ensuring perfect consistency. E-commerce sites exemplify this by showing possibly stale inventory counts rather than going offline."},{question:"Which mechanism does PostgreSQL primarily use for concurrency control?",options:["Two-Phase Locking only","MVCC (Multi-Version Concurrency Control)","Optimistic locking only","No concurrency control"],correctIndex:1,explanation:"PostgreSQL uses MVCC as its primary concurrency control mechanism. Each row can have multiple versions, identified by transaction IDs (xmin and xmax). When a transaction updates a row, it creates a new version rather than overwriting the old one. Read transactions see a snapshot based on their transaction ID, ensuring they get a consistent view without blocking writers. Dead versions are later cleaned up by the VACUUM process. This allows PostgreSQL to achieve high concurrency with minimal lock contention."},{question:"What is a lost update anomaly?",options:["A database losing data during backup","Two transactions reading the same data, then both writing updates, causing one update to overwrite the other","An update that takes too long to execute","A failed replication of updates to a secondary node"],correctIndex:1,explanation:"A lost update occurs when two transactions read the same data, perform modifications based on what they read, and then both write back their results  the second write overwrites the first, effectively losing that update. For example, two transactions both read a counter value of 10, each increment it to 11, and write 11 back  the counter should be 12 but is 11. This can be prevented with SELECT FOR UPDATE, atomic operations, or higher isolation levels like Repeatable Read with proper conflict detection."},{question:"What is the purpose of undo logs in a database?",options:["To speed up read queries","To roll back uncommitted transactions and provide consistent read views for MVCC","To compress data files","To encrypt transaction data"],correctIndex:1,explanation:"Undo logs store the previous version of data before it was modified by a transaction, serving two critical purposes. First, they enable rollback: if a transaction is aborted, the undo log entries restore the original values. Second, they support MVCC by providing older versions of data to transactions that started before the modification. In InnoDB, undo logs are stored in the undo tablespace and are purged once no active transaction needs them. Without undo logs, neither rollback nor consistent snapshots would be possible."},{question:"What distinguishes a CP system from an AP system in the CAP theorem?",options:["CP systems use caching; AP systems don't","CP systems sacrifice availability during partitions to maintain consistency; AP systems sacrifice consistency to remain available","CP systems are faster than AP systems","AP systems cannot store data"],correctIndex:1,explanation:"In a CP (Consistent-Partition tolerant) system, when a network partition occurs, the system may refuse to respond to requests (sacrificing availability) to ensure all accessible nodes return the same, correct data. Examples include HBase and MongoDB (in default config). An AP (Available-Partition tolerant) system continues to serve requests during partitions but may return stale or conflicting data. Examples include Cassandra and DynamoDB. The choice depends on whether your application can tolerate stale data or downtime."},{question:"What is Serializable Snapshot Isolation (SSI)?",options:["A technique that locks all tables during transactions","An optimistic approach that detects serialization conflicts at commit time using snapshot reads","A method of compressing transaction logs","A way to partition data across shards"],correctIndex:1,explanation:"Serializable Snapshot Isolation (SSI) is an advanced concurrency control mechanism that provides serializable isolation without the heavy locking overhead of traditional two-phase locking. It works by allowing transactions to execute using snapshot reads (like in MVCC) and then detecting potential serialization anomalies at commit time. If a conflict is detected (read-write dependency cycles), the transaction is aborted and must retry. PostgreSQL uses SSI for its Serializable isolation level since version 9.1, offering much better performance than lock-based approaches."},{question:"What is the ACID property that prevents concurrent transactions from interfering with each other?",options:["Atomicity","Consistency","Isolation","Durability"],correctIndex:2,explanation:"Isolation ensures that concurrent transactions execute as if they were running sequentially, without seeing each other's intermediate states. Without isolation, transactions could read partially completed data from other transactions, leading to anomalies. Different isolation levels (Read Uncommitted, Read Committed, Repeatable Read, Serializable) offer varying degrees of protection, trading off between data correctness and performance. The database uses mechanisms like locks, MVCC, or SSI to enforce the chosen isolation level."},{question:"What happens if a participant in 2PC votes 'no' during the prepare phase?",options:["The coordinator ignores the vote","Only the voting participant rolls back","The coordinator sends an abort message to all participants","The transaction is retried automatically"],correctIndex:2,explanation:"In 2PC, the commit decision requires unanimous agreement. If any single participant votes 'no' during the prepare phase, the coordinator must send a global abort message to all participants, causing everyone to roll back their portion of the transaction. The participant that voted no has already rolled back its changes. Participants that voted yes must undo their prepared changes upon receiving the abort message. This ensures atomicity across the distributed system  the transaction is all-or-nothing."},{question:"What is the difference between redo and undo logs?",options:["They are the same thing","Redo logs replay committed changes after a crash; undo logs reverse uncommitted changes during rollback","Redo logs are for reads; undo logs are for writes","Redo logs work on disk; undo logs work in memory"],correctIndex:1,explanation:"Redo logs and undo logs serve complementary purposes in crash recovery. Redo logs (also called WAL) contain the new values of committed transactions and are replayed after a crash to ensure committed data is restored  this provides durability. Undo logs contain the old values before modification and are used to roll back uncommitted transactions during recovery  this provides atomicity. Together, they implement the ARIES recovery algorithm used in most relational databases. The redo phase replays all changes, then the undo phase reverses uncommitted ones."},{question:"Which consistency model provides the strongest guarantees?",options:["Eventual consistency","Causal consistency","Linearizability","Read-your-writes consistency"],correctIndex:2,explanation:"Linearizability (also called strong consistency or atomic consistency) is the strongest consistency model. It guarantees that once a write completes, all subsequent reads (from any node) will return that value or a more recent one, as if there were a single copy of the data. Operations appear to take effect instantaneously at some point between their invocation and completion. This is the most intuitive model for programmers but the most expensive to implement in distributed systems, as it typically requires consensus protocols like Raft or Paxos."},{question:"What is a write skew anomaly?",options:["Data being written to the wrong table","Two transactions read overlapping data, make decisions based on it, and write to different rows, violating a constraint","Writes being slower than reads","A write that causes a deadlock"],correctIndex:1,explanation:"Write skew is a subtle concurrency anomaly where two transactions both read a set of rows satisfying some condition, then each updates different rows in a way that violates the condition. For example, if a hospital requires at least one doctor on call, and two doctors each check that the other is on call before removing themselves, both could end up off call. Unlike lost updates (which affect the same row), write skew involves different rows. Only Serializable isolation prevents write skew; Repeatable Read does not."},{question:"How does Three-Phase Commit (3PC) improve upon 2PC?",options:["It's faster than 2PC","It adds a pre-commit phase to avoid the blocking problem when the coordinator fails","It eliminates the need for a coordinator","It uses fewer network round trips"],correctIndex:1,explanation:"Three-Phase Commit adds a 'pre-commit' phase between the vote and final commit phases of 2PC. After all participants vote yes, the coordinator sends a pre-commit message before the final commit. This ensures that if the coordinator fails, any participant can determine the transaction's fate by communicating with other participants. If no participant received a pre-commit, they can safely abort. However, 3PC is still not perfectly safe under network partitions and adds latency, so it's rarely used in practice compared to alternatives like Paxos-based commits."},{question:"What is the purpose of savepoints in a transaction?",options:["To save the transaction to disk","To create intermediate rollback points within a transaction without aborting the entire transaction","To save query execution plans","To create database backups"],correctIndex:1,explanation:"Savepoints allow you to set markers within a transaction that you can roll back to without aborting the entire transaction. If a portion of the transaction fails, you can ROLLBACK TO SAVEPOINT to undo only the work after that savepoint while preserving earlier operations. This is especially useful in complex transactions with multiple steps where partial failure should not require starting over. Most databases support savepoints with SAVEPOINT, ROLLBACK TO SAVEPOINT, and RELEASE SAVEPOINT commands."},{question:"What is the difference between strong consistency and eventual consistency?",options:["Strong consistency is only for SQL databases","Strong consistency guarantees reads see the latest write immediately; eventual consistency guarantees convergence over time","Eventual consistency is faster in all cases","They are interchangeable terms"],correctIndex:1,explanation:"Strong consistency (linearizability) ensures that after a write completes, any subsequent read from any node will return that value, giving the appearance of a single copy of data. Eventual consistency only guarantees that if no new writes occur, all replicas will eventually converge to the same value, but reads in the interim may return stale data. Strong consistency requires coordination (consensus protocols, synchronous replication) which adds latency. Eventual consistency allows higher availability and lower latency but shifts complexity to the application layer."},{question:"What is a deadlock in the context of database transactions?",options:["A transaction that runs forever","Two or more transactions waiting for each other to release locks, creating a cycle of dependencies","A transaction that cannot find the required data","A database running out of memory"],correctIndex:1,explanation:"A deadlock occurs when two or more transactions form a circular chain of lock dependencies  Transaction A holds a lock that B needs, while B holds a lock that A needs. Neither can proceed, and without intervention, they'd wait forever. Databases detect deadlocks using wait-for graphs or timeout mechanisms and resolve them by aborting one transaction (the 'victim') to break the cycle. Prevention strategies include always acquiring locks in a consistent order, using lock timeouts, or using optimistic concurrency control."},{question:"Which of the following best describes the 'Soft State' in BASE?",options:["The database always maintains a consistent state","The system state may change over time due to eventual consistency, even without new inputs","The database is in a soft-delete mode","The system uses soft locks instead of hard locks"],correctIndex:1,explanation:"Soft state means the system's state can change without any explicit input, simply because data is propagating between replicas to achieve eventual consistency. At any given moment, different nodes might have different values for the same data item. This is in contrast to 'hard state' in ACID systems where the state changes only in response to explicit transactions. The softness reflects the transient nature of the data's accuracy across the distributed system until convergence is reached."},{question:"What is a transaction log (also known as a journal)?",options:["A file that records user login events","A sequential record of all database modifications used for crash recovery and replication","A summary of query execution times","A list of all database tables"],correctIndex:1,explanation:"A transaction log is a sequential file that records every modification made to the database, including the before and after images of changed data. It serves multiple critical purposes: crash recovery (replaying committed transactions and undoing uncommitted ones), point-in-time recovery (restoring the database to any moment), and replication (streaming changes to replica nodes). The log is append-only for performance, and databases like PostgreSQL, MySQL, and SQL Server all maintain transaction logs as a fundamental component of their architecture."},{question:"How does Read Committed isolation level work?",options:["It allows dirty reads","Each query within a transaction sees only data committed before that query began","It prevents all concurrency anomalies","It locks all rows accessed by the transaction"],correctIndex:1,explanation:"Read Committed ensures that any data read by a transaction has been committed by the time of the read. Each statement within the transaction gets a fresh snapshot, meaning different statements may see different committed data. This prevents dirty reads but allows non-repeatable reads (reading the same row twice may return different values if another transaction committed a change in between) and phantom reads. It's the default isolation level in PostgreSQL and Oracle, offering a good balance between consistency and concurrency."},{question:"What is the role of a lock manager in a database?",options:["To encrypt database files","To manage lock requests, detect deadlocks, and coordinate access to shared resources","To manage user authentication","To optimize query execution"],correctIndex:1,explanation:"The lock manager is a critical database component that tracks all active locks, processes lock requests from transactions, and handles lock conflicts. When a transaction requests a lock that conflicts with an existing one, the lock manager places it in a wait queue. It also runs deadlock detection algorithms (typically using wait-for graphs) to identify and resolve circular dependencies. The lock manager supports various lock types (shared, exclusive) and granularities (row, page, table) to balance concurrency and overhead."},{question:"What is the difference between shared locks and exclusive locks?",options:["Shared locks are faster than exclusive locks","Shared locks allow multiple readers; exclusive locks allow only one writer and block all other access","Shared locks work on tables; exclusive locks work on rows","There is no practical difference"],correctIndex:1,explanation:"Shared locks (S locks) are acquired for read operations and are compatible with other shared locks, allowing multiple transactions to read the same data concurrently. Exclusive locks (X locks) are acquired for write operations and are incompatible with both shared and other exclusive locks, ensuring only one transaction can modify the data. A shared lock blocks exclusive locks (readers block writers), and an exclusive lock blocks both shared and exclusive locks (writers block everyone). This protocol is fundamental to Two-Phase Locking (2PL)."},{question:"What is the ARIES recovery algorithm?",options:["An algorithm for optimizing joins","A database crash recovery algorithm using WAL that performs analysis, redo, and undo phases","An algorithm for distributing data across shards","A method for compressing database backups"],correctIndex:1,explanation:"ARIES (Algorithms for Recovery and Isolation Exploiting Semantics) is the industry-standard crash recovery algorithm. It operates in three phases: Analysis scans the log from the last checkpoint to determine which transactions were active at crash time. Redo replays all logged changes (even from uncommitted transactions) to restore the database to its pre-crash state. Undo then rolls back all uncommitted transactions. ARIES uses LSNs (Log Sequence Numbers) to avoid redundant work and supports fine-grained locking for high concurrency."},{question:"What is a distributed transaction?",options:["A transaction that runs very slowly","A transaction that spans multiple databases, services, or network nodes and must maintain atomicity across all of them","A transaction distributed across multiple threads","A transaction that is logged to multiple files"],correctIndex:1,explanation:"A distributed transaction is one that involves operations across multiple independent systems  different databases, microservices, or network nodes  that must all succeed or all fail as a single atomic unit. This is significantly more complex than local transactions because network failures, node crashes, and partial failures must be handled. Protocols like 2PC, 3PC, and Saga patterns are used to coordinate distributed transactions. The challenge is maintaining ACID properties across unreliable networks, which is fundamentally difficult per the CAP theorem."},{question:"What is snapshot isolation?",options:["Taking a backup of the database","Each transaction works with a consistent snapshot of the database taken at the transaction's start time","Isolating the database from network access","Creating a read-only copy of the database"],correctIndex:1,explanation:"Snapshot isolation provides each transaction with a consistent point-in-time view of the database as it existed when the transaction started. All reads within the transaction see this snapshot, regardless of concurrent modifications. Writes from other transactions committed after the snapshot was taken are invisible. This is implemented using MVCC, where multiple versions of rows coexist. Snapshot isolation prevents dirty reads, non-repeatable reads, and phantoms but can still allow write skew anomalies, making it weaker than true serializable isolation."},{question:"What is the purpose of intent locks?",options:["To indicate a transaction's intent to acquire a finer-grained lock at a lower level","To lock data temporarily","To prevent deadlocks entirely","To optimize read queries"],correctIndex:0,explanation:"Intent locks are used in hierarchical locking schemes (table  page  row) to signal that a transaction holds or intends to hold locks at a finer granularity. For example, an Intent Exclusive (IX) lock on a table means a transaction holds exclusive locks on some rows within that table. This allows other transactions to quickly determine whether they can acquire a table-level lock without scanning every row's lock status. Intent locks dramatically improve performance in multi-granularity locking by avoiding expensive lock compatibility checks at every level."},{question:"What is the difference between two-phase locking (2PL) and two-phase commit (2PC)?",options:["They are the same protocol","2PL controls concurrency within a single database; 2PC coordinates commits across distributed systems","2PL is for distributed systems; 2PC is for single databases","2PL is newer than 2PC"],correctIndex:1,explanation:"Two-Phase Locking (2PL) is a concurrency control protocol where a transaction has a growing phase (acquiring locks) and a shrinking phase (releasing locks), ensuring serializability within a single database. Two-Phase Commit (2PC) is a distributed consensus protocol that coordinates the commit decision across multiple nodes to ensure atomicity of distributed transactions. Despite the similar names, they solve completely different problems: 2PL manages concurrent access to shared data, while 2PC manages agreement across distributed participants."},{question:"What is the purpose of a redo log buffer?",options:["To cache frequently read data","To temporarily store redo log entries in memory before flushing them to disk for better write performance","To store the undo log","To hold query execution plans"],correctIndex:1,explanation:"The redo log buffer is a memory area that holds redo log entries before they're written to the redo log files on disk. Instead of writing each log entry individually to disk (which would be very slow), the buffer accumulates entries and flushes them in batches. The buffer is flushed when a transaction commits, when it becomes full, or periodically. This batching of writes significantly improves performance while still maintaining durability, since the buffer is flushed at commit time. InnoDB's innodb_log_buffer_size controls its size."},{question:"What is a compensation transaction in the Saga pattern?",options:["A transaction that adds extra data for completeness","A transaction that semantically undoes the effect of a previously completed step in the saga","A transaction that retries a failed operation","A transaction that compensates for slow network speeds"],correctIndex:1,explanation:"In the Saga pattern, each step has an associated compensating transaction that semantically reverses its effect if a later step fails. Unlike a database rollback which physically undoes changes, compensating transactions perform new operations to logically negate previous ones. For example, if a payment was processed but shipping fails, the compensation would issue a refund rather than rolling back the payment record. Compensating transactions must be idempotent to handle retries safely. Designing correct compensation logic is one of the main challenges of implementing Sagas."},{question:"What is the difference between logical and physical logging in WAL?",options:["Logical logging records the operation performed; physical logging records the exact bytes changed on disk pages","Logical logging is faster","Physical logging supports distributed systems; logical logging doesn't","They produce identical log entries"],correctIndex:0,explanation:"Physical logging records the exact before and after images of the data pages modified by a transaction, making redo and undo straightforward but generating large log volumes. Logical logging records the high-level operation (e.g., 'INSERT INTO table VALUES(...)'), which is more compact but harder to replay correctly, especially with concurrent operations. Most modern databases use physiological logging  a hybrid that records physical page identifiers with logical changes within those pages. This combines the benefits of both approaches."},{question:"What is the purpose of a transaction isolation level?",options:["To determine how fast transactions execute","To define the degree to which a transaction is protected from the effects of concurrent transactions","To set the maximum number of concurrent transactions","To control transaction log size"],correctIndex:1,explanation:"Transaction isolation levels define what phenomena (dirty reads, non-repeatable reads, phantom reads) a transaction might experience from concurrent transactions. Higher isolation levels provide stronger guarantees about data consistency but typically reduce concurrency and increase the chance of transaction conflicts or deadlocks. The SQL standard defines four levels: Read Uncommitted, Read Committed, Repeatable Read, and Serializable. Choosing the right level is a critical design decision that balances data correctness requirements against performance needs."},{question:"What is the role of LSN (Log Sequence Number) in WAL?",options:["To identify database users","To uniquely identify each log record and establish ordering for crash recovery","To count the number of transactions","To measure database size"],correctIndex:1,explanation:"A Log Sequence Number is a monotonically increasing identifier assigned to each entry in the write-ahead log. LSNs establish a total order of all changes and are crucial for crash recovery: the database compares the LSN of each data page with log records to determine which changes need to be redone. Each data page stores the LSN of the last log record that modified it. During recovery, if a page's LSN is less than a redo log record's LSN, that change must be replayed. LSNs also help determine the checkpoint position for efficient recovery."},{question:"How does quorum-based replication relate to consistency?",options:["It has no relation to consistency","By requiring writes to W replicas and reads from R replicas where W + R > N, it ensures reads see the latest write","It only works with ACID databases","It guarantees serializable isolation"],correctIndex:1,explanation:"Quorum-based replication achieves tunable consistency by requiring that write and read operations succeed on a minimum number of replicas. With N total replicas, if writes succeed on W nodes and reads query R nodes, then when W + R > N, at least one node in every read quorum must have the latest write, ensuring strong consistency. Common configurations include W=N, R=1 (fast reads) or W=N/2+1, R=N/2+1 (balanced). Reducing W or R below this threshold trades consistency for availability and lower latency."},{question:"What is a database constraint, and how does it relate to ACID consistency?",options:["Constraints limit the number of connections","Constraints are rules (unique, foreign key, check) that the database enforces to maintain consistency during transactions","Constraints control transaction isolation levels","Constraints manage disk space"],correctIndex:1,explanation:"Database constraints are declarative rules that define what constitutes a valid database state  including primary keys, foreign keys, unique constraints, check constraints, and not-null constraints. They are the enforcement mechanism for the 'C' (Consistency) in ACID: when a transaction would violate a constraint, the database rejects it, ensuring only valid state transitions occur. For example, a foreign key constraint prevents inserting an order referencing a non-existent customer. Without constraints, maintaining consistency would fall entirely on application code."},{question:"What is the difference between strict and non-strict two-phase locking?",options:["Strict 2PL releases all locks only at transaction end; non-strict 2PL can release some locks before commit","Strict 2PL is slower than non-strict","Non-strict 2PL allows dirty reads","They acquire locks differently"],correctIndex:0,explanation:"In strict two-phase locking (Strict 2PL), all locks are held until the transaction commits or aborts, ensuring that no other transaction can read the modified data until it's committed. In non-strict 2PL (basic 2PL), a transaction can release locks during the shrinking phase before committing, which can lead to cascading aborts  if the releasing transaction aborts, transactions that read its released data must also abort. Strict 2PL is more commonly used because it avoids cascading aborts and simplifies recovery, though it holds locks longer."},{question:"What is the XA protocol?",options:["A compression protocol","A standard interface for coordinating distributed transactions between a transaction manager and resource managers","A network protocol for database replication","An encryption standard for databases"],correctIndex:1,explanation:"XA is a standard defined by the Open Group for distributed transaction processing. It defines the interface between a Transaction Manager (TM) and Resource Managers (RMs, such as databases). The TM uses the XA interface to coordinate 2PC across multiple RMs, calling xa_prepare, xa_commit, and xa_rollback. Most major databases (PostgreSQL, MySQL, Oracle) support XA transactions. However, XA transactions have significant overhead and can cause blocking, so many modern architectures prefer Saga-based approaches for cross-service transactions."},{question:"What is the difference between READ_COMMITTED and SNAPSHOT isolation in SQL Server?",options:["They are identical","READ_COMMITTED uses locks for reads; SNAPSHOT uses row versioning so reads never block writes","SNAPSHOT is less consistent","READ_COMMITTED is faster in all cases"],correctIndex:1,explanation:"In SQL Server, READ_COMMITTED (default) acquires shared locks on rows being read, which can block if a writer holds an exclusive lock. SNAPSHOT isolation uses row versioning (MVCC) to provide each transaction with a consistent view as of its start time, so readers never block writers and vice versa. SNAPSHOT provides a stronger guarantee (no non-repeatable reads) but uses more tempdb space for version storage. SQL Server also offers READ_COMMITTED_SNAPSHOT, which combines Read Committed semantics with row versioning for statement-level consistency."},{question:"What is the purpose of group commit in transaction processing?",options:["To group multiple transactions into a single transaction","To batch multiple transaction log flushes into a single disk write for improved throughput","To group related tables together","To commit transactions across multiple databases simultaneously"],correctIndex:1,explanation:"Group commit is a performance optimization where the database batches multiple transactions' log records into a single disk flush operation. Since fsync (ensuring data reaches disk) is expensive, writing one batch of log entries is much faster than multiple individual flushes. Transactions that commit around the same time share the cost of a single disk sync. This can improve throughput dramatically under high concurrency. The trade-off is a small increase in commit latency for individual transactions as they wait for the batch to fill. PostgreSQL, MySQL, and most modern databases implement group commit."},{question:"What is a phantom read and which isolation levels prevent it?",options:["Reading deleted data; prevented by all levels","New rows appearing in repeated range queries; only prevented by Serializable","Reading NULL values; prevented by Read Committed","Reading encrypted data; prevented by Repeatable Read"],correctIndex:1,explanation:"A phantom read occurs when a transaction re-executes a range query and discovers rows that were inserted by another committed transaction. For example, querying 'SELECT * FROM orders WHERE amount > 100' might return 10 rows initially and 11 rows on re-execution. According to the SQL standard, only the Serializable isolation level prevents phantoms, though some implementations (like MySQL InnoDB's Repeatable Read with gap locking) also prevent them. Serializable uses techniques like predicate locking or SSI to block or detect phantom-producing inserts."},{question:"What is the difference between implicit and explicit transactions?",options:["Implicit transactions are faster","Implicit transactions are automatically started by the database for each statement; explicit transactions are manually started with BEGIN","Explicit transactions don't support rollback","There is no difference in modern databases"],correctIndex:1,explanation:"In autocommit mode (default in many databases), each SQL statement is implicitly wrapped in its own transaction  it's automatically committed upon success or rolled back on failure. Explicit transactions are manually started with BEGIN (or START TRANSACTION) and ended with COMMIT or ROLLBACK, allowing multiple statements to be grouped as a single atomic unit. Explicit transactions are essential when multiple related operations must succeed or fail together, such as transferring money between accounts. Understanding this distinction is crucial for correct application behavior."},{question:"What is the consistency guarantee of reading from a replica in a primary-replica setup?",options:["Always strongly consistent","Eventually consistent, as the replica may lag behind the primary","The replica always has newer data","Replicas don't support read operations"],correctIndex:1,explanation:"In a primary-replica (master-slave) setup with asynchronous replication, replicas may lag behind the primary because changes are applied with a delay. Reading from a replica provides eventual consistency  the data will eventually match the primary but may be stale at any given moment. Replication lag can range from milliseconds to seconds (or more under load). For use cases requiring strong consistency, reads must go to the primary, or synchronous replication must be used (at the cost of write latency). Some systems offer 'read-your-writes' consistency as a middle ground."},{question:"What is the purpose of a transaction timeout?",options:["To optimize query performance","To automatically abort transactions that exceed a time limit, preventing resource hogging and potential deadlocks","To schedule transactions for later execution","To measure transaction latency"],correctIndex:1,explanation:"Transaction timeouts set a maximum duration for a transaction to complete. If the timeout is exceeded, the database automatically aborts the transaction and releases all its locks and resources. This prevents long-running transactions from holding locks indefinitely, which could block other transactions and degrade system performance. Timeouts also serve as a simple deadlock resolution mechanism  if a transaction waits too long for a lock, the timeout breaks the potential deadlock. Setting appropriate timeouts is important for system reliability and resource management."},{question:"What is the PACELC theorem and how does it extend CAP?",options:["It replaces CAP entirely","It adds that even without partitions (E), a system must choose between latency (L) and consistency (C)","It's a different name for the BASE model","It only applies to SQL databases"],correctIndex:1,explanation:"PACELC extends the CAP theorem by addressing system behavior both during and without network partitions. It states: if there is a Partition (P), choose between Availability (A) and Consistency (C); Else (E), choose between Latency (L) and Consistency (C). This captures an important real-world trade-off that CAP ignores: even when the network is healthy, enforcing strong consistency requires coordination between nodes, which adds latency. For example, DynamoDB is PA/EL (favors availability and latency), while systems using synchronous replication are PC/EC (favors consistency always)."},{question:"What is write amplification in the context of WAL?",options:["Writing data multiple times across the system (to the WAL, buffer pool, and data files)","Writing very large transactions","Amplifying write speed","Writing to multiple databases simultaneously"],correctIndex:0,explanation:"Write amplification refers to the phenomenon where a single logical write results in multiple physical writes to storage. In WAL-based systems, a data modification is first written to the WAL, then eventually written to the actual data pages, and potentially written again during compaction or checkpointing. LSM-tree based storage engines (used in RocksDB, Cassandra) can have even higher write amplification due to compaction. Understanding write amplification is crucial for capacity planning and choosing appropriate storage engines for write-heavy workloads."},{question:"What is the difference between immediate and deferred constraint checking?",options:["They produce different results","Immediate checks constraints after each statement; deferred checks constraints only at commit time","Deferred is always faster","Immediate doesn't support foreign keys"],correctIndex:1,explanation:"With immediate constraint checking (the default), the database validates constraints after each individual SQL statement. If a statement violates a constraint, it's immediately rejected. Deferred constraint checking postpones validation until the transaction commits, allowing temporarily inconsistent states within the transaction. This is useful for operations like inserting mutually referencing rows (circular foreign keys) where neither row can exist without the other. PostgreSQL supports SET CONSTRAINTS DEFERRED for constraints defined as DEFERRABLE. Oracle supports similar functionality."},{question:"What is a read-write conflict in MVCC?",options:["Two transactions reading the same data","A transaction reading data that another concurrent transaction has written, potentially causing a serialization anomaly","A conflict between read and write file permissions","Reading and writing to different tables"],correctIndex:1,explanation:"In MVCC, a read-write conflict (also called a rw-dependency) occurs when one transaction reads a version of a row and another concurrent transaction writes a new version of that same row. While MVCC allows both to proceed without blocking, this creates a dependency that could lead to serialization anomalies. Serializable Snapshot Isolation (SSI) tracks these dependencies and aborts transactions when it detects dangerous patterns (like rw-dependency cycles). Detecting and handling these conflicts is the key mechanism that makes SSI work without locks."},{question:"How does MySQL InnoDB handle gap locking?",options:["InnoDB doesn't support gap locking","Gap locks prevent insertions into index gaps, helping prevent phantom reads at Repeatable Read isolation","Gap locks only apply to primary keys","Gap locking is the same as table locking"],correctIndex:1,explanation:"InnoDB uses gap locks to lock the gaps between existing index records, preventing other transactions from inserting new rows into those gaps. This is how InnoDB prevents phantom reads at the Repeatable Read isolation level  even though the SQL standard only requires Serializable to prevent phantoms. A next-key lock combines a record lock with a gap lock on the gap before the record. While gap locks effectively prevent phantoms, they can also increase lock contention and cause deadlocks, especially with range queries on non-unique indexes."},{question:"What is the role of a prepare record in 2PC WAL?",options:["To mark the start of a transaction","To durably record a participant's 'yes' vote so it can honor its commitment even after a crash","To prepare data for compression","To prepare the log for archiving"],correctIndex:1,explanation:"When a 2PC participant votes 'yes' during the prepare phase, it writes a prepare record to its local WAL before sending the vote. This is critical because the participant is making a promise: 'I can commit this transaction.' If it crashes and restarts, the prepare record tells the recovery process that this transaction is in a prepared state and must wait for the coordinator's final decision (commit or abort). Without the prepare record, the participant wouldn't know about the pending transaction after a crash and might incorrectly undo changes that should be committed."},{question:"What is the difference between synchronous and asynchronous replication in terms of consistency?",options:["They provide the same consistency","Synchronous replication waits for replicas to confirm writes, providing strong consistency; asynchronous allows replicas to lag","Asynchronous replication is more consistent","Synchronous replication doesn't support transactions"],correctIndex:1,explanation:"Synchronous replication waits for one or more replicas to acknowledge receiving and persisting a write before confirming the commit to the client. This ensures replicas are always up-to-date, providing strong consistency, but adds latency equal to the round-trip time to the replica. Asynchronous replication confirms the commit immediately after the primary persists the write, then sends changes to replicas in the background. This is faster but means replicas may lag, providing only eventual consistency. Semi-synchronous replication (waiting for one replica) is a common compromise."},{question:"What is a global transaction ID (GTID)?",options:["A unique identifier for database tables","A unique identifier assigned to each transaction that is consistent across all nodes in a replicated setup","A global lock identifier","An identifier for database users"],correctIndex:1,explanation:"A Global Transaction ID is a unique identifier assigned to each committed transaction that remains the same across all replicas in a replication topology. GTIDs simplify replication management by making it easy to determine which transactions have been applied to each replica, facilitating failover and replication setup. In MySQL, GTIDs consist of a source UUID and a sequence number (e.g., 3E11FA47-71CA-11E1-9E33-C80AA9429562:23). PostgreSQL has a similar concept with LSN-based replication slots. GTIDs eliminate the need to track binary log file positions manually."},{question:"What is a cascading rollback?",options:["Rolling back transactions in order of their start time","When aborting one transaction forces the abort of other transactions that read its uncommitted data","Rolling back changes across multiple databases","A gradual rollback of changes"],correctIndex:1,explanation:"A cascading rollback occurs when Transaction A writes data that Transaction B reads before A commits. If A then aborts, B has read invalid data and must also abort. If Transaction C read B's uncommitted data, C must abort too, creating a cascade. This is a significant problem because it can cause widespread transaction failures. Strict Two-Phase Locking prevents cascading rollbacks by holding all write locks until commit, ensuring no transaction can read uncommitted data. This is one reason strict 2PL is preferred over basic 2PL in practice."},{question:"What is the difference between a latch and a lock in database internals?",options:["They are the same thing","Latches are lightweight, short-term synchronization primitives for internal structures; locks are heavier, long-term mechanisms for transaction isolation","Locks are faster than latches","Latches support deadlock detection; locks don't"],correctIndex:1,explanation:"Latches and locks serve different purposes in database systems. Latches are low-level synchronization primitives (like mutexes) that protect internal data structures such as buffer pool pages, hash tables, and log buffers for very short durations (microseconds). They have minimal overhead and no deadlock detection. Locks are higher-level mechanisms managed by the lock manager to enforce transaction isolation  they protect logical data objects (rows, tables) and can be held for the entire transaction duration. Locks support deadlock detection and various compatibility modes (shared, exclusive)."},{question:"What is a transaction's visibility in MVCC?",options:["Whether a transaction is logged","The set of data versions a transaction can see based on its snapshot timestamp and the commit status of other transactions","Whether the transaction is visible in monitoring tools","The size of the transaction's changes"],correctIndex:1,explanation:"In MVCC, each transaction has a visibility rule that determines which row versions it can see. Generally, a transaction can see versions created by transactions that committed before it started, plus its own changes. It cannot see versions from transactions that started after it or that haven't committed yet. PostgreSQL implements this using xmin/xmax fields on each row tuple and a snapshot that lists in-progress transaction IDs. This visibility rule is what enables consistent, non-blocking reads in MVCC systems."},{question:"How does the Saga orchestration pattern differ from choreography?",options:["They are the same approach","Orchestration uses a central coordinator to manage steps; choreography has services react to events independently","Choreography is always more reliable","Orchestration doesn't support compensating transactions"],correctIndex:1,explanation:"In orchestration-based Sagas, a central orchestrator service explicitly commands each participant to execute its step and handles the logic for compensating transactions on failure. The orchestrator knows the full workflow. In choreography-based Sagas, there is no central controller  each service listens for events and decides what action to take next, publishing its own events when done. Orchestration is easier to understand and debug but creates a single point of dependency. Choreography is more decoupled but can become hard to trace and maintain as the number of services grows."},{question:"What is idempotency and why is it important for distributed transactions?",options:["A mathematical concept with no practical use","An operation that produces the same result whether executed once or multiple times, critical for safe retries in distributed systems","A type of database index","A locking strategy"],correctIndex:1,explanation:"An idempotent operation produces the same result regardless of how many times it's executed. In distributed systems, network failures can cause uncertainty about whether a request was processed, leading to retries. Without idempotency, retrying a 'transfer $100' operation could transfer $200. Idempotent designs use unique request IDs or conditional operations (e.g., 'set balance to $100' instead of 'subtract $100') so duplicates are harmless. This is essential for Saga compensating transactions, message queue consumers, and any operation that might be retried."},{question:"What is the purpose of VACUUM in PostgreSQL?",options:["To compress database files","To reclaim storage from dead tuples left by MVCC and update visibility information","To optimize query execution plans","To back up the database"],correctIndex:1,explanation:"PostgreSQL's MVCC creates new tuple versions for every update and marks old versions as dead rather than overwriting them. VACUUM reclaims the storage occupied by these dead tuples, making it available for reuse. It also updates the visibility map (tracking which pages contain only tuples visible to all active transactions) and the free space map. Without regular vacuuming, tables bloat with dead tuples, wasting disk space and degrading query performance. Autovacuum runs automatically, but understanding its behavior is crucial for PostgreSQL performance tuning."},{question:"What is a distributed lock and when would you use one?",options:["A lock that is very spread out","A locking mechanism that coordinates access to a shared resource across multiple nodes or services","A database table lock","A file system lock"],correctIndex:1,explanation:"A distributed lock ensures mutual exclusion across multiple nodes or services that don't share memory. It's used when only one process should perform an action at a time, such as processing a scheduled job, accessing an external resource, or preventing duplicate operations. Implementations include Redis-based locks (Redlock), ZooKeeper ephemeral nodes, and etcd lease-based locks. Distributed locks are inherently more complex than local locks due to network failures, clock skew, and the need for TTLs to prevent deadlocks from crashed lock holders."},{question:"What problem does the 'heuristic decision' solve in 2PC?",options:["Query optimization","When the coordinator is unreachable for too long, a participant unilaterally decides to commit or abort","Data compression decisions","Schema migration choices"],correctIndex:1,explanation:"A heuristic decision is a last-resort mechanism in 2PC where a participant that has voted 'yes' but hasn't received the coordinator's final decision makes its own commit or abort choice to unblock itself. This violates the 2PC protocol and may cause data inconsistency  if the participant commits but the coordinator eventually sends an abort (or vice versa). Heuristic decisions are recorded for manual reconciliation and should only be used when the coordinator is unreachable for an unacceptable duration. They represent a trade-off between availability and strict atomicity."},{question:"What is the difference between a transaction's read set and write set?",options:["They always contain the same data","The read set is data the transaction read; the write set is data it modified  used for conflict detection","Read sets are larger than write sets always","Write sets include only deleted data"],correctIndex:1,explanation:"A transaction's read set includes all data items it has read, and the write set includes all data items it has modified (inserted, updated, or deleted). These sets are fundamental to concurrency control: optimistic concurrency control validates that no other transaction's write set overlaps with this transaction's read set since it started. SSI uses read and write sets to detect dangerous dependency cycles. In 2PL, locks on the read set are shared locks, and locks on the write set are exclusive locks. Understanding these sets is key to reasoning about transaction anomalies."},{question:"What is multi-version timestamp ordering?",options:["Ordering queries by execution time","A concurrency control method where each transaction gets a timestamp and versions are maintained to allow reads of appropriate past values","Sorting database records by creation date","Ordering log entries by time"],correctIndex:1,explanation:"Multi-version timestamp ordering (MVTO) assigns each transaction a unique timestamp at the start and uses it to determine which version of a data item each transaction should see. When a transaction reads, it gets the latest version created by a transaction with a smaller timestamp. Writes create new versions tagged with the writing transaction's timestamp. If a write would affect a version that a newer transaction has already read, the writing transaction must abort. MVTO provides serializable isolation without locks but may have higher abort rates under contention."},{question:"What is the difference between point-in-time recovery and crash recovery?",options:["They are the same process","Crash recovery restores to the moment of failure; point-in-time recovery restores to any specified past moment","Point-in-time recovery is automatic","Crash recovery requires backups"],correctIndex:1,explanation:"Crash recovery is an automatic process that occurs when a database restarts after an unexpected failure. It replays the WAL from the last checkpoint to redo committed transactions and undo uncommitted ones, restoring the database to its state just before the crash. Point-in-time recovery (PITR) allows restoring the database to any specific moment in time by restoring a base backup and then replaying WAL records up to the desired timestamp. PITR requires a combination of periodic base backups and continuous WAL archiving, and is used for recovering from logical errors like accidental data deletion."},{question:"What is the relationship between ACID and performance?",options:["ACID properties always improve performance","ACID properties have no impact on performance","Stronger ACID guarantees generally require more coordination, locking, and I/O, reducing throughput and increasing latency","Performance depends only on hardware"],correctIndex:2,explanation:"Each ACID property introduces overhead: Atomicity requires undo logging and rollback mechanisms. Consistency requires constraint checking. Isolation requires concurrency control (locks, MVCC, or SSI). Durability requires synchronous disk writes (fsync). Stronger isolation levels demand more locking or conflict checking, reducing concurrency. This is why many applications choose lower isolation levels and why NoSQL databases often adopt the BASE model  to gain performance by relaxing ACID guarantees. The key is choosing the minimum level of consistency your application requires."},{question:"What is predicate locking and why is it used?",options:["Locking rows that match a predicate condition","Locking all data that matches a query's WHERE clause, including rows that don't yet exist, to prevent phantoms","Locking the database predicate parser","Locking only primary key columns"],correctIndex:1,explanation:"Predicate locking locks all data items that match a particular condition (predicate), including items that don't yet exist. This is the theoretically correct way to prevent phantom reads at the Serializable isolation level  if you lock the predicate 'salary > 100000', no one can insert a new row matching that predicate. However, true predicate locking is expensive to implement because it requires evaluating whether new inserts satisfy existing predicates. In practice, databases use approximations like index-range locks (next-key locking) or gap locks, which may be slightly more restrictive but are much more efficient."},{question:"What is the purpose of a fence token in distributed locking?",options:["To encrypt the lock","A monotonically increasing number attached to each lock acquisition to detect stale lock holders","To limit the number of lock holders","To measure lock duration"],correctIndex:1,explanation:"A fence token is a monotonically increasing number issued each time a lock is acquired. When a client accesses a shared resource, it includes its fence token. The resource server rejects requests with a token lower than the highest it has seen. This solves a critical problem: if Client A acquires a lock, gets delayed (GC pause, network), and the lock expires, Client B acquires the lock with a higher token. When A finally sends its request with the old token, the server rejects it. Martin Kleppmann demonstrated that without fencing, distributed locks cannot provide true mutual exclusion."},{question:"What is the difference between row-level and table-level locking?",options:["They have the same performance characteristics","Row-level locks are granular allowing more concurrency; table-level locks are coarser but have less overhead","Table-level locking is always better","Row-level locking doesn't prevent dirty reads"],correctIndex:1,explanation:"Row-level locking locks individual rows, allowing different transactions to modify different rows in the same table concurrently. This maximizes concurrency but requires more memory to track locks and more CPU for lock management. Table-level locking locks the entire table, which is simpler and cheaper to manage but blocks all concurrent access to that table. Most OLTP databases (PostgreSQL, InnoDB) default to row-level locking for maximum concurrency. Some operations (like ALTER TABLE) require table-level locks. MyISAM uses only table-level locking, which is one reason it's been largely superseded by InnoDB."},{question:"What is the role of the transaction manager in a database?",options:["To manage database users","To coordinate transaction execution, ensure ACID properties, manage concurrency, and handle recovery","To manage disk space","To optimize network connections"],correctIndex:1,explanation:"The transaction manager is the core database component responsible for maintaining the ACID properties. It coordinates with the lock manager for isolation, the log manager for atomicity and durability, and the buffer manager for efficiently accessing data. It assigns transaction IDs, manages transaction state transitions (active, partially committed, committed, aborted), handles savepoints, and coordinates recovery after crashes. In distributed databases, the transaction manager also handles distributed commit protocols like 2PC. It's essentially the orchestrator of all transactional behavior."},{question:"What is the 'write-write conflict' in MVCC?",options:["Two writes to the same disk sector","Two concurrent transactions attempting to modify the same row, where the second must abort or wait","Writing duplicate data","Writing to a read-only database"],correctIndex:1,explanation:"A write-write conflict in MVCC occurs when two concurrent transactions attempt to update the same row. Since MVCC typically uses 'first writer wins,' the second transaction to attempt the write detects that the row has been modified since it read it and must either abort (in snapshot isolation) or wait (in some implementations). In PostgreSQL's Repeatable Read, the second writer gets a serialization failure and must retry. This is different from lost updates  MVCC detects the conflict rather than silently overwriting. Proper handling of write-write conflicts is essential for data integrity."},{question:"What is the Paxos consensus algorithm and how does it relate to distributed transactions?",options:["A sorting algorithm","A consensus protocol that allows distributed nodes to agree on a value, used as a building block for distributed transaction commits","A compression algorithm","A query optimization algorithm"],correctIndex:1,explanation:"Paxos is a family of consensus protocols that enable a group of distributed nodes to agree on a single value despite node failures. Unlike 2PC, Paxos can make progress as long as a majority of nodes are available, avoiding the blocking problem. It works through a proposer-acceptor-learner model with prepare and accept phases. Systems like Google's Spanner use Paxos for distributed transaction commits, replacing 2PC's coordinator with a Paxos group for fault tolerance. Raft is a more understandable alternative to Paxos that provides the same guarantees."},{question:"How does Google Spanner achieve external consistency?",options:["By using eventual consistency","By combining Paxos consensus with TrueTime (GPS/atomic clock-synchronized timestamps) for globally consistent transactions","By using 2PC without modification","By avoiding distributed transactions"],correctIndex:1,explanation:"Google Spanner achieves external consistency (real-time ordering of transactions) by using TrueTime, an API that provides globally synchronized timestamps with bounded uncertainty using GPS and atomic clocks. When a transaction commits, Spanner assigns it a TrueTime timestamp and waits out the uncertainty interval to ensure no other transaction can be assigned an earlier timestamp that hasn't yet committed. This wait (typically a few milliseconds) guarantees that transactions are ordered consistently with real-time. Combined with Paxos for replication, Spanner provides the strongest consistency guarantees of any globally distributed database."},{question:"What is the purpose of the undo tablespace in InnoDB?",options:["To store temporary tables","To store undo logs that enable MVCC read views and transaction rollbacks","To store database backups","To store query results"],correctIndex:1,explanation:"The undo tablespace in InnoDB stores undo log records that serve two purposes: rolling back transactions when they're aborted, and providing consistent read views for MVCC. When a row is updated, the old version is written to the undo tablespace, allowing other transactions to read the previous version. The purge thread periodically removes undo records that are no longer needed by any active transaction. If long-running transactions prevent purging, the undo tablespace can grow significantly, a condition known as 'undo log bloat.' Monitoring undo tablespace size is important for production database management."},{question:"What is the difference between two-phase commit and consensus protocols like Raft?",options:["They are identical protocols","2PC requires all participants to agree and blocks if the coordinator fails; Raft requires only a majority and can elect a new leader","Raft is slower than 2PC in all cases","2PC supports more nodes than Raft"],correctIndex:1,explanation:"Two-Phase Commit and consensus protocols solve related but different problems. 2PC ensures all participants commit or abort together but requires every participant to be available  if the coordinator or any participant fails, the protocol blocks. Raft (and Paxos) require only a majority (quorum) of nodes to agree, allowing progress even when some nodes are down. 2PC coordinates heterogeneous operations (each node does something different), while Raft replicates the same state across nodes. Modern systems often combine them: using Raft for replication within a shard and 2PC for cross-shard transactions."},{question:"What is a materialized conflict in optimistic concurrency control?",options:["A conflict that exists in a materialized view","A situation where validation at commit time detects that another transaction has modified data this transaction read","A conflict between materialized and virtual tables","A pre-computed list of potential conflicts"],correctIndex:1,explanation:"In optimistic concurrency control, a materialized conflict is detected during the validation phase when a committing transaction discovers that data it read has been modified by another transaction that committed since the read. The validating transaction must then abort and retry because its computations were based on stale data. The validation typically compares the transaction's read set against the write sets of all transactions that committed during its execution. While optimistic control avoids lock overhead, materialized conflicts lead to wasted work, making it less suitable for high-contention workloads."},{question:"How does CockroachDB handle distributed transactions differently from traditional 2PC?",options:["It doesn't support distributed transactions","It uses a parallel commit protocol that allows transactions to be considered committed before all intents are resolved","It uses the exact same 2PC protocol","It uses eventual consistency instead"],correctIndex:1,explanation:"CockroachDB uses a parallel commit protocol that improves on traditional 2PC by allowing a transaction to be considered committed as soon as all writes (called 'intents') are staged and the transaction record is committed, without waiting for intents to be resolved (cleaned up). This reduces the critical path of a distributed commit to a single round of parallel writes. Intents are resolved asynchronously afterward. CockroachDB also uses a hybrid logical clock (HLC) for timestamp ordering and serializable isolation by default, making it one of the few distributed databases offering strong consistency with reasonable performance."},{question:"What is the 'phantom problem' in index locking?",options:["Indexes that disappear after restart","Range queries missing newly inserted rows because index locks don't cover the gaps where new entries could be inserted","Corrupted index entries","Indexes taking up phantom disk space"],correctIndex:1,explanation:"The phantom problem in index locking occurs when a transaction's range scan locks only existing index entries but not the gaps between them. Another transaction can then insert a new row that falls within the range, and a subsequent scan by the first transaction will see this 'phantom' row. To solve this, databases use next-key locking (InnoDB) or predicate locking, which locks not just existing entries but also the gaps where new qualifying entries could be inserted. This is one of the most subtle concurrency issues and is the reason Serializable isolation requires special locking techniques."},{question:"What is the relationship between WAL and replication?",options:["WAL cannot be used for replication","WAL records can be shipped to replicas to replay changes, providing physical replication","Replication replaces the need for WAL","WAL is only used for backup, not replication"],correctIndex:1,explanation:"WAL-based (physical) replication works by streaming WAL records from the primary to replica servers, which replay them to maintain identical copies of the data. PostgreSQL's streaming replication works this way, sending WAL segments in near real-time. This is efficient because the same log needed for crash recovery is reused for replication. However, physical replication requires replicas to have identical binary formats. Logical replication (used in MySQL's binlog replication and PostgreSQL's logical decoding) operates at a higher level, describing row changes independently of physical storage format, enabling cross-version replication."},{question:"What is the purpose of a read view (or snapshot) in InnoDB?",options:["To create a materialized view","To establish which transaction versions are visible to a given transaction based on active transaction IDs at snapshot creation time","To optimize read queries","To read data from multiple tables simultaneously"],correctIndex:1,explanation:"An InnoDB read view is a snapshot that records the state of active transactions at the time it's created. It contains the list of active (uncommitted) transaction IDs, the lowest active transaction ID, and the next transaction ID to be assigned. Using this information, the read view determines visibility: changes from transactions that were committed before the read view was created are visible; changes from active or future transactions are not. At Read Committed, a new read view is created for each statement; at Repeatable Read, one read view is used for the entire transaction."},{question:"What is the Calvin distributed transaction protocol?",options:["A caching protocol","A deterministic transaction processing system that pre-orders all transactions to avoid distributed coordination during execution","A consensus protocol for leader election","A data compression protocol"],correctIndex:1,explanation:"Calvin is a distributed transaction protocol that takes a fundamentally different approach from traditional protocols by establishing a global transaction order before execution. A sequencing layer collects transactions and assigns them a deterministic order. Each node then executes transactions in this pre-determined order, guaranteeing that all nodes reach the same state without runtime coordination for commit decisions. This eliminates the need for 2PC during execution since the outcome is predetermined. The trade-off is added latency in the sequencing phase and the requirement that transaction read/write sets be known in advance."},{question:"What is an anti-dependency (write-read conflict) in serialization theory?",options:["A dependency that cancels out another","When a transaction writes a value that another concurrent transaction has already read, creating a potential serialization anomaly","A conflict between antivirus and database software","A dependency that doesn't matter"],correctIndex:1,explanation:"An anti-dependency (or write-read dependency) occurs when Transaction T2 reads a data item, and then Transaction T1 writes a new version of that same item. In serialization order, T1 must appear before T2 (since T2 read the old version). Anti-dependencies are one of three types of dependencies in serialization graphs (along with read-write and write-write). A cycle involving anti-dependencies indicates a potential serialization anomaly. SSI specifically tracks anti-dependencies (called 'rw-conflicts') to detect and prevent serialization failures in snapshot isolation."},{question:"What is the difference between logical replication and physical replication?",options:["They are the same thing","Physical replication copies raw WAL bytes; logical replication decodes changes into logical operations (INSERT, UPDATE, DELETE)","Logical replication is always faster","Physical replication supports cross-version setups"],correctIndex:1,explanation:"Physical replication ships and replays raw WAL records, creating byte-identical replicas. It's efficient and simple but requires identical PostgreSQL versions, architecture, and even the same tablespace layout. Logical replication decodes WAL into logical change records (operations on rows), allowing replication between different versions, selective table replication, and even different database engines. PostgreSQL supports both: streaming replication (physical) and logical replication via publications/subscriptions. MySQL's binary log replication is inherently logical. Each approach has trade-offs in performance, flexibility, and complexity."}],hf=[{question:"What is the primary difference between SQL and NoSQL databases?",options:["SQL is faster than NoSQL","SQL enforces a fixed schema with structured tables; NoSQL offers flexible schemas with various data models","NoSQL is newer and always better","SQL doesn't support indexing"],correctIndex:1,explanation:"SQL databases enforce a predefined schema with structured tables, rows, and columns, ensuring data consistency through constraints and relationships. NoSQL databases provide flexible schemas, allowing different records to have different structures  this includes document stores (MongoDB), key-value stores (Redis), column-family (Cassandra), and graph databases (Neo4j). The choice depends on your needs: SQL excels at complex queries and transactions with strong consistency, while NoSQL shines for flexible schemas, horizontal scaling, and high write throughput. Instagram uses both PostgreSQL (for user/relationship data needing consistency) and Cassandra (for high-volume feed data needing scale)."},{question:"What does ACID stand for in database transactions?",options:["Automated, Consistent, Isolated, Durable","Atomicity, Consistency, Isolation, Durability","Atomic, Cached, Indexed, Distributed","Available, Consistent, Isolated, Durable"],correctIndex:1,explanation:"ACID stands for Atomicity (all operations in a transaction succeed or all fail), Consistency (the database moves from one valid state to another), Isolation (concurrent transactions don't interfere with each other), and Durability (committed data survives system failures). These properties are the foundation of reliable relational databases like PostgreSQL and MySQL. For example, in a bank transfer, atomicity ensures money is deducted from one account AND added to another  never just one. NoSQL databases often relax some ACID properties (particularly isolation and consistency) in exchange for better performance and availability, following the BASE model instead."},{question:"What is a B-tree index and why is it the default index type in most relational databases?",options:["A binary tree for fast sorting","A balanced tree structure that keeps data sorted and allows searches, insertions, and deletions in O(log n) time","A tree that stores only boolean values","A backup tree for redundancy"],correctIndex:1,explanation:"A B-tree is a self-balancing tree data structure that maintains sorted data and allows searches, sequential access, insertions, and deletions in O(log n) time. It's the default index in PostgreSQL, MySQL, and most relational databases because it handles both equality lookups (WHERE id = 5) and range queries (WHERE age BETWEEN 20 AND 30) efficiently. B-trees have high fan-out (many children per node), which minimizes the number of disk reads  a B-tree with branching factor 500 can index billions of rows in just 3-4 levels. Each level requires one disk read, so even very large tables need only a few disk accesses. This is why B-tree indexes dramatically improve query performance on large tables."},{question:"What is an LSM-tree and which databases use it?",options:["A tree for log storage management","Log-Structured Merge-tree that optimizes write performance by buffering writes in memory then flushing sorted runs to disk","A tree for least-recently-used data","A tree structure for read optimization"],correctIndex:1,explanation:"An LSM-tree (Log-Structured Merge-tree) optimizes write performance by first writing data to an in-memory sorted structure (memtable), then periodically flushing it to immutable sorted files (SSTables) on disk. Reads check the memtable first, then progressively older SSTables. Background compaction merges SSTables to reduce read amplification. LSM-trees are used by Cassandra, RocksDB, LevelDB, and HBase. They excel at write-heavy workloads because writes are sequential (appending to memtable and writing sorted runs) rather than random (as with B-tree page updates). The tradeoff is that reads can be slower because they may check multiple SSTables, which is mitigated by Bloom filters that quickly determine if a key exists in a given SSTable."},{question:"What is database normalization?",options:["Making the database run at normal speed","Organizing data to minimize redundancy by decomposing tables according to normal forms","Converting data to a normal distribution","Resetting the database to defaults"],correctIndex:1,explanation:"Normalization is the process of organizing a relational database to reduce data redundancy and improve data integrity by decomposing tables according to a series of normal forms (1NF, 2NF, 3NF, BCNF, etc.). For example, instead of storing a customer's address in every order row, you store it once in a customers table and reference it via a foreign key. This prevents update anomalies (changing an address in one order but not others), insertion anomalies, and deletion anomalies. The tradeoff is that normalized databases require more JOINs to reconstruct data, which can impact read performance. Most production systems normalize to 3NF as a good balance between data integrity and query performance."},{question:"When would you choose denormalization over normalization?",options:["Always, because it's faster","When read performance is critical and the cost of JOINs is too high for your query patterns","When you want better data integrity","When disk space is limited"],correctIndex:1,explanation:"Denormalization intentionally introduces redundancy by storing derived or copied data to avoid expensive JOINs at query time. It's chosen when read performance is the primary concern and the application can tolerate some data redundancy. For example, storing a user's name directly in the orders table (instead of joining to users table) eliminates a JOIN for the common 'show order with customer name' query. The cost is increased storage, more complex updates (must update the name in multiple places), and potential inconsistency. Social media feeds, analytics dashboards, and e-commerce product pages are common use cases where denormalization significantly improves read latency. NoSQL databases often embrace denormalization as a primary design pattern."},{question:"What is a document store database?",options:["A database for storing PDF documents","A NoSQL database that stores data as semi-structured documents (JSON/BSON) with flexible schemas","A database that stores documentation","A relational database with text columns"],correctIndex:1,explanation:"A document store is a type of NoSQL database that stores data as semi-structured documents, typically in JSON or BSON format. Each document can have a different structure  one user document might have an 'address' field while another doesn't. This flexibility is ideal for evolving schemas and varied data. MongoDB is the most popular document store, using BSON (Binary JSON) internally. Documents are grouped into collections (analogous to tables), and you can query on any field without predefined schemas. Document stores excel when your data naturally fits a hierarchical/nested structure, like a blog post with embedded comments. The tradeoff is that cross-document queries (equivalent to JOINs) are less efficient than in relational databases."},{question:"What is a column-family database?",options:["A database that only stores columns","A NoSQL database that stores data in columns grouped by column families, optimized for queries over large datasets","A relational database with many columns","A database for spreadsheet data"],correctIndex:1,explanation:"Column-family databases (also called wide-column stores) organize data by columns rather than rows, with columns grouped into families. Unlike relational databases where a row stores all columns together, column-family stores keep each column family's data contiguous on disk. This makes them extremely efficient for queries that read specific columns across many rows (like analytics) and for sparse data where most columns are empty. Cassandra and HBase are the main examples. Cassandra's data model uses partition keys for distribution and clustering columns for sorting within partitions. Column-family databases excel at time-series data, event logging, and analytics workloads where you typically query specific attributes across millions of records."},{question:"What is the CAP theorem?",options:["Caching And Processing theorem","A distributed system can only guarantee two of three: Consistency, Availability, and Partition tolerance","Concurrency, Atomicity, and Persistence theorem","A theorem about database capacity"],correctIndex:1,explanation:"The CAP theorem states that in a distributed system, you can only guarantee two of three properties simultaneously: Consistency (all nodes see the same data at the same time), Availability (every request receives a response), and Partition tolerance (the system continues operating despite network failures). Since network partitions are inevitable in distributed systems, the real choice is between CP (consistency + partition tolerance, like HBase) and AP (availability + partition tolerance, like Cassandra with eventual consistency). For example, during a network partition, a CP system might refuse to serve requests to maintain consistency, while an AP system serves potentially stale data to remain available. It's important to note that CAP applies only during partition events."},{question:"What is the difference between OLTP and OLAP?",options:["They are the same thing with different names","OLTP handles real-time transactional operations; OLAP handles analytical queries on large datasets","OLTP is for old systems; OLAP is for new systems","OLTP is faster than OLAP"],correctIndex:1,explanation:"OLTP (Online Transaction Processing) systems handle high volumes of short, real-time transactions like inserts, updates, and deletes  think of a bank processing transfers or an e-commerce site processing orders. OLAP (Online Analytical Processing) systems are optimized for complex analytical queries that scan large amounts of data  think of a data warehouse generating quarterly revenue reports. OLTP uses row-oriented storage for fast writes and point lookups, while OLAP uses columnar storage for efficient aggregation queries. PostgreSQL and MySQL are OLTP databases; Snowflake, BigQuery, and ClickHouse are OLAP systems. Many architectures use both: OLTP for the application and ETL pipelines feeding data into an OLAP system for analytics."},{question:"What is a composite index and when should you use one?",options:["An index made of composite materials","An index on multiple columns that speeds up queries filtering on those columns together","An index that combines multiple tables","An index that's created automatically"],correctIndex:1,explanation:"A composite (compound) index covers multiple columns in a specific order, optimizing queries that filter or sort on those columns together. For example, an index on (country, city) speeds up queries like 'WHERE country = 'US' AND city = 'NYC'' and also 'WHERE country = 'US'' (leftmost prefix). However, it doesn't help 'WHERE city = 'NYC'' alone because indexes follow left-to-right column order. The column order matters enormously  put the highest-cardinality or most-frequently-filtered column first. In PostgreSQL, a composite index on (user_id, created_at) is perfect for 'SELECT * FROM orders WHERE user_id = 123 ORDER BY created_at DESC' because it satisfies both the filter and the sort in a single index scan."},{question:"What is a covering index?",options:["An index that covers the entire table","An index that contains all columns needed by a query, avoiding table lookups entirely","An index with encryption coverage","An index used for covering edge cases"],correctIndex:1,explanation:"A covering index includes all the columns that a query needs, so the database can satisfy the query entirely from the index without reading the actual table rows (an 'index-only scan'). For example, if you frequently run 'SELECT name, email FROM users WHERE user_id = ?', an index on (user_id, name, email) covers this query completely. PostgreSQL shows this as 'Index Only Scan' in EXPLAIN output. Covering indexes can dramatically improve performance for read-heavy queries because index structures are typically smaller and more cache-friendly than full table pages. The tradeoff is that wider indexes consume more disk space and slow down writes since each INSERT/UPDATE must update the index."},{question:"What is database sharding?",options:["Breaking a database into pieces for disposal","Horizontally partitioning data across multiple database instances to distribute load","Creating read replicas","Backing up the database"],correctIndex:1,explanation:"Sharding horizontally partitions data across multiple independent database instances (shards), each holding a subset of the total data. For example, users with IDs 1-1M go to shard 1, 1M-2M to shard 2, etc. This enables horizontal scaling beyond a single machine's capacity for both storage and throughput. Instagram sharded their PostgreSQL database by user ID to handle billions of photos. The key challenges are choosing a good shard key (to avoid hot shards), handling cross-shard queries (JOINs across shards are very expensive), and rebalancing data when adding shards. Many modern databases like CockroachDB and Vitess (MySQL) handle sharding automatically, while others like MongoDB provide built-in sharding support."},{question:"What is the difference between horizontal and vertical partitioning?",options:["They are the same","Horizontal partitioning splits rows across tables/databases; vertical partitioning splits columns across tables","Horizontal is for NoSQL; vertical is for SQL","Horizontal adds more servers; vertical adds more storage"],correctIndex:1,explanation:"Horizontal partitioning (sharding) distributes rows across multiple tables or database instances  each partition has the same columns but different rows. For example, orders from January in one partition, February in another. Vertical partitioning splits columns across tables  frequently accessed columns stay in a 'hot' table while rarely accessed large columns (like BLOBs) go to a separate table. For example, separating user profile data (name, email  read often) from user preferences (JSON blob  read rarely). Horizontal partitioning enables scaling across machines, while vertical partitioning optimizes I/O by keeping hot data compact. PostgreSQL supports both via declarative partitioning (horizontal) and manual table decomposition (vertical)."},{question:"What is eventual consistency?",options:["The database will eventually become consistent when you fix bugs","All replicas will converge to the same state over time, but reads may return stale data temporarily","The database is always consistent","Consistency that applies only to eventual queries"],correctIndex:1,explanation:"Eventual consistency is a consistency model where, given enough time without new updates, all replicas will converge to the same state. During the convergence period, different replicas may return different values for the same data. This is the model used by AP systems like Cassandra and DynamoDB (by default). For example, after updating a user's profile, one replica might return the old name while another returns the new name for a brief period. Eventually, all replicas sync up. The benefit is higher availability and lower latency (no need to wait for all replicas to acknowledge). For many use cases like social media likes or product view counts, eventual consistency is perfectly acceptable  users won't notice a count being off by one for a few seconds."},{question:"What is a graph database best suited for?",options:["Storing graphs and charts","Data with complex many-to-many relationships where traversal queries are common","Time-series data","Simple key-value lookups"],correctIndex:1,explanation:"Graph databases store data as nodes (entities) and edges (relationships), making them ideal for highly connected data where relationship traversal is the primary query pattern. They excel at queries like 'find all friends of friends who work at company X' or 'what's the shortest path between person A and person B'  queries that would require multiple expensive JOINs in a relational database. Neo4j is the most popular graph database. Real-world use cases include social networks (Facebook's TAO), fraud detection (finding circular money transfers), recommendation engines (products bought by similar users), and knowledge graphs. The key advantage is that traversing relationships is O(1) per hop regardless of total data size, unlike SQL JOINs which degrade with table size."},{question:"What is a write-ahead log (WAL) in databases?",options:["A log of future writes planned","A durability mechanism that logs changes before applying them to the database, enabling crash recovery","A log that's written ahead of time","A performance optimization log"],correctIndex:1,explanation:"A write-ahead log (WAL) records all changes to a log file before they're applied to the actual database pages. If the database crashes, it can replay the WAL to recover committed transactions that weren't yet written to the data files, and roll back incomplete transactions. This is how databases achieve durability (the 'D' in ACID) without writing every change synchronously to data files (which would be very slow due to random I/O). PostgreSQL's WAL writes sequentially to disk (fast) while data file updates happen asynchronously in the background. WAL also enables replication  replicas can stream and replay the WAL to stay in sync. MySQL's equivalent is the InnoDB redo log."},{question:"What is the difference between PostgreSQL and MySQL?",options:["They are identical","PostgreSQL is more standards-compliant with advanced features (JSONB, CTE, window functions); MySQL is simpler with historically better replication","PostgreSQL is NoSQL; MySQL is SQL","MySQL is always faster"],correctIndex:1,explanation:"PostgreSQL is known for strict SQL standards compliance, advanced data types (JSONB, arrays, hstore), sophisticated query planner, extensibility (custom types, functions, operators), and features like CTEs, window functions, and full-text search. MySQL (especially with InnoDB) has historically offered simpler setup, better replication support, and faster performance for simple queries. PostgreSQL supports MVCC more completely and handles complex queries better, while MySQL's simpler architecture made it the default for web applications. In modern versions, the gap has narrowed significantly  MySQL 8.0 added CTEs and window functions. Companies like Instagram and Discord use PostgreSQL for its JSONB support and reliability, while many legacy web apps (WordPress, Drupal) use MySQL."}],pf=[{question:"What is the primary advantage of using base62 encoding for URL shortener keys?",options:["It produces the shortest possible keys","It uses only URL-safe characters (a-z, A-Z, 0-9)","It prevents hash collisions entirely","It enables automatic expiration"],correctIndex:1,explanation:"Base62 encoding uses characters a-z (26), A-Z (26), and 0-9 (10), totaling 62 URL-safe characters. This is critical because URL shortener keys appear directly in URLs, so they must not contain special characters that could break URL parsing or require percent-encoding. Base64, by contrast, includes '+' and '/' which are not URL-safe without modification. While base62 does produce compact keys, the primary advantage is URL-safety  a 7-character base62 string gives 62^7  3.5 trillion unique URLs, which is more than sufficient for most services."},{question:"How many unique short URLs can a 7-character base62 key generate?",options:["About 3.5 million","About 3.5 billion","About 3.5 trillion","About 3.5 quadrillion"],correctIndex:2,explanation:"A 7-character base62 key generates 62^7 = 3,521,614,606,208, which is approximately 3.5 trillion unique combinations. This calculation is straightforward: each of the 7 positions can hold any of 62 characters. For context, bit.ly processes billions of links but is nowhere near this limit, making 7 characters a practical choice. If you needed fewer characters, 6 gives only ~57 billion combinations, while 8 gives ~218 trillion  so 7 is the sweet spot for most URL shortening services."},{question:"What HTTP status code should a URL shortener return for temporary redirects where analytics tracking is important?",options:["200 OK","301 Moved Permanently","302 Found","404 Not Found"],correctIndex:2,explanation:"A 302 (Found) redirect tells the browser that the redirect is temporary, meaning the browser will NOT cache the redirect and will hit the URL shortener server on every request. This is critical for analytics because each request passes through your server, allowing you to log click counts, referrers, geolocation, and timestamps. A 301 (Moved Permanently) would cause browsers to cache the redirect and skip your server on subsequent visits, making your analytics incomplete. Most URL shorteners like bit.ly use 302 redirects (or 307) to ensure accurate click tracking, even though 301 would be slightly faster for end users."},{question:"What is the main problem with using MD5 or SHA-256 hashing directly for generating short URLs?",options:["They are too slow to compute","They produce fixed-length outputs that are too long for short URLs","They cannot handle Unicode input","They require a salt"],correctIndex:1,explanation:"MD5 produces a 128-bit (32 hex characters) hash, and SHA-256 produces a 256-bit (64 hex characters) hash  both far too long for a 'short' URL. Even if you take just the first 7 characters of the hash, you dramatically increase collision probability compared to using all the bits. The birthday paradox means collisions become likely much sooner than you'd expect  with 7 characters of hex (16^7  268 million), you'd expect a collision after roughly 23,000 URLs. A Key Generation Service (KGS) that pre-generates unique keys avoids this collision problem entirely, which is why it's the preferred approach in system design interviews."},{question:"What is a Key Generation Service (KGS) in the context of a URL shortener?",options:["A service that encrypts URLs before storage","A service that pre-generates unique short keys and stores them in a database","A service that manages API authentication keys","A service that compresses URLs"],correctIndex:1,explanation:"A Key Generation Service (KGS) pre-generates a large pool of unique random keys (e.g., 7-character base62 strings) and stores them in a database with two tables: one for unused keys and one for used keys. When a new short URL is needed, KGS simply moves a key from the unused table to the used table, guaranteeing uniqueness without any collision checking at creation time. This is far more efficient than hash-then-check-collision approaches because the uniqueness guarantee is built into the pre-generation step. Services like bit.ly and TinyURL use similar approaches  pre-generation decouples the key uniqueness problem from the URL creation hot path, making the write operation O(1) with no retry logic needed."},{question:"In a URL shortener, why is the read-to-write ratio typically very high (e.g., 100:1)?",options:["Because URLs expire frequently","Because each short URL is created once but redirected many times","Because the database performs read replicas","Because caching is not used"],correctIndex:1,explanation:"A short URL is created exactly once (one write) but may be clicked hundreds or thousands of times (many reads). For example, a popular tweet with a shortened link might generate millions of clicks from a single creation event. This 100:1 or even 1000:1 read-to-write ratio fundamentally shapes the system architecture: you should optimize for reads with extensive caching (Redis/Memcached), use read replicas for the database, and consider a CDN for geographic distribution. The write path can be simpler and slower since it's invoked far less frequently  this is why many URL shortener designs focus their optimization efforts on the redirect (read) path."},{question:"Which caching strategy is most appropriate for a URL shortener's redirect service?",options:["Write-through cache","Write-around cache with LRU eviction","Write-back cache","No caching needed"],correctIndex:1,explanation:"Write-around cache with LRU (Least Recently Used) eviction is ideal because URL shortener access patterns follow a power-law distribution  a small percentage of URLs receive the vast majority of traffic. With write-around, new URL mappings are written directly to the database, and the cache is populated only on read misses. LRU eviction ensures hot (frequently accessed) URLs stay in cache while rarely-accessed URLs are evicted. Write-through would unnecessarily cache every newly created URL, most of which may never be accessed. A Redis or Memcached layer with this strategy can serve 99%+ of redirect requests from memory, reducing database load dramatically  bit.ly reportedly caches its hottest URLs this way."},{question:"What database type is most commonly recommended for a URL shortener's primary storage?",options:["Graph database like Neo4j","NoSQL key-value store like DynamoDB","Time-series database like InfluxDB","Document database like MongoDB"],correctIndex:1,explanation:"A URL shortener's data model is fundamentally a key-value mapping: short_key  long_url. This maps perfectly to a NoSQL key-value store like DynamoDB, Cassandra, or Riak, which provide O(1) lookups by key, horizontal scalability, and high throughput. Relational databases work but add overhead for features (joins, transactions, schemas) that aren't needed for this simple mapping. The access pattern is almost entirely point lookups (get URL by key) with no complex queries, making key-value stores the natural fit. Companies like Pinterest (which shortened billions of URLs) chose similar NoSQL approaches for their URL shortening infrastructure."},{question:"How should you handle hash collisions when using a hash-based approach for short URL generation?",options:["Ignore them  collisions are extremely rare","Append a sequence number to the URL before re-hashing","Use a longer hash output","Switch to a different hashing algorithm"],correctIndex:1,explanation:"When a collision is detected (the generated short key already exists in the database), the standard approach is to append a monotonically increasing sequence number or timestamp to the original URL and re-hash. For example, if hash('example.com') = 'abc1234' and that key is taken, you try hash('example.com1'), then hash('example.com2'), etc. This is preferable to using a longer hash (which defeats the purpose of short URLs) or switching algorithms (which doesn't prevent future collisions). However, this retry approach has performance implications under high write load, which is exactly why a KGS is preferred  it eliminates collisions entirely by pre-generating guaranteed-unique keys."},{question:"What is the purpose of database sharding in a URL shortener, and what is the best sharding key?",options:["Shard by user ID to keep each user's URLs together","Shard by the hash/short key for even distribution","Shard by creation date for time-based queries","Shard by the long URL's domain"],correctIndex:1,explanation:"Sharding by the short key (hash) provides the most even distribution of data across shards because short keys are random (especially with base62 encoding or pre-generated random keys). This avoids hot spots  if you sharded by user ID, a single power user could overload one shard. Date-based sharding would create hot spots on the current day's shard. The short key is also the primary lookup key for redirects, so sharding by it means every redirect request hits exactly one shard with no scatter-gather needed. Consistent hashing is often used to map short keys to shards, allowing smooth scaling when adding or removing nodes  this is the approach used by large-scale URL shorteners."},{question:"What happens when a user requests a custom alias that already exists?",options:["Silently assign a different alias","Return an error indicating the alias is taken","Overwrite the existing alias","Add a random suffix to make it unique"],correctIndex:1,explanation:"When a custom alias collides with an existing one, the correct behavior is to return a clear error (e.g., HTTP 409 Conflict) telling the user the alias is taken and to choose another. Silently changing the alias would violate the user's intent and break their expected URL. Overwriting would be a security disaster  someone could hijack existing short URLs. Adding random suffixes defeats the purpose of a custom alias. This check requires a database lookup before creation, and for KGS-based systems, you need to also check that the custom alias doesn't conflict with pre-generated keys  some systems use separate namespaces or length conventions to avoid this (e.g., custom aliases must be 8+ characters while auto-generated are exactly 7)."},{question:"Why might a URL shortener use 301 redirects instead of 302?",options:["To reduce server load since browsers cache the redirect","To improve SEO for the short URL","To prevent analytics tracking","To support HTTPS"],correctIndex:0,explanation:"A 301 (Moved Permanently) redirect tells the browser to cache the redirect mapping locally, so subsequent clicks on the same short URL bypass the shortener's server entirely and go directly to the destination. This dramatically reduces server load and improves user experience with faster redirects. The trade-off is that you lose analytics visibility for repeat visitors. Services that prioritize performance and low infrastructure costs over detailed analytics may prefer 301. Google's goo.gl (now discontinued) originally used 301 redirects, while bit.ly uses 302 to maintain analytics  the choice depends on whether your product prioritizes speed or tracking."},{question:"How should a URL shortener handle URL expiration?",options:["Delete expired URLs immediately from the database","Use a lazy cleanup approach  check expiration on read and run periodic batch cleanup","Set database TTL and let the database handle it","Never expire URLs  storage is cheap"],correctIndex:1,explanation:"A lazy cleanup approach is most practical: when a redirect request comes in, check if the URL has expired, and if so, return a 410 Gone or 404. Separately, run a periodic batch job (e.g., nightly) to delete expired entries and reclaim keys. Immediate deletion on expiration would require a scheduled task per URL, which doesn't scale to billions of URLs. While database-level TTL (like DynamoDB's) can work, it provides less control and doesn't allow graceful error responses. The lazy approach ensures no redirect latency impact for non-expired URLs while still eventually cleaning up storage  this is similar to how Redis handles key expiration with lazy + periodic deletion strategies."},{question:"What rate limiting strategy should a URL shortener use for its creation endpoint?",options:["No rate limiting needed","Rate limit by IP address with a token bucket algorithm","Rate limit only authenticated users","Rate limit based on URL domain"],correctIndex:1,explanation:"Rate limiting the creation endpoint by IP address (for anonymous users) or API key (for authenticated users) using a token bucket algorithm is essential to prevent abuse. Without rate limiting, attackers could exhaust your key space, use your service for spam/phishing link distribution, or simply overwhelm your write infrastructure. Token bucket is preferred because it allows burst traffic (a user creating several URLs quickly) while enforcing a sustained rate limit. The redirect endpoint typically doesn't need aggressive rate limiting since it's designed for high-throughput reads. Real-world services like bit.ly enforce strict creation rate limits  free tier users get limited link creation per month, which serves both business and infrastructure protection purposes."},{question:"In a URL shortener system, what is the role of a CDN?",options:["To store the database closer to users","To cache redirect responses at edge locations for faster response times","To compress the short URLs","To generate short keys at edge locations"],correctIndex:1,explanation:"A CDN (Content Delivery Network) caches the redirect responses at edge servers distributed globally, so a user in Tokyo clicking a short URL gets the redirect response from a nearby edge server rather than a data center in Virginia. With 302 redirects, CDN caching requires careful configuration (short TTLs or using the Cache-Control header) to balance performance with analytics accuracy. With 301 redirects, the CDN can aggressively cache since the redirect is permanent. This is especially impactful for viral links that receive millions of clicks from diverse geographic locations. Services like Cloudflare or AWS CloudFront can reduce redirect latency from hundreds of milliseconds to single-digit milliseconds for cached URLs."},{question:"What is the birthday paradox and how does it relate to URL shortener design?",options:["It's about generating unique URLs for users born on the same day","It means hash collisions become probable much sooner than intuition suggests, affecting key space sizing","It's a method for generating random keys","It's related to URL expiration timing"],correctIndex:1,explanation:"The birthday paradox states that in a set of n randomly chosen values from a space of N possibilities, collisions become likely when n  N. For a URL shortener, if your key space has N possible keys, you'll likely see your first collision after approximately N URLs. For example, with a 7-character hex key (16^7  268M possibilities), collisions become probable after only ~16,000 URLs  far sooner than the intuitive 268M/2. This is why base62 with 7 characters (62^7  3.5T) is preferred  collisions become likely only after ~1.87M URLs. Understanding this paradox is critical for sizing your key space correctly and justifying the use of KGS over hash-based approaches in system design interviews."},{question:"How should a URL shortener validate the destination URL before creating a short link?",options:["Only check that it starts with http:// or https://","Validate URL format, check against malware/phishing blacklists, and optionally verify the URL is reachable","No validation needed  just store whatever the user provides","Only check URL length"],correctIndex:1,explanation:"Comprehensive URL validation involves multiple layers: format validation (proper URL syntax), protocol checking (restrict to http/https), blacklist checking against known malware/phishing databases (like Google Safe Browsing API), and optionally an HTTP HEAD request to verify the destination is reachable. This is crucial because URL shorteners are frequently abused for phishing  a legitimate-looking short URL can hide a malicious destination. Bit.ly and other major shorteners integrate with threat intelligence feeds and scan destinations periodically (not just at creation time, since a destination could become malicious later). Skipping validation exposes your service to abuse and could get your domain blacklisted by browsers and email providers."},{question:"What is the advantage of using a NoSQL database over a relational database for a URL shortener?",options:["NoSQL supports SQL queries","NoSQL provides horizontal scalability and handles the simple key-value access pattern efficiently","NoSQL is always faster than relational databases","NoSQL provides ACID transactions"],correctIndex:1,explanation:"A URL shortener's primary data access pattern is a simple key-value lookup: given a short key, return the long URL. NoSQL databases like DynamoDB or Cassandra are purpose-built for this pattern, offering O(1) lookups and linear horizontal scalability by adding more nodes. Relational databases would work but carry overhead from features like enforcing schemas, supporting complex joins, and maintaining ACID transactions  none of which are needed for a key-value mapping. NoSQL isn't 'always faster' (that's a myth), but for this specific access pattern at scale (billions of URLs, millions of reads/second), it's the more natural and cost-effective choice. Companies like Instagram and Pinterest have successfully used NoSQL stores for similar simple-mapping use cases."},{question:"How can a URL shortener track analytics without significantly impacting redirect latency?",options:["Log analytics synchronously before redirecting","Write analytics events to a message queue (like Kafka) asynchronously and process them separately","Store analytics in the same database as URL mappings","Skip analytics for high-traffic URLs"],correctIndex:1,explanation:"Writing analytics events asynchronously to a message queue like Kafka or Amazon Kinesis allows the redirect response to return immediately (within milliseconds) while analytics data is processed separately by downstream consumers. Synchronous logging would add database write latency to every redirect, directly impacting user experience. Storing analytics in the same database as URL mappings would create write contention on the read-optimized URL store. The async approach also enables rich analytics processing  a stream processor can compute real-time dashboards, aggregate by geography, referrer, device type, and time windows. This is a classic CQRS (Command Query Responsibility Segregation) pattern used by virtually all production URL shorteners."},{question:"What is consistent hashing and why is it useful for a URL shortener?",options:["It ensures all hash values are the same length","It minimizes key redistribution when adding or removing database shards","It prevents hash collisions","It ensures URLs are distributed alphabetically"],correctIndex:1,explanation:"Consistent hashing maps both keys and servers onto a hash ring, so each key is assigned to the nearest server clockwise on the ring. When a server is added or removed, only the keys that map to the affected segment of the ring need to be redistributed  typically only K/N keys (where K is total keys and N is number of servers) rather than remapping everything. For a URL shortener with billions of entries across many database shards, this means adding a new shard to handle growth only requires migrating a fraction of existing data. Without consistent hashing, adding a shard with simple modular hashing (key % N) would require redistributing nearly all keys. Virtual nodes (multiple points per server on the ring) further improve distribution uniformity."},{question:"What is the difference between a URL shortener using random key generation vs. sequential counter-based keys?",options:["Random keys are shorter than sequential ones","Random keys don't reveal information about URL creation order or volume, while sequential keys are predictable","Sequential keys are impossible to implement at scale","Random keys never collide"],correctIndex:1,explanation:"Random keys (e.g., 'xK9mQ2z') provide security through unpredictability  an attacker cannot guess other valid short URLs by incrementing a counter. Sequential keys (e.g., '0000001', '0000002') reveal business metrics (total URL count, creation rate) and allow enumeration attacks where someone scrapes all URLs by iterating through the sequence. However, sequential keys have the advantage of guaranteed uniqueness without collision checking, which is why KGS uses pre-generated random keys  combining the uniqueness guarantee of sequential generation with the unpredictability of random keys. In practice, most production URL shorteners use random or pseudo-random keys, sometimes with a distributed ID generator like Twitter's Snowflake as a seed."},{question:"How should a URL shortener handle the same long URL being shortened multiple times?",options:["Always return the same short URL (deduplication)","Always generate a new short URL each time","It depends on the product requirements  dedup saves space but prevents per-context analytics","Return an error for duplicate URLs"],correctIndex:2,explanation:"This is a product decision with engineering trade-offs. Deduplication (returning the same short URL for the same long URL) saves storage and key space but means you can't track analytics separately for each context where the URL was shared. Generating a new short URL each time uses more storage but enables per-link analytics  a marketer sharing the same article in an email vs. a tweet wants separate click counts. Most commercial shorteners like bit.ly create a new short URL each time because analytics per-link is their core value proposition. If you do want dedup, you'd need an index on the long URL column, which adds write overhead and storage for the index itself. The interviewer wants to hear you articulate this trade-off."},{question:"What is the purpose of the X-Forwarded-For header in URL shortener analytics?",options:["To forward the short URL to another server","To identify the original client IP address when the request passes through proxies or load balancers","To forward the URL to the destination","To encrypt the client's IP address"],correctIndex:1,explanation:"When a request passes through load balancers, reverse proxies, or CDN edge servers before reaching the URL shortener's application server, the source IP seen by the application is the proxy's IP, not the end user's. The X-Forwarded-For header preserves the original client IP chain, which is essential for geo-analytics (determining where clicks come from), rate limiting, and fraud detection. Without this header, all analytics would show your load balancer's IP as the source, making geographic and per-user analytics useless. In production, you must validate this header carefully  it can be spoofed by clients, so you typically trust only the rightmost IP added by your own infrastructure. This is a common detail that differentiates senior engineers in system design discussions."},{question:"Why is eventual consistency acceptable for a URL shortener's read path?",options:["Because URLs change frequently","Because a short delay in propagating new URL mappings to read replicas is tolerable  the URL will resolve correctly within seconds","Because consistency doesn't matter for any system","Because users don't notice errors"],correctIndex:1,explanation:"When a user creates a short URL, there may be a brief delay (milliseconds to seconds) before the mapping propagates to all read replicas and cache nodes. This is acceptable because users rarely create a short URL and click it within the same second  typically they share it and others click it later. The trade-off is worth it because strong consistency would require synchronous replication to all nodes before returning the creation response, significantly increasing write latency and reducing availability. This is a textbook application of the CAP theorem  for a URL shortener, we prefer availability and partition tolerance (AP) over strict consistency. In the rare case a click arrives before replication completes, a cache miss falls through to the primary database, which has the data."},{question:"How would you design the database schema for a URL shortener?",options:["Single table with columns: short_key (PK), long_url, user_id, created_at, expires_at","Normalized schema with separate tables for URLs, users, clicks, and tags","Store everything in a single JSON document","Use a graph schema with URL nodes and user edges"],correctIndex:0,explanation:"A simple single-table design with short_key as the primary key is ideal because the dominant access pattern is point lookups by short key. The table contains: short_key (partition key), long_url, user_id (who created it), created_at (timestamp), and expires_at (optional TTL). Analytics data should be stored separately (in a time-series database or data warehouse) rather than in the URL mapping table to avoid write contention. A normalized schema with joins would add unnecessary latency to the redirect hot path. Additional secondary indexes on user_id enable 'list my URLs' functionality but aren't needed for the core redirect path. This denormalized design is a perfect fit for DynamoDB or Cassandra."},{question:"What is the role of a load balancer in a URL shortener architecture?",options:["To shorten URLs faster","To distribute incoming redirect requests across multiple application servers for high availability and throughput","To cache URL mappings","To generate short keys"],correctIndex:1,explanation:"A load balancer sits between clients and the URL shortener's application servers, distributing incoming requests (both URL creation and redirect) across multiple server instances. This provides horizontal scalability (add more servers to handle more traffic), high availability (if one server dies, traffic routes to others), and even load distribution. For a URL shortener with potentially millions of redirects per second, a single server cannot handle the load. Layer 7 (application) load balancers can also perform health checks, SSL termination, and request routing. In production, you'd typically use multiple layers  a DNS-based global load balancer (like AWS Route 53) for geographic routing, and an L7 load balancer (like nginx or AWS ALB) within each region."},{question:"What is the significance of using PUT vs POST for creating short URLs in a RESTful API?",options:["PUT is faster than POST","PUT is idempotent (same request = same result), making it suitable for custom aliases, while POST is for server-generated keys","PUT supports longer URLs","There is no difference"],correctIndex:1,explanation:"In REST semantics, PUT is idempotent  sending the same PUT request multiple times produces the same result. This maps well to custom aliases: PUT /urls/my-custom-alias with a body containing the long URL always creates or updates the same mapping. POST is non-idempotent, meaning each call can create a different resource  appropriate when the server generates the short key, as each POST creates a new unique short URL. This distinction matters for retry logic: if a PUT request times out, the client can safely retry without creating duplicates. With POST, a retry might create a duplicate entry. In system design interviews, discussing this shows understanding of API design principles and their implications for distributed systems reliability."},{question:"How would you implement a URL shortener that needs to support 1 billion URLs with 10,000 redirects per second?",options:["Single powerful server with lots of RAM","Microservices architecture with separate read/write services, NoSQL storage, caching layer, and CDN","Serverless functions for each redirect","Peer-to-peer architecture"],correctIndex:1,explanation:"At this scale, you need a distributed architecture with clear separation of concerns. The write service handles URL creation through a KGS, storing mappings in a horizontally-sharded NoSQL database (like DynamoDB or Cassandra). The read service handles redirects, first checking a distributed cache layer (Redis cluster) that holds hot URLs  with a typical 80/20 distribution, caching the top 20% of URLs serves 80% of requests. A CDN layer in front handles geographically distributed traffic. 10,000 redirects/second is modest for this architecture  each Redis node can handle 100K+ operations/second. A single server would be a single point of failure and couldn't scale. Serverless has cold start issues that would add unacceptable latency to redirects. This layered approach is how real URL shorteners operate at scale."},{question:"What is URL canonicalization and why is it important for a URL shortener?",options:["Making URLs prettier","Converting equivalent URLs to a standard form to prevent duplicate entries for the same destination","Encrypting URLs for security","Compressing URLs to save space"],correctIndex:1,explanation:"URL canonicalization converts URLs to a standard form so that equivalent URLs map to the same short URL (if deduplication is desired). For example, 'HTTP://Example.COM/page' and 'http://example.com/page' are equivalent but textually different. Canonicalization includes: lowercasing the scheme and host, removing default ports (80 for HTTP, 443 for HTTPS), removing trailing slashes, sorting query parameters, and removing unnecessary fragments. Without canonicalization, the same destination could consume multiple short keys unnecessarily. However, over-aggressive canonicalization can be dangerous  removing query parameters or fragments might change the page content. The implementation should be configurable and well-tested against edge cases like URLs with authentication credentials or unusual encoding."},{question:"What security concern arises from URL shorteners and how can it be mitigated?",options:["Short URLs use too much bandwidth","URL shorteners can be used to disguise malicious/phishing URLs behind trusted-looking short domains","Short URLs expire too quickly","URL shorteners slow down page loads"],correctIndex:1,explanation:"URL shorteners fundamentally hide the destination behind a short, opaque link, making them ideal tools for phishing attacks. An attacker can shorten a malicious URL (e.g., a fake banking login page) and share the innocent-looking short link. Mitigations include: scanning destination URLs against malware/phishing blacklists (Google Safe Browsing, PhishTank) at creation time AND periodically re-scanning existing URLs, implementing preview pages (like bit.ly's '+' suffix) that show the destination before redirecting, adding interstitial warning pages for suspicious destinations, and flagging URLs with multiple reports. Some browsers and email clients also preview short URLs before following them. This is why URL shortener services bear significant responsibility for internet safety."},{question:"How does a distributed counter work for URL shortener analytics?",options:["Each server maintains its own counter independently","Use a centralized counter with locks","Approximate counting with HyperLogLog for unique visitors and append-only logs for total clicks","Counters are stored in the URL database"],correctIndex:2,explanation:"At scale, maintaining exact real-time counters for every short URL is impractical because it would require a synchronized write for every redirect. Instead, production systems use approximate counting structures like HyperLogLog for unique visitor counts (which uses only ~12KB of memory per counter regardless of cardinality) and append-only event logs (Kafka/Kinesis) for total click streams. Click events are asynchronously consumed and aggregated in batch or stream processing systems (like Apache Flink or Spark Streaming) to update dashboards. For rough real-time counts, Redis INCR commands with eventual aggregation work well. This separation of the real-time redirect path from the analytics aggregation path is a fundamental principle of scalable system design."},{question:"What is the advantage of using a Bloom filter in a URL shortener?",options:["It compresses URLs","It provides a space-efficient probabilistic check for whether a short key already exists, reducing database lookups","It encrypts URL mappings","It speeds up URL redirects"],correctIndex:1,explanation:"A Bloom filter is a probabilistic data structure that can tell you with certainty when an element is NOT in a set, and with high probability when it IS in a set (allowing small false positive rates). For a URL shortener using hash-based key generation, before checking the database for a collision, you first check the Bloom filter  if it says the key doesn't exist, you skip the database lookup entirely. This dramatically reduces database reads during URL creation. The false positive rate can be tuned by adjusting the filter size (typically a few bits per element). For 1 billion URLs, a Bloom filter with 1% false positive rate requires only about 1.2 GB of memory. However, Bloom filters cannot delete entries, which complicates URL expiration  counting Bloom filters or cuckoo filters address this limitation."},{question:"What is the purpose of using a separate read replica database in a URL shortener?",options:["To store different types of data","To handle high read throughput by distributing redirect queries across multiple database copies","To provide backup in case of data loss","To speed up write operations"],correctIndex:1,explanation:"Read replicas are copies of the primary database that handle read queries (redirect lookups) while the primary handles writes (URL creation). Given the extreme read-to-write ratio (100:1 or higher) of URL shorteners, this architecture allows you to scale read capacity independently by adding more replicas. Each replica can serve redirect queries, and a load balancer distributes read traffic across them. While replicas also provide some backup capability, that's not their primary purpose  dedicated backup solutions are separate. Write operations aren't sped up by replicas; in fact, writes may be slightly slower due to replication overhead. This is a standard pattern for read-heavy workloads and is used extensively in production systems at companies like Twitter and Pinterest."},{question:"How would you handle a database failure in a URL shortener?",options:["Show a 500 error for all requests","Use circuit breaker pattern with fallback to cache, and redirect to a status page for cache misses","Retry indefinitely until the database recovers","Queue all requests until the database comes back"],correctIndex:1,explanation:"The circuit breaker pattern detects database failures and stops sending requests to the failing database, preventing cascade failures. For redirects, the system falls back to the cache layer  if the short URL is cached (likely for hot URLs), the redirect works normally despite the database being down. For cache misses, return a friendly error page or redirect to a status page. For URL creation, requests can be queued in a message queue (like Kafka) for processing when the database recovers. Infinite retries would overwhelm the recovering database and degrade the entire system. This graceful degradation approach ensures that the majority of redirects (those hitting cache) continue working during database outages, maintaining high availability where it matters most."},{question:"What is the trade-off between using auto-increment IDs vs. random IDs for short URL keys?",options:["Auto-increment is always better because it's simpler","Auto-increment reveals business metrics and is predictable; random IDs are secure but require collision handling","Random IDs are always better because they're faster","There is no meaningful trade-off"],correctIndex:1,explanation:"Auto-increment IDs (1, 2, 3...) converted to base62 are simple and guarantee uniqueness, but they expose business information  anyone can estimate total URLs created and creation rate by decoding the latest short URL. They're also sequentially predictable, enabling enumeration attacks. Random IDs provide security through unpredictability but require collision detection (checking if the random key already exists). The KGS approach solves this by pre-generating random unique keys, giving you both unpredictability and guaranteed uniqueness. In a distributed system, auto-increment also requires coordination (like Twitter's Snowflake) to avoid duplicate IDs across servers, while random keys from KGS can be distributed to multiple servers from a shared pool."},{question:"Why is it important to URL-encode the long URL before storing it in a URL shortener?",options:["To make it shorter","To ensure special characters in the URL don't break storage, retrieval, or redirect operations","To encrypt the URL","To compress the URL"],correctIndex:1,explanation:"URLs can contain special characters (spaces, Unicode, ampersands, quotes) that could break database queries, HTTP redirect headers, or JSON API responses if not properly encoded. Storing the properly encoded form ensures the URL is faithfully preserved and can be safely included in the HTTP Location header during redirect. For example, a URL containing a space should be stored with %20 encoding. Additionally, different databases handle special characters differently  proper encoding provides a consistent representation regardless of the storage backend. However, you should also store the original (decoded) URL for display purposes, as showing '%20' instead of spaces in analytics dashboards provides a poor user experience."},{question:"What monitoring metrics are most important for a URL shortener service?",options:["Only tracking total number of URLs created","Redirect latency (p50/p95/p99), cache hit ratio, creation throughput, error rates, and database query latency","Only monitoring disk space usage","Only tracking the number of active users"],correctIndex:1,explanation:"Comprehensive monitoring is critical for a URL shortener's reliability. Redirect latency percentiles (p50, p95, p99) tell you about user experience  p99 matters because 1% of millions of daily users is still thousands of people. Cache hit ratio indicates cache effectiveness  a drop might signal a change in traffic patterns or cache issues. Creation throughput and error rates detect abuse or system problems. Database query latency detects storage layer issues before they become user-facing. Additional metrics include: KGS key pool size (to prevent running out of keys), queue depth for analytics events, and per-URL redirect rates for detecting abuse. Setting up alerts on these metrics enables proactive incident response  for example, if cache hit ratio drops below 90%, the on-call engineer investigates immediately."},{question:"How would you implement URL shortener expiration at database level in DynamoDB?",options:["Use a cron job to scan the entire table","Use DynamoDB's built-in TTL (Time-To-Live) feature which automatically deletes expired items","Manually delete items on each read","Set up a separate expiration service"],correctIndex:1,explanation:"DynamoDB's TTL feature allows you to specify an attribute containing an expiration timestamp (Unix epoch). DynamoDB automatically scans for and deletes expired items in the background at no additional cost, without consuming write capacity. This is ideal for URL shortener expiration because it requires zero application-level code for cleanup. The deletion is eventually consistent  items may persist briefly after their TTL  so your application should still check expiration on reads. A full table scan would be prohibitively expensive at scale (billions of items), and manual deletion on reads would only clean up accessed items, leaving orphaned expired URLs consuming storage. The freed-up short keys can be recycled back to the KGS pool through a DynamoDB Streams trigger that captures deletion events."},{question:"What is the role of a message queue in a URL shortener architecture?",options:["To queue redirect requests during peak traffic","To decouple URL creation from analytics processing and handle asynchronous tasks like link scanning and expiration","To store URL mappings temporarily","To communicate between frontend and backend"],correctIndex:1,explanation:"A message queue (like Kafka, RabbitMQ, or SQS) serves multiple asynchronous purposes in a URL shortener. First, click analytics: every redirect publishes an event to the queue, and separate analytics consumers aggregate the data without impacting redirect latency. Second, link safety scanning: newly created URLs are queued for asynchronous malware/phishing analysis, so the creation API returns quickly while scanning happens in the background. Third, expired URL cleanup: expiration events trigger key recycling back to the KGS pool. This decoupling is fundamental to building scalable systems  the synchronous redirect path stays fast and simple while complex processing happens asynchronously. Without a message queue, you'd need to perform all these operations synchronously, dramatically increasing latency and coupling between components."},{question:"How do you handle the 'thundering herd' problem for a popular short URL?",options:["Block all requests until one completes","Use request coalescing (single-flight) where concurrent requests for the same key share one database lookup","Increase database capacity","Disable the URL temporarily"],correctIndex:1,explanation:"When a viral short URL's cache entry expires, hundreds of concurrent requests might simultaneously miss the cache and hit the database  this is the 'thundering herd' or 'cache stampede' problem. Request coalescing (also called single-flight or request collapsing) ensures that only the first request actually queries the database, while all concurrent requests for the same key wait for and share that single result. Libraries like Go's singleflight or custom implementations with distributed locks (Redis SETNX) achieve this. Additional mitigations include: staggered cache TTLs (adding random jitter to prevent synchronized expiration), cache-aside with early refresh (refreshing entries before they expire), and background cache warming for known hot URLs. This is a critical concept that shows deep understanding of caching challenges in system design interviews."},{question:"What is the CAP theorem trade-off for a URL shortener?",options:["URL shorteners need strong consistency above all","URL shorteners typically choose AP (Availability + Partition Tolerance) with eventual consistency, since brief stale reads are acceptable","URL shorteners need CP (Consistency + Partition Tolerance)","The CAP theorem doesn't apply to URL shorteners"],correctIndex:1,explanation:"The CAP theorem states that during a network partition, a distributed system must choose between Consistency (all nodes see the same data) and Availability (every request gets a response). For a URL shortener, availability is paramount  returning a 503 error for a redirect is a worse user experience than a brief period where a newly-created URL isn't yet resolvable on all nodes. Eventual consistency means that after a new URL is created, it might take milliseconds to seconds for the mapping to propagate to all read replicas and cache nodes, which is acceptable because users rarely click a short URL within milliseconds of creating it. This AP choice aligns with DynamoDB's and Cassandra's default modes, which is another reason these databases are popular choices for URL shorteners."},{question:"How should a URL shortener handle internationalized (non-ASCII) long URLs?",options:["Reject non-ASCII URLs","Accept and store them with proper percent-encoding (Punycode for domains, UTF-8 percent-encoding for paths)","Convert them to ASCII by removing non-ASCII characters","Store them as-is without any encoding"],correctIndex:1,explanation:"Internationalized URLs contain non-ASCII characters in domain names (IDN) and paths. The correct approach is to convert domain names using Punycode (e.g., 'mnchen.de'  'xn--mnchen-3ya.de') and percent-encode path segments using UTF-8 encoding (e.g., ''  '%D0%BF%D1%83%D1%82%D1%8C'). This produces a valid ASCII URL that browsers and HTTP clients can handle. Rejecting non-ASCII URLs would exclude a huge portion of the international web. Storing them as-is could cause issues with HTTP headers (which must be ASCII) and database encoding. The display URL shown to users should preserve the original Unicode characters for readability, while the stored/redirected URL uses the encoded form. This internationalization support is increasingly important as internet usage grows globally."},{question:"What happens if the Key Generation Service (KGS) becomes unavailable?",options:["All URL creation stops immediately","Pre-fetched key batches on application servers allow URL creation to continue temporarily until KGS recovers or local batches are exhausted","The system switches to hash-based generation as a fallback","Nothing  KGS is not a critical service"],correctIndex:1,explanation:"To avoid KGS being a single point of failure, application servers pre-fetch batches of keys from KGS (e.g., 1000 keys at a time) and store them in local memory. When KGS goes down, servers continue creating URLs using their local key batches, buying time for KGS recovery. Once local batches are exhausted, the system can either fail gracefully with a 503 or fall back to a hash-based approach with collision detection. For high availability, KGS itself should be replicated with a primary-standby setup and use a persistent database (not just in-memory) for the unused key pool. The key batch size represents a trade-off: larger batches provide more buffer during outages but mean more wasted keys if a server crashes before using its batch."},{question:"Why is it important to log the User-Agent header for URL shortener analytics?",options:["To verify the URL format","To identify the device type, browser, and OS of users clicking short URLs, enabling device-specific analytics","To authenticate users","To speed up redirects"],correctIndex:1,explanation:"The User-Agent header contains information about the client's browser, operating system, and device type (mobile, desktop, bot). For URL shortener analytics, this data enables: device breakdown charts (60% mobile, 35% desktop, 5% tablet), browser statistics, OS distribution, and critically, bot detection  distinguishing real human clicks from automated crawlers and link preview generators (like Facebook's, Twitter's, and Slack's preview bots). Without filtering bot traffic, click analytics would be severely inflated and misleading. Modern User-Agent parsing libraries (like ua-parser) can extract structured device information from the raw header string. This granular analytics is often the premium feature that URL shortener services monetize."},{question:"How would you design the URL shortener's API for creating short URLs?",options:["GET /create?url=<long_url>","POST /api/v1/urls with JSON body containing the long URL, optional custom alias, and expiration","PUT /shorten/<long_url>","DELETE /api/v1/urls"],correctIndex:1,explanation:"A well-designed REST API uses POST for resource creation (since each call creates a new short URL), versioned endpoints (/api/v1/) for backward compatibility, and a JSON request body for structured parameters. The body should include: 'long_url' (required), 'custom_alias' (optional), 'expires_at' (optional), and any metadata. The response should return the short URL, creation timestamp, and expiration details. Using GET for creation is an anti-pattern because GET should be idempotent and safe (no side effects), and URLs have length limits. API versioning prevents breaking existing clients when you evolve the API. Authentication via API key or OAuth token in the Authorization header enables per-user rate limiting and analytics. This RESTful design is what interviewers expect in system design discussions."},{question:"What is the benefit of using Redis as a caching layer for a URL shortener?",options:["Redis supports complex SQL queries","Redis provides sub-millisecond read latency, built-in TTL support, and data structures like HyperLogLog for analytics","Redis automatically scales horizontally without configuration","Redis provides strong ACID guarantees"],correctIndex:1,explanation:"Redis is an in-memory data store that provides sub-millisecond read latency  critical for URL shortener redirects where every millisecond matters. Its built-in TTL (Time-To-Live) support naturally handles cache expiration, and it supports data structures beyond simple key-value: HyperLogLog for approximate unique visitor counting (using only 12KB per counter), sorted sets for top-URL leaderboards, and bitmaps for daily active URL tracking. Redis Cluster provides horizontal scaling through hash-slot-based sharding. While Redis doesn't 'automatically' scale (it requires cluster configuration), it's operationally simpler than many alternatives. Combined with the URL shortener's simple key-value access pattern, Redis achieves cache hit response times of 0.1-0.5ms, compared to 5-20ms for database lookups. This order-of-magnitude improvement is why Redis is ubiquitous in URL shortener architectures."},{question:"How would you implement a 'link preview' feature for a URL shortener?",options:["Scrape the destination page in real-time when the short URL is accessed","Pre-fetch and cache Open Graph metadata (title, description, image) when the short URL is created, with periodic refresh","Embed the destination page in an iframe","Ask the user to provide preview information manually"],correctIndex:1,explanation:"Pre-fetching Open Graph (og:title, og:description, og:image) and Twitter Card metadata at URL creation time provides instant link previews without impacting redirect latency. The metadata is stored alongside the URL mapping and served via an API endpoint (e.g., GET /api/v1/urls/{key}/preview). Periodic background refreshes ensure metadata stays current if the destination page changes. Real-time scraping at access time would add seconds of latency and create a DoS vector. Iframes have cross-origin security restrictions and poor mobile experience. Requiring manual input adds friction to URL creation. This pre-fetch approach is exactly how Slack, Discord, and social media platforms generate link previews  they fetch the destination's metadata and cache it for fast display."},{question:"What is the difference between URL shortening and URL aliasing?",options:["They are exactly the same thing","Shortening generates a random/auto key; aliasing lets users choose a custom, meaningful short path (e.g., bit.ly/my-brand)","Aliasing only works with certain URLs","Shortening is faster than aliasing"],correctIndex:1,explanation:"URL shortening generates an opaque, system-assigned key (like 'xK9mQ2z') optimized for brevity, while URL aliasing (also called custom short links or vanity URLs) lets users choose a meaningful, branded path (like 'bit.ly/summer-sale-2024'). Aliasing requires additional validation: checking that the custom alias is available, doesn't conflict with reserved words or system paths, meets length/character requirements, and isn't offensive. Custom aliases are a premium feature in most URL shortener business models because they provide brand value. The technical implementation differs too: auto-generated keys come from KGS, while custom aliases require a real-time uniqueness check against the database. Both map to the same underlying redirect mechanism, but the creation paths are distinct."},{question:"How would you prevent a URL shortener from being used as an open redirect vulnerability?",options:["Only allow URLs from a whitelist of approved domains","Validate destination URLs, implement abuse detection, add warning interstitials for suspicious URLs, and support user reporting","Block all external URLs","Require CAPTCHA for every redirect"],correctIndex:1,explanation:"Open redirect vulnerabilities occur when an attacker uses your trusted short URL domain to redirect users to malicious sites, bypassing security filters that trust your domain. A multi-layered defense includes: URL validation at creation time (checking against phishing/malware databases), real-time abuse detection (monitoring for patterns like bulk creation of URLs pointing to the same suspicious domain), interstitial warning pages for URLs flagged as potentially dangerous, and a user reporting mechanism for URLs that bypass automated detection. Domain whitelisting is too restrictive for a general-purpose shortener, and CAPTCHAs on redirects would destroy user experience. This defense-in-depth approach is standard practice  Google, Microsoft, and other major shortener operators continuously scan and block millions of malicious URLs."},{question:"In a URL shortener, what is a 'warm-up' cache strategy?",options:["Heating up the server hardware","Pre-loading frequently accessed URL mappings into cache before traffic arrives, typically after a deployment or cache flush","Gradually increasing rate limits","Slowly starting the application server"],correctIndex:1,explanation:"Cache warm-up involves pre-populating the cache with known hot URL mappings before the cache starts receiving production traffic. This is critical after events like: deploying a new cache cluster, recovering from a cache failure, or performing cache maintenance. Without warm-up, the cold cache would cause all requests to hit the database simultaneously (a thundering herd), potentially overwhelming it. Warm-up strategies include: replaying recent access logs to identify hot keys, querying the database for the most-accessed URLs, or gradually shifting traffic to the new cache while the old one is still serving. Some systems maintain a 'hot key list' specifically for this purpose. Cache warm-up is an operational detail that demonstrates production experience in system design interviews."},{question:"What is the advantage of using a CDN with anycast for a URL shortener?",options:["It makes URLs shorter","Anycast routes users to the nearest CDN edge server automatically via BGP, minimizing redirect latency globally","It prevents DDoS attacks","It compresses URL data"],correctIndex:1,explanation:"Anycast is a network routing technique where multiple servers share the same IP address, and BGP routing directs each client to the topologically nearest server. For a URL shortener CDN, this means a user in Sydney automatically reaches an edge server in Australia rather than a server in the US, reducing redirect latency from ~200ms to ~5ms. Unlike DNS-based geographic routing which has TTL-based delays, anycast routing is immediate and transparent. Additionally, anycast provides natural DDoS resilience by distributing attack traffic across all edge locations. While DDoS protection is a side benefit, the primary advantage is latency reduction. Cloudflare and other major CDN providers use anycast extensively, and URL shortener services running on these CDNs automatically benefit from this routing optimization."},{question:"How do you handle database migration or schema changes in a running URL shortener service?",options:["Shut down the service and perform the migration","Use online schema migration tools (like pt-online-schema-change or gh-ost) with blue-green or rolling deployments","Create a new database and copy all data","Schema changes are not needed for URL shorteners"],correctIndex:1,explanation:"Zero-downtime schema migrations are essential for a URL shortener serving millions of requests per second. Tools like gh-ost (GitHub's online schema tool) or pt-online-schema-change create a shadow copy of the table, apply the schema change, copy data incrementally, and swap tables atomically. For NoSQL databases, schema evolution is handled at the application level with backward-compatible changes (adding optional fields, handling missing fields gracefully). Blue-green deployments maintain two production environments and switch traffic after verification. Shutting down is unacceptable for a service that's part of the web's redirect infrastructure  a major shortener outage breaks millions of links across the internet. This operational awareness is what distinguishes senior engineers in system design interviews."},{question:"What is the impact of HTTPS on URL shortener performance?",options:["HTTPS has no impact on performance","TLS handshakes add latency to each new connection, but connection reuse and TLS 1.3 minimize the overhead","HTTPS makes URLs shorter","HTTPS prevents URL shortening"],correctIndex:1,explanation:"HTTPS requires a TLS handshake before any data is exchanged, adding 1-2 round trips (with TLS 1.2) or 1 round trip (with TLS 1.3's 0-RTT) to each new connection. For a URL shortener where each redirect is often a new connection, this overhead is significant  potentially adding 50-200ms per redirect depending on geographic distance. Mitigations include: TLS 1.3 with 0-RTT resumption, TLS session tickets for returning visitors, HTTP/2 connection pooling, and CDN edge termination (which terminates TLS close to the user). Despite the overhead, HTTPS is mandatory for modern URL shorteners  browsers increasingly warn about HTTP-only sites, and HTTPS prevents intermediate network manipulation of redirects. Certificate management at scale (via Let's Encrypt or AWS ACM) is also an operational consideration."},{question:"How would you design a URL shortener to support A/B testing with split redirects?",options:["Create two separate short URLs","Allow a single short URL to redirect to different destinations based on configurable traffic percentages","Randomly change the destination URL","Use cookies to track A/B assignments"],correctIndex:1,explanation:"Split redirect (also called split URL testing) allows a single short URL to redirect different percentages of traffic to different destination URLs. For example, 50% of clicks on 'sho.rt/campaign' go to page A and 50% to page B. Implementation requires: storing multiple destination URLs with traffic weights per short key, a deterministic assignment mechanism (hashing the request IP or generating a random number per request), and analytics that track conversions per variant. This is a powerful marketing feature  marketers can test different landing pages without creating multiple short URLs. Cookie-based tracking helps maintain consistent assignment for returning users but requires the redirect response to set a cookie before redirecting, adding complexity. This feature differentiates premium URL shortener services from basic ones."},{question:"What are the implications of GDPR on URL shortener design?",options:["GDPR doesn't apply to URL shorteners","URL shorteners must handle personal data (IP addresses, analytics) with consent, retention limits, and data deletion capabilities","GDPR only requires HTTPS","GDPR requires URLs to be encrypted"],correctIndex:1,explanation:"GDPR considers IP addresses and device information as personal data, which URL shorteners collect during redirects for analytics. Compliance requires: a clear privacy policy explaining what data is collected and why, a legal basis for processing (typically legitimate interest for basic functionality, consent for detailed analytics), data retention limits (not storing click data indefinitely), data deletion capabilities (right to erasure  users can request deletion of their data), and data processing agreements with third-party analytics providers. For EU users, you may need to avoid logging IP addresses or anonymize them (truncating the last octet). The 'right to be forgotten' means your analytics pipeline must support deleting specific user data retroactively. These privacy requirements significantly impact the analytics architecture and should be discussed in system design interviews."},{question:"How would you implement URL shortener high availability across multiple data centers?",options:["Run everything in one data center with backups","Deploy in multiple regions with active-active configuration, cross-region database replication, and global DNS routing","Use a single database shared across regions","Only replicate the cache layer across regions"],correctIndex:1,explanation:"Multi-region active-active deployment ensures the URL shortener remains available even if an entire data center or region fails. Each region runs a complete stack: load balancers, application servers, cache layer, and database nodes. Cross-region database replication (using Cassandra's multi-DC replication or DynamoDB Global Tables) keeps URL mappings synchronized. Global DNS routing (AWS Route 53 latency-based routing or Cloudflare's anycast) directs users to the nearest healthy region. Write conflicts are handled with last-writer-wins or vector clocks depending on the database. This architecture provides both high availability (surviving regional outages) and low latency (serving users from nearby regions). A single data center, no matter how well-provisioned, represents a single point of failure  natural disasters, network partitions, or power outages can take down an entire region."},{question:"What is the purpose of the HTTP 307 status code in URL shortening?",options:["It means the URL was not found","It's a temporary redirect that preserves the HTTP method (POST stays POST), unlike 302 which may change POST to GET","It's a permanent redirect like 301","It means the server is unavailable"],correctIndex:1,explanation:"HTTP 307 (Temporary Redirect) is similar to 302 but with a crucial difference: 307 guarantees that the HTTP method and body are preserved across the redirect. With 302, browsers historically changed POST requests to GET requests when following the redirect (despite the spec saying they shouldn't). For a URL shortener, this distinction matters if short URLs are used for API endpoints or form submissions where the POST method must be preserved. For simple click redirects (GET requests), 302 and 307 behave identically. The 308 status code similarly preserves the method but for permanent redirects (equivalent to 301). Understanding these subtle HTTP semantics demonstrates deep web protocol knowledge in system design interviews."},{question:"How would you implement rate limiting for the URL shortener's redirect endpoint?",options:["Apply the same strict limits as the creation endpoint","Use lenient limits (e.g., 1000 requests/minute/IP) to protect against DDoS while allowing normal viral traffic patterns","No rate limiting needed for redirects","Block IPs after 10 redirects"],correctIndex:1,explanation:"The redirect endpoint needs rate limiting but with much higher thresholds than the creation endpoint because legitimate use cases include: a popular link being clicked thousands of times per minute from the same corporate proxy IP, automated monitoring checking link health, and social media platforms generating link previews. Overly aggressive limits would block legitimate traffic and degrade the service's core value proposition. A reasonable approach uses tiered rate limiting: lenient per-IP limits (e.g., 1000/min) with stricter per-URL limits if a single URL receives suspicious traffic patterns (like exactly 100 requests/second from different IPs  a botnet pattern). DDoS protection should be handled at the infrastructure layer (CDN/WAF) rather than the application layer for the redirect path, as the goal is to keep redirect latency minimal."},{question:"Why might you choose Cassandra over MongoDB for a URL shortener's primary database?",options:["Cassandra supports SQL queries","Cassandra provides linear horizontal scalability, tunable consistency, and no single point of failure, ideal for the simple partition-key lookup pattern","MongoDB is not a NoSQL database","Cassandra is always faster for all workloads"],correctIndex:1,explanation:"Cassandra excels at the URL shortener's access pattern for several reasons: its partition-key-based architecture maps perfectly to short-key lookups (O(1) reads), it scales linearly by adding nodes (no master node bottleneck), it supports multi-datacenter replication natively, and it offers tunable consistency (use ONE for fast reads, QUORUM for consistent writes). MongoDB would work but uses a primary-secondary architecture that creates a single point of failure for writes and requires more complex sharding configuration. Cassandra's ring-based architecture distributes data evenly and handles node failures gracefully  any node can serve any request. However, Cassandra's write-optimized LSM-tree storage is overkill for a read-heavy URL shortener, and it lacks the rich query capabilities of MongoDB, which aren't needed for this use case anyway."},{question:"What is the difference between URL shortener keys generated with counter-based vs. random approaches in a distributed system?",options:["Counter-based is impossible in distributed systems","Counter-based requires coordination between servers (like Snowflake IDs) to avoid duplicates, while random approaches need collision detection but require no coordination","Random approaches never produce duplicates","Counter-based is always preferred"],correctIndex:1,explanation:"In a distributed system, a simple auto-increment counter doesn't work because multiple servers would generate the same numbers. Coordination mechanisms like Twitter's Snowflake solve this by combining: a timestamp, a worker ID (unique per server), and a per-server sequence number  guaranteeing globally unique IDs without inter-server communication. The trade-off is that Snowflake IDs are 64-bit (12+ characters in base62)  longer than desired for a URL shortener. Random approaches (random 7-character base62 strings) produce shorter keys but require collision detection (check if key exists before using it). KGS bridges both worlds by pre-generating random unique keys centrally. The key insight for interviews: understand the coordination vs. collision trade-off and why KGS is the elegant solution that avoids both problems."},{question:"How does a URL shortener handle the 'hot key' problem in a sharded database?",options:["Ignore it  all keys are accessed equally","Replicate hot keys across multiple shards and use caching to absorb the majority of reads for popular URLs","Move hot keys to a separate database","Delete hot keys after they become too popular"],correctIndex:1,explanation:"In a sharded database, a viral URL can create a 'hot key'  a single shard receiving disproportionate traffic while others sit idle. The primary defense is caching: a Redis cluster absorbs the vast majority of reads for hot URLs, ensuring most requests never reach the database. For extreme cases (a URL going viral on the homepage of Reddit), you can: replicate the hot key's data across multiple cache nodes (read from any), use client-side caching on application servers, or prepend the cache key with a random prefix (e.g., 'shard-1-abc123', 'shard-2-abc123') to distribute the key across multiple cache nodes. This is the same problem Twitter faces with celebrity tweets and Instagram faces with popular accounts  caching with key replication is the universal solution."},{question:"What encryption should a URL shortener use for storing URLs at rest?",options:["No encryption needed  URLs are public data","Encrypt sensitive metadata (user IDs, analytics data) but URL mappings themselves may not need encryption since the URLs are publicly accessible","Encrypt everything with AES-256","Use homomorphic encryption"],correctIndex:1,explanation:"This is a nuanced question. The URL mappings themselves (short key  long URL) are essentially public data  anyone with the short URL can discover the long URL by clicking it. However, associated metadata deserves encryption: user account information, detailed analytics (who clicked what, when, from where), API keys, and any personally identifiable information. For compliance-sensitive deployments (healthcare, finance), encrypting everything at rest with AES-256 via database-level encryption (like DynamoDB's or RDS's encryption at rest) provides defense-in-depth with minimal performance impact. The key point for interviews: don't apply a one-size-fits-all approach. Threat model first, then decide what needs protection. Encrypting everything adds operational complexity (key management, rotation) that should be justified by actual security requirements."},{question:"How would you implement URL shortener search functionality (e.g., searching through created URLs)?",options:["Full table scan on every search","Use Elasticsearch or similar search engine indexed on long URL, custom alias, and creation metadata","Store URLs in alphabetical order","Use LIKE queries on the primary database"],correctIndex:1,explanation:"Full-text search across millions of URL mappings requires a dedicated search engine like Elasticsearch or Apache Solr. When a URL is created, an event is published to index the mapping in Elasticsearch, including: the long URL, custom alias (if any), creation timestamp, user ID, and any tags/metadata. This enables searching by destination domain, partial URL matches, tags, and creation date ranges. LIKE queries on the primary database (NoSQL or SQL) would be catastrophically slow at scale and could impact production traffic. Elasticsearch provides full-text search, fuzzy matching, and aggregations with sub-second response times even over billions of documents. This is a classic polyglot persistence pattern  use the right storage engine for each access pattern: key-value store for redirects, Elasticsearch for search, time-series for analytics."},{question:"What is base62 encoding?",options:["An encoding that uses 62 characters: a-z, A-Z, 0-9","An encoding that uses 62 bits","A compression algorithm","A hashing algorithm"],correctIndex:0,explanation:"Base62 encoding maps numbers to a 62-character alphabet consisting of lowercase letters (a-z, 26 chars), uppercase letters (A-Z, 26 chars), and digits (0-9, 10 chars), totaling 62 characters. Unlike base64, base62 excludes '+' and '/' which are not URL-safe, making it ideal for URL shortener keys. A numeric ID like 125 would be converted by repeatedly dividing by 62 and mapping remainders to characters. This encoding is reversible  given a base62 string, you can decode it back to the original number, which is useful when the short key is derived from a database auto-increment ID."},{question:"What is the main bottleneck in a URL shortener system?",options:["CPU processing","Database reads during redirect operations at peak traffic","Network bandwidth","Disk storage"],correctIndex:1,explanation:"The primary bottleneck is the database read path during redirects, especially at peak traffic. Each redirect requires looking up the short key to find the long URL, and at millions of redirects per second, even fast databases struggle. This is why caching is the single most important optimization  a well-tuned Redis cache with high hit ratios can reduce database load by 95%+. CPU is minimal (just a lookup and redirect), network bandwidth is small (redirect responses are tiny), and disk storage grows slowly. The read-heavy workload pattern (100:1 read-to-write ratio) means optimizing the read path has 100x more impact than optimizing writes."},{question:"What is the purpose of a 410 Gone HTTP response in a URL shortener?",options:["The server is temporarily unavailable","It indicates the short URL existed but has been permanently removed or expired","The URL was redirected","Authentication is required"],correctIndex:1,explanation:"HTTP 410 (Gone) explicitly tells clients and search engines that the resource existed previously but has been intentionally and permanently removed. This is more informative than a 404 (Not Found), which doesn't distinguish between 'never existed' and 'existed but was deleted.' For expired or manually deleted short URLs, 410 is semantically correct and helps search engines remove the URL from their index faster. This matters for SEO  if someone published the short URL and it's been shared widely, search engines need to know to stop indexing it. Returning 404 might cause search engines to keep retrying, while 410 is a clear signal to give up."},{question:"How many database reads does a single redirect operation require in an optimized URL shortener?",options:["At least 3 reads (key lookup, user lookup, analytics lookup)","One read  a single key-value lookup by the short key, or zero if cached","At least 5 reads including authentication","It varies based on URL length"],correctIndex:1,explanation:"In an optimized URL shortener, the redirect path involves a single key-value lookup: given the short key, retrieve the long URL. If the result is cached in Redis, it's zero database reads. The operation is O(1) in both the cache and a key-value database. There's no need to look up user information (that's only for the management API), no authentication required (short URLs are public), and analytics are written asynchronously rather than read. This simplicity is by design  the redirect path must be as fast as possible, and every additional lookup adds latency. This is why the data model should be denormalized for the redirect use case, with all necessary redirect information in a single record."},{question:"What is the purpose of the Location header in a URL shortener redirect response?",options:["It specifies the server's geographic location","It contains the destination URL that the browser should navigate to","It contains the short URL","It specifies the cache location"],correctIndex:1,explanation:"The HTTP Location header is a required component of 3xx redirect responses. When a URL shortener returns a 301 or 302 response, the Location header contains the full destination URL (the original long URL). The browser reads this header and automatically navigates to the specified URL. Without the Location header, the browser wouldn't know where to redirect. The response body is typically empty or contains a small HTML page with a meta refresh tag as a fallback for older clients that don't handle redirects properly. The Location header must contain a valid, absolute URL  relative URLs are technically allowed by HTTP/1.1 but can cause issues with some clients."},{question:"What is the impact of using a SQL database with indexes for a URL shortener at scale?",options:["SQL databases are always better than NoSQL","SQL databases work but B-tree indexes add write overhead and JOIN capabilities go unused for the simple key-value pattern","SQL databases cannot handle URL shortener workloads","SQL databases don't support indexing on string columns"],correctIndex:1,explanation:"SQL databases like PostgreSQL or MySQL can handle URL shortener workloads at moderate scale (millions of URLs). However, B-tree indexes on the short_key column add write amplification (each insert updates the index), and the relational features (JOINs, foreign keys, transactions) add overhead without providing value for the simple key-value access pattern. At billions of URLs with thousands of reads per second, SQL database sharding becomes complex (no native support in most SQL databases) compared to NoSQL databases that shard automatically. That said, if your scale doesn't require billions of entries, PostgreSQL with proper indexing and connection pooling (PgBouncer) is a perfectly valid choice  over-engineering with NoSQL for a small-scale service is equally problematic."},{question:"How does DNS resolution affect URL shortener latency?",options:["DNS has no impact on URL shortener latency","Each new connection requires DNS resolution of the short URL domain, adding 10-100ms that can be mitigated with low TTLs and DNS prefetching","DNS only affects the creation of short URLs","DNS resolution is handled by the database"],correctIndex:1,explanation:"Before a browser can connect to your URL shortener server, it must resolve the short URL domain name to an IP address via DNS. This DNS lookup adds 10-100ms depending on whether the result is cached in the user's DNS resolver. For a URL shortener where speed is critical, DNS optimization matters: use low but reasonable TTL values (300 seconds), ensure DNS records are available from multiple authoritative nameservers, and use a DNS provider with global anycast (like Cloudflare or Route 53). Websites can also add DNS prefetch hints (<link rel='dns-prefetch'>) for their short URL domains. After the first resolution, DNS caching makes subsequent lookups near-instant  but the first click on a short URL from a new device always pays the DNS tax."},{question:"What is the maximum URL length that a URL shortener should accept?",options:["No limit","A practical limit of 2,000-8,000 characters, matching browser and server constraints","Exactly 255 characters","Exactly 100 characters"],correctIndex:1,explanation:"While HTTP doesn't define a maximum URL length, practical limits exist: Internet Explorer historically supported only 2,083 characters, most modern browsers support 8,000-65,000 characters, and many web servers default to 8KB maximum header size (which includes the URL). A URL shortener should enforce a reasonable limit (e.g., 2,048 or 8,192 characters) that works across all major browsers and servers. Accepting unlimited-length URLs could be exploited for storage abuse or buffer overflow attacks. The limit should be documented in the API specification and enforced with a clear 414 (URI Too Long) error response. In practice, URLs longer than 2,000 characters are rare and often indicate encoding issues or tracking parameter bloat."},{question:"What is the role of connection pooling in a URL shortener's database layer?",options:["It pools multiple databases together","It reuses database connections across requests to avoid the overhead of creating new connections for each redirect","It compresses database queries","It caches query results"],correctIndex:1,explanation:"Creating a new database connection for each request involves TCP handshake, authentication, and SSL negotiation  adding 5-50ms of overhead per request. Connection pooling (using tools like PgBouncer for PostgreSQL, or built-in pools in application frameworks) maintains a pool of pre-established connections that are reused across requests. For a URL shortener handling thousands of redirects per second, this eliminates connection creation overhead entirely. The pool size should be tuned based on the database's max_connections setting and the number of application server instances. A common mistake is setting pool size too high, which can overwhelm the database with too many concurrent connections. A good starting point is: pool_size = (database_max_connections - reserved) / number_of_app_servers."},{question:"How should a URL shortener handle concurrent requests to create the same custom alias?",options:["Allow both requests to succeed","Use database-level unique constraints and optimistic locking  the first request succeeds, subsequent ones get a conflict error","Queue all requests and process them sequentially","Ignore the conflict and overwrite"],correctIndex:1,explanation:"Database-level unique constraints on the custom alias column provide atomic conflict detection: when two concurrent requests try to insert the same alias, the database guarantees exactly one succeeds while the other receives a unique constraint violation. The application catches this error and returns HTTP 409 (Conflict) to the losing request. This approach is more efficient than pessimistic locking (which would serialize all creation requests) or application-level checking (which has a race condition between check and insert). Optimistic concurrency is the standard pattern for handling concurrent writes in web applications. For distributed databases without unique constraints (like Cassandra), you can use lightweight transactions (IF NOT EXISTS) to achieve the same atomicity, though with higher latency."},{question:"What is the purpose of a WAF (Web Application Firewall) for a URL shortener?",options:["To shorten URLs faster","To protect against common web attacks like SQL injection, XSS, DDoS, and bot abuse targeting the shortener's API","To compress HTTP responses","To cache URL mappings"],correctIndex:1,explanation:"A WAF sits in front of the URL shortener's API and inspects incoming requests for malicious patterns. It protects against: SQL injection in URL parameters (even with NoSQL, injection attacks exist), cross-site scripting (XSS) in custom alias fields, DDoS attacks by rate-limiting at the edge, bot abuse (automated bulk URL creation for spam), and request smuggling attacks. Cloud WAF services (like Cloudflare WAF or AWS WAF) can also enforce geographic restrictions, block known malicious IPs, and implement custom rules (e.g., blocking requests containing known phishing domains). For a URL shortener that's publicly accessible on the internet, a WAF is a critical security layer that handles threats before they reach the application code."},{question:"How do you test a URL shortener system for correctness and performance?",options:["Only manual testing is needed","Unit tests for encoding logic, integration tests for redirect flows, load tests simulating realistic traffic patterns, and chaos engineering for resilience","Only load testing matters","Testing isn't important for URL shorteners"],correctIndex:1,explanation:"A comprehensive testing strategy includes multiple layers: unit tests verify base62 encoding/decoding, URL validation, and key generation logic. Integration tests verify the full redirect flow (create  redirect  analytics), custom alias creation, expiration behavior, and error handling. Load tests using tools like k6, Locust, or Gatling simulate realistic traffic patterns  high read-to-write ratios, burst traffic during viral events, and geographic distribution. Chaos engineering (killing cache nodes, simulating database failures, introducing network latency) verifies graceful degradation. Performance benchmarks establish baselines for redirect latency (target: <50ms p99), cache hit ratios (target: >95%), and throughput (target: sustain 10x average load). This multi-layered approach catches bugs at every level and gives confidence in the system's behavior under stress."},{question:"What is the 'stampede' problem when a cache entry for a popular URL expires?",options:["Users rush to create the same short URL simultaneously","Many concurrent requests miss the expired cache entry and simultaneously query the database, potentially overwhelming it","The database runs out of storage","Short URLs expire too quickly"],correctIndex:1,explanation:"Cache stampede occurs when a cached entry expires and many concurrent requests for the same key arrive before the cache is repopulated. All requests find a cache miss and query the database simultaneously. For a viral short URL with 10,000 requests/second, a cache expiry could instantly send 10,000 queries to the database for the same key. Mitigations include: probabilistic early expiration (each request has a small random chance of refreshing the cache before TTL expires), distributed locking (only one request fetches from DB while others wait for the cache to be repopulated), and staggered TTLs (adding random jitter to TTL values so entries don't expire simultaneously). This is also known as the 'thundering herd' problem and is a critical concern for any read-heavy cached system."},{question:"How does a URL shortener handle Unicode/emoji in custom aliases?",options:["Reject all non-ASCII characters","Allow Unicode with proper percent-encoding, or use Punycode-like encoding to support emoji/international aliases in a URL-safe way","Convert to ASCII by removing special characters","Store as-is without encoding"],correctIndex:1,explanation:"Supporting Unicode custom aliases (like 'bit.ly/party' or 'bit.ly/') requires careful handling. The alias must be percent-encoded for the actual URL (emoji  %F0%9F%8E%89...) but displayed in its readable form in the UI. You need to normalize Unicode (NFC normalization) to prevent visually-identical-but-different-byte-sequence aliases from coexisting. Security concerns include homograph attacks (using Cyrillic '' that looks like Latin 'a'). Some shorteners restrict aliases to ASCII to avoid these complexities, while others embrace Unicode as a differentiating feature. If supporting Unicode, validate against confusable characters, normalize consistently, and store both the display form and the encoded form."},{question:"What is the benefit of using HTTP/2 for a URL shortener's server?",options:["HTTP/2 creates shorter URLs","HTTP/2's multiplexing, header compression, and server push can reduce connection overhead for clients making multiple requests","HTTP/2 is required for redirects","HTTP/2 encrypts URLs"],correctIndex:1,explanation:"HTTP/2 provides several performance benefits for URL shorteners: header compression (HPACK) reduces the overhead of repeated headers across requests, which matters when a page contains multiple short URLs. Multiplexing allows multiple redirect requests over a single TCP connection, avoiding head-of-line blocking. For the shortener's API (creating URLs, checking analytics), HTTP/2's binary framing and multiplexing reduce latency compared to HTTP/1.1's sequential request processing. While a single redirect doesn't benefit much from multiplexing, analytics API endpoints returning multiple data streams do. Most modern CDNs and load balancers support HTTP/2 automatically. The practical impact is modest for simple redirect operations but meaningful for API-heavy interactions."},{question:"What is the trade-off between storing analytics data in the same database as URL mappings vs. a separate analytics store?",options:["Always store everything together for simplicity","Separating analytics from URL mappings prevents write-heavy analytics from degrading read-heavy redirect performance","Analytics data should never be stored","The same database is always better for consistency"],correctIndex:1,explanation:"URL mappings are read-heavy (millions of lookups/second) while analytics data is write-heavy (every redirect generates an analytics event). Storing both in the same database creates resource contention  analytics writes compete with redirect reads for I/O, CPU, and memory. Separating them allows each database to be optimized for its workload: the URL mapping store uses read-optimized settings (large buffer pool, read replicas), while the analytics store uses write-optimized settings (write-ahead log, batch inserts, columnar storage for aggregation queries). Time-series databases (InfluxDB, TimescaleDB) or columnar analytics databases (ClickHouse, BigQuery) are ideal for analytics data. This polyglot persistence approach is standard in production systems at scale."},{question:"How would you implement a URL shortener preview feature (like bit.ly's '+' suffix)?",options:["Create a separate short URL for the preview","Append a special character (like '+') to the short URL path that serves an HTML page showing destination URL and metadata instead of redirecting","Add a query parameter for preview mode","Preview is not possible with short URLs"],correctIndex:1,explanation:"The preview feature works by reserving a URL pattern (e.g., 'sho.rt/abc123+' or 'sho.rt/abc123/info') that returns an HTML page showing: the destination URL, page title and description (from Open Graph metadata), click statistics, creation date, and a 'proceed to destination' button. The server routes these requests differently from redirect requests  matching the '+' suffix pattern returns an HTML page rather than a 302 redirect. This feature is important for security-conscious users who want to verify the destination before being redirected. Query parameters (like '?preview=true') would also work technically but could conflict with URLs that contain similar parameters. Bit.ly popularized the '+' convention, and it has become a de facto standard in the URL shortener space."},{question:"What is the role of a reverse proxy (like Nginx) in front of a URL shortener application?",options:["To generate short URLs","To handle SSL termination, static content serving, request buffering, and act as a gateway before requests reach the application servers","To store URL mappings","To generate analytics reports"],correctIndex:1,explanation:"A reverse proxy like Nginx sits between clients and application servers, providing several critical functions. SSL/TLS termination handles HTTPS decryption at the proxy level so application servers process plain HTTP, reducing their CPU load. Request buffering absorbs slow client connections so application servers aren't tied up waiting for slow uploads. Connection pooling maintains a smaller number of persistent connections to backend servers. Static content serving returns error pages and status pages without hitting the application. Rate limiting and access control provide a first line of defense against abuse. For URL shorteners, Nginx can also handle the redirect logic directly for cached URLs using its built-in proxy_cache module, making redirects extremely fast without involving the application server at all."},{question:"How would you design a URL shortener to be serverless?",options:["Serverless architectures can't support URL shorteners","Use API Gateway for routing, Lambda/Cloud Functions for logic, DynamoDB for storage, and CloudFront as CDN  but cold starts may increase redirect latency","Replace all servers with a single large instance","Use only edge computing"],correctIndex:1,explanation:"A serverless URL shortener architecture uses: API Gateway (AWS API Gateway or Cloudflare Workers) for request routing, Lambda or Cloud Functions for creation logic and analytics processing, DynamoDB or FaunaDB for URL storage with built-in TTL for expiration, and CloudFront or another CDN for caching redirect responses. Benefits include zero infrastructure management, automatic scaling, and pay-per-request pricing. The main concern is cold start latency  a Lambda function that hasn't been invoked recently may take 100-500ms to start, which is unacceptable for redirect latency. Mitigations include provisioned concurrency (keeping functions warm) and edge-based execution (Cloudflare Workers run on the edge with near-zero cold starts). For low-to-moderate traffic URL shorteners, serverless is cost-effective; at high scale, dedicated infrastructure may be cheaper."},{question:"What is the difference between soft delete and hard delete for expired URLs?",options:["There is no difference","Soft delete marks the URL as inactive (preserving data for analytics/legal), while hard delete permanently removes the record from the database","Soft delete is slower","Hard delete preserves data"],correctIndex:1,explanation:"Soft delete adds a 'deleted_at' timestamp or 'is_active' flag to the record without removing it from the database. The redirect logic checks this flag and returns 410 Gone for soft-deleted URLs. This preserves historical data for analytics, compliance, and potential restoration. Hard delete permanently removes the record, freeing storage space and allowing the short key to be recycled. The choice depends on requirements: regulated industries (finance, healthcare) may require retaining records for audit trails, while privacy regulations (GDPR right to erasure) may require actual deletion. A common pattern is soft delete with a grace period (30-90 days) followed by hard delete and key recycling. This allows users to recover accidentally deleted URLs while eventually reclaiming resources."},{question:"How would you handle a URL shortener migration from one database to another?",options:["Shut down and migrate everything at once","Use the strangler fig pattern: dual-write to both databases, gradually shift reads to the new database, verify consistency, then decommission the old one","Simply point the application to the new database","Migrations are unnecessary"],correctIndex:1,explanation:"The strangler fig pattern provides zero-downtime database migration. Phase 1: Set up the new database and begin dual-writing (every URL creation writes to both old and new databases). Phase 2: Backfill historical data from the old database to the new one using a batch migration job. Phase 3: Verify consistency by comparing read results from both databases (shadow reads). Phase 4: Gradually shift read traffic to the new database using feature flags (e.g., 1%  10%  50%  100%). Phase 5: Stop writing to the old database and decommission it. This approach ensures zero data loss and allows rollback at any phase. The dual-write phase requires careful handling of failures  if one write succeeds but the other fails, you need compensating logic or an eventual consistency reconciliation job."},{question:"What is the significance of the Referer header in URL shortener analytics?",options:["It specifies the URL shortener's domain","It indicates where the user was before clicking the short URL, revealing which websites or platforms are driving traffic","It contains the user's name","It specifies the redirect destination"],correctIndex:1,explanation:"The HTTP Referer (historically misspelled) header contains the URL of the page that linked to the current request. For URL shortener analytics, this reveals traffic sources  whether clicks are coming from Twitter, Facebook, email clients, direct messaging apps, or other websites. This information is invaluable for marketers tracking campaign performance across channels. However, Referer is not always available: HTTPS-to-HTTP transitions strip it (though HTTPS-to-HTTPS preserves it), some privacy-focused browsers suppress it, and Referrer-Policy headers can restrict it. Despite these limitations, Referer data typically covers 60-80% of traffic and remains one of the most valuable analytics dimensions. Modern URL shorteners supplement Referer with UTM parameters for more reliable source tracking."},{question:"How would you implement a QR code generation feature for a URL shortener?",options:["Store pre-generated QR codes for all URLs","Generate QR codes on-demand by encoding the short URL, cache the result, and optionally allow customization (colors, logo embedding)","QR codes cannot encode URLs","Use a third-party QR code service for every request"],correctIndex:1,explanation:"QR code generation is a natural complement to URL shortening  the short URL is encoded into the QR code, keeping the QR pattern simple and scannable. Implementation involves: generating QR codes on-demand using a library (like qrcode in Node.js or Python), encoding the short URL (not the long URL, for a simpler QR pattern), caching generated QR code images (since the same short URL always produces the same QR code), and optionally allowing customization (brand colors, embedded logos, rounded dots). The QR code should be served with appropriate cache headers for browser caching. For high-traffic generation, a dedicated microservice with image caching (S3 + CDN) keeps the main redirect service unaffected. This feature is increasingly important as QR codes have seen massive adoption since COVID-19."},{question:"What is the purpose of using a write-ahead log (WAL) in a URL shortener's database?",options:["To log URL redirect analytics","To ensure durability by writing changes to a sequential log before applying them to the database, preventing data loss on crashes","To compress URL data","To cache frequently accessed URLs"],correctIndex:1,explanation:"A write-ahead log (WAL) ensures that no committed data is lost even if the database server crashes mid-operation. Before any change is applied to the actual data files, it's first written to the WAL  a sequential, append-only log on disk. If the server crashes, it replays the WAL on startup to recover committed transactions. For a URL shortener, this means a successfully created short URL won't be lost due to a server crash between the creation confirmation and the data being flushed to disk. WAL also improves write performance because sequential writes to the log are much faster than random writes to data files. Both PostgreSQL and MySQL use WAL (MySQL calls it the 'redo log'), and even NoSQL databases like Cassandra use a similar commit log mechanism."},{question:"How would you implement geographic routing for a multi-region URL shortener?",options:["Route all traffic to a single region","Use GeoDNS or latency-based DNS routing to direct users to the nearest region, with health checks for failover","Use client-side routing","Route based on the URL content"],correctIndex:1,explanation:"Geographic routing directs users to the closest healthy deployment region, minimizing latency. AWS Route 53's latency-based routing measures actual latency from DNS resolvers to each region, while GeoDNS maps source IPs to geographic regions. Implementation requires: deploying the full shortener stack in multiple regions, cross-region data replication (using DynamoDB Global Tables or Cassandra multi-DC), health checks that automatically remove unhealthy regions from DNS, and a global CDN layer (CloudFront, Cloudflare) for edge caching. The DNS TTL should be low enough (e.g., 60 seconds) to enable fast failover but high enough to not add excessive DNS resolution overhead. This ensures a user in Tokyo gets served from an Asia-Pacific region (~20ms latency) rather than US-East (~200ms latency)."},{question:"What is the concept of 'backpressure' in a URL shortener's analytics pipeline?",options:["Pushing back the user's browser","A mechanism where overwhelmed downstream analytics consumers signal upstream producers to slow down, preventing system overload","Compressing analytics data","Reversing URL redirects"],correctIndex:1,explanation:"Backpressure is a flow control mechanism in stream processing systems. In a URL shortener's analytics pipeline, click events flow from the redirect service  message queue  stream processor  analytics database. If the stream processor or analytics database becomes overwhelmed, backpressure signals the message queue to slow down event delivery, which may in turn signal the redirect service to buffer events locally. Without backpressure, the analytics pipeline could crash under load, losing click data. Kafka handles this naturally through consumer lag (consumers read at their own pace, and Kafka retains events). Redis Streams and RabbitMQ provide similar mechanisms. The key principle: it's better to process analytics events slowly than to lose them, since the redirect path is independent and unaffected."},{question:"How would you implement URL shortener link rotation (cycling through multiple destination URLs)?",options:["Create separate short URLs for each destination","Store an ordered list of destination URLs with rotation rules (round-robin, weighted, time-based) in the URL mapping record","Randomly pick a destination on each redirect","URL rotation is not possible"],correctIndex:1,explanation:"Link rotation allows a single short URL to redirect to different destinations based on configurable rules. The URL mapping record stores an array of destinations with rotation strategy metadata. Round-robin rotation assigns each click to the next destination in sequence (using an atomic counter). Weighted rotation assigns different traffic percentages to each destination (similar to A/B testing). Time-based rotation changes the destination at scheduled times (e.g., redirect to morning page before noon, afternoon page after). Implementation requires atomic read-modify-write operations for round-robin counters (Redis INCR works well here). This feature is valuable for marketers running multi-variant campaigns and for load distribution across multiple landing page servers."},{question:"What are idempotency keys and how do they apply to URL shortener creation?",options:["Keys that generate identical short URLs","Client-generated unique identifiers sent with creation requests to ensure the same request processed multiple times produces only one short URL","Keys that expire after one use","Keys used for URL encryption"],correctIndex:1,explanation:"Idempotency keys solve the problem of duplicate creation in unreliable networks. When a client sends a URL creation request, it includes a unique idempotency key (typically a UUID) in the header. The server stores this key with the creation result. If the client retries the same request (due to a timeout or network error), the server recognizes the duplicate idempotency key and returns the original result without creating a new short URL. Without idempotency keys, network retries could create multiple short URLs for the same long URL, wasting key space and confusing analytics. This pattern is used by Stripe, AWS, and other API-first companies. Implementation typically uses a Redis cache with a TTL (e.g., 24 hours) to store idempotency_key  response mappings."},{question:"What is the purpose of health checks in a URL shortener's load balancer configuration?",options:["To check the health of URLs","To verify application servers are functioning correctly so the load balancer routes traffic only to healthy instances","To check database health","To monitor URL creation rates"],correctIndex:1,explanation:"Health checks are periodic probes sent by the load balancer to each application server to verify it's capable of handling requests. For a URL shortener, a health check endpoint (e.g., GET /health) should verify: the application process is running, the database connection is active, and the cache is reachable. If a server fails health checks, the load balancer stops routing traffic to it, preventing users from hitting a broken server. Health checks should be fast (<100ms) and not perform heavy operations. A deep health check (testing all dependencies) is useful but risky  if the database is temporarily slow, all servers might fail deep health checks simultaneously, causing total service outage. A common pattern is shallow checks for load balancer routing and deep checks for monitoring/alerting."},{question:"How does consistent hashing with virtual nodes improve data distribution in a sharded URL shortener?",options:["Virtual nodes are faster than real nodes","Virtual nodes create multiple points per server on the hash ring, evening out distribution and reducing hotspots when servers are added or removed","Virtual nodes provide encryption","Virtual nodes reduce storage requirements"],correctIndex:1,explanation:"Without virtual nodes, each physical server gets one point on the hash ring, which can lead to uneven data distribution  especially with a small number of servers. Virtual nodes assign multiple points (e.g., 150-256) per physical server on the ring, creating a much more uniform distribution. When a server is removed, its virtual nodes' key ranges are distributed across many other servers rather than being absorbed by a single neighbor. This prevents sudden load spikes on individual servers. For a URL shortener with billions of entries, this uniform distribution is critical  a 10% imbalance across 10 shards means one shard handles 10% more traffic, which could exceed its capacity during peak hours. Amazon's Dynamo paper popularized this technique, and it's used in Cassandra, Riak, and most modern distributed databases."},{question:"What is the difference between proactive and reactive cache invalidation in a URL shortener?",options:["There is no difference","Proactive invalidation removes cache entries before they become stale (e.g., when a URL is updated/deleted), while reactive uses TTL expiration","Proactive is always slower","Reactive requires more code"],correctIndex:1,explanation:"Proactive (or explicit) invalidation immediately removes or updates cache entries when the underlying data changes  for example, when a user updates a short URL's destination or deletes it, the cache entry is immediately invalidated. Reactive invalidation relies on TTL expiration, meaning stale data may be served until the TTL expires. For a URL shortener, both approaches are needed: TTL-based expiration for normal operation (prevents indefinite caching and handles gradual consistency), and proactive invalidation for user-initiated changes (a user updating their URL's destination expects the change to take effect immediately, not after a 5-minute TTL). The implementation uses cache-aside pattern: on URL update, delete the cache entry and let the next request repopulate it from the database. This combination is a standard practice in any caching architecture."},{question:"What is the security risk of sequential/predictable short URL keys?",options:["Sequential keys are slower to look up","Attackers can enumerate all URLs by iterating through the key space, accessing potentially sensitive destination URLs","Sequential keys use more storage","Sequential keys cause more cache misses"],correctIndex:1,explanation:"If a URL shortener uses sequential keys (1, 2, 3... converted to base62), an attacker can systematically try every possible key to discover all shortened URLs. This is a privacy and security risk because people often shorten sensitive URLs  private documents, internal company tools, personal photos, or confidential business links  assuming the short URL is effectively 'secret.' Enumeration attacks on sequential keys are trivial to automate and can expose thousands of sensitive URLs per minute. Random keys make enumeration impractical  with a 7-character base62 key space of 3.5 trillion possibilities, randomly guessing valid keys has a negligible success rate. This is one of the strongest arguments against sequential key generation and in favor of random/KGS-based approaches in system design interviews."},{question:"How should a URL shortener handle a destination URL that becomes unreachable?",options:["Immediately delete the short URL","Continue redirecting users (the destination might be temporarily down), but flag the URL for review and optionally show a warning after prolonged unavailability","Return a 404 error","Replace the destination with a cached version"],correctIndex:1,explanation:"A URL shortener's job is to redirect, not to guarantee destination availability. The destination might be temporarily down for maintenance, experiencing a brief outage, or behind a firewall that blocks the shortener's monitoring probe. Immediately deleting or blocking the redirect would break legitimate use cases. Instead, implement a monitoring system that periodically checks destination health and flags URLs that have been unreachable for extended periods (e.g., 7+ days). For flagged URLs, you could: show an interstitial warning ('This link's destination appears to be unavailable'), send a notification to the URL's creator, or mark it in analytics. The redirect itself should always attempt to work  the user might have access even if the monitoring system doesn't. This nuanced approach balances user experience with proactive monitoring."},{question:"What is the maximum QPS (Queries Per Second) a single Redis instance can typically handle for URL shortener lookups?",options:["About 1,000 QPS","About 10,000 QPS","About 100,000+ QPS","About 1,000,000+ QPS"],correctIndex:2,explanation:"A single Redis instance can typically handle 100,000 to 250,000+ simple GET operations per second, depending on hardware, network configuration, and value sizes. For URL shortener lookups (GET by short key returning a URL string of ~100-500 bytes), Redis comfortably handles 100K+ QPS per instance. This means even a small Redis cluster (3-5 nodes) can serve the redirect traffic of most URL shorteners. Redis achieves this through: single-threaded event loop (no lock contention), in-memory data storage, efficient data structures, and minimal protocol overhead. Redis 6+ introduced I/O threading for even higher throughput. For comparison, a well-tuned PostgreSQL instance might handle 10,000-50,000 simple lookups per second  an order of magnitude less. This performance difference is why Redis is the de facto caching layer for read-heavy systems."},{question:"How would you implement URL shortener access controls (private short URLs)?",options:["All short URLs must be public","Add an authentication layer where private URLs require an API key or token in a query parameter or header before redirecting","Use longer keys for private URLs","Encrypt the destination URL"],correctIndex:1,explanation:"Private short URLs add an access control layer before the redirect. Implementation options include: API key in a query parameter (sho.rt/abc123?key=secret123), bearer token in a cookie/header (requires the user to be authenticated), password-protected redirect (interstitial page asking for a password), or organization-based access (only users authenticated with the organization's SSO can access the redirect). The trade-off is user experience  adding authentication to redirects adds friction and breaks the simplicity of short URLs. For API-based access, tokens are cleaner. For browser-based access, a lightweight interstitial page with password entry works well. The short URL's metadata record needs an 'access_type' field (public/private/password-protected) and associated credentials. This feature is valuable for enterprise use cases where companies share internal links externally but want to control access."},{question:"What is the purpose of request tracing (distributed tracing) in a URL shortener system?",options:["To trace the destination URL","To track a single request's journey through all system components (load balancer  app server  cache/DB  analytics), enabling debugging and performance optimization","To trace user identity","To trace URL creation history"],correctIndex:1,explanation:"Distributed tracing assigns a unique trace ID to each incoming request and propagates it through every component the request touches. For a URL shortener redirect: the load balancer logs the trace ID, the application server logs cache hit/miss with the trace ID, the database query (if any) is tagged with the trace ID, and the analytics event includes it. When a user reports slow redirects, you can look up the trace ID and see exactly where time was spent  was it DNS resolution? TLS handshake? Cache miss? Slow database query? Tools like Jaeger, Zipkin, or AWS X-Ray provide visualization of trace data. Without distributed tracing, debugging performance issues in a multi-component system becomes guesswork. This observability capability is essential for any production system and demonstrates operational maturity in system design interviews."},{question:"What is the role of feature flags in a URL shortener deployment?",options:["To flag malicious URLs","To enable gradual rollout of new features (like new redirect logic or caching strategies) to a percentage of traffic without redeploying","To mark URLs for deletion","To flag popular URLs for caching"],correctIndex:1,explanation:"Feature flags (also called feature toggles) allow you to enable or disable specific code paths at runtime without redeploying the application. For a URL shortener, this enables: gradual rollout of a new caching strategy (test with 1% of traffic before full deployment), A/B testing different redirect logic (302 vs 307), canary deployments of new database clients, quick rollback of problematic features without redeployment, and enabling premium features for specific user tiers. Tools like LaunchDarkly, Split.io, or simple Redis-backed configurations provide feature flag infrastructure. The key benefit is risk reduction  if a new feature causes issues, you flip the flag off instantly rather than waiting for a deployment. This operational pattern is used extensively at companies like Facebook, Google, and Netflix for safe continuous deployment."}],mf=[{question:"What is the fundamental principle of event-driven architecture?",options:["Services communicate by directly calling each other's APIs","Components communicate by producing and consuming events, reacting to state changes asynchronously","All services share a single database that triggers events","Events are only used for logging and monitoring"],correctIndex:1,explanation:"Event-driven architecture (EDA) is a design pattern where components communicate through eventsnotifications that something has happened. Producers emit events without knowing who will consume them, and consumers react to events they're interested in. This creates loose coupling: producers and consumers can evolve independently, and new consumers can be added without modifying producers. Unlike direct API calls (request/response), EDA is inherently asynchronousthe producer doesn't wait for a response. This enables higher resilience, better scalability, and natural support for workflows that span multiple services. Real-world examples include order processing pipelines, IoT telemetry systems, and real-time analytics."},{question:"What is event sourcing?",options:["Sourcing events from third-party APIs","Storing the state of an entity as a sequence of immutable events rather than the current state","Using source control to track event changes","A technique for finding the source of errors in event logs"],correctIndex:1,explanation:"Event sourcing stores every state change as an immutable event in an append-only log (event store), rather than storing just the current state. To get the current state of an entity, you replay all its events from the beginning. For example, instead of storing 'account balance = $500,' you store 'deposited $1000,' 'withdrew $300,' 'deposited $100,' 'withdrew $300.' This provides a complete audit trail, enables temporal queries ('what was the balance on Tuesday?'), and allows rebuilding state from scratch. The trade-off is complexity: reading current state requires replaying events (solved by snapshots), and the event schema must be carefully managed since events are immutable. Event sourcing is the foundation of CQRS in many systems."}],ff=[{question:"What OSI layer does an L4 load balancer operate at?",options:["Layer 2 (Data Link)","Layer 4 (Transport)","Layer 7 (Application)","Layer 3 (Network)"],correctIndex:1,explanation:"An L4 load balancer operates at the Transport layer (Layer 4), making routing decisions based on TCP/UDP information such as IP addresses and port numbers. It does not inspect the actual content of packets like an L7 load balancer would. This makes L4 balancers faster since they don't need to parse HTTP headers, cookies, or other application-level data. AWS Network Load Balancer (NLB) is a real-world example of an L4 load balancer that can handle millions of requests per second with ultra-low latency."},{question:"Which load balancing algorithm distributes requests sequentially across servers in order?",options:["Least connections","IP hash","Round-robin","Weighted random"],correctIndex:2,explanation:"Round-robin distributes incoming requests sequentially across the pool of servers  first request to server 1, second to server 2, and so on, cycling back to the start. It's the simplest algorithm and works well when all servers have similar capacity and requests have similar processing costs. However, it doesn't account for current server load, so a server handling a long-running request still gets new ones. Nginx uses round-robin as its default load balancing method."},{question:"What is the main advantage of the 'least connections' algorithm over round-robin?",options:["It's faster to compute","It considers current server load","It preserves session affinity","It uses less memory"],correctIndex:1,explanation:"Least connections routes new requests to the server with the fewest active connections, effectively considering current server load. Unlike round-robin which blindly cycles through servers, least connections adapts to situations where some requests take longer than others. For example, if one server is handling several long-running database queries, new requests will be directed to less-busy servers. This makes it ideal for applications with variable request processing times, such as WebSocket connections or API endpoints with mixed query complexity."},{question:"What does SSL termination at the load balancer mean?",options:["The load balancer blocks all SSL traffic","The load balancer decrypts SSL and forwards plain HTTP to backends","The load balancer re-encrypts traffic with a different certificate","The load balancer only accepts non-SSL traffic"],correctIndex:1,explanation:"SSL termination means the load balancer handles the SSL/TLS decryption, removing the encryption overhead from backend servers. The load balancer decrypts incoming HTTPS requests and forwards plain HTTP to the backend servers over the internal network. This offloads the CPU-intensive cryptographic operations from application servers, allowing them to focus on business logic. It's widely used in production  for example, AWS ALB can terminate SSL and even handle certificate rotation via ACM, while backend EC2 instances communicate in plaintext within the VPC."},{question:"Which load balancer type can route based on URL path or HTTP headers?",options:["L3 load balancer","L4 load balancer","L7 load balancer","DNS load balancer"],correctIndex:2,explanation:"L7 (Application layer) load balancers can inspect HTTP/HTTPS content including URL paths, headers, cookies, and even request bodies to make routing decisions. This enables content-based routing  for example, sending /api/* requests to API servers and /static/* to file servers. L4 load balancers cannot do this because they only see TCP/UDP packet information without parsing the application protocol. AWS Application Load Balancer (ALB) is a classic L7 load balancer that supports path-based and host-based routing rules."},{question:"What is sticky session (session affinity) in load balancing?",options:["Sessions that cannot be terminated","Routing all requests from the same client to the same server","Sharing session data across all servers","Encrypting session data at the load balancer"],correctIndex:1,explanation:"Sticky sessions ensure that all requests from a particular client are directed to the same backend server for the duration of the session. This is typically implemented using cookies  the load balancer sets a cookie on the first response indicating which server to use. While this solves session state problems (e.g., shopping carts stored in server memory), it can lead to uneven load distribution if some users generate more traffic. Modern architectures prefer externalized session stores like Redis instead of sticky sessions, as they don't create single points of failure."},{question:"What happens when a health check fails for a backend server?",options:["The load balancer restarts the server","The load balancer stops sending traffic to that server","The load balancer sends more traffic to test recovery","The load balancer shuts down completely"],correctIndex:1,explanation:"When a health check fails, the load balancer marks that server as unhealthy and stops routing new requests to it. Existing connections may be drained gracefully depending on the configuration. The load balancer continues to periodically check the unhealthy server, and once it passes health checks again, it's returned to the active pool. For example, an ALB health check might ping /health every 30 seconds, requiring 3 consecutive failures to mark a target unhealthy and 3 successes to mark it healthy again."},{question:"IP hash load balancing is most useful when you need:",options:["Maximum throughput","Client-server affinity without cookies","Equal distribution regardless of client","Minimum latency"],correctIndex:1,explanation:"IP hash computes a hash of the client's IP address to determine which server should handle the request, ensuring the same client always reaches the same server. This provides session affinity without relying on cookies or application-layer information, making it work at L4 where cookies aren't visible. However, it can cause uneven distribution when many clients share an IP (like behind a corporate NAT), as all those clients would hit the same server. It's commonly used in scenarios where stateful connections are needed but cookie-based affinity isn't feasible, such as TCP-based protocols."},{question:"Which software is primarily known as a high-performance L4/L7 load balancer and proxy?",options:["Apache Tomcat","HAProxy","MySQL Proxy","Varnish"],correctIndex:1,explanation:"HAProxy (High Availability Proxy) is one of the most widely-used open-source load balancers, supporting both L4 (TCP) and L7 (HTTP) load balancing. It's known for extremely high performance, handling millions of concurrent connections with low resource usage. HAProxy powers major sites like GitHub, Reddit, and Stack Overflow. While Nginx also functions as a load balancer, HAProxy was purpose-built for load balancing and proxying, offering more advanced health checking, connection draining, and traffic management features out of the box."},{question:"What is the primary difference between AWS ALB and NLB?",options:["ALB is cheaper than NLB","ALB operates at L7 while NLB operates at L4","NLB supports HTTP while ALB does not","ALB is faster than NLB"],correctIndex:1,explanation:"AWS Application Load Balancer (ALB) operates at Layer 7, understanding HTTP/HTTPS and supporting features like path-based routing, host-based routing, and WebSocket support. AWS Network Load Balancer (NLB) operates at Layer 4, routing based on TCP/UDP and offering ultra-low latency with millions of requests per second. NLB is actually faster than ALB because it doesn't need to parse HTTP content. Choose ALB when you need content-based routing, and NLB when you need raw performance, static IPs, or non-HTTP protocols like gRPC or custom TCP."},{question:"What is Global Server Load Balancing (GSLB)?",options:["Load balancing within a single data center","DNS-based load balancing across geographically distributed data centers","A single load balancer handling global traffic","Round-robin within a server cluster"],correctIndex:1,explanation:"GSLB distributes traffic across multiple geographically distributed data centers, typically using DNS to direct users to the nearest or healthiest data center. When a client resolves a domain name, the GSLB-aware DNS returns the IP of the optimal data center based on factors like geographic proximity, health status, and current load. This is different from local load balancing which distributes within a single data center. Services like AWS Route 53 with latency-based routing, Cloudflare Load Balancing, and F5 BIG-IP DNS are real-world GSLB implementations."},{question:"In weighted round-robin, what does the 'weight' represent?",options:["The physical size of the server","The proportion of traffic a server should receive","The priority in case of failover","The maximum number of connections"],correctIndex:1,explanation:"In weighted round-robin, each server is assigned a weight that determines the proportion of requests it receives relative to other servers. A server with weight 3 receives three times as many requests as a server with weight 1. This is useful when servers have different capacities  a powerful machine with 16 CPU cores should handle more traffic than a smaller 4-core machine. For example, in Nginx you'd configure 'server backend1 weight=3; server backend2 weight=1;' to send 75% of traffic to backend1 and 25% to backend2."},{question:"What problem does connection draining solve during server removal?",options:["It prevents new servers from being added","It allows in-flight requests to complete before removing a server","It speeds up the removal process","It caches responses for offline servers"],correctIndex:1,explanation:"Connection draining (also called deregistration delay) ensures that when a server is being removed from the pool, existing in-flight requests are allowed to complete rather than being abruptly terminated. The load balancer stops sending new requests to the server but maintains existing connections until they finish or a timeout is reached. Without connection draining, users might see 502 errors or dropped connections during deployments. AWS ALB has a configurable deregistration delay (default 300 seconds) that defines how long to wait before forcefully closing remaining connections."},{question:"Which health check type actually makes an HTTP request to verify application health?",options:["TCP health check","ICMP ping check","HTTP health check","Port check"],correctIndex:2,explanation:"HTTP health checks send an actual HTTP request (usually GET) to a specific endpoint like /health and verify the response status code (typically expecting 200 OK). This is more thorough than TCP checks (which only verify the port is open) or ICMP pings (which only verify network reachability). An HTTP health check can detect application-level failures  for instance, a server where the web process is running but the database connection is broken would pass TCP checks but fail an HTTP health check that queries the DB. This is why production systems almost always use HTTP health checks for web applications."},{question:"What is the 'thundering herd' problem in load balancing?",options:["Too many load balancers competing for traffic","All backend servers failing simultaneously","Many clients reconnecting simultaneously after a failure, overwhelming the system","Servers processing requests too quickly"],correctIndex:2,explanation:"The thundering herd problem occurs when many clients simultaneously attempt to reconnect or retry after a failure or during a recovery event, potentially overwhelming the system. For example, if a load balancer goes down for 30 seconds, thousands of clients will retry simultaneously when it comes back, creating a massive spike. Solutions include exponential backoff with jitter on client retries, gradual ramp-up (slow start) for recovered servers, and connection rate limiting. AWS ALB's slow start feature addresses this by gradually increasing the proportion of requests sent to a newly healthy target."},{question:"What does Nginx use as its default load balancing algorithm?",options:["Least connections","IP hash","Round-robin","Random"],correctIndex:2,explanation:"Nginx uses round-robin as its default load balancing algorithm when you define an upstream block without specifying a method. Requests are distributed sequentially across the defined servers in order. You can change this by adding directives like 'least_conn;' for least connections or 'ip_hash;' for IP-based hashing inside the upstream block. Round-robin is a sensible default because it's simple, fast, and works reasonably well when servers have similar capacities. For heterogeneous server pools, you'd typically switch to weighted round-robin or least connections."},{question:"What is a reverse proxy, and how does it relate to load balancing?",options:["A proxy that clients use to access the internet anonymously","A server that sits in front of backend servers, forwarding client requests  load balancers are a type of reverse proxy","A proxy that reverses the direction of network traffic","A backup load balancer"],correctIndex:1,explanation:"A reverse proxy sits between clients and backend servers, receiving client requests and forwarding them to appropriate backends. Load balancers are essentially reverse proxies with the added intelligence of distributing requests across multiple servers. Beyond load distribution, reverse proxies provide benefits like SSL termination, caching, compression, and security (hiding backend server details). Nginx and HAProxy are commonly used as both reverse proxies and load balancers. The 'reverse' distinguishes it from a forward proxy, which sits in front of clients to access external resources."},{question:"In a Blue-Green deployment, how does the load balancer facilitate zero-downtime releases?",options:["It runs both versions simultaneously on the same servers","It switches all traffic from the old (blue) environment to the new (green) environment at once","It gradually increases traffic to the new version","It pauses all traffic during deployment"],correctIndex:1,explanation:"In Blue-Green deployment, you maintain two identical production environments. The load balancer points all traffic to the current (blue) environment while the new version is deployed to the green environment. Once the green environment is verified, the load balancer switches all traffic from blue to green instantly. If problems are found, you can immediately switch back to blue. This differs from canary deployments where traffic is gradually shifted. The load balancer is the key component that makes the instant cutover possible, and tools like AWS ALB with target group switching make this straightforward."},{question:"What is the purpose of the X-Forwarded-For HTTP header in load balancing?",options:["To specify which server should handle the request","To preserve the original client IP address when traffic passes through a proxy/load balancer","To indicate the load balancing algorithm being used","To forward authentication tokens"],correctIndex:1,explanation:"When a load balancer forwards a request to a backend server, the backend sees the load balancer's IP as the source, not the original client's IP. The X-Forwarded-For header preserves the original client IP by appending it as the request passes through each proxy. This is critical for logging, rate limiting, geolocation, and security  you need to know the real client IP, not the load balancer's. For example, a request passing through two proxies might have 'X-Forwarded-For: client-ip, proxy1-ip'. Backend applications must be configured to trust and parse this header from known load balancer IPs."},{question:"What is the 'slow start' feature in load balancing?",options:["Delaying the load balancer startup","Gradually increasing traffic to a newly added or recovered server","Slowing down request processing for better reliability","Starting health checks at a slower rate"],correctIndex:1,explanation:"Slow start gradually increases the proportion of requests sent to a newly added or recently recovered server over a configured time period, rather than immediately sending it a full share of traffic. This prevents overwhelming a cold server that may need to warm up caches, establish database connections, or JIT-compile code. Without slow start, a freshly started Java application server might receive thousands of requests before its JVM has warmed up, leading to high latency or failures. AWS ALB supports slow start mode where you can configure the ramp-up duration (e.g., 30-900 seconds)."},{question:"Which protocol does an L4 load balancer primarily use to make routing decisions?",options:["HTTP","TCP/UDP","FTP","SMTP"],correctIndex:1,explanation:"An L4 load balancer makes routing decisions based on TCP and UDP protocol information  specifically source/destination IP addresses and port numbers. It operates on network packets without understanding the application protocol carried within them. This means it can load balance any TCP or UDP-based protocol (HTTP, FTP, SMTP, custom protocols) without needing protocol-specific knowledge. The tradeoff is that it can't make smart routing decisions based on content, but it's significantly faster than L7 balancing because it doesn't need to parse application-level data. AWS NLB is a prime example of an L4 load balancer."},{question:"What is an active-passive (failover) load balancer configuration?",options:["Both load balancers handle traffic simultaneously","One load balancer handles all traffic while the other stands by as backup","One handles read traffic, the other handles write traffic","Both are passive until traffic spikes"],correctIndex:1,explanation:"In active-passive configuration, one load balancer (active) handles all incoming traffic while another (passive) monitors the active one and stands ready to take over if it fails. The passive load balancer uses heartbeat mechanisms to detect when the active one goes down. When failover occurs, the passive takes over the active's virtual IP address (VIP), making the switch transparent to clients. This is simpler than active-active but wastes the passive node's capacity during normal operation. Tools like keepalived with VRRP protocol are commonly used to implement this pattern with HAProxy."},{question:"What is an active-active load balancer configuration?",options:["Only one load balancer is active at a time","Multiple load balancers handle traffic simultaneously","Load balancers alternate between active and passive states","Load balancers are active only during peak hours"],correctIndex:1,explanation:"In active-active configuration, multiple load balancers simultaneously handle traffic, distributing the load among themselves. This provides both high availability and increased capacity compared to active-passive where one node sits idle. DNS round-robin or an upstream GSLB typically distributes traffic across the active load balancers. If one fails, the others absorb its traffic. The challenge is synchronizing state (like session persistence tables) across active instances. Active-active is preferred in high-traffic environments because it utilizes all available hardware, and services like AWS ALB are inherently active-active across multiple availability zones."},{question:"What is the primary disadvantage of DNS-based load balancing?",options:["It's too expensive","DNS TTL causes slow failover because clients cache old DNS records","It can only balance HTTP traffic","It requires special client software"],correctIndex:1,explanation:"DNS-based load balancing has a fundamental limitation: DNS responses are cached by clients, ISPs, and resolvers for the duration of the TTL (Time To Live). Even with short TTLs, many resolvers and operating systems ignore low TTL values, meaning clients may continue connecting to a failed server for minutes or hours. This makes DNS-based failover slow and unreliable compared to hardware or software load balancers that can detect failures in seconds. Additionally, DNS doesn't perform health checks, so it can resolve to unhealthy servers. Services like AWS Route 53 mitigate this with health check integration, but TTL caching remains a fundamental limitation."},{question:"What is consistent hashing used for in the context of load balancing?",options:["Encrypting traffic between load balancer and backends","Minimizing redistribution of requests when servers are added or removed","Ensuring all servers get exactly equal traffic","Hashing passwords for authentication"],correctIndex:1,explanation:"Consistent hashing maps both servers and request keys onto a hash ring, so each request is routed to the nearest server clockwise on the ring. When a server is added or removed, only the requests that map to that segment of the ring are redistributed  typically 1/N of all requests, where N is the number of servers. This is far better than traditional hash-mod-N approaches where adding a server remaps almost all requests. It's especially important for stateful or cached backends where you want the same client or request key to usually hit the same server. Memcached and many distributed caches use consistent hashing for this reason."},{question:"What does 'SSL passthrough' mean in load balancing?",options:["The load balancer generates new SSL certificates","The load balancer forwards encrypted traffic directly to backends without decryption","The load balancer converts SSL to a different encryption","The load balancer caches SSL sessions"],correctIndex:1,explanation:"SSL passthrough means the load balancer forwards the encrypted TLS traffic directly to the backend server without decrypting it. The backend server handles the SSL/TLS termination itself. This is the opposite of SSL termination where the load balancer decrypts traffic. SSL passthrough is used when end-to-end encryption is required (e.g., for compliance reasons like PCI-DSS) or when the backend needs to see the client certificate for mutual TLS authentication. The downside is the load balancer can't inspect HTTP content, so L7 features like path-based routing or header manipulation are unavailable. HAProxy supports this via 'mode tcp' configuration."},{question:"What is a Virtual IP (VIP) in the context of load balancing?",options:["An IP address only accessible to admin users","A shared IP address that floats between load balancers for high availability","A private IP address used within the data center","An IP address reserved for future use"],correctIndex:1,explanation:"A Virtual IP (VIP) is an IP address that is not tied to a specific physical server but can float between multiple load balancer instances. In an active-passive setup, the active load balancer owns the VIP, and if it fails, the passive takes over the VIP using protocols like VRRP (Virtual Router Redundancy Protocol). Clients always connect to the VIP, so failover is transparent  they don't need to know which physical load balancer is active. This is a cornerstone of high-availability configurations. Keepalived is the most common open-source tool used to manage VIPs in Linux-based load balancer setups with HAProxy or Nginx."},{question:"How does a load balancer handle WebSocket connections differently from regular HTTP?",options:["It blocks WebSocket connections","It must maintain persistent connections and use connection-aware balancing","It converts WebSocket to HTTP","It treats them exactly the same as HTTP"],correctIndex:1,explanation:"WebSocket connections are long-lived, bidirectional, and persistent  unlike typical HTTP request-response cycles that are short-lived. A load balancer must be aware that once a WebSocket connection is established (after the HTTP upgrade handshake), it should maintain that connection to the same backend server for its entire lifetime. Using round-robin per-request would break WebSocket since each message isn't a new connection. L7 load balancers like ALB and Nginx support WebSocket by recognizing the Upgrade header and maintaining connection affinity. This is why least-connections is often preferred for WebSocket workloads, as it accounts for long-held connections."},{question:"What is the 'hot spot' problem in load balancing?",options:["Servers overheating physically","One server receiving disproportionately more traffic than others","The load balancer itself becoming a bottleneck","Network switches becoming saturated"],correctIndex:1,explanation:"A hot spot occurs when one backend server receives significantly more traffic than others, often due to uneven distribution from the load balancing algorithm. This can happen with IP hash when many clients share a single IP (e.g., behind corporate NAT), with round-robin when request processing times vary greatly, or with consistent hashing when the hash space isn't evenly distributed. The hot server may become overloaded while others sit idle. Solutions include weighted balancing, virtual nodes in consistent hashing, and least-connections algorithms. Celebrity Twitter accounts causing hot spots on specific cache servers is a classic real-world example."},{question:"What is Direct Server Return (DSR) in load balancing?",options:["The server directly returns error messages to clients","Response traffic goes directly from the backend to the client, bypassing the load balancer","The server returns the request back to the load balancer","A debugging mode for load balancers"],correctIndex:1,explanation:"In Direct Server Return (DSR), the load balancer only handles incoming requests  the response traffic from the backend server goes directly back to the client, bypassing the load balancer entirely. This dramatically reduces the load on the load balancer since response data (which is typically much larger than requests) doesn't pass through it. DSR is commonly used for video streaming or large file downloads where response payloads are orders of magnitude larger than requests. The backend server must be configured to accept traffic for the VIP address, typically using a loopback interface. L4 load balancers like LVS (Linux Virtual Server) commonly implement DSR."},{question:"In canary deployment, how does the load balancer route traffic?",options:["All traffic goes to the new version immediately","A small percentage of traffic is sent to the new version, gradually increasing","Traffic alternates between old and new versions","Only test traffic goes to the new version"],correctIndex:1,explanation:"In canary deployment, the load balancer initially routes a small percentage (e.g., 1-5%) of real production traffic to the new version while the majority continues to the stable version. If metrics (error rates, latency, etc.) look good, the percentage is gradually increased until 100% reaches the new version. This is named after canaries used in coal mines to detect danger early. The load balancer implements this via weighted routing rules  for example, AWS ALB supports weighted target groups where you can assign 95% weight to the old version and 5% to the new. This provides a safer rollout compared to blue-green's all-at-once switch."},{question:"What is the purpose of a load balancer's 'idle timeout'?",options:["To shut down the load balancer when not in use","To close inactive connections after a specified period to free resources","To delay starting up after a reboot","To slow down traffic during off-peak hours"],correctIndex:1,explanation:"The idle timeout defines how long a load balancer keeps an inactive connection open before closing it. If no data is sent over a connection within this period, it's terminated to free up resources (memory, file descriptors, port numbers). This prevents resource exhaustion from abandoned connections. For example, AWS ALB has a default idle timeout of 60 seconds. For WebSocket or long-polling applications, you need to increase this timeout, while for typical REST APIs the default is usually sufficient. It's important to ensure the backend server's timeout is longer than the load balancer's to avoid the LB sending traffic to a connection the backend has already closed."},{question:"Which header does an L7 load balancer use for host-based routing?",options:["X-Forwarded-For","Host","Content-Type","Accept-Encoding"],correctIndex:1,explanation:"The Host header in HTTP requests specifies the domain name the client is trying to reach, and L7 load balancers use this for host-based routing. This allows a single load balancer to route traffic for multiple domains  for example, api.example.com to API servers and www.example.com to web servers. This is essentially virtual hosting at the load balancer level. AWS ALB supports host-based routing rules where you define conditions like 'if Host header matches api.example.com, forward to target group A.' Without the Host header, you'd need separate load balancers (or IP addresses) for each domain."},{question:"What is the advantage of using multiple availability zones with a load balancer?",options:["Lower cost","Fault tolerance  if one AZ fails, traffic is routed to healthy AZs","Faster response times for all users","Simplified configuration"],correctIndex:1,explanation:"Deploying across multiple availability zones (AZs) with a load balancer provides fault tolerance against entire data center failures. If one AZ experiences an outage (power failure, network issues), the load balancer automatically routes traffic to healthy instances in other AZs. Each AZ is an isolated data center with independent power, cooling, and networking. AWS ALB, for example, is inherently multi-AZ  it deploys nodes in each enabled AZ and performs cross-zone load balancing. This is a fundamental best practice for production workloads, as it protects against the most common cause of large-scale outages: AZ-level failures."},{question:"What is cross-zone load balancing?",options:["Load balancing between different cloud providers","Distributing traffic evenly across all targets in all enabled availability zones","Balancing traffic between development and production zones","Load balancing between different geographic regions"],correctIndex:1,explanation:"Cross-zone load balancing ensures that traffic is distributed evenly across all registered targets in all enabled availability zones, regardless of which AZ the load balancer node received the traffic. Without cross-zone balancing, each LB node only distributes to targets in its own AZ, which can cause uneven distribution if AZs have different numbers of targets. For example, if AZ-A has 2 instances and AZ-B has 8, without cross-zone balancing each AZ gets 50% of traffic, meaning AZ-A's 2 instances each handle 25% while AZ-B's 8 each handle only 6.25%. AWS ALB has cross-zone enabled by default, while NLB has it disabled by default."},{question:"What is a load balancer sandwich architecture?",options:["A load balancer placed between two firewalls","Multiple tiers of load balancers for different layers of an application","A load balancer between two identical server groups","A redundant pair of load balancers"],correctIndex:1,explanation:"A load balancer sandwich (or multi-tier load balancing) uses load balancers at multiple levels in the application architecture. For example, an external L7 ALB routes to web servers, which then connect through an internal L4 NLB to application servers, which connect through another internal NLB to database read replicas. This provides granular scaling and health checking at each tier. It's common in microservices architectures where each service layer needs independent scaling and traffic management. AWS recommends this pattern for three-tier architectures, using public ALBs for the web tier and private NLBs for internal service-to-service communication."},{question:"How does HAProxy differ from Nginx in terms of primary design philosophy?",options:["HAProxy is only L4, Nginx is only L7","HAProxy was purpose-built for proxying/load balancing; Nginx was originally a web server","Nginx is faster than HAProxy in all scenarios","HAProxy doesn't support HTTP"],correctIndex:1,explanation:"HAProxy was designed from the ground up as a high-performance TCP/HTTP load balancer and proxy, while Nginx was originally created as a web server that later gained load balancing capabilities. This means HAProxy has more sophisticated load balancing features out of the box: advanced health checking, detailed connection statistics, hitless reloads, and fine-grained traffic management. Nginx excels as a combined web server, reverse proxy, and load balancer, making it a great all-in-one solution. In practice, many architectures use both: Nginx serving static content and HAProxy handling load balancing for dynamic traffic. Both can handle millions of concurrent connections."},{question:"What is the 'least response time' load balancing algorithm?",options:["Routes to the server that started most recently","Routes to the server with the fastest response time and fewest active connections","Routes to the server closest geographically","Routes to the server with the most memory available"],correctIndex:1,explanation:"The least response time algorithm considers both the number of active connections and the server's recent response time to choose the best backend. It routes requests to the server that can respond fastest, combining the benefits of least connections with actual performance data. This is more sophisticated than pure least connections because a server with few connections might still be slow due to hardware issues or heavy background processing. Nginx Plus offers this as 'least_time' where you can optimize for header response time or full response time. It's particularly useful for heterogeneous server pools where machines have different performance characteristics."},{question:"What is preconnection (TCP connection pooling) in load balancers?",options:["Connecting to servers before any client requests arrive","Maintaining a pool of pre-established connections to backend servers","Pre-allocating IP addresses for new servers","Connecting multiple load balancers together"],correctIndex:1,explanation:"Connection pooling (or multiplexing) means the load balancer maintains a pool of pre-established, persistent TCP connections to backend servers. Instead of creating a new TCP connection (with its 3-way handshake overhead) for every client request, the load balancer reuses existing connections from the pool. This dramatically reduces latency and server load, especially under high traffic. AWS ALB uses connection multiplexing by default  it may send multiple client requests over a single backend connection. This is particularly impactful for HTTPS backends where each new connection would require a full TLS handshake, which can take 2-3 round trips."},{question:"What is the purpose of the X-Forwarded-Proto header?",options:["To specify the protocol version","To indicate the original protocol (HTTP/HTTPS) used by the client before the load balancer","To specify which backend protocol to use","To forward the prototype of the request object"],correctIndex:1,explanation:"X-Forwarded-Proto tells the backend server whether the original client connection used HTTP or HTTPS. When SSL termination occurs at the load balancer, the backend receives plain HTTP, so it can't tell if the original request was secure. This header is crucial for applications that need to enforce HTTPS redirects, generate correct absolute URLs, or set secure cookie flags. Without it, an app might generate 'http://' links even though the user accessed it via HTTPS. For example, Django uses the SECURE_PROXY_SSL_HEADER setting to check X-Forwarded-Proto, and Rails uses it to determine if a request was made over SSL."},{question:"What happens when all backend servers fail health checks?",options:["The load balancer shuts down","Behavior varies: some return 503, others fall back to sending traffic to all servers","All traffic is dropped silently","The load balancer creates new server instances"],correctIndex:1,explanation:"When all backends are unhealthy, load balancer behavior depends on configuration. Some load balancers return 503 Service Unavailable to all clients. Others implement a 'panic mode' or 'all backends down' fallback where they send traffic to all servers anyway  the reasoning being that health checks might be too strict and some servers might still be partially functional. HAProxy supports 'option allbackups' and backup server configurations for this scenario. AWS ALB returns 503 when no healthy targets exist. Nginx can be configured with 'backup' servers that only receive traffic when all primary servers are down. It's critical to plan for this scenario in your architecture."},{question:"What is a 'least bandwidth' load balancing algorithm?",options:["Routes to the server using the least network bandwidth currently","Routes to the server with the smallest bandwidth capacity","Routes to reduce overall bandwidth usage","Routes to the server with the most available bandwidth"],correctIndex:0,explanation:"The least bandwidth algorithm routes new requests to the server currently consuming the least amount of network bandwidth (measured in Mbps). This is useful for workloads with varying response sizes  for example, a file download service where some requests return 10MB files and others return 100KB files. Unlike least connections which treats all connections equally, least bandwidth accounts for the actual data transfer happening on each server. This prevents scenarios where one server is handling fewer but larger transfers while others handle many small ones. It's available in enterprise load balancers like F5 BIG-IP and Citrix ADC."},{question:"What is rate limiting at the load balancer level?",options:["Limiting the speed of the load balancer itself","Restricting the number of requests a client can make within a time period","Limiting the number of backend servers","Slowing down all traffic equally"],correctIndex:1,explanation:"Rate limiting at the load balancer restricts how many requests a particular client (identified by IP, API key, or other attributes) can make within a defined time window. This protects backend servers from abuse, DDoS attacks, and runaway clients. For example, you might limit an IP to 100 requests per minute. Requests exceeding the limit receive a 429 Too Many Requests response. Implementing rate limiting at the load balancer is more efficient than at the application level because rejected requests never reach backend servers. Nginx supports rate limiting with the 'limit_req' module, and AWS ALB integrates with AWS WAF for advanced rate limiting rules."},{question:"What is the difference between Layer 4 NAT mode and Layer 4 DR (Direct Return) mode?",options:["NAT modifies both request and response packets; DR only modifies request packets since responses go directly to clients","They are identical","NAT is faster than DR","DR requires L7 inspection"],correctIndex:0,explanation:"In NAT mode, the load balancer rewrites packet headers for both incoming requests (changing destination to the backend) and outgoing responses (changing source back to the VIP). All traffic flows through the load balancer, making it a potential bottleneck. In Direct Return (DR) mode, the load balancer only modifies incoming request packets, and the backend server sends responses directly to the client, bypassing the load balancer. DR is much more scalable because response traffic (typically 10x larger than request traffic) doesn't burden the load balancer. However, DR requires backends to be on the same L2 network and accept traffic for the VIP address. LVS (Linux Virtual Server) supports both modes."},{question:"What is a 'target group' in AWS load balancer terminology?",options:["A group of users targeted by the application","A logical grouping of backend targets (instances, IPs, or lambdas) that receive traffic from a load balancer","A group of load balancers","A security group for targets"],correctIndex:1,explanation:"A target group is an AWS concept that defines a set of backend targets (EC2 instances, IP addresses, Lambda functions, or other ALBs) along with health check settings and routing configuration. The load balancer routes requests to targets within a target group based on the listener rules. You can have multiple target groups attached to a single ALB with different routing rules  for example, one target group for /api/* and another for /web/*. Target groups also enable blue-green and canary deployments by adjusting weights between two target groups. This abstraction decouples the load balancer configuration from the specific backend instances."},{question:"How does a load balancer handle gRPC traffic?",options:["gRPC cannot be load balanced","Using L7 load balancing with HTTP/2 support","Only L4 load balancing works for gRPC","By converting gRPC to REST"],correctIndex:1,explanation:"gRPC uses HTTP/2 as its transport protocol, so a load balancer needs HTTP/2 support for effective gRPC load balancing. L7 load balancers that understand HTTP/2 can route individual gRPC requests within a single HTTP/2 connection to different backends  this is crucial because HTTP/2 multiplexes many requests over one connection, so L4 balancing would send all requests on a connection to the same backend. AWS ALB supports gRPC natively since 2020, and Envoy proxy is widely used for gRPC load balancing in service meshes. Without proper HTTP/2-aware L7 balancing, you'd lose the benefits of distributing gRPC calls across backends."},{question:"What is the 'power of two choices' load balancing algorithm?",options:["Choosing between only two backend servers","Randomly selecting two servers and routing to the one with fewer connections","Alternating between two algorithms","Using two load balancers simultaneously"],correctIndex:1,explanation:"The 'power of two choices' (P2C) algorithm randomly selects two backend servers and routes the request to whichever has fewer active connections. Despite its simplicity, it provides near-optimal load distribution  mathematically proven to reduce maximum load from O(log n / log log n) with random to O(log log n) with P2C. It's much cheaper to compute than full least-connections (which must check all servers) while providing similar benefits. Nginx Plus uses this as its 'random two least_conn' method. It's particularly effective in large clusters where checking every server's status on every request would be too expensive, and it avoids the herd behavior where multiple load balancers simultaneously route to the same 'least loaded' server."},{question:"What is a 'listener' in load balancer configuration?",options:["A monitoring tool","A process that checks for incoming connections on a specific port and protocol","A logging component","A backend server waiting for connections"],correctIndex:1,explanation:"A listener is a configuration component that defines a port and protocol that the load balancer listens on for incoming client connections. For example, you might have one listener on port 80 (HTTP) that redirects to HTTPS, and another on port 443 (HTTPS) that forwards to your target group. Each listener has rules that determine how to route requests to backend targets. AWS ALB supports multiple listeners on a single load balancer, each with its own set of routing rules. Listeners are where you configure SSL certificates, default actions, and the initial entry point for all client traffic hitting the load balancer."},{question:"Why might you choose an L4 load balancer over an L7 for a database cluster?",options:["L4 is cheaper","Databases use TCP protocols that don't need HTTP inspection, and L4 offers lower latency","L7 doesn't support TCP","L4 provides better security"],correctIndex:1,explanation:"Database protocols like MySQL (port 3306) and PostgreSQL (port 5432) use raw TCP connections, not HTTP. An L7 load balancer that inspects HTTP would be useless and add unnecessary overhead for database traffic. An L4 load balancer simply forwards TCP connections based on IP and port, which is exactly what's needed. It adds minimal latency since there's no packet inspection or protocol parsing. AWS NLB is commonly used to load balance database read replicas or to provide a single endpoint for a database cluster. The lower latency of L4 is particularly important for databases where every millisecond counts for query response times."},{question:"What is 'session persistence' and how is it different from 'sticky sessions'?",options:["They are the same concept  both ensure requests from one client go to the same backend","Session persistence stores data; sticky sessions route traffic","Session persistence is for databases; sticky sessions for web servers","Sticky sessions are permanent; session persistence is temporary"],correctIndex:0,explanation:"Session persistence and sticky sessions are two names for the same concept  ensuring that all requests from a particular client are routed to the same backend server during a session. The terms are used interchangeably across different load balancer vendors: F5 calls it 'persistence,' AWS calls it 'stickiness,' and HAProxy calls it 'stick-tables.' Implementation methods include cookie-based (load balancer sets a cookie identifying the backend), source IP-based (hash of client IP), and SSL session ID-based persistence. The choice of method depends on whether you're doing L4 or L7 balancing and whether clients support cookies."},{question:"What is the risk of having a single load balancer in your architecture?",options:["Higher cost","It becomes a single point of failure (SPOF)","Slower performance","More complex configuration"],correctIndex:1,explanation:"A single load balancer creates a single point of failure  if it goes down, all traffic stops regardless of how many healthy backend servers exist. This defeats the purpose of using a load balancer for high availability. The solution is to deploy load balancers in pairs (active-passive or active-active) with failover mechanisms. In cloud environments, managed load balancers like AWS ALB are inherently redundant across multiple availability zones, eliminating this concern. For on-premise deployments, tools like keepalived implement VRRP to manage VIP failover between HAProxy or Nginx instances, ensuring continuous availability even if one load balancer node fails."},{question:"What is Maglev hashing in the context of load balancing?",options:["A hashing algorithm for SSL certificates","Google's consistent hashing algorithm designed for software load balancers with fast lookup","A magnetic storage-based caching algorithm","A hash algorithm for compressing HTTP headers"],correctIndex:1,explanation:"Maglev hashing is a consistent hashing algorithm developed by Google for their Maglev network load balancer. It provides O(1) lookup time using a precomputed lookup table, making it significantly faster than ring-based consistent hashing which requires O(log n) lookups. Maglev hashing generates a permutation table for each backend, creating a fixed-size lookup table where each entry maps to a backend. When a backend is added or removed, minimal disruption occurs (similar to consistent hashing). It also provides excellent load uniformity across backends. Google's Maglev handles over a million requests per second per machine, and the algorithm was published in their 2016 NSDI paper."},{question:"What is the purpose of HTTP keep-alive in the context of load balancing?",options:["To keep the load balancer running","To reuse TCP connections for multiple HTTP requests, reducing connection overhead","To monitor server health","To maintain session state"],correctIndex:1,explanation:"HTTP keep-alive (persistent connections) allows multiple HTTP requests to be sent over a single TCP connection instead of opening a new connection for each request. In the context of load balancing, this reduces the overhead of TCP handshakes and TLS negotiations between clients and the load balancer, and between the load balancer and backends. However, it creates a tension with load balancing: if a client's keep-alive connection always goes to the same backend, new requests won't be distributed. L7 load balancers like ALB solve this by multiplexing  they can maintain keep-alive connections on both sides but route individual requests within those connections to different backends."},{question:"How does Envoy proxy differ from traditional load balancers like HAProxy?",options:["Envoy doesn't support load balancing","Envoy was designed as a sidecar proxy for microservices with advanced observability features","Envoy only supports L4 balancing","Envoy is faster than HAProxy in all cases"],correctIndex:1,explanation:"Envoy was designed by Lyft specifically for modern microservices architectures, functioning as a sidecar proxy deployed alongside each service instance. Unlike HAProxy which is typically deployed as a centralized load balancer, Envoy is the data plane in service meshes like Istio. Envoy provides advanced features like automatic retries, circuit breaking, zone-aware routing, and rich L7 observability with distributed tracing integration. It supports dynamic configuration via xDS APIs, allowing configuration changes without restarts. While HAProxy excels as a high-performance edge proxy, Envoy shines in service-to-service communication within distributed systems where observability and dynamic configuration are crucial."},{question:"What is a 'backend' vs 'frontend' in HAProxy terminology?",options:["Frontend is the UI, backend is the database","Frontend defines how requests are received; backend defines where they are forwarded","Frontend is external facing; backend is internal only","They are interchangeable terms"],correctIndex:1,explanation:"In HAProxy, a 'frontend' section defines how incoming connections are received  it specifies the bind address/port, protocol mode (HTTP or TCP), timeouts, and ACL rules for routing decisions. A 'backend' section defines the pool of servers that will handle forwarded requests, including the server list, load balancing algorithm, and health check configuration. A frontend can route to multiple backends based on ACL rules. For example, a frontend listening on port 443 might route /api/* to an api-backend and /static/* to a static-backend. This separation of concerns makes HAProxy configurations clean and modular."},{question:"What is circuit breaking in load balancing?",options:["Physically disconnecting network cables","Automatically stopping traffic to a failing backend to prevent cascade failures","Breaking long-running connections","A backup power system for load balancers"],correctIndex:1,explanation:"Circuit breaking is a pattern where the load balancer (or proxy) monitors error rates for each backend and automatically stops sending traffic when failures exceed a threshold, preventing cascade failures. Like an electrical circuit breaker, it 'trips open' when too many errors occur, returning immediate errors to clients instead of waiting for timeouts. After a cooldown period, it enters a 'half-open' state and allows a few test requests through. If they succeed, the circuit closes and normal traffic resumes. Envoy and Istio implement sophisticated circuit breaking with configurable thresholds for maximum connections, pending requests, and retries. This is essential in microservices to prevent one failing service from taking down the entire system."},{question:"What is the 'source' load balancing algorithm in HAProxy?",options:["Routes based on the source code of the application","Hashes the source IP to always route the same client to the same server","Selects the source server with the most resources","Routes based on the country of origin"],correctIndex:1,explanation:"The 'source' algorithm in HAProxy hashes the client's source IP address to select a backend server, ensuring the same client consistently reaches the same server. It's essentially IP hash load balancing. This provides session affinity at L4 without requiring cookies or application changes. The formula is typically hash(source_ip) mod num_servers. The main disadvantage is that if a server is added or removed, the hash changes for many clients, disrupting existing sessions. You can mitigate this with 'hash-type consistent' which uses consistent hashing to minimize remapping. It's commonly used for TCP load balancing where cookie-based affinity isn't available."},{question:"What is 'request queuing' in a load balancer?",options:["Queuing requests for later processing when all backends are busy","Sorting requests by priority","Buffering requests to batch them","Caching requests for replay"],correctIndex:0,explanation:"Request queuing occurs when the load balancer holds incoming requests in a queue when all backend servers have reached their maximum connection limits, rather than immediately rejecting them. The queue releases requests as backends become available. This smooths out traffic spikes and prevents immediate 503 errors during brief overload periods. HAProxy implements this with 'maxconn' on server definitions (limiting per-server connections) combined with the 'queue' timeout. However, queuing increases latency and can mask capacity problems  if the queue grows too long, it's better to shed load with 503 responses than to keep clients waiting. Setting appropriate queue timeouts is critical."},{question:"How does a load balancer handle HTTP/2 server push?",options:["It blocks server push entirely","It must support HTTP/2 end-to-end to proxy push frames correctly","It converts push to regular requests","Server push doesn't work with load balancers"],correctIndex:1,explanation:"HTTP/2 server push allows servers to proactively send resources to clients before they're requested. For a load balancer to correctly handle this, it must support HTTP/2 on both the client-facing side and the backend connections, properly proxying PUSH_PROMISE frames. If the load balancer terminates HTTP/2 and connects to backends via HTTP/1.1, server push won't work because HTTP/1.1 doesn't support it. AWS ALB supports HTTP/2 on the frontend but communicates with backends using HTTP/1.1, so server push doesn't work through ALB. Nginx supports HTTP/2 end-to-end (with 'grpc_pass' or HTTP/2 upstream) enabling server push passthrough."},{question:"What is the 'agent check' feature in HAProxy?",options:["A security scanning tool","An auxiliary health check where the backend server reports its own status and weight","Checking if the HAProxy agent is running","A monitoring agent installed on each server"],correctIndex:1,explanation:"HAProxy's agent check is a supplementary health check mechanism where the backend server runs a small agent (listening on a configured port) that reports its status and desired weight. The agent can respond with values like 'up', 'down', 'ready', a percentage to set weight (e.g., '75%'), or 'drain' to stop new connections. This allows application-aware load management  for example, a server under heavy CPU load can tell HAProxy to reduce its weight to 25%, and a server during maintenance can report 'drain' to stop receiving new traffic. Agent checks work alongside regular health checks, giving backend servers active participation in traffic management."},{question:"What is Anycast and how is it used with load balancing?",options:["Broadcasting to all servers simultaneously","Multiple servers share the same IP address; the network routes clients to the nearest one","Sending requests to any available server randomly","A multicast protocol for load balancers"],correctIndex:1,explanation:"Anycast is a network addressing method where the same IP address is announced by multiple servers in different geographic locations. The internet's BGP routing automatically directs clients to the nearest (in terms of network hops) server advertising that IP. This provides natural geographic load balancing without any application-level load balancer. Cloudflare uses anycast extensively  their IP addresses are announced from 300+ data centers worldwide, so a user in Tokyo reaches a Tokyo server while a user in London reaches a London server. Anycast also provides built-in DDoS resilience since attack traffic is distributed across all locations. It's commonly combined with traditional load balancing within each location."},{question:"What is 'connection multiplexing' in an L7 load balancer?",options:["Using multiple network interfaces","Sending requests from multiple clients over a single backend connection","Connecting to multiple load balancers simultaneously","Multiplying the number of available connections"],correctIndex:1,explanation:"Connection multiplexing means the L7 load balancer can take HTTP requests from many different client connections and send them over fewer backend connections. For example, 1000 client connections might be served by just 10 backend connections, with the load balancer interleaving requests. This dramatically reduces the connection overhead on backend servers, which is especially valuable for servers with high connection setup costs. AWS ALB does this by default  it maintains a small pool of connections to each backend and multiplexes client requests over them. This is one reason why backend servers see far fewer connections than expected when behind an ALB."},{question:"When would you use a TCP (L4) proxy mode in HAProxy instead of HTTP (L7)?",options:["When serving web pages","When load balancing non-HTTP protocols like databases, SMTP, or custom TCP protocols","When you need URL-based routing","When you need cookie-based persistence"],correctIndex:1,explanation:"TCP mode (mode tcp) in HAProxy is used when you need to load balance non-HTTP protocols. Databases (MySQL, PostgreSQL, Redis), mail servers (SMTP), LDAP, custom binary protocols, and any other TCP-based service requires L4 proxy mode because HAProxy wouldn't understand the application protocol for L7 inspection. TCP mode is also used for SSL passthrough where you don't want HAProxy to decrypt traffic. It's faster than HTTP mode since there's no header parsing. For example, to load balance MySQL read replicas, you'd use 'mode tcp' with 'option mysql-check' for health checking. The tradeoff is losing L7 features like path routing and header manipulation."},{question:"What is the role of a load balancer in a microservices architecture?",options:["Only external traffic management","Both external traffic routing and internal service-to-service discovery and routing","Database connection pooling only","Log aggregation"],correctIndex:1,explanation:"In microservices, load balancers serve dual roles: external-facing load balancers (edge proxies) route client traffic, while internal load balancers handle service-to-service communication. An API gateway (L7 LB) at the edge routes requests to appropriate microservices, while internal LBs or service meshes (like Envoy sidecar proxies) balance traffic between service instances. For example, an order service calling the inventory service needs load balancing across inventory instances. Kubernetes Services with kube-proxy provide built-in L4 load balancing, while Istio adds sophisticated L7 balancing. This multi-layer load balancing is essential because microservices create many more internal network hops than monolithic architectures."},{question:"What is 'graceful degradation' in the context of load balancing?",options:["Gradually shutting down the load balancer","Maintaining partial service when backends fail rather than total outage","Slowing down request processing during peak times","Downgrading to a simpler load balancing algorithm"],correctIndex:1,explanation:"Graceful degradation means the system continues to provide reduced but functional service when some components fail, rather than failing completely. In load balancing, this might mean returning cached or static content when backend servers are down, directing overflow traffic to a 'sorry' page, or serving a read-only version when write servers fail. For example, Netflix uses graceful degradation extensively  if their recommendation service is down, they show generic popular content instead of personalized recommendations. Load balancers implement this through fallback backends, custom error pages, and integration with circuit breakers that redirect to fallback services when primary backends are failing."},{question:"What is the difference between 'health check' and 'liveness probe'?",options:["They are identical concepts","Health checks are performed by load balancers; liveness probes are used by container orchestrators like Kubernetes","Health checks are manual; liveness probes are automatic","Health checks are for hardware; liveness probes are for software"],correctIndex:1,explanation:"While both verify if a service is running, health checks are performed by load balancers to decide whether to route traffic to a backend, while liveness probes are Kubernetes concepts used to determine whether to restart a container. A load balancer health check failure removes the target from the routing pool but doesn't restart it. A Kubernetes liveness probe failure causes kubelet to restart the container. They can even use different endpoints  a liveness probe might check basic process health while a health check verifies the service can handle requests. In practice, you often have both: a /livez endpoint for Kubernetes and a /readyz endpoint for the load balancer's health check."},{question:"What problem does 'least outstanding requests' solve compared to 'least connections'?",options:["It handles more protocols","It accounts for requests in the load balancer's queue, not just established connections","It's faster to compute","It uses less memory"],correctIndex:1,explanation:"Least outstanding requests considers all pending requests including those queued at the load balancer, not just established TCP connections to backends. A server might show few active connections if it's fast at accepting them but slow at processing them. Least outstanding requests tracks the total number of in-flight requests (queued + being processed) per backend. This gives a more accurate picture of actual server load. AWS ALB uses 'least outstanding requests' as an alternative to round-robin for target group routing. It's especially effective when request processing times vary significantly and you want to ensure faster servers naturally receive more traffic."},{question:"What is 'traffic mirroring' (shadowing) in load balancing?",options:["Creating backup copies of network traffic for storage","Duplicating live traffic to a test environment for testing without impacting production","Redirecting all traffic to a mirror server","Encrypting traffic twice for security"],correctIndex:1,explanation:"Traffic mirroring copies live production requests and sends them to a separate test/staging environment simultaneously, allowing you to test new service versions with real traffic patterns without affecting the production response. The client only receives the response from the primary backend; the mirror's response is discarded. This is invaluable for validating new deployments  you can compare the mirror's behavior against production to catch bugs before they impact users. Envoy, Istio, and AWS ALB all support traffic mirroring. For example, before launching a new recommendation engine, you'd mirror production traffic to it and compare results against the existing system."},{question:"What is 'retry budgets' in the context of load balancer retries?",options:["A financial budget for retry infrastructure","Limiting the total percentage of requests that can be retries to prevent retry storms","The number of times a request can be retried","A budget allocated for timeout extensions"],correctIndex:1,explanation:"A retry budget limits the ratio of retry requests to total requests (e.g., retries should not exceed 20% of original traffic). Without a budget, retries can cascade catastrophically: if a backend is overloaded and failing, retries add even more load, causing more failures and more retries  a 'retry storm.' The budget ensures retries are helpful during transient errors but are automatically throttled during systematic failures. Envoy implements retry budgets with the 'retry_budget' configuration, setting max_retries as a percentage of active requests. This is a critical safety mechanism in microservices where multiple services retrying simultaneously can amplify a minor issue into a complete outage."},{question:"How does AWS ALB handle 'slow loris' attacks?",options:["It cannot handle them","It sets idle timeouts and manages connections independently from backend servers","It blocks all slow connections","It requires a separate WAF"],correctIndex:1,explanation:"Slow loris attacks work by opening many connections and sending data very slowly, tying up server resources. AWS ALB mitigates this because it acts as a full proxy  it buffers the entire client request before forwarding it to the backend, so the backend server isn't held up by slow clients. ALB also has idle connection timeouts that close stale connections. Since ALB is a managed service with massive capacity, it can absorb the connection overhead that would overwhelm a single server. Additionally, combining ALB with AWS WAF provides rate-limiting rules that can further protect against such attacks. This proxy-based buffering is a fundamental advantage of L7 load balancers over L4."},{question:"What does 'least pending requests' mean in the context of Envoy proxy?",options:["Requests waiting to be processed at the proxy","Requests that haven't been retried yet","Requests pending cancellation","Requests pending authentication"],correctIndex:0,explanation:"In Envoy, 'least pending requests' (also called 'least request') routes new requests to the backend with the fewest outstanding (pending) requests. This includes requests currently being processed by the backend and those queued at the proxy level. It's Envoy's equivalent of least connections but operates at the request level rather than connection level  this distinction matters with HTTP/2 where multiple requests share a single connection. Envoy's implementation uses the 'power of two choices' optimization by default: it randomly picks two backends and selects the one with fewer pending requests, providing near-optimal distribution with minimal overhead. This is the recommended algorithm for most HTTP workloads in Envoy and Istio service meshes."},{question:"What is 'priority-based routing' in load balancing?",options:["Routing VIP customers first","Routing to primary backends first, falling back to secondary when primaries are unhealthy","Processing high-priority requests faster","Assigning higher network priority to certain packets"],correctIndex:1,explanation:"Priority-based routing assigns priority levels to different groups of backend servers. Traffic is first routed to the highest-priority group; only when that group becomes unhealthy (below a minimum healthy threshold) does traffic overflow to the next priority level. This is useful for preferring local servers over remote ones, or primary servers over backup servers. For example, you might assign priority 1 to servers in the same AZ and priority 2 to servers in other AZs, routing cross-AZ only when local capacity is insufficient. AWS ALB doesn't natively support this, but Envoy implements it through priority levels in clusters, and HAProxy achieves it with 'backup' server directives."},{question:"What is 'locality-aware routing' in service mesh load balancing?",options:["Routing based on language locale","Preferring backends in the same zone/region to reduce latency and cross-zone costs","Routing to locally installed software","Routing based on local time"],correctIndex:1,explanation:"Locality-aware routing preferentially routes requests to backends in the same zone, availability zone, or region as the client to minimize latency and avoid cross-zone data transfer costs. In cloud environments, cross-AZ traffic incurs charges and adds 1-2ms of latency. Istio and Envoy implement locality-aware routing by considering zone, region, and sub-zone labels. When enough healthy local endpoints exist, traffic stays local; when local capacity is insufficient, it overflows to other zones proportionally. For example, in a Kubernetes cluster spanning 3 AZs, a pod in us-east-1a preferentially communicates with other pods in us-east-1a. This can save significant costs on high-traffic internal services."},{question:"What is a 'service mesh' and how does it relate to load balancing?",options:["A physical network topology","An infrastructure layer that handles service-to-service communication including load balancing, with sidecar proxies","A type of load balancer hardware","A mesh of interconnected load balancers"],correctIndex:1,explanation:"A service mesh is a dedicated infrastructure layer for managing service-to-service communication in microservices architectures. It deploys sidecar proxies (like Envoy) alongside each service instance, handling load balancing, retries, circuit breaking, mutual TLS, and observability transparently. Instead of each service implementing its own load balancing logic, the sidecar handles it uniformly. Istio (using Envoy sidecars) is the most popular service mesh. The mesh provides consistent, centrally-managed traffic policies across all services without modifying application code. This decouples networking concerns from business logic, but adds complexity and resource overhead  each sidecar consumes CPU and memory."},{question:"What is 'outlier detection' in Envoy/Istio load balancing?",options:["Detecting unusual user behavior","Automatically ejecting backends that show abnormal error rates or latency","Finding misconfigured servers","Detecting DDoS attacks"],correctIndex:1,explanation:"Outlier detection is Envoy's mechanism for automatically identifying and ejecting misbehaving backends from the load balancing pool. It monitors metrics like consecutive errors (5xx responses) or gateway errors, and temporarily removes backends that exceed thresholds. For example, if a backend returns 5 consecutive 503 errors, it's ejected for 30 seconds. This is similar to circuit breaking but operates per-backend rather than on the overall cluster. Ejected hosts are periodically allowed back in to test if they've recovered. It's configured in Istio via DestinationRule resources. Outlier detection is critical in microservices where a single failing instance can slow down the entire system if traffic keeps being sent to it."},{question:"What is the purpose of 'retry policies' in load balancing?",options:["Retrying the load balancer configuration","Automatically retrying failed requests on different backends to handle transient failures","Retrying health checks more frequently","Retrying DNS resolution"],correctIndex:1,explanation:"Retry policies allow the load balancer to automatically retry failed requests on different backend servers, recovering from transient failures without client involvement. When a request to backend A fails with a 503, the load balancer can transparently retry it on backend B. This is especially valuable for handling single-server failures, transient network issues, and server restarts during deployments. However, retries must be configured carefully  they should only apply to idempotent requests (GET, not POST) to avoid duplicate side effects, and must include retry budgets to prevent retry storms. Envoy supports sophisticated retry policies including per-try timeouts, retry conditions (5xx, connection failure), and maximum retry attempts."},{question:"What is 'header-based routing' in an L7 load balancer?",options:["Routing based on packet headers","Routing decisions based on specific HTTP header values like API version or user agent","Adding headers to every request","Removing headers from responses"],correctIndex:1,explanation:"Header-based routing allows an L7 load balancer to examine specific HTTP headers and route requests to different backend groups based on their values. For example, routing requests with 'X-API-Version: v2' to a new API backend while 'X-API-Version: v1' goes to the legacy backend. This enables sophisticated traffic management like A/B testing (routing based on experiment headers), canary releases (routing based on beta-user headers), and multi-tenant routing. AWS ALB supports header-based routing in its listener rules, and Envoy provides extremely flexible header matching including regex, prefix, and exact matching. This is a powerful L7 capability that's impossible with L4 load balancers."},{question:"What is 'TLS re-encryption' (SSL bridging) at the load balancer?",options:["Using two different SSL certificates","Decrypting client TLS at the LB and re-encrypting with a different certificate to backends","Encrypting already encrypted traffic","Converting TLS to a newer version"],correctIndex:1,explanation:"TLS re-encryption (SSL bridging) means the load balancer terminates the client's TLS connection, inspects/routes the HTTP request at L7, and then establishes a new TLS connection to the backend server. This provides the benefits of both worlds: L7 inspection capabilities (content routing, header manipulation) AND encrypted backend communication. The backend TLS certificate is typically an internal CA certificate different from the public-facing certificate. This is required in environments where regulations mandate encryption of data in transit even within the internal network. AWS ALB supports this  you can configure HTTPS listeners with HTTPS target groups, achieving end-to-end encryption while still getting L7 features."},{question:"What is the difference between 'active' and 'passive' health checks?",options:["Active checks are faster; passive are slower","Active checks proactively send requests; passive checks monitor actual traffic for failures","Active checks run during the day; passive checks run at night","Active checks require agents; passive checks don't"],correctIndex:1,explanation:"Active health checks proactively send periodic test requests (HTTP GET, TCP connect) to backend servers to verify their health, regardless of whether real traffic is flowing. Passive health checks (also called 'real traffic' checks) monitor the responses from actual client requests to detect failures  if a backend returns too many errors, it's marked unhealthy. Active checks detect issues even on idle backends but add extra traffic. Passive checks detect issues based on real behavior but can't identify problems on backends that aren't receiving traffic. The best practice is to use both together  Envoy and Nginx Plus support combining active and passive health checking for robust failure detection."},{question:"How do load balancers handle HTTP/3 (QUIC)?",options:["They don't support it","They terminate QUIC at the edge and typically proxy to backends over HTTP/2 or HTTP/1.1","They pass QUIC through unchanged","They convert QUIC to TCP"],correctIndex:1,explanation:"HTTP/3 uses QUIC (a UDP-based transport protocol) instead of TCP, which presents challenges for load balancers. Most L7 load balancers terminate QUIC at the edge, benefiting from QUIC's faster connection establishment (0-RTT) and better handling of packet loss for end users, then proxy requests to backends over HTTP/2 or HTTP/1.1 over TCP. This is practical because the internal network typically has low latency and packet loss, making QUIC's benefits less relevant for backend connections. Cloudflare, Google Cloud, and AWS CloudFront support HTTP/3 termination. L4 load balancers face challenges because QUIC's connection IDs don't map to traditional IP:port tuples, requiring QUIC-aware load balancing."},{question:"What is the 'hash ring' approach to load balancing?",options:["Hashing requests in a circular sequence","Using consistent hashing where servers and keys are mapped to positions on a virtual ring","Arranging servers in a physical ring topology","A hardware ring buffer in the load balancer"],correctIndex:1,explanation:"The hash ring is the data structure underlying consistent hashing for load balancing. Servers are assigned positions on a circular hash space (0 to 2^32-1), and each request key is hashed to a position on the same ring. The request is routed to the nearest server clockwise on the ring. When a server is removed, only its portion of the ring is remapped to the next server; other mappings stay intact. Virtual nodes (multiple hash positions per physical server) improve uniformity. This approach is particularly valuable for caching proxies where you want the same content to consistently hit the same server to maximize cache hit rates. Nginx supports this with the 'hash' directive and 'consistent' parameter."},{question:"What is 'request hedging' in load balancing?",options:["Protecting requests with encryption","Sending the same request to multiple backends simultaneously and using the first response","Queuing requests as insurance against failure","Prioritizing certain requests over others"],correctIndex:1,explanation:"Request hedging sends redundant copies of a request to multiple backends simultaneously, returning whichever response arrives first and discarding the rest. This reduces tail latency  if one backend is slow, the duplicate on another backend may respond faster. Google uses hedging extensively to reduce p99 latency. The tradeoff is increased resource usage since you're doing N times the work for one response. A common optimization is 'delayed hedging'  send the first request normally and only hedge if no response arrives within a timeout (e.g., the p50 latency). gRPC and Envoy support hedging policies. It's most effective for read-only, idempotent operations where the extra backend load is acceptable."},{question:"What is 'load shedding' at the load balancer?",options:["Reducing the physical weight of the load balancer hardware","Deliberately dropping excess traffic to protect the system from overload","Shedding responsibility to backend servers","Reducing the number of backend servers"],correctIndex:1,explanation:"Load shedding is the practice of intentionally dropping or rejecting excess requests when the system is approaching or at capacity, preventing total system failure. It's better to reject 10% of requests cleanly with 503 responses than to accept everything and have 100% of requests fail due to overload. Load balancers implement this via connection limits, request rate limits, and queue depth limits. For example, Envoy's circuit breaker can limit maximum concurrent connections and pending requests per backend. Netflix and Google implement adaptive load shedding that adjusts thresholds based on current system health. Think of it like a nightclub with a maximum capacity  turning people away at the door keeps the experience good for everyone inside."},{question:"What is the 'proxy protocol' used by some load balancers?",options:["A protocol for communication between multiple load balancers","A protocol that prepends client connection info (IP, port) to the TCP connection for backends","A protocol for proxy server authentication","A protocol for load balancer health checks"],correctIndex:1,explanation:"The Proxy Protocol (developed by HAProxy's creator) is a simple protocol that prepends a small header to TCP connections containing the original client's IP address and port. This solves the problem of preserving client information when using L4 (TCP) load balancing, where HTTP headers like X-Forwarded-For aren't available because the load balancer doesn't inspect HTTP content. The header is added at the start of the TCP connection and contains: protocol version, source/destination IPs, and source/destination ports. AWS NLB supports Proxy Protocol v2, allowing backend applications to see the real client IP even through an L4 load balancer. The backend application must be configured to parse and strip this header."},{question:"What is an ingress controller in Kubernetes, and how does it relate to load balancing?",options:["A firewall for incoming traffic","A Kubernetes component that manages external access to services, typically implementing L7 load balancing","A controller that manages server ingress ports","A monitoring tool for incoming connections"],correctIndex:1,explanation:"A Kubernetes Ingress Controller is a component that implements the Kubernetes Ingress resource, providing L7 load balancing, SSL termination, and path/host-based routing for external traffic entering the cluster. Popular implementations include Nginx Ingress Controller, Traefik, HAProxy Ingress, and AWS ALB Ingress Controller. The Ingress resource defines routing rules (e.g., host: api.example.com, path: /v1  service-a), and the Ingress Controller translates these into actual load balancer configuration. This is different from Kubernetes Services (which provide basic L4 load balancing via kube-proxy). In production, the Ingress Controller is typically the entry point for all HTTP traffic into a Kubernetes cluster."},{question:"What is the 'random' load balancing algorithm best suited for?",options:["Production environments with strict SLAs","Simple scenarios or as a baseline; also forms the basis of 'power of two choices'","Databases requiring consistency","Financial trading systems"],correctIndex:1,explanation:"The random algorithm selects a backend server at random for each request. While seemingly primitive, it provides surprisingly good distribution due to the law of large numbers  over many requests, each server gets approximately equal traffic. It's stateless (no need to track connections or positions), making it the simplest algorithm to implement in distributed load balancers where state sharing is difficult. More importantly, random selection forms the basis of the 'power of two choices' algorithm (pick two random servers, choose the less loaded one), which is used in production by systems like Nginx Plus. Pure random is suitable for homogeneous servers with uniform request costs, but least-connections or P2C generally perform better in practice."},{question:"What is 'zone-aware routing' in AWS ALB?",options:["Routing based on DNS zones","Preferentially routing to targets in the same AZ as the LB node to reduce cross-zone traffic","Routing based on timezone","Routing between different AWS regions"],correctIndex:1,explanation:"Zone-aware routing in AWS means the ALB node in a particular availability zone preferentially routes to targets in the same AZ, reducing cross-zone data transfer (which costs money and adds latency). If the local AZ doesn't have enough healthy targets, traffic overflows to other AZs. This is related to the cross-zone load balancing setting  when cross-zone is disabled, each ALB node only routes to targets in its own AZ. For NLB, cross-zone is disabled by default to save costs, meaning zone-aware routing is the default behavior. Understanding this is important because it affects both cost optimization and latency, especially for high-traffic internal services communicating across AZs."},{question:"What are 'connection limits' on a load balancer and why are they important?",options:["Marketing limits on the number of customers","Maximum concurrent connections the LB or backends can handle, preventing resource exhaustion","Limits on physical network cables","Restrictions on who can connect"],correctIndex:1,explanation:"Connection limits define the maximum number of concurrent connections at both the load balancer level and per-backend level. Without limits, a traffic spike or DDoS attack could exhaust the load balancer's resources (memory, file descriptors) or overwhelm backend servers. HAProxy's 'maxconn' setting limits per-server connections  excess requests are queued. AWS ALB has built-in connection limits per target. When a backend hits its connection limit, the LB routes new requests to other backends or returns 503. These limits are essential for protecting both the load balancer and backends from cascade failures. The key is setting limits based on actual server capacity testing, not arbitrary numbers."},{question:"How does a load balancer support A/B testing?",options:["By running two separate load balancers","By routing a defined percentage or subset of users to different backend versions based on headers, cookies, or weights","By alternating between two server pools every day","By serving different content from the same servers"],correctIndex:1,explanation:"Load balancers enable A/B testing by splitting traffic between different backend versions based on rules. You can route based on cookies (users in experiment group A get version 1), headers (specific header values trigger routing to version 2), or weighted distribution (70% to version A, 30% to version B). AWS ALB supports weighted target groups for percentage-based splitting, and header/cookie conditions for rule-based routing. Envoy and Istio provide even more granular control through traffic splitting policies. This is crucial for data-driven product development  companies like Netflix, Google, and Amazon run thousands of simultaneous A/B tests by routing different user segments to different service versions through their load balancer configurations."},{question:"What is a 'sidecar proxy' pattern in load balancing?",options:["A backup load balancer","A proxy deployed alongside each application instance to handle its network communication","A secondary monitoring proxy","A proxy for offline processing"],correctIndex:1,explanation:"In the sidecar proxy pattern, a lightweight proxy (like Envoy) is deployed as a companion to each application instance, handling all inbound and outbound network communication. Instead of the application directly connecting to other services, it sends requests to its local sidecar, which handles load balancing, retries, circuit breaking, mutual TLS, and observability. This is the fundamental building block of service meshes like Istio. The sidecar intercepts traffic transparently using iptables rules, requiring no application code changes. The advantage is that complex networking logic is centralized in the proxy; the disadvantage is the added latency (typically <1ms) and resource overhead of running a proxy alongside every service instance."},{question:"What is the 'least time' algorithm in Nginx Plus?",options:["Routes to the most recently started server","Routes based on a combination of fewest active connections and lowest average response time","Routes to minimize total request time","Routes requests at specific time intervals"],correctIndex:1,explanation:"Nginx Plus's 'least_time' algorithm selects the backend with the best combination of fewest active connections and lowest average response time. You can configure it to optimize for either 'header' time (time to receive the first byte from the backend) or 'last_byte' (time to receive the complete response). This is more intelligent than pure least connections because it accounts for actual backend performance  a server with few connections might still be slow due to hardware issues or heavy processing. It's particularly effective for heterogeneous server pools where machines have different performance characteristics, automatically sending more traffic to faster servers without manual weight tuning."},{question:"What is 'request routing' vs 'connection routing' in load balancing?",options:["They are the same thing","Connection routing assigns a connection to a backend once; request routing can send each request within a connection to a different backend","Request routing is for HTTP; connection routing is for HTTPS","Connection routing is faster"],correctIndex:1,explanation:"Connection routing (L4) assigns an entire TCP connection to a backend server  all data on that connection goes to the same server. Request routing (L7) can inspect individual HTTP requests within a connection and route each one to potentially different backends. This distinction is critical with HTTP keep-alive and HTTP/2, where a single connection carries multiple requests. With connection routing, all requests on a keep-alive connection hit the same backend. With request routing, the load balancer can distribute individual requests from the same connection across multiple backends, achieving much better load distribution. This is why L7 load balancers generally provide better balance for HTTP workloads."},{question:"What is 'server warming' in the context of load balancing?",options:["Physically heating servers","Gradually increasing traffic to a new server so it can initialize caches and JIT compilations","Pre-installing software on servers","Running diagnostic tests before deployment"],correctIndex:1,explanation:"Server warming refers to the process of allowing a newly started or restarted server to gradually build up its operational state before receiving full production traffic. Cold JVM servers need time for JIT compilation to optimize hot paths, application caches need to be populated, database connection pools need to be established, and DNS caches need to fill. Sending full traffic to a cold server can cause high latency or failures. Load balancers support this through 'slow start' features that ramp up traffic over a configurable period. For example, ALB's slow start ramps traffic to a new target from 0% to full share over 30-900 seconds. This is especially critical for Java applications where JIT warmup can take several minutes."},{question:"What is 'upstream hashing' in Nginx?",options:["Hashing the Nginx configuration","Using a hash of a specified key (like URI or client IP) to consistently route requests to the same backend","Hashing passwords for upstream authentication","Encrypting data sent upstream"],correctIndex:1,explanation:"Nginx's 'hash' directive in upstream blocks computes a hash of a specified variable to determine which backend receives each request. For example, 'hash $request_uri consistent;' routes all requests for the same URI to the same backend, which is excellent for caching scenarios. You can hash on $remote_addr for IP-based affinity, $request_uri for cache optimization, or even custom variables. The 'consistent' parameter uses ketama consistent hashing (a hash ring approach), which minimizes remapping when servers are added or removed. Without 'consistent', adding a server remaps most requests. This is commonly used in front of caching servers where you want the same content to always hit the same cache node for maximum cache hit rates."},{question:"What is the 'keepalive' directive in Nginx upstream configuration?",options:["Enabling HTTP keep-alive for clients","Setting the maximum number of idle keep-alive connections to upstream servers per worker","Keeping upstream servers alive","A health check interval setting"],correctIndex:1,explanation:`The 'keepalive' directive in Nginx upstream blocks sets the maximum number of idle persistent connections to upstream servers that are cached per Nginx worker process. For example, 'keepalive 32;' means each worker can maintain up to 32 idle connections to the upstream group. These pre-established connections are reused for new requests, eliminating TCP handshake and TLS negotiation overhead. Without keepalive, Nginx opens a new connection for every request to the backend. This is critical for performance  establishing a new TLS connection can take 2-5 round trips. You must also set 'proxy_http_version 1.1;' and 'proxy_set_header Connection "";' in the location block for HTTP keep-alive to work with the upstream.`},{question:"What is 'horizontal scaling' and how does a load balancer enable it?",options:["Adding more CPU to existing servers","Adding more server instances and using a load balancer to distribute traffic across them","Scaling the load balancer itself","Adding more disk storage to servers"],correctIndex:1,explanation:"Horizontal scaling means adding more server instances to handle increased load, as opposed to vertical scaling which increases the resources of existing servers. The load balancer is the key enabler  it distributes incoming requests across all instances, making the group appear as a single endpoint to clients. Without a load balancer, you'd need to manually assign clients to specific servers. Horizontal scaling is preferred in modern architectures because it has no theoretical limit (just add more servers), provides fault tolerance (losing one server doesn't mean downtime), and is cost-effective (use many cheap commodity servers). Cloud auto-scaling groups work with load balancers to automatically add/remove instances based on demand."},{question:"What is the difference between 'proxy_pass' in Nginx and 'use_backend' in HAProxy?",options:["They serve completely different purposes","Both route requests to backends, but proxy_pass is a location-level directive while use_backend is a frontend-level conditional routing rule","proxy_pass is for TCP; use_backend is for HTTP","proxy_pass is deprecated"],correctIndex:1,explanation:"Both directives route traffic to backend servers, but they work within their respective architectures differently. Nginx's 'proxy_pass' is used in 'location' blocks within 'server' blocks  routing is determined by URL path matching in the server configuration. HAProxy's 'use_backend' is used in 'frontend' sections with ACL conditions  routing is determined by evaluating conditional rules against request attributes. HAProxy's ACL system is generally more flexible for complex routing logic (combining multiple conditions), while Nginx's location-based routing is more intuitive for path-based routing. For example, HAProxy can easily route based on a combination of path, header, and source IP in a single rule, which requires more configuration in Nginx."},{question:"What is 'TCP multiplexing' in the context of load balancing?",options:["Using multiple TCP ports","Combining multiple client TCP connections into fewer backend connections","Running multiple protocols over one TCP connection","Using TCP and UDP simultaneously"],correctIndex:1,explanation:"TCP multiplexing at the load balancer level means combining requests from many client connections into fewer backend connections. An L7 load balancer can accept thousands of client connections and forward their HTTP requests over a much smaller pool of persistent backend connections. This is possible because the load balancer buffers complete requests and multiplexes them over available backend connections. The benefit is massive: backend servers only need to manage a fraction of the total client connections, reducing memory usage and context switching overhead. AWS ALB performs TCP multiplexing by default. For a backend serving 100,000 clients through an ALB, it might only see 100 connections from the ALB."},{question:"What monitoring metrics are most important for a load balancer?",options:["Only request count","Request rate, error rate, latency percentiles, active connections, and backend health","Only CPU usage","Only bandwidth usage"],correctIndex:1,explanation:"Comprehensive load balancer monitoring requires several key metrics: request rate (throughput), HTTP error rates (4xx/5xx), latency percentiles (p50, p95, p99), active/new connections, backend health status, spillover count (rejected requests), and connection queue depth. Latency percentiles are particularly important  p99 latency reveals tail latency issues that averages would hide. Error rate spikes indicate backend problems. Connection metrics help with capacity planning. AWS ALB emits these as CloudWatch metrics: RequestCount, HTTPCode_Target_5XX_Count, TargetResponseTime, ActiveConnectionCount, and UnHealthyHostCount. Setting up alerts on these metrics (e.g., alert when 5xx rate exceeds 1% or p99 latency exceeds 2 seconds) is essential for maintaining reliability."},{question:"What is 'surge queue' in AWS Classic Load Balancer?",options:["A queue for surge pricing","A queue that holds requests when all backend instances are at capacity","A queue for handling traffic surges","A priority queue for important requests"],correctIndex:1,explanation:"The surge queue in AWS Classic Load Balancer (CLB) holds pending requests when all registered backend instances have reached their maximum connection capacity. The CLB can queue up to 1,024 requests. If the queue is full, additional requests receive 503 Service Unavailable errors (counted as 'SpilloverCount' metric). This was a significant limitation of CLB. AWS ALB replaced this with a more sophisticated approach  it doesn't have a fixed surge queue but uses its own connection pooling and request queuing internally with higher limits. Monitoring SpilloverCount was critical for CLB operators; a non-zero value indicated capacity problems requiring immediate attention. This was one of the reasons AWS recommended migrating from CLB to ALB."},{question:"What is a 'virtual server' in F5 BIG-IP load balancer terminology?",options:["A VM running behind the load balancer","A listener that represents the combination of IP address and port that clients connect to","A simulated test server","A server in the cloud"],correctIndex:1,explanation:"In F5 BIG-IP, a 'virtual server' is the front-end configuration object that defines the IP address and port combination where the load balancer accepts client connections. It's conceptually equivalent to HAProxy's 'frontend' or AWS ALB's 'listener.' A virtual server ties together the listening address, traffic processing profiles (SSL, HTTP compression, caching), and the pool of backend servers. For example, a virtual server at 10.0.0.1:443 with an SSL profile and HTTP profile would accept HTTPS connections and load balance them across a pool of web servers. F5 virtual servers support iRules, which are custom scripts that can perform complex traffic manipulation beyond standard load balancing."},{question:"What is the 'server_name' directive in Nginx used for in load balancing?",options:["Setting the hostname of the Nginx server","Matching the incoming Host header to select the appropriate server block for virtual hosting","Naming the backend servers","Setting DNS names for health checks"],correctIndex:1,explanation:"The 'server_name' directive in Nginx matches the Host header in incoming HTTP requests to determine which server block should process the request. This enables name-based virtual hosting  a single Nginx instance can serve multiple domains. Combined with upstream blocks, this becomes host-based load balancing: 'server_name api.example.com;' with 'proxy_pass http://api-backend;' routes API traffic to API servers, while 'server_name www.example.com;' routes to web servers. Without server_name matching, all traffic would hit the default server block regardless of the requested domain. This is equivalent to AWS ALB's host-based routing rules."},{question:"What is 'SSL session caching' and why is it important for load balancers?",options:["Caching web pages served over SSL","Storing TLS session parameters so subsequent connections skip the full handshake, reducing latency","Caching SSL certificates","Storing encrypted data"],correctIndex:1,explanation:"SSL session caching stores the negotiated TLS session parameters (session tickets or session IDs) so that when a client reconnects, it can resume the previous session instead of performing a full TLS handshake. A full handshake requires 2 round trips (TLS 1.2) of CPU-intensive key exchange. Session resumption reduces this to 1 round trip with minimal CPU usage. For a load balancer handling thousands of TLS connections per second, this dramatically reduces CPU usage and connection latency. The challenge with multiple load balancers is ensuring session data is shared  either using shared memory, memcached backends for session tickets, or sticky sessions. Nginx supports ssl_session_cache with shared memory zones."},{question:"What is 'ECMP' (Equal-Cost Multi-Path) routing and its relation to load balancing?",options:["An encryption protocol","A network-layer technique that distributes traffic across multiple equal-cost routes, enabling L3 load balancing","A load balancing algorithm for HTTP","A monitoring protocol"],correctIndex:1,explanation:"ECMP is a routing technique where network switches/routers distribute packets across multiple paths that have the same routing cost to a destination. This provides L3 (network layer) load balancing without any dedicated load balancer hardware. For example, if there are 4 equal-cost paths to a server subnet, the router distributes traffic across all 4 using a hash of the packet's 5-tuple (source/dest IP, source/dest port, protocol). ECMP is commonly used in data center leaf-spine architectures and in front of L4 load balancers to distribute traffic across multiple LB nodes. Maglev, Google's load balancer, uses ECMP with consistent hashing to ensure connection affinity even as paths change."},{question:"What happens when you configure too short a health check interval?",options:["Better reliability","Increased network overhead and potential for false positives, especially during minor GC pauses","No effect","Faster failover with no drawbacks"],correctIndex:1,explanation:"Setting health check intervals too short (e.g., 1 second with 2-failure threshold) creates several problems. First, it generates significant network overhead  checking 100 backends every second is 100 health check requests per second. Second, it increases false positives: a server experiencing a brief JVM garbage collection pause (200ms stop-the-world) or momentary CPU spike might fail consecutive checks and be incorrectly marked unhealthy. This causes unnecessary traffic shifting and potential cascading issues. AWS recommends health check intervals of 10-30 seconds with 2-3 failure thresholds for a good balance between detection speed and stability. The total detection time equals interval  failure threshold, so 10s interval  3 failures = 30 seconds to detect a real failure."},{question:"What is 'backend server weight auto-tuning'?",options:["Manually adjusting server weights daily","Automatically adjusting backend weights based on real-time performance metrics","Tuning the physical weight of servers","Automatic firmware updates"],correctIndex:1,explanation:"Backend server weight auto-tuning dynamically adjusts the load balancing weight of each backend server based on real-time performance metrics like response time, error rate, or CPU usage. Instead of static weights, the load balancer continuously adapts to actual server performance. If a server starts responding slowly (perhaps due to noisy neighbors in a cloud environment), its weight is automatically reduced. HAProxy's agent checks enable this  the backend agent reports a percentage weight based on local conditions. Envoy's adaptive load balancing and Netflix's gradient-based load balancing are more sophisticated implementations. This approach handles heterogeneous and dynamic environments better than static configuration."},{question:"What is 'traffic splitting' and how is it different from load balancing?",options:["They are identical","Load balancing distributes across identical servers; traffic splitting routes percentages to different service versions for deployment strategies","Traffic splitting is faster","Load balancing is more advanced"],correctIndex:1,explanation:"Traditional load balancing distributes traffic across identical servers for scalability and reliability. Traffic splitting directs specific percentages of traffic to different versions of a service for deployment strategies like canary releases, A/B testing, or traffic migration. For example, 95% to version 1.0 and 5% to version 2.0. While both involve distributing traffic, the intent is different  load balancing maximizes utilization and availability, while traffic splitting validates changes safely. Istio's VirtualService supports traffic splitting with exact percentages. AWS ALB implements it via weighted target groups. Traffic splitting is a deployment and experimentation tool that happens to use load balancer infrastructure."},{question:"What is 'headless service' in Kubernetes and when would you use it instead of a load balancer?",options:["A service without a UI","A service that returns all pod IPs directly via DNS instead of load balancing through a virtual IP","A service that runs without a controller","A temporary service"],correctIndex:1,explanation:"A headless Service in Kubernetes (clusterIP: None) doesn't allocate a virtual IP or use kube-proxy for load balancing. Instead, DNS resolution returns the IP addresses of all backing pods directly. This is useful when the client needs to know all endpoints and do its own load balancing or connection management  for example, databases like Cassandra or Elasticsearch that use their own cluster discovery. StatefulSets typically use headless services because each pod needs a stable, unique DNS name (pod-0.service, pod-1.service). Regular Services provide built-in L4 load balancing via kube-proxy, but headless services are necessary when the application layer needs direct control over which pod it connects to."},{question:"What is the impact of load balancer placement on SSL/TLS certificate management?",options:["No impact","Certificates only need to be managed on the load balancer (with SSL termination), simplifying certificate lifecycle management","Certificates become more complex with load balancers","Each backend needs its own unique certificate"],correctIndex:1,explanation:"With SSL termination at the load balancer, TLS certificates only need to be installed and renewed on the load balancer, not on every backend server. This dramatically simplifies certificate management  instead of updating certificates on 50 servers during renewal, you update one place. AWS ALB integrates with ACM (AWS Certificate Manager) for free, auto-renewing certificates. Without SSL termination, every backend server needs a valid certificate, and rotating them across a fleet is operationally complex. This centralization also makes it easier to enforce TLS policies (minimum version, cipher suites) consistently. The only downside is that if you need end-to-end encryption, you'll also need internal certificates on backends, though these can be self-signed from an internal CA."},{question:"What is the 'max_fails' directive in Nginx upstream configuration?",options:["Maximum number of backend servers that can fail","Number of failed requests to a backend before Nginx considers it unavailable","Maximum number of retries","Maximum number of 4xx errors allowed"],correctIndex:1,explanation:"The 'max_fails' directive in Nginx defines how many consecutive failed requests (timeouts or errors) to a backend server are needed before Nginx marks it as unavailable. Combined with 'fail_timeout' (the period during which failures are counted and the duration the server is marked down), it forms Nginx's passive health check mechanism. For example, 'server backend1 max_fails=3 fail_timeout=30s;' means after 3 failures within 30 seconds, the server is marked down for 30 seconds. After the timeout, Nginx tries the server again. This is simpler than active health checks (which require Nginx Plus) but has the drawback that real user requests are used to detect failures."},{question:"What is 'load balancer affinity' at the kernel level (IPVS)?",options:["CPU affinity for load balancer processes","Linux kernel-level load balancing using IP Virtual Server for high-performance L4 balancing","Affinity between load balancers in a cluster","Memory affinity for connection tables"],correctIndex:1,explanation:"IPVS (IP Virtual Server) is a Linux kernel module that provides L4 load balancing at the kernel level, offering much higher performance than userspace load balancers. Because IPVS operates in kernel space, it avoids the overhead of copying packets between kernel and user space. It supports multiple algorithms (round-robin, least connections, weighted, etc.) and modes (NAT, DR, IP tunneling). Kubernetes uses IPVS as an alternative to iptables for kube-proxy, providing better performance with large numbers of services. IPVS can handle millions of concurrent connections with O(1) complexity for connection scheduling, compared to iptables' O(n) chain traversal. It's the technology behind LVS (Linux Virtual Server) used by many large-scale deployments."},{question:"What is the difference between 'passive' and 'active' SSL/TLS termination?",options:["Active terminates SSL; passive just monitors","Active termination decrypts at the LB; passive termination uses SSL passthrough with traffic copying for inspection","Active is for production; passive is for testing","They are the same"],correctIndex:1,explanation:"Active SSL termination fully decrypts TLS at the load balancer, forwarding plain HTTP to backends  this is the standard SSL termination approach. Passive SSL termination (or SSL inspection/tap) copies the encrypted traffic stream for analysis (like intrusion detection) while letting the original encrypted traffic pass through to the backend. The passive approach requires access to the private key to decrypt the copy. Active termination is by far more common in load balancing. Passive inspection is primarily used in security appliances (IDS/IPS) deployed alongside load balancers. Some enterprise load balancers like F5 BIG-IP can perform both simultaneously  terminating SSL for load balancing while also sending decrypted copies to security analysis tools."},{question:"What is 'adaptive load balancing'?",options:["Adapting to new server hardware automatically","Dynamically adjusting the load balancing algorithm based on real-time system conditions and performance","A specific load balancing algorithm","Adapting to different network protocols"],correctIndex:1,explanation:"Adaptive load balancing goes beyond static algorithms by continuously monitoring backend health, response times, and resource utilization to dynamically adjust routing decisions. Rather than using a fixed algorithm like round-robin, it might shift more traffic to faster backends, reduce traffic to backends showing latency increases, and react to changing conditions in real time. Netflix's implementation uses gradient-based signals: if a backend's latency increases, it receives proportionally less traffic. Envoy's adaptive concurrency limits and AWS ALB's 'least outstanding requests' are related concepts. Adaptive load balancing is particularly valuable in cloud environments where server performance can vary due to noisy neighbors, thermal throttling, or background maintenance."},{question:"What is 'request collapsing' (coalescing) at the load balancer?",options:["Combining multiple small requests into one large request","Deduplicating identical concurrent requests and serving all clients from a single backend response","Collapsing failed requests into retries","Compressing requests to reduce size"],correctIndex:1,explanation:"Request collapsing (or coalescing) detects when multiple clients simultaneously request the same resource and sends only one request to the backend, serving all waiting clients with the same response. This is particularly valuable for cache-miss scenarios in CDNs and caching proxies  without collapsing, 1000 simultaneous requests for the same uncached resource would generate 1000 backend requests. Varnish and Nginx both implement this pattern. Nginx does it naturally for proxy_cache when proxy_cache_lock is enabled  concurrent requests for the same cache key are held while the first request populates the cache. This prevents the 'thundering herd' problem where cache misses amplify backend load."},{question:"What is the difference between ELB, ALB, and NLB in AWS?",options:["They are different names for the same service","ELB (Classic) is legacy L4/L7; ALB is modern L7; NLB is modern L4  each optimized for different use cases","ELB is for EC2, ALB is for ECS, NLB is for Lambda","They differ only in pricing"],correctIndex:1,explanation:"AWS has three load balancer types: Classic Load Balancer (CLB/ELB) is the legacy option supporting basic L4 and L7 features; Application Load Balancer (ALB) is the modern L7 load balancer with advanced features like path-based routing, WebSocket support, and Lambda targets; Network Load Balancer (NLB) is a high-performance L4 load balancer handling millions of requests per second with static IPs and TLS passthrough. AWS recommends ALB for HTTP/HTTPS workloads and NLB for TCP/UDP workloads or when you need extreme performance, static IPs, or preservation of source IP. CLB is effectively deprecated  AWS encourages migration to ALB or NLB for all new deployments."},{question:"How does a load balancer handle 'long-polling' requests?",options:["It rejects them","It must have sufficiently long idle timeouts to maintain the held connections without prematurely closing them","It converts them to WebSocket","It doesn't need any special handling"],correctIndex:1,explanation:"Long-polling involves clients sending a request that the server holds open until new data is available or a timeout occurs (typically 30-60 seconds). The load balancer must have idle timeout values higher than the long-poll timeout to avoid prematurely closing these intentionally held connections. For example, if your long-poll timeout is 60 seconds, the load balancer's idle timeout should be at least 65 seconds. Additionally, the load balancer should use least-connections rather than round-robin, since long-poll connections tie up server resources longer. AWS ALB's default idle timeout of 60 seconds may need to be increased for long-polling applications. Connection count per backend also matters more with long-polling since connections are held much longer."},{question:"What is 'consistent hashing with bounded loads'?",options:["Hashing with a maximum file size","A variant of consistent hashing that caps the maximum load on any single server to ensure even distribution","Hashing with bandwidth limits","Consistent hashing for large-scale systems only"],correctIndex:1,explanation:"Consistent hashing with bounded loads (proposed by Mirrokni et al.) adds a load-balancing constraint to consistent hashing: no server can receive more than (1 + ) times the average load. When a server reaches its bound, requests that would normally hash to it are redirected to the next available server on the ring. This solves the hot-spot problem that standard consistent hashing can suffer from (where unlucky hash distributions overload some servers) while maintaining the minimal-disruption property when servers are added or removed. Google Vimeo uses this algorithm for their load balancers. The  parameter (typically small, like 0.25) provides a tuning knob between strict load balance and hash consistency."},{question:"What is the role of 'back-end connection pooling' in load balancers?",options:["Pooling hardware resources","Maintaining reusable connections to backends to avoid per-request TCP/TLS handshake overhead","Pooling multiple backends into one","Creating backup connection paths"],correctIndex:1,explanation:"Backend connection pooling means the load balancer maintains a pool of pre-established TCP connections to each backend server, reusing them for multiple client requests rather than creating new connections each time. This eliminates the per-request overhead of TCP 3-way handshake (1 RTT) and TLS handshake (2-3 RTTs for TLS 1.2). For a load balancer handling 10,000 requests/second, this saves 10,000 handshakes per second. The pool typically has configurable minimum and maximum sizes, with idle connections kept alive using TCP keepalive packets. HAProxy's 'http-reuse' directive controls connection reuse strategies. This is one of the most impactful performance optimizations in any load balancer configuration."}],gf=[{question:"What is the primary motivation for decomposing a monolith into microservices?",options:["To reduce the total lines of code in the system","To enable independent deployment and scaling of services","To eliminate the need for a database","To ensure all teams use the same programming language"],correctIndex:1,explanation:"The primary motivation for microservices is enabling independent deployment and scaling. Each service can be deployed, updated, and scaled independently without affecting others, which accelerates release cycles and allows teams to work autonomously. Reducing total lines of code is not a goalmicroservices often increase overall code due to infrastructure overhead. Microservices still use databases (often one per service), and a key advantage is that teams can choose different programming languages (polyglot architecture)."},{question:"In Domain-Driven Design (DDD), what defines a Bounded Context?",options:["A physical server boundary where code is deployed","An explicit boundary within which a domain model is defined and applicable","The maximum number of entities allowed in a single database","A network firewall rule separating services"],correctIndex:1,explanation:"A Bounded Context in DDD is a logical boundary within which a particular domain model is consistent and meaningful. The same term (e.g., 'Order') can mean different things in different bounded contextsin the Sales context it represents a purchase intent, while in Shipping it represents a package to deliver. This is not about physical server boundaries or network rules; it's a semantic boundary that helps teams avoid model confusion. Bounded Contexts are one of the most effective tools for identifying service boundaries in microservices architectures."},{question:"Which communication pattern introduces the tightest coupling between microservices?",options:["Asynchronous messaging via a message broker","Event-driven publish/subscribe","Synchronous HTTP REST calls","Event sourcing with an append-only log"],correctIndex:2,explanation:"Synchronous HTTP REST calls create the tightest coupling because the caller must wait for the callee to respond, creating temporal couplingboth services must be available simultaneously. If the downstream service is slow or down, the caller is directly affected. Asynchronous messaging and pub/sub decouple services in time; the sender doesn't wait for a response. Event sourcing further decouples by recording state changes as events. In practice, synchronous calls are sometimes necessary (e.g., user-facing queries), but overusing them in service-to-service communication recreates the coupling problems of a monolith."},{question:"What is the Circuit Breaker pattern used for in microservices?",options:["Encrypting data in transit between services","Preventing cascading failures by failing fast when a downstream service is unhealthy","Load balancing traffic across multiple instances of a service","Compressing request payloads to reduce bandwidth"],correctIndex:1,explanation:"The Circuit Breaker pattern prevents cascading failures by monitoring calls to a downstream service and 'opening' the circuit (failing fast) when failures exceed a threshold. This stops the calling service from wasting resources on requests likely to fail, and gives the downstream service time to recover. It has three states: Closed (normal operation), Open (requests fail immediately), and Half-Open (test requests to check recovery). Libraries like Resilience4j and Netflix Hystrix implement this pattern. Without it, a single slow service can exhaust thread pools and bring down the entire system."},{question:"What is the Sidecar pattern in a microservices architecture?",options:["A secondary database replica running alongside the primary","A helper container deployed alongside the main application container to handle cross-cutting concerns","A backup service that takes over when the primary fails","A secondary API endpoint for internal-only traffic"],correctIndex:1,explanation:"The Sidecar pattern deploys a helper process or container alongside the main application container within the same pod (in Kubernetes) or host. The sidecar handles cross-cutting concerns like logging, monitoring, TLS termination, and service mesh proxying without modifying the application code. Envoy proxy in Istio is a classic exampleit intercepts all network traffic for observability and security. This pattern promotes separation of concerns and allows infrastructure teams to update sidecar functionality independently of the application. It's fundamental to service mesh architectures."},{question:"In the Saga pattern, what is a compensating transaction?",options:["A transaction that runs in parallel to speed up processing","An action that undoes the effect of a previously committed local transaction when a later step fails","A transaction that automatically retries on failure","A database rollback using ACID guarantees"],correctIndex:1,explanation:"A compensating transaction is an action that semantically reverses the effect of a previously committed step in a saga when a subsequent step fails. Unlike a database rollback, compensating transactions are application-level logicfor example, if payment was captured but shipping fails, the compensation would issue a refund. This is necessary because each service has its own database with local transactions; there's no distributed ACID transaction spanning services. Compensating transactions may not perfectly undo everything (e.g., a sent email can't be unsent), which is why saga design requires careful consideration of idempotency and ordering."},{question:"What problem does service discovery solve in a microservices environment?",options:["How to encrypt traffic between services","How clients and services locate the network addresses of dynamically changing service instances","How to split a monolith into services","How to store service source code in a repository"],correctIndex:1,explanation:"Service discovery solves the problem of locating service instances whose network addresses change dynamically due to auto-scaling, deployments, and failures. In a cloud-native environment, IP addresses are ephemeralcontainers start and stop constantly. Service discovery mechanisms (like Consul, Eureka, or Kubernetes DNS) maintain a registry of available instances so that clients can find healthy endpoints. Without service discovery, you'd need to hardcode IP addresses, which is fragile and doesn't work with elastic scaling. There are two patterns: client-side discovery (client queries registry) and server-side discovery (load balancer queries registry)."},{question:"What is the Bulkhead pattern inspired by?",options:["Electrical circuit breakers in buildings","Watertight compartments in a ship's hull that contain flooding","The human immune system's white blood cells","Traffic lanes on a highway"],correctIndex:1,explanation:"The Bulkhead pattern is named after the watertight compartments (bulkheads) in a ship's hull. If one compartment is breached, the flooding is contained and doesn't sink the entire ship. In microservices, this translates to isolating resources (thread pools, connection pools, CPU) so that a failure in one component doesn't exhaust shared resources and bring down everything. For example, you might allocate separate thread pools for calls to different downstream services, so a slow Service A can't starve requests to Service B. Netflix famously used bulkheads in their architecture to prevent cascading failures across their streaming platform."},{question:"Which type of Saga execution uses a central coordinator to manage the workflow?",options:["Choreography-based saga","Orchestration-based saga","Two-phase commit saga","Peer-to-peer saga"],correctIndex:1,explanation:"An orchestration-based saga uses a central orchestrator (saga coordinator) that tells each participant what local transaction to execute and in what order. The orchestrator maintains the saga's state and handles compensations if a step fails. In contrast, choreography-based sagas have no central coordinatoreach service listens for events and decides what to do next. Orchestration is easier to understand and debug for complex workflows but introduces a single point of coordination. Two-phase commit is a different distributed transaction protocol that blocks resources and isn't suitable for microservices. Real-world examples include order processing workflows in e-commerce systems using tools like Temporal or AWS Step Functions."},{question:"What does distributed tracing help you understand in a microservices system?",options:["The database schema of each service","The end-to-end path and latency of a request as it flows through multiple services","The number of lines of code in each service","The programming language used by each team"],correctIndex:1,explanation:"Distributed tracing tracks a request's journey across multiple services, recording timing data at each hop to create a trace. This allows engineers to visualize the complete request path, identify latency bottlenecks, and pinpoint which service is causing slowdowns. Tools like Jaeger, Zipkin, and AWS X-Ray implement the OpenTelemetry standard for this purpose. Each request gets a unique trace ID that propagates through all service calls, with each service adding a span. Without distributed tracing, debugging performance issues in a system with dozens of interconnected services would be nearly impossibleyou'd be guessing which of many services is the culprit."},{question:"In Kubernetes, what is a Pod?",options:["A virtual machine running multiple containers","The smallest deployable unit, consisting of one or more tightly coupled containers sharing network and storage","A cluster of physical servers","A DNS entry for a service"],correctIndex:1,explanation:"A Pod is the smallest deployable unit in Kubernetes, consisting of one or more containers that share the same network namespace (same IP, can communicate via localhost) and storage volumes. Pods are not virtual machinesthey're lightweight process groups. The most common pattern is a single application container per pod, but multi-container pods are used for sidecars (e.g., log collectors, service mesh proxies). Pods are ephemeral; they can be terminated and recreated at any time, which is why you use higher-level abstractions like Deployments to manage their lifecycle. Understanding pods is fundamental to running microservices on Kubernetes."},{question:"What is the primary disadvantage of a monolithic architecture as the team and codebase grow?",options:["It's impossible to write unit tests","Deployment of any change requires rebuilding and redeploying the entire application","Monoliths cannot use relational databases","Monoliths cannot handle more than 100 concurrent users"],correctIndex:1,explanation:"As a monolith grows, even a small change requires rebuilding and redeploying the entire application, which slows down release cycles and increases risk. A bug in one module can block deployment of unrelated features. This creates coordination overhead as teams growmerge conflicts, long build times, and coupled release schedules. Monoliths absolutely support unit testing, relational databases, and can handle massive traffic (many successful companies run monoliths at scale). The key issue is organizational: Conway's Law suggests that as teams scale, independent deployability becomes critical for velocity."},{question:"Which service mesh implementation uses Envoy as its data plane proxy?",options:["Netflix Zuul","Istio","Apache Kafka","RabbitMQ"],correctIndex:1,explanation:"Istio is a service mesh that uses Envoy proxy as its data plane, deploying it as a sidecar container alongside each service. Envoy handles all inbound and outbound traffic, providing features like mutual TLS, load balancing, circuit breaking, and observability without application code changes. Netflix Zuul is an API gateway, not a service mesh. Kafka and RabbitMQ are message brokers for asynchronous communication, not service meshes. Other service meshes like Linkerd use their own lightweight proxy (linkerd2-proxy), but Istio's choice of Envoy has made Envoy the de facto standard for cloud-native proxy infrastructure."},{question:"What is the 'Database per Service' pattern?",options:["All services share a single large database for consistency","Each microservice owns its private database, and other services cannot access it directly","Each service uses a different database vendor","Services store data only in memory, never on disk"],correctIndex:1,explanation:"The Database per Service pattern means each microservice has its own private data store that no other service can access directlyonly through the service's API. This ensures loose coupling: services can evolve their schemas independently, choose the most appropriate database technology, and scale their data stores independently. The trade-off is that cross-service queries become harder (no joins across service boundaries) and maintaining data consistency requires patterns like sagas or event-driven synchronization. Using different vendors is optional (polyglot persistence), not required. This pattern is considered foundational to achieving true microservice independence."},{question:"In the context of inter-service communication, what does 'temporal coupling' mean?",options:["Services must be written in the same programming language","Services must be available at the same time for communication to succeed","Services must share the same database schema","Services must be deployed in the same time zone"],correctIndex:1,explanation:"Temporal coupling means both the caller and callee must be running and available simultaneously for communication to succeed. This is inherent in synchronous communication (HTTP REST, gRPC) where the caller blocks waiting for a response. If the downstream service is down, the call fails immediately. Asynchronous messaging eliminates temporal couplingthe sender publishes a message to a broker, and the receiver processes it whenever it's available. Temporal coupling is one of the main reasons architects favor asynchronous communication for service-to-service interactions, especially for operations that don't need an immediate response."},{question:"What does a Kubernetes Deployment resource manage?",options:["Network routing rules between pods","The desired state of pod replicas, including rolling updates and rollbacks","Persistent storage volumes for databases","DNS resolution for external services"],correctIndex:1,explanation:"A Kubernetes Deployment is a higher-level abstraction that manages a ReplicaSet of pods, ensuring the desired number of identical pod replicas are running at all times. It handles rolling updates (gradually replacing old pods with new ones), rollbacks (reverting to a previous version), and self-healing (restarting failed pods). Network routing is handled by Services and Ingress resources. Persistent storage is managed by PersistentVolumes and PersistentVolumeClaims. Deployments are the standard way to run stateless microservices on Kubernetes, as they provide declarative updates and maintain availability during deployments."},{question:"What is the Strangler Fig pattern used for?",options:["Rapidly building a greenfield microservices system","Incrementally migrating a monolith to microservices by gradually replacing functionality","Monitoring service health in production","Encrypting all inter-service communication"],correctIndex:1,explanation:"The Strangler Fig pattern (named after a tropical vine that gradually envelops and replaces a tree) is a migration strategy where you incrementally replace monolith functionality with microservices. New features are built as microservices, and existing features are migrated one at a time while the monolith continues to serve unmodified functionality. A routing layer (often an API gateway) directs traffic to either the monolith or the new services. This avoids the risk of a big-bang rewrite, which historically has a high failure rate. Martin Fowler popularized this pattern, and companies like Amazon and Netflix famously used it to migrate from monoliths."},{question:"What is the key difference between choreography and orchestration in saga patterns?",options:["Choreography uses REST while orchestration uses gRPC","Choreography has no central coordinatorservices react to events; orchestration has a central coordinator directing the flow","Orchestration is always faster than choreography","Choreography can only handle two services while orchestration handles unlimited"],correctIndex:1,explanation:"In choreography, there is no central coordinatoreach service publishes events and other services subscribe and react independently, creating a decentralized flow. In orchestration, a central saga orchestrator explicitly tells each service what to do and when. Choreography is simpler for few steps but becomes hard to understand as complexity grows (the 'event spaghetti' problem). Orchestration provides a single place to see the entire workflow but adds a coordination dependency. The choice of protocol (REST vs gRPC) is orthogonal to the saga style. Both patterns can handle any number of services; the trade-off is about complexity management and coupling."},{question:"What is the primary purpose of a Service Mesh?",options:["To provide a UI dashboard for developers","To handle service-to-service communication concerns (security, observability, traffic management) transparently at the infrastructure layer","To compile microservices code into containers","To replace the need for a message broker"],correctIndex:1,explanation:"A service mesh handles cross-cutting communication concerns like mutual TLS, load balancing, circuit breaking, retries, and distributed tracing at the infrastructure layer, without requiring changes to application code. It consists of a data plane (sidecar proxies like Envoy) and a control plane (like Istio's istiod) that configures the proxies. This is valuable because these concerns would otherwise need to be implemented in every service using language-specific libraries. A service mesh doesn't replace message brokers (which handle async messaging) or compile code. It's particularly powerful in polyglot environments where services use different languages and frameworks."},{question:"Which Kubernetes resource provides a stable network endpoint for a set of pods?",options:["ConfigMap","Service","PersistentVolume","Namespace"],correctIndex:1,explanation:"A Kubernetes Service provides a stable virtual IP (ClusterIP) and DNS name that routes traffic to a set of pods selected by label selectors. Since pods are ephemeral and their IPs change when they restart, the Service abstraction gives clients a consistent endpoint. There are several types: ClusterIP (internal only), NodePort (exposes on each node's IP), LoadBalancer (provisions cloud load balancer), and ExternalName (DNS alias). ConfigMaps store configuration data, PersistentVolumes provide storage, and Namespaces provide logical isolation. Services are essential for service discovery within a Kubernetes cluster."},{question:"What is API versioning important for in microservices?",options:["It allows services to evolve their contracts without breaking existing consumers","It speeds up API response times","It reduces the number of services needed","It eliminates the need for documentation"],correctIndex:0,explanation:"API versioning allows services to evolve their APIs while maintaining backward compatibility with existing consumers. In a microservices ecosystem, services are developed and deployed independently by different teams, so breaking changes to an API can cascade and disrupt multiple consumers. Common versioning strategies include URL path versioning (/v1/users), header-based versioning, and content negotiation. Each approach has trade-offsURL versioning is most visible but clutters routes, while header-based versioning keeps URLs clean but is less discoverable. Without versioning, every API change requires coordinated deployments across all consuming services, negating a key microservices benefit."},{question:"What does the term 'polyglot persistence' mean?",options:["Using multiple programming languages in the same service","Using different database technologies for different services based on their specific needs","Persisting data in multiple geographic regions simultaneously","Translating database queries into multiple languages"],correctIndex:1,explanation:"Polyglot persistence means choosing the most appropriate database technology for each service's specific data access patterns, rather than forcing all services to use the same database. For example, a product catalog might use Elasticsearch for full-text search, a social graph might use Neo4j, a session store might use Redis, and an order service might use PostgreSQL. This is a natural fit for microservices' database-per-service pattern. The trade-off is operational complexityyour team needs expertise in multiple database technologies. However, it allows each service to optimize for its unique read/write patterns, consistency requirements, and data model."},{question:"What is a key risk of synchronous request chains (Service A  B  C  D)?",options:["It guarantees strong consistency, which is always undesirable","The overall availability is the product of individual availabilities, creating a fragile chain","It reduces network traffic to zero","It forces all services to use the same database"],correctIndex:1,explanation:"When services form synchronous chains, the system's availability becomes the product of each service's availability. If each service has 99.5% uptime, a chain of four services has ~98% uptime (0.995^4). The deeper the chain, the worse the compound availability. Additionally, latency adds upeach hop adds network time plus processing time. A timeout in the last service cascades back through the entire chain, blocking threads at every level. This is why architects try to minimize synchronous call depth, prefer asynchronous communication for non-blocking operations, and use patterns like circuit breakers and bulkheads to contain failures when synchronous calls are necessary."},{question:"In Kubernetes, what is an Ingress?",options:["A container runtime interface","An API object that manages external HTTP/HTTPS access to services, typically providing routing, TLS termination, and virtual hosting","A tool for building container images","A secrets management system"],correctIndex:1,explanation:"A Kubernetes Ingress is an API object that defines rules for routing external HTTP/HTTPS traffic to internal services. It provides features like path-based routing (/api  service-a, /web  service-b), host-based virtual hosting, TLS termination, and basic load balancing. An Ingress Controller (like NGINX Ingress Controller, Traefik, or AWS ALB Ingress Controller) implements these rules. Without Ingress, you'd need to expose each service individually via LoadBalancer services, which is expensive and unmanageable. Ingress is essentially the entry point for external traffic into your Kubernetes cluster, similar to a reverse proxy or API gateway at the cluster edge."},{question:"What is the primary benefit of asynchronous messaging between microservices?",options:["It guarantees messages are never lost","It decouples services in timethe sender doesn't wait for the receiver to process the message","It eliminates the need for serialization","It makes debugging easier than synchronous calls"],correctIndex:1,explanation:"Asynchronous messaging decouples services temporallythe sender publishes a message and continues without waiting for the receiver. This means services don't need to be available simultaneously, improving resilience and allowing independent scaling. If a consumer is down, messages queue up and are processed when it recovers. Message durability (not losing messages) depends on broker configurationit's not automatic. Serialization is still required (JSON, Avro, Protobuf). Debugging can actually be harder than synchronous calls because the flow is non-linear and distributed. Despite the debugging complexity, async messaging is preferred for most service-to-service communication because it significantly improves system resilience."},{question:"What is the 'smart endpoints, dumb pipes' principle in microservices?",options:["Use intelligent network switches and simple service logic","Put business logic in the services (endpoints) and use lightweight messaging infrastructure (pipes) for transport only","Encrypt endpoints and leave pipes unencrypted","Use smart DNS and simple HTTP"],correctIndex:1,explanation:"This principle, coined by Martin Fowler, means that business logic should reside in the microservices themselves (smart endpoints) while the communication infrastructure (pipes) should be simple and lightweightjust transporting messages without adding business logic. This contrasts with the ESB (Enterprise Service Bus) approach where the bus contained routing rules, transformations, and business logic. In microservices, you use simple protocols like HTTP/REST or lightweight message brokers (RabbitMQ, Kafka) for transport, keeping them 'dumb.' This prevents the communication layer from becoming a bottleneck and a tangled mess of routing rules, which was a common problem with SOA-era ESBs."},{question:"What is the difference between client-side and server-side service discovery?",options:["Client-side uses JavaScript, server-side uses Java","In client-side, the client queries the registry and selects an instance; in server-side, a load balancer queries the registry on behalf of the client","Client-side is for mobile apps, server-side is for web apps","Client-side uses UDP, server-side uses TCP"],correctIndex:1,explanation:"In client-side discovery, the client (calling service) directly queries the service registry to get available instances and uses a load-balancing algorithm to pick one. Netflix Eureka with Ribbon is a classic example. In server-side discovery, the client sends requests to a load balancer or router, which queries the registry and forwards the request to an appropriate instance. AWS ELB and Kubernetes Services use this approach. Client-side discovery gives the client more control over load-balancing strategy but couples it to the registry. Server-side discovery is simpler for clients but adds a network hop through the load balancer. Kubernetes DNS-based discovery is server-side and is the most common approach in cloud-native environments."},{question:"What is a common anti-pattern when designing microservice boundaries?",options:["Aligning services with business capabilities","Creating services that are too fine-grained (nano-services), leading to excessive inter-service communication","Using bounded contexts from DDD to define service boundaries","Having a single team own a single service"],correctIndex:1,explanation:"Creating nano-servicesservices that are too small and fine-grainedis a common anti-pattern. When services are too small, simple operations require many network calls, increasing latency and failure points. For example, having separate services for 'validate address,' 'format address,' and 'geocode address' creates unnecessary network overhead. A single 'Address Service' would be more appropriate. Good boundaries align with business capabilities or DDD bounded contexts, grouping related functionality that changes together. The goal is to minimize inter-service communication while maximizing independent deployability. Sam Newman's advice: start with a monolith and extract services as boundaries become clear."},{question:"What does the CAP theorem state?",options:["A system can have Consistency, Availability, and Partition tolerance simultaneously","A distributed system can provide at most two of three guarantees: Consistency, Availability, and Partition tolerance","Caching Always Performs better than database queries","Concurrent Access Patterns must be serialized"],correctIndex:1,explanation:"The CAP theorem (Brewer's theorem) states that in a distributed system experiencing a network partition, you must choose between Consistency (all nodes see the same data) and Availability (every request receives a response). Since network partitions are inevitable in distributed systems, the real choice is CP (consistent but may be unavailable during partitions, like ZooKeeper) or AP (available but may return stale data, like Cassandra). This is fundamental to microservices because the database-per-service pattern creates a distributed data system. Understanding CAP helps architects make informed trade-offs between consistency and availability for each service based on business requirements."},{question:"What is the purpose of health check endpoints in microservices?",options:["To display the service's source code","To allow orchestrators and load balancers to determine if a service instance is healthy and able to receive traffic","To encrypt service communications","To compress response payloads"],correctIndex:1,explanation:"Health check endpoints (e.g., /health or /ready) allow orchestrators like Kubernetes and load balancers to determine whether a service instance can handle traffic. Kubernetes uses liveness probes (is the process alive?) and readiness probes (is it ready to accept traffic?). If a liveness check fails, Kubernetes restarts the container; if readiness fails, it removes the pod from the service's endpoints. This enables self-healingunhealthy instances are automatically replaced. Health checks should verify critical dependencies (database connectivity, cache availability) to provide an accurate picture. Without proper health checks, traffic may be routed to broken instances, causing errors for users."},{question:"What is a 'shared nothing' architecture in the context of microservices?",options:["Services don't share any code, data stores, or infrastructure components","All services run on a single shared server","Services share a common message format","Teams don't share knowledge between them"],correctIndex:0,explanation:"Shared nothing architecture means each microservice owns all its resources independentlyits own database, its own data, its own dependencieswithout sharing these with other services. This eliminates resource contention and coupling: one service's database maintenance doesn't affect others, and schema changes are local. The trade-off is data duplication and the complexity of keeping data synchronized across services. Sharing message formats (contracts) is actually expected for interoperability. In practice, some sharing is pragmatic (shared libraries for common utilities, shared infrastructure like Kubernetes), but the data layer should remain private to each service."},{question:"Which pattern helps maintain data consistency across multiple microservices without distributed transactions?",options:["Two-phase commit (2PC)","Saga pattern","Singleton pattern","Factory pattern"],correctIndex:1,explanation:"The Saga pattern maintains data consistency across services by executing a sequence of local transactions, each within a single service's database. If any step fails, compensating transactions undo the effects of prior steps. Unlike 2PC, sagas don't lock resources across serviceseach transaction commits locally immediately. Two-phase commit technically works for distributed transactions but is impractical for microservices due to resource locking, reduced availability, and tight coupling. Singleton and Factory are OOP design patterns unrelated to distributed data consistency. Sagas are the standard approach in microservices, implemented via either choreography (events) or orchestration (coordinator)."},{question:"What is the purpose of an API Gateway in a microservices architecture?",options:["To store all service configurations","To provide a single entry point for clients, handling routing, authentication, rate limiting, and request aggregation","To compile microservices code","To replace all inter-service communication"],correctIndex:1,explanation:"An API Gateway serves as a single entry point for external clients, abstracting the complexity of the microservices topology behind a unified API. It handles cross-cutting concerns like authentication, rate limiting, SSL termination, request routing, response caching, and request/response transformation. It can also aggregate responses from multiple backend services into a single response (reducing client round trips). Without a gateway, clients would need to know the addresses of individual services and handle authentication separately for each. Popular implementations include Kong, AWS API Gateway, and Netflix Zuul/Spring Cloud Gateway."},{question:"What is the 'data consistency' challenge specific to microservices?",options:["Microservices cannot use SQL databases","With each service owning its database, maintaining consistency across services requires careful coordination since you can't use ACID transactions across service boundaries","All microservices must use eventual consistency","Data consistency is not a concern in microservices"],correctIndex:1,explanation:"When each service owns its own database, you lose the ability to use ACID transactions that span multiple services. An operation like 'place order' might involve the Order Service, Inventory Service, and Payment Serviceeach with its own database. There's no single transaction to atomically update all three. This requires patterns like sagas for managing multi-service transactions and event-driven approaches for synchronizing data. Not all microservices must use eventual consistencyindividual services can use strong consistency internally. The challenge is inter-service consistency, and the right consistency model depends on business requirements (e.g., payment needs strong guarantees, recommendation feeds can be eventually consistent)."},{question:"What does 'idempotency' mean in the context of microservices communication?",options:["A request always returns the same response time","Processing the same request multiple times produces the same result as processing it once","All services must use the same programming language","Messages are always delivered in order"],correctIndex:1,explanation:"Idempotency means that performing the same operation multiple times has the same effect as performing it once. This is critical in microservices because network failures, retries, and message broker redeliveries can cause the same request to be received multiple times. For example, a payment service must ensure that charging a customer twice for the same order doesn't actually debit their account twice. Common implementations include using idempotency keys (unique request IDs), checking if the operation was already performed before executing it, and designing operations to be naturally idempotent (like setting a value rather than incrementing). Without idempotency, retry mechanisms become dangerous."},{question:"What is the Backends for Frontends (BFF) pattern?",options:["Running backend services in the frontend browser","Creating separate backend services tailored for each type of frontend client (web, mobile, etc.)","Using the same API for all frontend platforms","A pattern where the frontend directly accesses the database"],correctIndex:1,explanation:"The BFF pattern creates dedicated backend services for each frontend platformone for the web app, one for the mobile app, one for third-party APIs, etc. Each BFF aggregates and transforms data from downstream microservices in the way that's optimal for its specific client. A mobile app might need less data and different formatting than a web dashboard. Without BFF, a general-purpose API either over-fetches for mobile (wasting bandwidth) or under-fetches for web (requiring multiple calls). Sam Newman popularized this pattern. The trade-off is maintaining multiple BFF services, but it prevents a single API from becoming a compromise that serves no client well."},{question:"What is 'contract testing' in microservices?",options:["Testing legal contracts between service teams","Verifying that a service's API meets the expectations defined by its consumers","Testing network contracts like TCP/UDP","Testing that all services use the same deployment pipeline"],correctIndex:1,explanation:"Contract testing (often called Consumer-Driven Contract Testing) verifies that a service provider's API satisfies the contracts expected by its consumers. Tools like Pact allow consumers to define their expectations (what endpoints they call, what data they need), and these contracts are verified against the provider during CI. This prevents a provider from accidentally breaking its consumers when making changes. Unlike integration tests that require running all services, contract tests run independently and are faster. This is critical in microservices where services are deployed independentlywithout contract tests, you might deploy a provider change that breaks consumers you didn't know about."},{question:"What is the recommended team structure for microservices according to Conway's Law?",options:["One large team manages all services","Teams organized around business capabilities, where each team owns one or more services end-to-end","Separate teams for frontend, backend, and database for each service","Teams organized by programming language expertise"],correctIndex:1,explanation:"Conway's Law states that organizations design systems that mirror their communication structures. For microservices, this means organizing teams around business capabilities (e.g., a Payments team, an Inventory team) where each team owns their services end-to-endfrom UI to database. Amazon's 'two-pizza teams' embody this principle. Horizontal teams (one team for all frontends, another for all backends) create handoffs and coordination overhead that slow delivery. When a team owns a service completely, they can deploy independently, make technology choices, and iterate quickly. This organizational alignment is often considered a prerequisite for successful microservices adoption."},{question:"What happens in the 'Half-Open' state of a Circuit Breaker?",options:["All requests are blocked indefinitely","A limited number of test requests are allowed through to check if the downstream service has recovered","The circuit operates normally as if fully closed","The circuit randomly accepts or rejects requests"],correctIndex:1,explanation:"In the Half-Open state, the circuit breaker allows a small number of trial requests through to the downstream service to test if it has recovered. If these test requests succeed, the circuit transitions back to Closed (normal operation). If they fail, the circuit returns to Open (fail fast). This state is crucial for automatic recoverywithout it, the circuit would either stay open forever (requiring manual intervention) or immediately flood the recovering service with traffic. The number of test requests and the timeout before entering Half-Open are configurable parameters. This three-state model (Closed  Open  Half-Open  Closed) enables self-healing without human intervention."},{question:"What is a 'correlation ID' in microservices?",options:["A database primary key shared across services","A unique identifier attached to a request that is propagated through all service calls to enable end-to-end tracing","An encryption key for inter-service communication","A version number for API contracts"],correctIndex:1,explanation:"A correlation ID (or trace ID) is a unique identifier generated at the entry point of a request (usually the API gateway) and propagated through all subsequent service calls via HTTP headers or message metadata. This allows log aggregation tools to correlate all log entries belonging to the same business operation across multiple services. Without correlation IDs, matching logs from Service A with related logs from Service B and Service C would be virtually impossible in a system processing thousands of concurrent requests. This is the foundation of distributed tracing systems like Jaeger and Zipkin, and it's essential for debugging, auditing, and performance analysis in microservices."},{question:"What is the difference between horizontal and vertical scaling?",options:["Horizontal scaling adds more machines; vertical scaling adds more resources to existing machines","Horizontal scaling is for databases; vertical scaling is for applications","Horizontal scaling is cheaper; vertical scaling is always more expensive","There is no difference; they are synonymous"],correctIndex:0,explanation:"Horizontal scaling (scaling out) adds more instances/machines to distribute the load, while vertical scaling (scaling up) increases the resources (CPU, RAM, disk) of existing machines. Microservices inherently favor horizontal scaling because each service can be independently replicated. Vertical scaling has a ceilingthere's a maximum machine sizewhile horizontal scaling is theoretically unlimited. However, horizontal scaling requires the application to be stateless or handle distributed state. Kubernetes facilitates horizontal scaling through Horizontal Pod Autoscalers (HPA) that automatically adjust replica counts based on metrics. Both strategies have their place: stateless services scale horizontally, while some databases (like traditional RDBMS) initially benefit more from vertical scaling."},{question:"What is 'eventual consistency' in distributed microservices?",options:["Data is never consistent across services","Given enough time without new updates, all replicas of the data will converge to the same state","Data is immediately consistent across all services at all times","Only the latest write is stored; all previous versions are deleted"],correctIndex:1,explanation:"Eventual consistency means that if no new updates are made, all copies of the data across services will eventually converge to the same valuebut there's a window where different services may see different values. This is common in microservices because data is replicated asynchronously via events or messages. For example, after an order is placed, the inventory service might take a few seconds to reflect the stock reduction. This trade-off is acceptable for many use cases (product catalogs, notifications, analytics) but not for others (financial transactions, inventory counts for limited stock). Understanding where eventual consistency is acceptable vs. where stronger guarantees are needed is a key architectural skill."},{question:"What tool is commonly used for container orchestration in microservices?",options:["Jenkins","Kubernetes","Git","Terraform"],correctIndex:1,explanation:"Kubernetes (K8s) is the industry-standard container orchestration platform for managing microservices. It automates deployment, scaling, self-healing, service discovery, load balancing, and rolling updates of containerized applications. Jenkins is a CI/CD tool for building and testing code, Git is version control, and Terraform is infrastructure-as-code for provisioning cloud resourcesnone of them orchestrate running containers. Kubernetes provides the runtime environment where microservices containers actually execute, managing their lifecycle, networking, and resource allocation. Alternatives like Docker Swarm and Apache Mesos exist but Kubernetes has become the dominant choice, supported by all major cloud providers."},{question:"What is the Outbox pattern used for in microservices?",options:["Storing outgoing emails in a queue","Ensuring atomic updates to a database and reliable event publishing by writing events to an outbox table in the same transaction","A logging pattern for outbound API calls","Managing outgoing network connections"],correctIndex:1,explanation:"The Outbox pattern solves the dual-write problem: when a service needs to update its database AND publish an event, but doing both atomically is impossible without distributed transactions. The solution writes both the data change and the event to the same database in a single ACID transaction (the event goes to an 'outbox' table). A separate process (like Debezium using CDC) reads the outbox table and publishes events to the message broker. This guarantees that if the data is updated, the event will eventually be published (and vice versa). Without this pattern, crashes between the database write and event publish could leave the system in an inconsistent state."},{question:"What is gRPC and why is it used in microservices?",options:["A database query language","A high-performance RPC framework using Protocol Buffers and HTTP/2 for efficient inter-service communication","A container orchestration tool","A service mesh implementation"],correctIndex:1,explanation:"gRPC is a high-performance Remote Procedure Call framework developed by Google that uses Protocol Buffers (protobuf) for serialization and HTTP/2 for transport. It offers significant advantages for inter-service communication: binary serialization is faster and smaller than JSON, HTTP/2 enables multiplexing and streaming, and strongly-typed service definitions (via .proto files) generate client/server code automatically. This makes it ideal for internal microservice-to-microservice communication where performance matters. The trade-off is that gRPC is less human-readable than REST/JSON and has limited browser support (though gRPC-Web exists). Many companies use REST for external APIs and gRPC for internal service communication."},{question:"What is the purpose of a Container Registry in microservices deployment?",options:["To register domain names for services","To store and distribute container images that services are packaged into","To register services with a service discovery mechanism","To store database connection strings"],correctIndex:1,explanation:"A Container Registry stores and distributes Docker/OCI container images. When you build a microservice, it's packaged as a container image and pushed to a registry (like Docker Hub, AWS ECR, Google GCR, or Harbor). During deployment, Kubernetes pulls the image from the registry to create container instances. This is the distribution mechanism for microservice artifactssimilar to how Maven repositories distribute JAR files, but for containers. Registries also support image versioning (tags), vulnerability scanning, access control, and image signing. Without a registry, there would be no standardized way to distribute and version microservice deployments across environments."},{question:"What is 'service decomposition' and what are common strategies for it?",options:["Breaking down a monolith's codebase by file size","Splitting a system into services based on business capabilities, subdomains, or use cases","Distributing a service across multiple geographic regions","Converting synchronous calls to asynchronous ones"],correctIndex:1,explanation:"Service decomposition is the process of dividing a system into microservices. Common strategies include: decomposing by business capability (payments, shipping, inventory), by DDD subdomain (each bounded context becomes a service), or by use case/user journey. The goal is to create services with high cohesion (related functionality together) and loose coupling (minimal dependencies between services). Anti-patterns include decomposing by technical layer (a 'data service' and 'logic service') which creates tight coupling, or making services too granular (nano-services). A practical approach is starting with a modular monolith, identifying natural seams, and extracting services incrementally as team and domain understanding matures."},{question:"What is a ConfigMap in Kubernetes?",options:["A mapping of services to their IP addresses","A Kubernetes object used to store non-confidential configuration data as key-value pairs that can be consumed by pods","A visual map of the cluster topology","A routing table for network traffic"],correctIndex:1,explanation:"A ConfigMap stores non-confidential configuration data (environment variables, configuration files, command-line arguments) as key-value pairs that pods can consume via environment variables or mounted volumes. This separates configuration from container images, following the twelve-factor app methodology. For example, you can change a service's log level or feature flags without rebuilding the imagejust update the ConfigMap and restart the pods. For sensitive data like passwords and API keys, Kubernetes provides Secrets (which are base64-encoded, not encrypted by default). ConfigMaps are essential for managing microservice configuration across different environments (dev, staging, production) without baking environment-specific values into images."},{question:"What is the 'ambassador pattern' in microservices?",options:["A diplomatic protocol for service-to-service negotiations","A proxy that handles cross-cutting concerns on behalf of a service, similar to a sidecar but specifically for outbound traffic patterns","A service that translates between different programming languages","The first service deployed in a new cluster"],correctIndex:1,explanation:"The Ambassador pattern deploys a proxy alongside a service to handle outbound communication concerns like retries, circuit breaking, logging, and routing. It's conceptually similar to the sidecar pattern but specifically focuses on acting as an 'ambassador' for the service's outbound connections. For example, an ambassador container might handle connection pooling and retries to a legacy database, allowing the application to use a simple connection. This pattern offloads cross-cutting networking concerns from the application code, making it simpler and allowing infrastructure teams to manage these concerns independently. The pattern is particularly useful when you can't modify the application code."},{question:"What is 'blue-green deployment' in microservices?",options:["Deploying services to two different cloud providers simultaneously","Running two identical production environments (blue and green), switching traffic between them for zero-downtime deployments","Color-coding services based on their team ownership","Deploying test and production environments side by side"],correctIndex:1,explanation:"Blue-green deployment maintains two identical production environments. The 'blue' environment runs the current version while the 'green' environment is deployed with the new version. Once the green environment is verified, traffic is switched from blue to green (typically via DNS or load balancer change). If issues are detected, you instantly switch back to blue. This provides zero-downtime deployments and instant rollback. The trade-off is costyou need double the infrastructure during deployment. In microservices, this can be applied per-service or for the entire system. Kubernetes supports this via service label selectors. An alternative is canary deployment, which gradually shifts traffic rather than switching all at once."},{question:"What is 'canary deployment' and how does it differ from blue-green?",options:["Deploying to a canary island data center","Gradually routing a small percentage of traffic to the new version and increasing it if metrics are healthy, unlike blue-green's all-at-once switch","Deploying a lightweight monitoring service alongside the main service","Testing in production using synthetic traffic only"],correctIndex:1,explanation:"Canary deployment rolls out a new version to a small percentage of users first (say 5%), monitoring error rates, latency, and business metrics. If the canary is healthy, traffic is gradually increased (25%, 50%, 100%). If problems are detected, only the small canary group is affected, and the deployment is rolled back. Unlike blue-green (which switches 100% of traffic at once), canary provides a gradual, metrics-driven rollout that catches issues before they affect all users. Istio and Flagger can automate canary deployments in Kubernetes by progressively shifting traffic weights between old and new deployments based on defined success criteria."},{question:"What are liveness and readiness probes in Kubernetes?",options:["Probes that test network connectivity between pods","Liveness checks if a container should be restarted; readiness checks if it should receive traffic","Probes that monitor disk space and memory usage","Probes that verify container image signatures"],correctIndex:1,explanation:"Liveness probes determine if a container is still running properly. If a liveness probe fails, Kubernetes kills the container and restarts it (assuming the restart policy allows it). This handles deadlocks, infinite loops, and corrupted state. Readiness probes determine if a container is ready to accept traffic. If readiness fails, the pod's IP is removed from Service endpointsno traffic is sent to it, but the container isn't restarted. This is important during startup (when a service is loading data or warming caches) and during temporary issues (database connection lost). There's also a startup probe for slow-starting containers. Properly configuring these probes is critical for reliable microservice operation on Kubernetes."},{question:"What is the 'retry with exponential backoff' pattern?",options:["Retrying a failed request immediately as many times as possible","Retrying failed requests with progressively increasing delays between attempts to avoid overwhelming the failing service","Sending the same request to exponentially more services","Reducing the payload size exponentially with each retry"],correctIndex:1,explanation:"Retry with exponential backoff means that when a request fails, subsequent retries wait for progressively longer intervals (e.g., 1s, 2s, 4s, 8s). This prevents overwhelming a struggling service with a flood of immediate retries, giving it time to recover. Adding random jitter (small random variation to the delay) prevents the 'thundering herd' problem where many clients retry simultaneously. Without backoff, aggressive retries can create a feedback loop that makes outages worse. Most HTTP client libraries and messaging frameworks support configurable backoff. It's typically combined with a maximum retry count and circuit breakers to avoid infinite retries. AWS SDK and gRPC libraries implement this pattern by default."},{question:"What is the difference between a Kubernetes Deployment and a StatefulSet?",options:["Deployments are for Java services; StatefulSets are for Python services","Deployments manage stateless applications with interchangeable pods; StatefulSets manage stateful applications with stable identities, persistent storage, and ordered deployment","StatefulSets are deprecated in favor of Deployments","Deployments support rolling updates; StatefulSets do not"],correctIndex:1,explanation:"Deployments are designed for stateless applications where pods are interchangeableany pod can be replaced without consequence. StatefulSets are for stateful applications (like databases, Kafka brokers, ZooKeeper) that need stable network identities (pod-0, pod-1), persistent storage that survives pod restarts, and ordered deployment/scaling. In a StatefulSet, pods are created sequentially (pod-0 before pod-1) and have predictable DNS names. Deployments create pods with random names and no ordering guarantees. For microservices, most application services use Deployments (stateless), while data infrastructure components use StatefulSets. Both support rolling updates, but StatefulSets update in reverse order."},{question:"What is 'feature flagging' and why is it important for microservices?",options:["A security mechanism to flag suspicious features","A technique to enable/disable features at runtime without redeploying, supporting progressive rollouts and A/B testing","A code review process for flagging features that need attention","A Kubernetes label used to mark services"],correctIndex:1,explanation:"Feature flags (toggles) allow teams to enable or disable features at runtime without deploying new code. This is powerful in microservices for several reasons: it enables trunk-based development (merge code early, activate features later), progressive rollouts (enable for 10% of users first), A/B testing, and instant kill switches for problematic features. Tools like LaunchDarkly, Unleash, and Flagsmith manage flags centrally. In microservices, where independent deployment is key, feature flags decouple deployment from releaseyou can deploy code anytime and activate features when ready. The trade-off is technical debt: old flags must be cleaned up, or the codebase becomes cluttered with conditional logic."},{question:"What is the 'log aggregation' pattern?",options:["Writing all logs to a single monolithic log file","Collecting logs from all service instances into a centralized system for searching, analysis, and alerting","Aggregating log entries to reduce storage costs by removing duplicates","Printing logs to the console during development only"],correctIndex:1,explanation:"Log aggregation collects logs from all microservice instances into a centralized platform for unified searching, analysis, and alerting. The ELK/EFK stack (Elasticsearch, Logstash/Fluentd, Kibana) and solutions like Datadog, Splunk, and Grafana Loki are common implementations. In microservices, logs are scattered across hundreds of container instances that are ephemeralwhen a pod restarts, its local logs are lost. Centralized log aggregation with correlation IDs enables engineers to trace a request across all services it touched. This is not about writing to a single file or deduplication, but about making distributed system behavior observable and debuggable from a single pane of glass."},{question:"What does 'infrastructure as code' mean for microservices deployments?",options:["Writing microservices in infrastructure-level languages like C","Defining and managing infrastructure (servers, networks, deployments) through code files that can be versioned and automated","Storing infrastructure passwords in source code","Running infrastructure monitoring dashboards as microservices"],correctIndex:1,explanation:"Infrastructure as Code (IaC) means defining infrastructure (Kubernetes manifests, cloud resources, networking) in declarative code files that are version-controlled, reviewed, and automatically applied. Tools like Terraform, Pulumi, Helm, and Kustomize enable this. For microservices, IaC is critical because manually configuring hundreds of services, load balancers, databases, and networks is error-prone and unreproducible. With IaC, you can recreate an entire environment from scratch, track changes through Git history, and ensure consistency across development, staging, and production. GitOps tools like ArgoCD and Flux take this further by using Git as the single source of truth for cluster state."},{question:"What is the purpose of a Namespace in Kubernetes?",options:["To define the programming language namespace for service code","To provide logical isolation and resource scoping within a cluster, allowing multiple teams or environments to share it","To create physical network partitions between nodes","To define DNS domain names for external traffic"],correctIndex:1,explanation:"Kubernetes Namespaces provide logical isolation within a cluster, allowing you to partition resources into named groups. Different teams can use separate namespaces (team-a, team-b), or you can separate environments (dev, staging) within the same cluster. Namespaces scope resource names (avoiding conflicts), allow resource quotas (limiting CPU/memory per namespace), and enable network policies (restricting cross-namespace traffic). They don't create physical network isolation by defaultpods in different namespaces can still communicate unless network policies restrict it. For microservices, namespaces help organize services by team or environment while sharing the same underlying cluster infrastructure."},{question:"What is 'observability' and how does it differ from 'monitoring'?",options:["They are the same thingdifferent names for dashboards","Monitoring checks known failure modes; observability lets you understand system behavior from its outputs, including investigating unknown unknowns","Observability is for development; monitoring is for production","Monitoring uses metrics; observability uses logs only"],correctIndex:1,explanation:"Monitoring tracks predefined metrics and alerts on known failure conditions (CPU > 90%, error rate > 5%). Observability goes furtherit enables you to understand system behavior from its external outputs (metrics, logs, traces), including diagnosing novel, unforeseen issues. The three pillars of observability are metrics (numerical measurements), logs (detailed event records), and traces (request paths across services). In microservices, monitoring alone is insufficient because the failure modes are too varied and complex to predict. Observability tools like Grafana, Jaeger, and Datadog let engineers ask arbitrary questions about system behavior and drill down from symptoms to root causes without adding new instrumentation."},{question:"What is the 'timeout pattern' and why is it critical in microservices?",options:["A pattern to limit how long developers spend on a feature","Setting maximum wait times for service calls to prevent resource exhaustion when downstream services are slow","A pattern for scheduling service shutdowns during maintenance","Limiting the time a container can run before being replaced"],correctIndex:1,explanation:"The timeout pattern sets a maximum time a caller will wait for a response from a downstream service. Without timeouts, a slow service can cause the caller to hold resources (threads, connections) indefinitely, eventually exhausting them and cascading the failure. For example, if Service A calls Service B with no timeout and B hangs, A's thread pool fills up with waiting threads, making A unresponsive. Timeouts should be carefully tunedtoo short causes false failures, too long wastes resources. Timeouts work best in combination with circuit breakers and retries. In microservices, every outbound call should have a timeout configured; the default 'infinite timeout' of most HTTP clients is dangerous in distributed systems."},{question:"What is 'event storming' in the context of microservices design?",options:["A chaos engineering technique that floods services with events","A collaborative workshop technique where domain experts and developers discover domain events, commands, and aggregates to identify service boundaries","A load testing approach using event-driven traffic","A monitoring technique for event-driven systems"],correctIndex:1,explanation:"Event Storming is a collaborative workshop method created by Alberto Brandolini where domain experts and developers use sticky notes on a wall to map out domain events (things that happen), commands (what triggers them), aggregates (entities that handle commands), and bounded contexts. This is one of the most effective techniques for discovering microservice boundaries because it focuses on business processes and domain events rather than data models. The workshop reveals natural boundaries where different teams or contexts handle different events. It produces a shared understanding of the domain that directly maps to service decomposition. It's rooted in DDD principles and typically precedes any architecture decisions."},{question:"What is a DaemonSet in Kubernetes?",options:["A set of services that run only at night (daemon hours)","A resource that ensures a copy of a pod runs on every node in the cluster, commonly used for logging agents and monitoring","A deployment strategy for stateful services","A security group for privileged containers"],correctIndex:1,explanation:"A DaemonSet ensures that a copy of a specific pod runs on every node (or a selected subset of nodes) in the cluster. When new nodes are added, the DaemonSet automatically deploys pods to them. Common use cases include log collectors (Fluentd, Filebeat), monitoring agents (Prometheus Node Exporter, Datadog Agent), network plugins (Calico, Weave), and storage daemons (GlusterFS). For microservices, DaemonSets are infrastructure-levelyou typically don't deploy your business services as DaemonSets but rather use them for the cross-cutting infrastructure that all services depend on. Unlike Deployments that create a specified number of replicas, DaemonSets tie replica count to node count."},{question:"What is 'chaos engineering' and why is it relevant to microservices?",options:["Writing code without a plan or design","Intentionally injecting failures into a system to test its resilience and identify weaknesses before they cause real outages","Using random data in unit tests","Deploying services in random order"],correctIndex:1,explanation:"Chaos engineering is the discipline of experimenting on a production (or production-like) system to build confidence in its ability to withstand turbulent conditions. Tools like Chaos Monkey (Netflix), Gremlin, and LitmusChaos inject failures such as killing pods, introducing network latency, corrupting data, and exhausting CPU. For microservices, this is particularly important because the distributed nature creates countless failure modes that are impossible to predict through testing alone. By proactively discovering weaknesses (missing timeouts, inadequate circuit breakers, poor retry logic), teams can fix them before they cause real incidents. Netflix pioneered this approach, evolving it from killing random instances to sophisticated, hypothesis-driven experiments."},{question:"What is the 'Anti-Corruption Layer' pattern in DDD?",options:["A firewall that prevents malicious data from entering the system","A translation layer that prevents one bounded context's model from leaking into and corrupting another context's model","A data validation layer that prevents database corruption","An encryption layer that prevents data tampering"],correctIndex:1,explanation:"The Anti-Corruption Layer (ACL) is a DDD pattern that acts as a translation boundary between two bounded contexts, especially when integrating with a legacy system or external service. It translates between the external model and your internal domain model, preventing foreign concepts from 'corrupting' your domain. For example, if a legacy system represents a customer differently than your new microservice, the ACL transforms data at the boundary. Without it, external models gradually leak into your codebase, making it harder to evolve independently. In microservices, ACLs are commonly implemented at service boundaries where different teams have different domain models, preserving each service's model integrity."},{question:"What is 'trunk-based development' and how does it relate to microservices?",options:["Developing all services in a single monorepo","A branching strategy where developers commit to a single main branch frequently, using feature flags to manage incomplete work","Storing code in tree-structured databases","Developing services based on a tree-structured architecture"],correctIndex:1,explanation:"Trunk-based development is a source-control branching strategy where all developers commit to a single shared branch (main/trunk) at least daily, using short-lived feature branches (if any) that merge quickly. Combined with feature flags, this enables continuous integrationcode is always in a deployable state. This complements microservices because each service needs rapid, independent deployments. Long-lived branches lead to merge hell and delayed integration, undermining the independence microservices promise. Google, Facebook, and Netflix practice trunk-based development. It requires strong CI/CD pipelines, comprehensive automated testing, and feature flags to manage unreleased functionality. Monorepo vs. polyrepo is a separate concern from branching strategy."},{question:"What is a Horizontal Pod Autoscaler (HPA) in Kubernetes?",options:["A tool that horizontally splits pods across regions","A Kubernetes resource that automatically adjusts the number of pod replicas based on observed metrics like CPU utilization or custom metrics","A manual scaling command for Kubernetes administrators","A network policy that controls horizontal traffic between pods"],correctIndex:1,explanation:"The Horizontal Pod Autoscaler automatically scales the number of pod replicas in a Deployment or StatefulSet based on observed metrics. By default, it uses CPU utilization, but it can also use memory, custom metrics (like request queue depth), or external metrics (like messages in a Kafka topic). For example, if average CPU exceeds 70%, HPA adds more replicas; if it drops below, it removes them. This is essential for microservices because traffic patterns varyan order service might spike during sales events while other services remain steady. HPA enables each service to scale independently based on its own demand, optimizing both performance and cost. It works with Cluster Autoscaler, which adds/removes nodes."},{question:"Why should microservices avoid sharing a database?",options:["Databases can only handle one service's queries","Sharing a database creates tight couplingschema changes, performance issues, and scaling decisions affect all services sharing it","Modern databases don't support multiple connections","It's a security requirement from cloud providers"],correctIndex:1,explanation:"When services share a database, they become tightly coupled in several ways: schema changes in one service can break others, a slow query from one service degrades performance for all, scaling the database means over-provisioning for all services, and teams can't independently choose the best data technology for their needs. It also creates hidden runtime couplingservices that appear independent at the code level are actually connected through shared tables. This undermines the core benefit of microservices: independent development and deployment. The migration path is gradual: start by separating schemas (separate tables/schemas per service), then move to separate database instances. Modern databases can absolutely handle multiple connections; that's not the issue."},{question:"What is the role of an 'event bus' or 'message broker' in microservices?",options:["It stores the source code for all microservices","It acts as an intermediary for asynchronous communication, decoupling producers from consumers and enabling event-driven interactions","It compiles and deploys microservices automatically","It manages database schemas across services"],correctIndex:1,explanation:"A message broker (like RabbitMQ, Apache Kafka, or AWS SNS/SQS) acts as an intermediary for asynchronous communication between services. Producers publish messages/events without knowing who will consume them, and consumers subscribe to the messages they're interested in. This decouples services in space (don't need to know each other's addresses) and time (don't need to be running simultaneously). Brokers also provide buffering (absorbing traffic spikes), guaranteed delivery (persisting messages until consumed), and fan-out (one message consumed by multiple services). In microservices, the message broker is often the backbone of the architecture, enabling event-driven patterns that are more resilient than synchronous call chains."},{question:"What is the difference between 'north-south' and 'east-west' traffic in microservices?",options:["Traffic between different geographic regions","North-south is traffic entering/leaving the system (client to services); east-west is traffic between services within the system","North-south is upload traffic; east-west is download traffic","They refer to different network protocols"],correctIndex:1,explanation:"North-south traffic flows between external clients and the system's edge (through the API gateway or load balancer). East-west traffic flows between microservices within the system. In microservices architectures, east-west traffic typically far exceeds north-south traffica single client request might trigger dozens of inter-service calls. This distinction matters for security (north-south goes through the gateway; east-west needs mutual TLS via service mesh), performance (east-west latency compounds across service chains), and monitoring (you need different tools for external vs. internal traffic). Service meshes like Istio primarily address east-west traffic concerns, while API gateways handle north-south."},{question:"What is 'domain event' in microservices?",options:["A DNS change event for a domain name","A significant occurrence within a bounded context that other contexts might need to know about","An event triggered when a new service is deployed","A scheduled cron job within a service"],correctIndex:1,explanation:"A domain event represents something meaningful that happened in the business domainlike 'OrderPlaced,' 'PaymentProcessed,' or 'InventoryReserved.' These events are first-class citizens in DDD and microservices, used to communicate state changes between bounded contexts without tight coupling. When the Order Service places an order, it publishes an 'OrderPlaced' event; the Inventory, Payment, and Notification services can independently react to this event. Domain events should be named in past tense (something that already happened) and contain the relevant data needed by consumers. They're the foundation of event-driven microservices and are distinct from infrastructure events (like deployment notifications)."},{question:"What is 'data replication' used for in microservices?",options:["Copying source code between service repositories","Maintaining copies of relevant data from other services to enable local queries and reduce runtime dependencies","Creating backup tapes for disaster recovery","Duplicating microservices in multiple programming languages"],correctIndex:1,explanation:"In microservices, data replication means maintaining a local copy of data owned by another service so you can query it without making synchronous calls. For example, the Shipping Service might maintain a local copy of customer addresses (owned by the Customer Service) by consuming address-changed events. This eliminates runtime dependency on the Customer Service for every shipment. The data is eventually consistentthere's a small delay between the original update and the replica. This pattern trades consistency for availability and autonomy. It's commonly implemented using events (CDC or domain events) and is essential for query-heavy services that need to join data from multiple domains."},{question:"What is the 'Twelve-Factor App' methodology?",options:["A methodology requiring exactly twelve microservices","A set of twelve best practices for building cloud-native applications that are portable, scalable, and suitable for continuous deployment","A twelve-step process for migrating monoliths to microservices","A security framework with twelve compliance checkpoints"],correctIndex:1,explanation:"The Twelve-Factor App methodology (by Heroku co-founder Adam Wiggins) defines twelve best practices for cloud-native applications: codebase (one repo per app), dependencies (explicitly declared), config (stored in environment), backing services (treated as attached resources), build/release/run (strict separation), processes (stateless), port binding (self-contained), concurrency (scale via processes), disposability (fast startup/shutdown), dev/prod parity, logs (event streams), and admin processes (one-off tasks). These principles align perfectly with microservices: stateless processes enable horizontal scaling, externalized config supports multiple environments, and disposability supports container orchestration. Most Kubernetes best practices derive from these principles."},{question:"What is 'mutual TLS (mTLS)' in microservices?",options:["TLS encryption used only for database connections","A security protocol where both the client and server authenticate each other using certificates, ensuring encrypted and verified communication","A backup TLS certificate used when the primary expires","A simplified TLS that skips certificate verification for performance"],correctIndex:1,explanation:"Mutual TLS (mTLS) extends standard TLS by requiring both parties to present and verify certificates. In standard TLS, only the server proves its identity; in mTLS, the client also proves its identity to the server. In microservices, mTLS ensures that service-to-service communication is both encrypted (confidentiality) and authenticated (you know who's calling). Service meshes like Istio automate mTLSthe sidecar proxies handle certificate generation, rotation, and verification without application changes. Without mTLS, any process in the network could impersonate a service and access internal APIs. Zero-trust security models require mTLS as a baseline for all east-west traffic."},{question:"What is 'rate limiting' at the microservice level used for?",options:["Limiting how fast services can write code","Protecting services from being overwhelmed by too many requests, whether from external clients or other services","Limiting the amount of data stored in databases","Controlling the rate of log file generation"],correctIndex:1,explanation:"Rate limiting restricts the number of requests a service accepts within a time window, protecting it from being overwhelmed. This is critical for several scenarios: preventing denial-of-service attacks, ensuring fair usage among clients, protecting downstream dependencies from cascading overload, and managing costs for pay-per-use resources. Rate limiting can be applied at the API gateway (for external traffic) and at individual services (for inter-service traffic). Common algorithms include token bucket, leaky bucket, and sliding window. When a limit is exceeded, the service returns HTTP 429 (Too Many Requests). Rate limiting is a form of load sheddingit's better to reject excess requests cleanly than to let the service degrade for everyone."},{question:"What is 'graceful degradation' in microservices?",options:["Slowly shutting down all services during maintenance","Continuing to provide reduced but functional service when some components fail, rather than completely failing","Gradually migrating from microservices back to a monolith","Reducing code quality over time to ship faster"],correctIndex:1,explanation:"Graceful degradation means the system continues to function in a reduced capacity when some components fail, rather than failing entirely. For example, if the Recommendation Service is down, an e-commerce site can still show products and process ordersjust without personalized recommendations. This requires designing services to handle missing dependencies: using cached data, default responses, or disabling non-essential features. Circuit breakers and fallback mechanisms are key enablers. Netflix is a master of thisif their recommendation engine fails, they show popular titles instead. Graceful degradation is a design philosophy that accepts partial failure as inevitable in distributed systems and plans for it proactively."},{question:"What is the purpose of Kubernetes Secrets?",options:["To hide services from external clients","To store and manage sensitive data like passwords, tokens, and keys, providing a more secure mechanism than ConfigMaps","To encrypt all network traffic in the cluster","To store secret algorithms used by services"],correctIndex:1,explanation:"Kubernetes Secrets store sensitive data (passwords, API keys, TLS certificates) separately from pod specifications and container images. While similar to ConfigMaps, Secrets are intended for confidential data and offer additional protections: they're stored in tmpfs (not written to disk on nodes), can be encrypted at rest in etcd, and access can be restricted via RBAC. However, by default Secrets are only base64-encoded (not encrypted), so additional measures like sealed-secrets, external-secrets-operator, or HashiCorp Vault integration are recommended for production. For microservices, Secrets are essential for managing database credentials, API keys, and TLS certificates without embedding them in source code or container images."},{question:"What is the 'shared library' vs 'shared service' debate in microservices?",options:["Whether to use open-source or proprietary libraries","Whether common functionality should be distributed as a library linked into each service or centralized as a separate microservice","Whether to share a library building or use remote offices","Whether teams should share code review feedback publicly"],correctIndex:1,explanation:"When multiple services need the same functionality (e.g., validation logic, utility functions), you can either distribute it as a shared library (NuGet, npm, Maven package) that each service includes, or centralize it as a shared microservice that others call over the network. Shared libraries are simpler (no network call) but create couplingupdating the library requires redeploying all consumers. Shared services are independently deployable but add latency and a failure point. The general guidance: use libraries for truly stable, utility-level code (logging, serialization) and services for business logic that evolves independently. Over-sharing libraries can recreate monolith-like coupling; over-creating shared services can create dependency hell."},{question:"What is 'service mesh data plane' vs 'control plane'?",options:["Data plane handles database operations; control plane handles user sessions","Data plane consists of sidecar proxies handling actual traffic; control plane configures and manages the proxies","Data plane is for testing; control plane is for production","Data plane is east-west; control plane is north-south"],correctIndex:1,explanation:"In a service mesh, the data plane consists of lightweight proxy sidecars (typically Envoy) deployed alongside each service. These proxies intercept and handle all network traffic: load balancing, retries, TLS, observability, etc. The control plane (like Istio's istiod) is the management layer that configures all data plane proxies: distributing routing rules, security policies, and telemetry configuration. Think of it like air traffic control: the data plane is the actual aircraft carrying traffic, while the control plane is the ATC system telling them where to go. This separation allows the data plane to operate even if the control plane is temporarily unavailable (using the last known configuration), improving resilience."},{question:"What is a 'monorepo' vs 'polyrepo' approach for microservices?",options:["Monorepo stores all services in one repository; polyrepo uses a separate repository per service","Monorepo is for monoliths only; polyrepo is for microservices only","Monorepo uses one programming language; polyrepo uses multiple","There is no practical difference between them"],correctIndex:0,explanation:"A monorepo stores all microservice codebases in a single repository, while a polyrepo uses a separate repository for each service. Monorepos (used by Google, Meta) simplify atomic cross-service changes, code sharing, and consistent tooling but require sophisticated build tools (Bazel, Nx) to manage scale. Polyrepos (more common in smaller organizations) give each team complete autonomy over their repo, simpler CI/CD per service, and clear ownership boundaries but make cross-service changes harder and can lead to tool/dependency fragmentation. Neither is inherently betterthe choice depends on team size, organizational structure, and tooling maturity. Many organizations use a hybrid approach."},{question:"What is the purpose of 'distributed locking' in microservices?",options:["Locking down services during deployments","Coordinating access to shared resources across multiple service instances to prevent race conditions","Encrypting data at rest in distributed databases","Restricting which services can communicate with each other"],correctIndex:1,explanation:"Distributed locking ensures that only one service instance at a time can perform a specific operation on a shared resource, preventing race conditions. For example, two instances of a Payment Service processing the same order simultaneously could charge the customer twice. Distributed locks (implemented using Redis with Redlock, ZooKeeper, or etcd) provide mutual exclusion across instances. However, distributed locks are fragilenetwork partitions, clock skew, and GC pauses can cause issues. Martin Kleppmann's critique of Redlock highlights these dangers. Whenever possible, prefer idempotent operations and optimistic concurrency control over distributed locks. When locks are necessary, use fencing tokens to prevent stale lock holders from causing harm."},{question:"What is 'service-level objective (SLO)' in microservices?",options:["The minimum number of services that must be running","A target value for a service's reliability metric, like '99.9% of requests complete in under 200ms'","The maximum number of developers assigned to a service","A standard for service naming conventions"],correctIndex:1,explanation:"A Service-Level Objective (SLO) is a target for a specific reliability metric that a service aims to meetfor example, '99.9% of requests succeed' or 'p99 latency under 200ms.' SLOs are part of the SRE framework: SLIs (indicators) measure the actual metrics, SLOs set the targets, and SLAs (agreements) are contractual obligations with consequences. In microservices, SLOs help teams make informed trade-offs between reliability and velocity. Error budgets (the acceptable amount of unreliability, e.g., 0.1%) determine how aggressively teams can deploy changes. If the error budget is exhausted, teams focus on reliability instead of new features. Google's SRE book popularized this approach for managing complex distributed systems."},{question:"What is the 'scatter-gather' pattern in microservices?",options:["A pattern for distributing databases across regions","Broadcasting a request to multiple services concurrently and aggregating their responses into a single result","A garbage collection algorithm for containers","A logging pattern that scatters log entries across multiple files"],correctIndex:1,explanation:"The scatter-gather pattern sends a request to multiple services in parallel (scatter) and then collects and aggregates their responses (gather). For example, a price comparison service might query multiple supplier services simultaneously and return the best price. Or a search service might scatter queries across multiple index shards and merge the results. This pattern reduces latency compared to sequential calls but requires handling partial failureswhat if one service times out? Strategies include returning partial results with a warning, using cached data for the missing service, or waiting with a timeout. The aggregation logic can live in an API gateway, a BFF, or a dedicated orchestration service."},{question:"What is the difference between 'orchestration' and 'choreography' for microservice integration?",options:["Orchestration uses containers; choreography uses virtual machines","Orchestration has a central controller directing the workflow; choreography has services reacting to events independently with no central control","Orchestration is synchronous; choreography is always faster","They are the same pattern with different names"],correctIndex:1,explanation:"Orchestration uses a central controller (orchestrator) that explicitly directs the sequence of service interactionsit knows the full workflow and tells each service what to do and when. Choreography is decentralizedeach service listens for events and independently decides how to react, with no single entity knowing the full picture. Orchestration is easier to understand and modify for complex workflows but creates a central point that must be maintained. Choreography is more loosely coupled and resilient but can become hard to reason about as the number of event chains grows ('event spaghetti'). Real systems often combine both: choreography between bounded contexts and orchestration within complex processes. Neither is inherently better; the choice depends on workflow complexity and coupling requirements."},{question:"What is 'consumer-driven contract testing' and why is it valuable?",options:["Testing that consumers pay for API usage","A testing approach where API consumers define their expectations, which are then verified against the provider to catch breaking changes early","Testing that all consumers use the same client library","A legal testing process for service agreements"],correctIndex:1,explanation:"Consumer-driven contract testing (CDCT) lets consumers of an API specify exactly what they expect (which endpoints, what data format, what status codes). These expectations become 'contracts' that are tested against the provider in the provider's CI pipeline. If a provider change breaks any consumer's contract, the build fails before deployment. Pact is the most popular CDCT framework. This is crucial for microservices because services are deployed independentlywithout contract tests, a provider might unknowingly break consumers discovered only in production. It's faster and more reliable than end-to-end integration tests because it runs without deploying all services. It inverts the testing relationship: providers are held accountable to their consumers' actual needs."},{question:"What is the 'database migration' challenge when splitting a monolith?",options:["Choosing which cloud provider to host databases on","Extracting each service's data from the shared database into independent stores while maintaining referential integrity and data consistency during the transition","Learning new database query languages","Converting all data to JSON format"],correctIndex:1,explanation:"When splitting a monolith, one of the hardest challenges is decomposing the shared database. Tables often have foreign keys, joins, and stored procedures that cross service boundaries. You must identify which tables belong to which service, replace cross-boundary joins with API calls or data replication, handle the transition period where both the monolith and new services need access to data, and migrate without downtime. Strategies include: starting with separate schemas in the same database, using database views to maintain backward compatibility, and employing CDC (Change Data Capture) to synchronize data during the transition. This is often the most technically challenging aspect of monolith decomposition and is why the Strangler Fig pattern recommends a gradual approach."},{question:"What is a Kubernetes Job and CronJob?",options:["Tools for hiring Kubernetes administrators","Job runs a pod to completion for batch tasks; CronJob schedules Jobs to run periodically on a cron schedule","Job monitors service health; CronJob rotates log files","Job deploys new services; CronJob rolls back failed deployments"],correctIndex:1,explanation:"A Kubernetes Job creates one or more pods and ensures they run to successful completionunlike Deployments that maintain long-running services. Jobs are ideal for batch processing, data migrations, and one-off tasks. A CronJob creates Jobs on a recurring schedule (using cron syntax), useful for periodic tasks like database backups, report generation, and cleanup operations. For microservices, Jobs handle batch processing that doesn't fit the always-running service model, and CronJobs replace traditional cron daemons in a cloud-native way. Jobs support parallelism (running multiple pods simultaneously), completion counts, and configurable retry policies for failed pods."},{question:"What is the 'death star' architecture anti-pattern?",options:["A highly centralized architecture with a single point of failure","An architecture where every service calls every other service, creating a tangled web of dependencies that's impossible to understand or manage","An architecture designed to destroy legacy systems","A top-secret internal codename for classified architectures"],correctIndex:1,explanation:"The 'death star' anti-pattern (named for the visualization that looks like the Star Wars Death Star) occurs when microservices are so interconnected that every service depends on multiple others with no clear layering or boundaries. Visualizing service dependencies produces a dense, tangled web. This typically results from poor service boundary design, excessive synchronous calls, and lack of domain-driven design. The symptoms include: a change in one service cascading unpredictably, difficulty understanding request flows, and system-wide outages from a single service failure. The solution involves introducing event-driven communication, defining clear service layers, establishing API contracts, and potentially re-evaluating service boundaries using DDD bounded contexts."},{question:"What is the role of a 'sidecar proxy' in implementing observability?",options:["It replaces the application's logging framework","It transparently captures metrics, traces, and access logs from all service traffic without modifying application code","It stores observability data in a local database","It sends alerts directly to developers' phones"],correctIndex:1,explanation:"A sidecar proxy (like Envoy in Istio) intercepts all inbound and outbound traffic for a service, automatically capturing observability data: request/response metrics (latency, status codes, throughput), distributed traces (propagating trace context between services), and access logs. This provides consistent observability across all services regardless of programming language or framework, without requiring application code changes. The sidecar emits data to observability backends (Prometheus for metrics, Jaeger for traces, Elasticsearch for logs). This is one of the most compelling features of service meshespolyglot environments get uniform observability. The application can still add custom business metrics, but infrastructure-level observability comes 'for free' from the sidecar."},{question:"What is 'semantic versioning' and how does it apply to microservice APIs?",options:["Versioning based on the meaning of code changes","A versioning scheme (MAJOR.MINOR.PATCH) where MAJOR indicates breaking changes, MINOR adds backward-compatible features, and PATCH fixes bugs","Using descriptive names instead of numbers for versions","Automatically versioning APIs based on deployment date"],correctIndex:1,explanation:"Semantic versioning (semver) uses MAJOR.MINOR.PATCH where: MAJOR increments for breaking (incompatible) changes, MINOR for backward-compatible new features, and PATCH for backward-compatible bug fixes. For microservice APIs, this communicates the impact of changes to consumers: a PATCH or MINOR update is safe to adopt without changes, but a MAJOR update requires consumer modifications. This is critical because services are independently deployedconsumers need to understand whether a provider update will break them. In practice, microservices often use simpler versioning (v1, v2) for API endpoints and semver for shared libraries. The key principle: never make breaking changes without communicating them through the version number."},{question:"What is 'load shedding' in microservices?",options:["Distributing load evenly across all service instances","Intentionally rejecting excess requests when a service is at capacity to maintain performance for accepted requests","Moving workloads from one cloud region to another","Reducing the service's codebase to improve performance"],correctIndex:1,explanation:"Load shedding is the practice of deliberately dropping excess requests when a service is at or near capacity, rather than attempting to process everything and degrading performance for all requests. For example, when a service detects it's handling more than its healthy capacity, it returns HTTP 503 (Service Unavailable) for additional requests while maintaining good latency for the requests it's processing. This is different from rate limiting (which limits per-client rates) and load balancing (which distributes requests). Load shedding is a last-resort defense that keeps the service responsive under extreme load. Google's approach prioritizes important requests (like user-facing traffic over batch jobs) during shedding."},{question:"What is 'zero-trust networking' in a microservices context?",options:["Not trusting any code that wasn't written in-house","A security model that verifies every request regardless of origin, assuming the network is always hostileno implicit trust based on network location","Using zero-configuration networking between services","A network that requires zero maintenance"],correctIndex:1,explanation:"Zero-trust networking abandons the traditional perimeter-based security model (trusted inside the firewall, untrusted outside) in favor of verifying every request regardless of where it originates. In microservices, this means: every service-to-service call is authenticated (mutual TLS), authorized (fine-grained policies checking if Service A is allowed to call Service B's specific endpoint), and encrypted. Service meshes like Istio enable zero-trust by handling mTLS and authorization policies transparently via sidecar proxies. This is essential in containerized environments where services share network infrastructure and lateral movement by attackers must be prevented. Google's BeyondCorp is a pioneering implementation of zero-trust principles."},{question:"What are 'init containers' in Kubernetes?",options:["The first containers deployed in a new cluster","Specialized containers that run and complete before the main application containers start, used for setup tasks","Containers that initialize the Kubernetes control plane","Default containers provided by Kubernetes for monitoring"],correctIndex:1,explanation:"Init containers are specialized containers in a pod that run to completion before the main application containers start. They're used for initialization tasks like: waiting for a dependent service to be ready, downloading configuration files, running database migrations, or setting up file permissions. Init containers run sequentiallyeach must succeed before the next starts, and all must complete before the main containers launch. This is useful in microservices for ensuring prerequisites are met: for example, an init container can wait for a database migration to complete or for a config service to be available. Unlike main containers, init containers don't support liveness/readiness probes since they run to completion."},{question:"What is the 'ambassador' deployment model in Kubernetes?",options:["Deploying services to a special 'ambassador' namespace","Running a proxy container in the same pod that handles routing, TLS termination, and protocol translation for the main container","Designating one pod as the spokesperson for a group of pods","A diplomatic protocol for cross-cluster communication"],correctIndex:1,explanation:"The ambassador pattern in Kubernetes deploys a proxy container within the same pod as the main application container. The ambassador handles complex outbound connectivity: routing requests to the right backend, TLS termination, protocol translation, or connection pooling. The main container communicates with the ambassador via localhost, simplifying its network logic. For example, an ambassador might handle connecting to a sharded database, routing requests to the correct shardthe application just connects to localhost:5432. This is closely related to the sidecar pattern but specifically addresses outbound communication complexity. In service mesh architectures, the Envoy sidecar serves a similar ambassador role."},{question:"What is 'graceful shutdown' and why is it important for microservices in Kubernetes?",options:["Shutting down the entire Kubernetes cluster gracefully","Allowing a service to finish processing in-flight requests before terminating, preventing data loss and request failures during deployments","A polite way to notify users that a service is going offline","Gradually reducing traffic to zero before maintenance"],correctIndex:1,explanation:"Graceful shutdown means that when a pod receives a termination signal (SIGTERM), it stops accepting new requests, finishes processing in-flight requests, cleans up resources (closes connections, flushes buffers), and then exits. In Kubernetes, this happens during rolling updates, scaling down, and node draining. Kubernetes sends SIGTERM, waits for the terminationGracePeriodSeconds (default 30s), then sends SIGKILL. Without graceful shutdown, in-flight requests are abruptly terminated, causing errors for clients. Applications must handle SIGTERM properly: stop the HTTP server from accepting new connections, wait for current requests to complete, and exit cleanly. This is especially critical for microservices with constant rolling updates."},{question:"What does 'polyglot architecture' mean?",options:["An architecture that supports multiple human languages in the UI","An architecture where different services are built using different programming languages, frameworks, and data stores based on what's optimal for each","Using a single programming language across all microservices","Translating APIs between different format standards"],correctIndex:1,explanation:"Polyglot architecture allows each microservice team to choose the best programming language, framework, and data store for their specific needs. A compute-heavy service might use Go or Rust for performance, a data science service might use Python, a web-facing service might use Node.js, and each might use the most appropriate database (PostgreSQL, MongoDB, Redis, etc.). This is a key benefit of microservices: the contract between services is the API, not the implementation. The trade-off is operational complexityyour platform team must support multiple runtimes, build pipelines, and data stores. Organizations often allow choice within guardrails (e.g., 'choose from these three approved languages')."},{question:"What is 'topology-aware routing' in Kubernetes?",options:["Routing based on the shape of the network diagram","Preferring to route traffic to service instances in the same zone or region to reduce latency and cross-zone costs","Using geographic maps to visualize service locations","Routing based on the physical topology of server racks"],correctIndex:1,explanation:"Topology-aware routing (formerly known as topology hints or service topology) in Kubernetes prefers to route traffic to endpoints that are 'closer' in the network topologytypically in the same availability zone before crossing to other zones. Cross-zone traffic in cloud environments incurs both latency (a few extra milliseconds) and cost (cloud providers charge for cross-AZ data transfer). In microservices with high inter-service call volumes, these costs add up significantly. Kubernetes topology-aware routing annotates endpoints with zone information and the kube-proxy routes traffic accordingly. This optimization is transparent to services and can reduce both latency and cloud costs substantially for east-west traffic."},{question:"What is the 'strangler fig' approach to handling the frontend during monolith decomposition?",options:["Rewriting the entire frontend from scratch","Using a reverse proxy or API gateway to route requestsnew functionality goes to microservices, legacy functionality stays in the monolith until migrated","Keeping the frontend as a monolith forever","Converting the frontend to a CLI tool"],correctIndex:1,explanation:"In the Strangler Fig pattern, a reverse proxy or API gateway sits in front of both the monolith and new microservices, routing each request to the appropriate backend. New features are built as microservices, and existing features are migrated one by one. The frontend doesn't need to know about this splitit talks to the gateway, which handles routing transparently. Over time, more routes point to microservices and fewer to the monolith, until the monolith is completely replaced (or shrunk to a manageable size). This approach minimizes risk, allows incremental validation, and avoids the dangerous big-bang rewrite that has historically led to project failures at companies like Netscape."},{question:"What is the 'saga execution coordinator' (SEC)?",options:["The Securities and Exchange Commission's role in microservices","A component in orchestration-based sagas that manages the saga's state machine, deciding what step to execute next or what to compensate","A team lead who coordinates deployment schedules","A Kubernetes controller for managing persistent volumes"],correctIndex:1,explanation:"The Saga Execution Coordinator (SEC) is the central component in orchestration-based sagas that maintains the saga's state (which steps have completed, which are pending) and determines the next action. When a step succeeds, the SEC invokes the next step; when a step fails, it triggers compensating transactions for previously completed steps in reverse order. The SEC itself must be reliableit persists its state so it can recover from crashes. Tools like Temporal, Camunda, and AWS Step Functions serve as SEC implementations. The SEC's state machine defines the entire workflow, making it easy to visualize and modify. The trade-off is that the SEC becomes a critical component that must be highly available."},{question:"What is 'request collapsing' or 'request coalescing'?",options:["Compressing multiple fields into a single field","Merging multiple identical or similar concurrent requests into a single backend call and sharing the result among all callers","Collapsing a microservice back into the monolith","Combining request and response into a single message"],correctIndex:1,explanation:"Request collapsing (or coalescing) detects multiple concurrent requests for the same data and groups them into a single backend call, sharing the result among all waiting callers. For example, if 50 threads simultaneously request user profile #123, instead of making 50 database queries, a single query is made and the result is distributed to all 50 callers. Netflix Hystrix (now Resilience4j) popularized this pattern. This is particularly effective for microservices with high concurrency and overlapping requests. It reduces load on downstream services and databases, decreasing latency and resource usage. The trade-off is that all collapsed requests share the same result, including errors, and it adds a small windowing delay to batch requests."},{question:"What is 'GitOps' in the context of microservices deployment?",options:["Using GitHub for all service repositories","An operational model where Git is the single source of truth for infrastructure and deployment configuration, with automated reconciliation","A Git branching strategy for operations teams","Using Git hooks to trigger alerts"],correctIndex:1,explanation:"GitOps uses Git as the single source of truth for both application code and infrastructure/deployment configuration. A GitOps operator (like ArgoCD or Flux) continuously monitors the Git repository and automatically reconciles the cluster state to match the desired state declared in Git. When you want to deploy a new version, you update the Kubernetes manifest in Git (via pull request), and the operator detects the change and applies it to the cluster. This provides audit trails (Git history), rollback (revert a commit), consistency (cluster always matches Git), and security (no direct kubectl access needed). For microservices with dozens of services, GitOps brings order to deployment chaos."},{question:"What is 'service-level agreement (SLA)' vs 'service-level objective (SLO)'?",options:["They are the same thing","SLA is a contractual agreement with consequences for breaching; SLO is an internal target the team aims to meet","SLA is for internal services; SLO is for external customers","SLO is stricter than SLA"],correctIndex:1,explanation:"An SLA (Service-Level Agreement) is a formal contract between a service provider and its customers that specifies performance guarantees and consequences (usually financial penalties or credits) if those guarantees are not met. An SLO (Service-Level Objective) is an internal target set by the teamit's what the team actually aims for and is typically stricter than the SLA to provide a buffer. For example, the SLA might guarantee 99.9% uptime, but the team's SLO might be 99.95%. The gap between SLO and SLA is the team's safety margin. SLIs (Service-Level Indicators) are the actual measured metrics. In microservices, each service should have SLOs that account for dependenciesif Service A depends on Service B, A's SLO can't be higher than B's."},{question:"What is a 'Network Policy' in Kubernetes?",options:["A company policy about network usage","A Kubernetes resource that defines rules controlling which pods can communicate with each other, implementing microsegmentation","A DNS configuration for external domains","A bandwidth throttling mechanism"],correctIndex:1,explanation:"Network Policies in Kubernetes are resources that specify rules for pod-to-pod communication, essentially acting as a firewall at the pod level. By default, all pods can communicate with all other pods in a Kubernetes cluster. Network Policies restrict this by defining ingress (inbound) and egress (outbound) rules based on pod labels, namespace labels, or IP blocks. For example, you might restrict a database pod to only accept traffic from the application pods that need it. This implements microsegmentationa zero-trust principle where only explicitly allowed communication is permitted. Network Policies require a CNI plugin that supports them (Calico, Cilium, Weave Net). They're essential for securing microservices by limiting blast radius if a service is compromised."},{question:"What is the 'saga log' in a saga pattern?",options:["A log file recording all API calls made during development","A persistent record of a saga's progress that enables recovery if the saga coordinator crashes during execution","The logging output of the saga framework","A changelog of saga pattern modifications"],correctIndex:1,explanation:"The saga log is a durable record of a saga instance's execution statewhich steps have completed, which are in progress, and any compensation results. If the saga coordinator crashes mid-execution, it uses the saga log to determine where to resume when it restarts. Without a saga log, a crash could leave the system in an inconsistent state with some services having committed their transactions and others not. The saga log is typically stored in the coordinator's database or a dedicated event store. This is analogous to a database transaction log that enables recovery after crashes. Tools like Temporal persist workflow state automatically, effectively managing the saga log for you."},{question:"What is 'multi-tenancy' in microservices architecture?",options:["Running services in multiple data centers","A single service instance serving multiple customers (tenants) with logical data isolation between them","Having multiple development teams work on the same service","Running multiple versions of a service simultaneously"],correctIndex:1,explanation:"Multi-tenancy means a single deployment of a microservice serves multiple customers (tenants), with each tenant's data logically isolated from others. This is more resource-efficient than deploying separate instances per tenant (single-tenancy). Implementation approaches range from shared database with tenant ID columns (simplest but least isolated), separate schemas per tenant (moderate isolation), to separate databases per tenant (strongest isolation). For microservices, multi-tenancy adds complexity: every query must be scoped to the current tenant, cross-tenant data leaks are a critical security concern, and noisy neighbor effects (one tenant's heavy usage affecting others) must be managed through resource limits and rate limiting."},{question:"What is the 'backend for frontend' pattern's main benefit over a general-purpose API?",options:["It reduces the number of microservices needed","It tailors the API to each client's specific needs, avoiding over-fetching or under-fetching of data","It eliminates the need for an API gateway","It makes all clients use the same data format"],correctIndex:1,explanation:"The BFF pattern creates a dedicated API layer for each type of client (web, mobile, third-party), each optimized for that client's specific needs. A mobile app might need smaller payloads with fewer fields, while a web dashboard might need aggregated data from multiple services. A general-purpose API serving all clients becomes a compromise that serves none optimallyit either includes too much data (mobile wastes bandwidth) or too little (web needs multiple calls). The BFF handles aggregation, transformation, and filtering specific to its client. It can also implement client-specific authentication flows. The trade-off is maintaining multiple BFF services, but the improved client experience and independent evolution usually justify the cost."},{question:"What is 'distributed saga' vs 'two-phase commit' for cross-service transactions?",options:["They are the same concept with different names","Sagas use compensating transactions and local ACID; 2PC uses a global coordinator that locks resources across all participants until all vote to commit","2PC is simpler and always preferred over sagas","Sagas require a specialized database; 2PC works with any database"],correctIndex:1,explanation:"Two-phase commit (2PC) uses a transaction coordinator that first asks all participants to prepare (vote to commit), then either commits or aborts all participants atomically. This provides strong consistency but blocks resources during the vote phase, reducing availability and throughput. Sagas break the transaction into local steps with compensating actions, each committing independently. Sagas provide better availability and performance but only guarantee eventual consistency. In microservices, 2PC is generally avoided because it creates tight coupling, blocks resources, and becomes a bottleneck. Sagas are the standard approach despite their complexity. Some databases support 2PC for sharded writes, but across independently owned microservices, sagas are the pragmatic choice."},{question:"What is the purpose of 'pod disruption budgets' (PDB) in Kubernetes?",options:["Setting financial budgets for pod resource usage","Ensuring a minimum number of pods remain available during voluntary disruptions like upgrades and node draining","Limiting the number of pods a team can create","Tracking pod creation and deletion costs"],correctIndex:1,explanation:"Pod Disruption Budgets (PDBs) define the minimum number of pods (or maximum unavailable) that must remain available during voluntary disruptions like node drains, cluster upgrades, or maintenance. For example, a PDB might specify 'at least 2 out of 3 replicas must be available at all times.' Without a PDB, a node drain could simultaneously terminate all pods of a service, causing downtime. With a PDB, Kubernetes respects the budget and waits for new pods to be ready before terminating additional ones. PDBs don't protect against involuntary disruptions (hardware failures), only voluntary ones. They're essential for microservices running on clusters with regular maintenance activities."},{question:"What is 'event-carried state transfer' in microservices?",options:["Transferring state via HTTP GET requests","Events that carry enough data for consumers to update their local state without needing to call back to the source service","Moving event processing logic between services","Using events to trigger state machine transitions"],correctIndex:1,explanation:"Event-carried state transfer is a pattern where events contain sufficient data for consumers to maintain a local replica of the source data, eliminating the need for synchronous callbacks. For example, when a Customer Service publishes a 'CustomerUpdated' event, it includes the full customer data (name, address, email), so consuming services can update their local copies without querying the Customer Service. This reduces coupling and improves resilienceconsumers don't need the source service to be available for reads. The trade-off is larger event payloads and data duplication across services. Martin Fowler described this pattern as one of four event-driven patterns. It's the foundation of data replication in event-driven microservices."},{question:"What is 'contract-first design' for microservice APIs?",options:["Writing the legal contract between teams before coding","Designing and agreeing on the API specification (e.g., OpenAPI/Swagger) before implementing the service","Implementing the service first and generating the contract from the code","Having the consumer write the provider's code"],correctIndex:1,explanation:"Contract-first design means defining the API specification (using OpenAPI/Swagger, protobuf, or AsyncAPI for events) before writing any implementation code. This allows consumer and provider teams to agree on the interface upfront, work in parallel (consumers mock the API while the provider implements it), and catch design issues early. It's the opposite of code-first, where the API spec is generated from the implementation. For microservices, contract-first is particularly valuable because it forces explicit boundary design, enables parallel development, and produces high-quality API documentation automatically. The contract becomes a shared artifact that both teams reference, reducing miscommunication and integration surprises."},{question:"What is 'Kubernetes Operator' pattern?",options:["A human operator who manages Kubernetes clusters","A custom controller that extends Kubernetes to automate the management of complex applications using domain-specific knowledge","The default controller manager in Kubernetes","A command-line tool for operating Kubernetes clusters"],correctIndex:1,explanation:"A Kubernetes Operator is a custom controller that encodes operational knowledge (human operator expertise) into software. It uses Custom Resource Definitions (CRDs) to extend the Kubernetes API with domain-specific resources and a controller that watches these resources and takes action. For example, a PostgreSQL Operator can automatically handle database provisioning, backups, failover, and scalingtasks that would otherwise require a DBA. Operators follow the Kubernetes control loop pattern: observe current state, compare to desired state, take action. For microservices, operators manage complex stateful infrastructure (databases, message brokers, monitoring systems) declaratively. The Operator Framework and Kubebuilder are popular tools for building operators."},{question:"What is 'data mesh' and how does it relate to microservices?",options:["A mesh network for database replication","A decentralized approach to data architecture where domain teams own and serve their data as products, analogous to microservices principles applied to data","A service mesh specifically for database traffic","A grid computing framework for data processing"],correctIndex:1,explanation:"Data mesh, proposed by Zhamak Dehghani, applies microservices principles (domain ownership, decentralization, self-serve platform) to analytical data. Instead of centralizing all data in a single data warehouse/lake owned by a central team, domain teams own their analytical data as 'data products' with defined contracts, SLOs, and discoverability. This addresses the bottleneck of central data teams that can't keep up with demand. The four principles are: domain-oriented data ownership, data as a product, self-serve data infrastructure platform, and federated computational governance. For microservices teams, data mesh means they're responsible not only for their operational data but also for serving their domain's analytical data to the organization."},{question:"What is 'traffic mirroring' (shadowing) in microservices?",options:["Duplicating traffic to a backup data center for disaster recovery","Sending a copy of production traffic to a new version of a service for testing without affecting the live response","Encrypting traffic to hide it from monitoring","Reflecting DDoS attack traffic back to the attacker"],correctIndex:1,explanation:"Traffic mirroring (or shadowing) copies production traffic to a new version of a service running alongside the current version. The mirrored service processes real requests but its responses are discardedonly the current version's responses are sent to clients. This allows testing a new version with real production traffic patterns and data, catching issues that synthetic tests might miss, without any risk to users. Istio's VirtualService supports traffic mirroring natively. It's particularly valuable for microservices because it validates behavior under realistic load conditions. The trade-off is resource cost (you're running two versions) and potential side effects (the mirrored version might write to databases or call other services, so safeguards are needed)."},{question:"What is the difference between 'push' and 'pull' models for service communication?",options:["Push uses TCP; pull uses UDP","In push, the producer sends data to consumers when available; in pull, consumers request data from the producer when they need it","Push is faster than pull in all scenarios","Push works within a cluster; pull works across clusters"],correctIndex:1,explanation:"In the push model, the producer actively sends data to consumers as soon as it's available (e.g., webhooks, pub/sub messaging, server-sent events). In the pull model, consumers request data when they need it (e.g., HTTP polling, Kafka consumer pulling from partitions). Push provides lower latency (consumers get data immediately) but can overwhelm slow consumers. Pull lets consumers control their consumption rate but may introduce latency. Kafka uses a pull model where consumers pull messages at their own pace, which provides excellent backpressure handling. Many systems combine both: Kafka pushes to consumer groups but consumers pull from partitions. The choice depends on latency requirements, consumer processing speed, and scalability needs."},{question:"What is the 'strangler fig' pattern's relationship with feature flags?",options:["They are unrelated concepts","Feature flags can control which implementation (monolith or microservice) handles specific requests, enabling gradual migration with instant rollback","Feature flags replace the strangler fig pattern entirely","The strangler fig pattern is a type of feature flag"],correctIndex:1,explanation:"Feature flags and the Strangler Fig pattern are complementary. During monolith decomposition, feature flags can control whether a specific request is routed to the legacy monolith or the new microservice implementation. This provides fine-grained control: you can enable the new service for 10% of users, specific user segments, or specific tenants. If the new service has issues, you instantly toggle the flag to route all traffic back to the monolith. This is much safer than hard-cutting traffic at the proxy level. Combined with canary analysis (comparing error rates and latency between old and new), feature flags turn the strangler fig migration into a measured, data-driven process with minimal risk."},{question:"What is 'domain-driven design strategic patterns' and how do they guide microservice decomposition?",options:["Patterns for choosing domain name registrars","High-level DDD patterns (bounded contexts, context maps, shared kernels, anti-corruption layers) that help identify service boundaries and inter-service relationships","Strategies for database schema design within a service","Game theory patterns for team coordination"],correctIndex:1,explanation:"DDD strategic patterns operate at the system level, guiding how bounded contexts (and thus microservices) relate to each other. Context Maps visualize these relationships using patterns like: Shared Kernel (two contexts share a subset of the domain model), Customer-Supplier (one context's output feeds another), Conformist (downstream accepts upstream's model as-is), Anti-Corruption Layer (downstream translates upstream's model), and Published Language (shared communication format). These patterns directly map to microservice integration strategiesan ACL becomes an adapter service, a shared kernel becomes a shared library, and customer-supplier relationships define API ownership. Understanding these patterns prevents ad-hoc service boundaries and helps teams design sustainable inter-service relationships."},{question:"What is the 'resource limits' concept in Kubernetes and why is it critical for microservices?",options:["Limiting the number of Kubernetes resources (pods, services) in the cluster","Setting CPU and memory bounds on containers to prevent a single service from consuming all node resources and affecting other services","Limiting how many API resources a service can create","Restricting which resources a service can access in the cloud"],correctIndex:1,explanation:"Kubernetes resource limits set maximum CPU and memory a container can use. Resource requests specify the guaranteed minimum resources for scheduling, while limits cap the maximum. Without limits, a memory-leaking service could consume all node memory, triggering the OOM killer and crashing other services on the same node. Similarly, a CPU-intensive service could starve its neighbors. For microservices sharing cluster resources, proper limits are essential: they ensure fair resource sharing, enable accurate capacity planning, and prevent noisy-neighbor problems. Setting requests too high wastes resources (pods can't be efficiently packed), while setting them too low causes throttling. Getting this right requires monitoring actual usage and iterating."},{question:"What is 'event notification' pattern vs 'event-carried state transfer'?",options:["They are identical patterns","Event notification contains minimal data (just the event type and ID, requiring callback for details); event-carried state transfer includes the full data in the event","Event notification is for errors; event-carried state transfer is for success","Event notification is synchronous; event-carried state transfer is asynchronous"],correctIndex:1,explanation:"Event notification publishes lightweight events (e.g., 'OrderCreated: orderId=123') that tell consumers something happened but don't include the full data. Consumers must call back to the source service's API to get details. Event-carried state transfer includes the full data (e.g., 'OrderCreated: {orderId: 123, items: [...], total: 99.99}') so consumers can update their local state without callbacks. Notification is simpler (smaller events, single source of truth for data) but creates runtime coupling (consumer depends on producer being available for queries). State transfer is more decoupled but creates larger events and data duplication. Martin Fowler describes these as two of four event-driven interaction patterns, each appropriate for different coupling and consistency requirements."},{question:"What is the 'ambassador' vs 'sidecar' vs 'adapter' container pattern in Kubernetes?",options:["They are three names for the same pattern","Ambassador handles outbound traffic proxying, sidecar adds functionality (logging, monitoring), adapter standardizes output formatall are multi-container pod patterns","Ambassador is for HTTP, sidecar for gRPC, adapter for WebSocket","They differ only in the container image used"],correctIndex:1,explanation:"These are three distinct multi-container pod patterns. The Sidecar extends the main container with additional functionality (log shipping, configuration reloading, service mesh proxy) without modifying the application. The Ambassador proxies outbound network connections, handling complexity like connection pooling, routing, and TLS termination for the main container. The Adapter standardizes the main container's outputfor example, converting application-specific metrics into a Prometheus-compatible format or transforming log formats. All three patterns deploy helper containers alongside the main container in the same pod, sharing network and storage. In practice, the Envoy sidecar in service meshes performs all three roles: proxying (ambassador), adding observability (sidecar), and standardizing metrics (adapter)."},{question:"What is 'distributed transaction' and why is it problematic in microservices?",options:["A transaction that takes a long time to complete","A transaction spanning multiple services/databases that's problematic because it requires all participants to be available and locks resources, reducing system availability and scalability","A transaction distributed across multiple time zones","A transaction that is processed by multiple CPUs"],correctIndex:1,explanation:"A distributed transaction spans multiple services or databases and aims to maintain ACID properties across all of them. The most common protocol is two-phase commit (2PC), where a coordinator first asks all participants to prepare, then instructs them to commit or abort. This is problematic in microservices because: it requires all services to be available simultaneously (reducing availability), it locks resources during the prepare phase (reducing throughput), it couples services at the database level, and it becomes a single point of failure if the coordinator crashes. The CAP theorem shows that strong consistency and high availability can't coexist during partitions. That's why microservices use sagas (eventual consistency with compensations) instead of distributed transactions."},{question:"What is 'shard-nothing architecture' in microservices databases?",options:["A database that never shards its data","An architecture where each database shard operates independently without sharing memory, disk, or CPU with other shards","Sharing database shards between multiple services","An architecture where sharding is handled by the application code"],correctIndex:1,explanation:"Shared-nothing architecture means each database node (shard) has its own dedicated CPU, memory, and storage, with no shared resources between nodes. This allows linear scalabilityadding more shards adds more capacity without contention. Each microservice's database can be independently sharded based on its access patterns (e.g., sharding orders by customer ID, products by category). This contrasts with shared-disk architectures (like Oracle RAC) where multiple nodes share storage. Shared-nothing is the foundation of distributed databases like Cassandra, CockroachDB, and Citus. For microservices, this aligns with the independent scaling principle: each service can scale its data tier based on its own growth, without affecting other services."},{question:"What is the role of 'Helm' in Kubernetes microservices deployment?",options:["A monitoring tool for Kubernetes","A package manager for Kubernetes that uses charts (templated manifests) to define, install, and manage application deployments","A network plugin for inter-pod communication","A security scanner for container images"],correctIndex:1,explanation:"Helm is the Kubernetes package manager that uses 'charts' (collections of templated YAML manifests) to define complete application deployments. A chart might include a Deployment, Service, ConfigMap, and Ingress, all parameterized with a values.yaml file for customization across environments. For microservices, Helm simplifies deploying complex applications with many interconnected resources: instead of managing dozens of individual YAML files, you install a chart with environment-specific values. Helm supports versioning, rollbacks, and dependency management between charts. Popular alternatives include Kustomize (overlay-based, no templating) and Jsonnet. Many organizations create a base Helm chart template that all microservices extend, ensuring consistent deployment standards."},{question:"What is 'cell-based architecture' in microservices?",options:["Organizing microservices by cellular network standards","Grouping related services into isolated cells (units of deployment and failure isolation) that can be independently deployed and scaled","Running microservices on cell phone hardware","A biology-inspired pattern where services divide like cells"],correctIndex:1,explanation:"Cell-based architecture groups related microservices into isolated cells, where each cell is a complete, independently deployable unit with its own data store, compute, and routing. Traffic is routed to a specific cell based on a partition key (like customer ID or region). If a cell fails, only its users are affectedother cells continue operating. This limits the blast radius of failures: instead of a service-wide outage affecting all users, only users in the failed cell are impacted. AWS and other hyperscalers use cell-based architectures extensively. It combines microservices principles with infrastructure isolation, providing stronger fault isolation than services alone. The trade-off is increased operational complexity and resource overhead from running multiple independent cells."}],yf=[{question:"What does CQRS stand for?",options:["Command Query Responsibility Segregation","Command Queue Replication Service","Consistent Query Read Separation","Command Query Resource Sharing"],correctIndex:0,explanation:"CQRS stands for Command Query Responsibility Segregation. It is a pattern that separates the read (query) and write (command) operations into different models. This separation allows each side to be optimized independently  for example, the read model can be denormalized for fast queries while the write model maintains strict consistency. Greg Young popularized this pattern, building on Bertrand Meyer's Command Query Separation (CQS) principle."},{question:"In CQRS, what is the primary purpose of the 'command' side?",options:["To serve read-heavy dashboards","To handle state mutations and enforce business rules","To replicate data across regions","To generate analytics reports"],correctIndex:1,explanation:"The command side in CQRS is responsible for handling all write operations  creating, updating, and deleting data. It enforces business rules, validates invariants, and ensures that the domain logic is correctly applied before persisting state changes. By isolating writes, the command side can use a normalized, consistency-focused data model without worrying about read performance. This separation means you can scale and optimize the write path independently from reads."},{question:"What is a materialized view in the context of CQRS?",options:["A database index on the write model","A pre-computed, denormalized representation of data optimized for queries","A temporary cache that expires after TTL","A SQL view that runs on every query"],correctIndex:1,explanation:"A materialized view is a pre-computed, stored result set that represents data in a query-friendly format. Unlike a regular SQL view that recalculates on every access, a materialized view persists the result and updates it when the underlying data changes. In CQRS, the read model often consists of materialized views built from events or change notifications from the write side. This allows the read side to serve queries with minimal joins and maximum throughput, at the cost of slight staleness."},{question:"How does event sourcing complement CQRS?",options:["It replaces the need for a read model entirely","It stores every state change as an immutable event, providing a complete audit trail that feeds projections","It eliminates eventual consistency","It removes the need for a message broker"],correctIndex:1,explanation:"Event sourcing stores every change to application state as an immutable event in an append-only log, rather than storing just the current state. When combined with CQRS, events from the write side are projected into read-optimized views. This provides a complete audit trail, enables temporal queries ('what was the state at time T?'), and allows rebuilding read models from scratch by replaying events. The combination is powerful but adds complexity  you must handle event versioning, schema evolution, and eventual consistency between the write and read sides."},{question:"What is a projection in event sourcing + CQRS?",options:["A database backup strategy","A function that transforms a stream of events into a read-optimized data structure","A load balancer configuration","A write-ahead log entry"],correctIndex:1,explanation:"A projection is a function or process that consumes events from the event store and builds a read-optimized representation (the read model). For example, an OrderSummaryProjection might listen to OrderPlaced, ItemAdded, and OrderShipped events to maintain a denormalized orders table. Projections can be rebuilt from scratch by replaying all events, making it easy to create new read models or fix bugs in existing ones. They are the bridge between the event-sourced write side and the query-optimized read side in a CQRS architecture."},{question:"What consistency model typically exists between the write and read sides in CQRS?",options:["Strong consistency","Eventual consistency","Linearizable consistency","No consistency guarantee at all"],correctIndex:1,explanation:"In most CQRS implementations, the read model is updated asynchronously from the write model, resulting in eventual consistency. After a command is processed and an event is published, there is a propagation delay before the read model reflects the change. This means a user who writes data might not immediately see it in a query. Designers must account for this with techniques like read-your-own-writes consistency, optimistic UI updates, or polling. The trade-off is that eventual consistency enables independent scaling and optimization of each side."},{question:"Why might you choose separate databases for the read and write models?",options:["To save on licensing costs","To independently optimize storage engines  e.g., relational for writes, denormalized/NoSQL for reads","Because CQRS requires different databases by definition","To avoid needing an ORM"],correctIndex:1,explanation:"Using separate databases allows you to pick the best storage technology for each workload. The write side might use a relational database with strong transactional guarantees, while the read side might use Elasticsearch for full-text search, Redis for low-latency lookups, or a document store for denormalized views. This polyglot persistence approach maximizes performance for each access pattern. Note that CQRS does not require separate databases  you can use different tables in the same database  but separate stores unlock the most flexibility."},{question:"What is the 'read-your-own-writes' problem in CQRS?",options:["The write model cannot read its own state","A user writes data but the read model hasn't caught up yet, so they don't see their change","The read model overwrites the write model","Events are delivered out of order"],correctIndex:1,explanation:"Read-your-own-writes is a common UX challenge in CQRS. Because the read model updates asynchronously, a user who just submitted a command may query the read side and not see their change reflected yet. This can be confusing and feel like a bug. Solutions include: routing the user's next read to the write database temporarily, including a version token in the response so the UI waits for the read model to catch up, or using optimistic UI updates on the client side. This is one of the key trade-offs of the pattern."},{question:"In CQRS, what is a command handler responsible for?",options:["Rendering the UI","Receiving a command, validating it, and executing the corresponding domain logic","Querying the read model","Managing database migrations"],correctIndex:1,explanation:"A command handler receives an incoming command (e.g., PlaceOrderCommand), validates its data and business rules, and orchestrates the domain logic to process it. It typically loads the relevant aggregate, invokes methods on it, and persists the resulting state changes or events. Command handlers should be idempotent where possible and should not return query data  they return success/failure or the ID of the created resource at most. This strict separation ensures the write path stays focused on correctness and consistency."},{question:"What is the key difference between CQS and CQRS?",options:["CQS is about method-level separation; CQRS extends this to architectural separation of read and write models","They are the same thing","CQS is for distributed systems; CQRS is for monoliths","CQRS does not allow queries"],correctIndex:0,explanation:"CQS (Command Query Separation) is a principle coined by Bertrand Meyer stating that a method should either change state (command) or return data (query), but not both. CQRS takes this principle to the architectural level by using separate models, potentially separate databases, and separate services for reads and writes. While CQS is a code-level design guideline, CQRS is a system-level architectural pattern that enables independent scaling, optimization, and evolution of the read and write paths."},{question:"What is an aggregate in the context of CQRS and Domain-Driven Design?",options:["A SQL aggregation function like SUM or COUNT","A cluster of domain objects treated as a single unit for data changes with a root entity enforcing invariants","A read model cache","A message queue topic"],correctIndex:1,explanation:"An aggregate is a DDD concept that groups related entities and value objects into a consistency boundary. The aggregate root is the only entry point for modifications, and it enforces all business invariants within the boundary. In CQRS, commands target specific aggregates, and the aggregate decides whether to accept or reject the command based on its current state. When using event sourcing, the aggregate's state is rebuilt by replaying its events, and new events are appended after successful command processing."},{question:"How can you rebuild a read model in an event-sourced CQRS system?",options:["Restore from the latest database backup","Replay all events from the event store through the projection logic","Copy the write database tables","You cannot rebuild read models once created"],correctIndex:1,explanation:"One of the major benefits of event sourcing is that the event store serves as the single source of truth. To rebuild a read model, you simply replay all relevant events from the beginning (or a snapshot) through the projection logic, writing the results to a new read store. This is invaluable for fixing projection bugs, creating new read models for new features, or migrating to a different storage technology. The process can be done in parallel with the live system, switching over once the new model catches up to the current event stream."},{question:"What is the purpose of a process manager (saga) in CQRS?",options:["To manage database connections","To coordinate long-running business processes that span multiple aggregates or services","To compress event logs","To handle user authentication"],correctIndex:1,explanation:"A process manager (sometimes called a saga) listens to events and coordinates multi-step business workflows that involve multiple aggregates or services. For example, an OrderFulfillmentSaga might listen for OrderPlaced, then send ReserveInventory and ChargePayment commands, handling success/failure of each step. It maintains its own state to track progress and can implement compensating actions if a step fails. In CQRS systems, sagas are essential for managing distributed transactions without using two-phase commit."},{question:"What problem does snapshotting solve in event sourcing?",options:["It reduces the size of the event store","It speeds up aggregate loading by storing periodic state snapshots so you don't replay all events from the beginning","It prevents event duplication","It enforces schema validation on events"],correctIndex:1,explanation:"As an aggregate accumulates events over time, replaying all of them to rebuild state becomes increasingly slow. Snapshotting solves this by periodically saving the aggregate's current state as a snapshot. When loading the aggregate, the system loads the latest snapshot and only replays events that occurred after it. This dramatically reduces load time for long-lived aggregates. Snapshots are an optimization, not a replacement for events  the event log remains the authoritative source of truth and snapshots can be regenerated at any time."},{question:"Which of the following is NOT a typical benefit of CQRS?",options:["Independent scaling of read and write workloads","Simplified codebase with less overall complexity","Optimized data models for each access pattern","Better separation of concerns between reads and writes"],correctIndex:1,explanation:"CQRS actually increases the overall complexity of a system  you now have two models to maintain, synchronization mechanisms, eventual consistency to manage, and more moving parts. The benefits are independent scaling (you can add read replicas without affecting writes), optimized data models (denormalized reads, normalized writes), and cleaner separation of concerns. CQRS should only be applied to bounded contexts where the read and write patterns are significantly different enough to justify the added complexity. It is not a default architecture for simple CRUD applications."},{question:"In CQRS, what is the role of a domain event?",options:["A request to change state","A record of something that has already happened in the domain","A query for current state","A database transaction log entry"],correctIndex:1,explanation:"A domain event is an immutable record that describes something that has already occurred in the system  for example, OrderPlaced, PaymentProcessed, or ItemShipped. Unlike commands (which are requests that may be rejected), events are facts. In CQRS with event sourcing, events are persisted in the event store and used to update read models via projections. They also serve as integration events to notify other bounded contexts or services about changes, enabling loose coupling in a distributed system."},{question:"What is the difference between a thin event and a fat event?",options:["Thin events are faster to process","Thin events contain only IDs/references requiring lookups; fat events carry all relevant data inline","Fat events are compressed; thin events are not","There is no meaningful difference"],correctIndex:1,explanation:"A thin event contains minimal data  typically just identifiers and the type of change (e.g., OrderPlaced { orderId: 123 }). Consumers must query back to get full details. A fat event includes all the relevant data needed by consumers (e.g., OrderPlaced { orderId: 123, items: [...], total: 99.99, customer: {...} }). Fat events reduce coupling because consumers don't need to call back to the source, but they increase event size and can leak domain details. The choice depends on your consistency requirements and how tightly coupled your services can afford to be."},{question:"How should you handle event schema evolution in an event-sourced system?",options:["Delete old events and replace them with new schema events","Use event upcasting/versioning to transform old events to the current schema when reading","Ignore schema changes since events are immutable","Stop the system and migrate all events at once"],correctIndex:1,explanation:"Since events are immutable and stored forever, you cannot simply modify old events when the schema changes. Event upcasting is the standard approach: when reading events, a transformation layer converts old event versions to the current schema. For example, if OrderPlaced v1 had a 'price' field and v2 split it into 'subtotal' and 'tax', an upcaster converts v1 events on the fly. You can also use event versioning (OrderPlacedV1, OrderPlacedV2) with explicit migration logic. This allows the system to evolve without losing historical data."},{question:"What is a denormalizer in a CQRS read model?",options:["A tool that normalizes database schemas","A component that processes events and writes denormalized data into the read store","A query optimizer in the write model","A data compression algorithm"],correctIndex:1,explanation:"A denormalizer (also called a projector) subscribes to events from the write side and transforms them into denormalized structures stored in the read database. For instance, when an OrderPlaced event occurs, the denormalizer might update a flat 'order_summaries' table that includes customer name, item count, and total  all pre-joined for fast querying. This eliminates expensive joins at query time. Each read model can have its own denormalizer, allowing multiple specialized views of the same data optimized for different query patterns."},{question:"When is CQRS overkill?",options:["When your system has complex business logic","When your read and write patterns are simple, similar, and a basic CRUD approach suffices","When you need to scale reads independently","When you have multiple bounded contexts"],correctIndex:1,explanation:"CQRS adds significant complexity  separate models, eventual consistency, synchronization infrastructure, and more code to maintain. If your application is a straightforward CRUD system where reads and writes use similar data shapes and volumes, CQRS provides little benefit while adding substantial overhead. It shines when read and write workloads are vastly different (e.g., 1000:1 read-to-write ratio), when you need different storage technologies for each, or when the domain logic on the write side is complex enough to warrant isolation. Always start simple and introduce CQRS only where justified."},{question:"What is the 'write model' in CQRS typically optimized for?",options:["Fast full-text search","Transactional consistency, validation, and enforcing business invariants","Low-latency read queries","Data warehousing and analytics"],correctIndex:1,explanation:"The write model in CQRS focuses on correctness over performance. It uses normalized data structures that enforce referential integrity, business rules, and domain invariants. Transactions on the write side ensure that state changes are atomic and consistent. This is in contrast to the read model, which sacrifices normalization for query speed. By separating concerns, the write model doesn't need to compromise its consistency guarantees to support read patterns, and vice versa."},{question:"How do you handle idempotency in CQRS command processing?",options:["You don't  commands are always processed exactly once","By assigning unique command IDs and tracking which have been processed to prevent duplicate execution","By using optimistic locking on the read model","By batching commands together"],correctIndex:1,explanation:"In distributed systems, messages can be delivered more than once due to retries, network issues, or broker redelivery. To ensure a command isn't processed twice, each command is assigned a unique ID (often a UUID). The command handler checks if this ID has already been processed (e.g., in a deduplication table) before executing. If it was already handled, the handler returns the previous result without re-executing. This is critical in CQRS because duplicate command processing would emit duplicate events, corrupting the read model and potentially causing incorrect business outcomes."},{question:"What is a catch-up subscription in event sourcing?",options:["A subscription that only receives future events","A subscription that starts reading from a specific position in the event stream and processes all events from that point forward","A mechanism to delete old events","A real-time WebSocket connection to the event store"],correctIndex:1,explanation:"A catch-up subscription reads events from a specific position (or from the beginning) in the event store and processes them sequentially until it reaches the current end, then continues processing new events as they arrive. This is how projections rebuild or stay up-to-date: the projection tracks its last processed position, and on startup it resumes from there. If you're building a new read model, you start a catch-up subscription from position 0 to process the entire event history. This mechanism is fundamental to how EventStoreDB and similar systems support CQRS projections."},{question:"What is the 'task-based UI' pattern often associated with CQRS?",options:["A UI that shows database tables directly","A UI designed around user intentions/actions (commands) rather than CRUD forms editing raw data","A UI that only displays read model data","A drag-and-drop task management board"],correctIndex:1,explanation:"Task-based UIs capture user intent rather than raw data edits. Instead of a generic 'Edit Order' form that saves all fields at once, you'd have specific actions like 'Change Shipping Address', 'Add Item', or 'Apply Discount'. Each action maps directly to a command in the CQRS system, preserving the business intent. This is more expressive for the domain model because the command carries semantic meaning, not just a diff of changed fields. It also enables better validation, auditing, and event generation since the system knows exactly what the user intended."},{question:"How does CQRS enable polyglot persistence?",options:["By requiring all data be stored in JSON format","By allowing the read and write sides to use entirely different database technologies suited to their access patterns","By using a single database with multiple schemas","By compressing data differently on each side"],correctIndex:1,explanation:"Since CQRS separates reads and writes into distinct models, each can use its own database technology. The write side might use PostgreSQL for strong ACID transactions, while the read side uses Elasticsearch for full-text search, Redis for real-time dashboards, or Cassandra for high-throughput time-series queries. The synchronization happens through events  the write side emits events, and each read model's projection consumes them and writes to its respective store. This allows you to pick the perfect tool for each job rather than compromising with a single database."},{question:"What is an event store?",options:["A regular relational database used for CRUD operations","An append-only database optimized for storing and retrieving ordered sequences of immutable events","A caching layer for frequently accessed events","A message broker like Kafka"],correctIndex:1,explanation:"An event store is a specialized database designed to persist events in an append-only fashion. Events are stored in streams (typically per aggregate), ordered by sequence number, and are immutable once written. The event store supports reading events for a specific stream (to rebuild aggregate state) and subscribing to events across streams (for projections). Examples include EventStoreDB, Axon Server, and Marten (for PostgreSQL). While Kafka can serve as an event store, purpose-built event stores offer features like optimistic concurrency, stream-level subscriptions, and built-in projections."},{question:"What concurrency control mechanism is commonly used when writing events to an event store?",options:["Pessimistic locking on the entire database","Optimistic concurrency using expected stream version numbers","Two-phase commit across all streams","No concurrency control is needed"],correctIndex:1,explanation:"Optimistic concurrency is the standard approach: when appending events to a stream, you specify the expected version (the version you read when loading the aggregate). If another command has appended events since you read, the version won't match and the write fails with a concurrency conflict. The command handler can then retry by reloading the aggregate with the new events and re-executing the command. This is lightweight and scalable compared to pessimistic locking, and it naturally fits the append-only nature of event stores."},{question:"What is the 'projection lag' problem?",options:["Events being stored too slowly","The delay between an event being written and the corresponding read model being updated","The write model falling behind the read model","A network latency issue between services"],correctIndex:1,explanation:"Projection lag is the time delay between when an event is persisted in the event store and when the read model reflects that change. During this window, queries to the read model return stale data. The lag can vary from milliseconds to seconds depending on the projection infrastructure, event volume, and processing complexity. High lag can lead to poor user experience (users don't see their changes) and inconsistency in downstream processes. Monitoring projection lag is critical in production CQRS systems, and strategies like read-your-own-writes or version-aware queries can mitigate its impact."},{question:"What is a command in CQRS?",options:["A SQL statement","An intent to change the system state, expressed as an imperative-named message","A query that returns data","An event that has already occurred"],correctIndex:1,explanation:"A command is a message that represents the intent to perform an action  it's a request, not a fact. Commands are named imperatively (PlaceOrder, CancelReservation, UpdateProfile) and carry the data needed to execute the action. Unlike events (past tense, something that happened), commands can be rejected if validation fails or business rules are violated. In CQRS, commands flow through a command bus or handler pipeline that routes them to the appropriate handler for processing."},{question:"Can a single CQRS read model serve multiple query use cases?",options:["No, each query must have its own read model","Yes, but it's often better to create specialized read models per use case for optimal performance","Read models cannot serve queries directly","Only if using a relational database"],correctIndex:1,explanation:"While a single read model can technically serve multiple query patterns, the power of CQRS lies in creating purpose-built read models tailored to specific use cases. A product listing page might need a lightweight summary model, while a product detail page needs a rich model with reviews and recommendations. Creating separate read models for each eliminates compromises  each model contains exactly the data its consumers need, in the exact shape they need it. The trade-off is maintaining multiple projections and storage, but the performance and simplicity gains at query time are often worth it."},{question:"What happens to the read model if the projection logic has a bug?",options:["The write model is also corrupted","You fix the projection code and rebuild the read model by replaying events from the event store","The read model must be manually corrected row by row","The system must be rebuilt from scratch"],correctIndex:1,explanation:"This is one of the greatest advantages of combining CQRS with event sourcing. Since the event store is the source of truth and events are immutable, a buggy projection doesn't cause permanent damage. You fix the projection code, drop the corrupted read model, and replay all events through the corrected projection to rebuild it accurately. This capability makes the system remarkably resilient to bugs  you can always get back to a correct state. Without event sourcing, a corrupted read model might require complex data migration or manual fixes."},{question:"What is the difference between an integration event and a domain event?",options:["They are identical concepts","Domain events are internal to a bounded context; integration events cross context boundaries and are part of the public contract","Integration events are faster than domain events","Domain events are stored; integration events are not"],correctIndex:1,explanation:"Domain events represent things that happened within a specific bounded context and may contain internal implementation details. Integration events are designed to communicate across bounded contexts or services  they are part of the public API contract and should be stable, well-documented, and backward-compatible. In CQRS, domain events drive internal projections and process managers, while integration events are published to a message broker for external consumers. Keeping them separate prevents internal domain changes from breaking external consumers."},{question:"How does CQRS relate to the CAP theorem?",options:["CQRS violates the CAP theorem","CQRS typically trades strong consistency for availability and partition tolerance on the read side","CQRS guarantees all three CAP properties","CQRS is unrelated to the CAP theorem"],correctIndex:1,explanation:"The CAP theorem states that in a distributed system, you can only guarantee two of: Consistency, Availability, and Partition Tolerance. CQRS systems typically favor availability and partition tolerance (AP) on the read side by accepting eventual consistency. The write side can still maintain strong consistency within its boundary. This means reads may return slightly stale data during network partitions or replication lag, but the system remains available. This trade-off aligns well with many real-world applications where reads vastly outnumber writes and brief staleness is acceptable."},{question:"What is a command bus in CQRS?",options:["A physical network cable","A message routing infrastructure that dispatches commands to their appropriate handlers","A database connection pool","A load balancer for read queries"],correctIndex:1,explanation:"A command bus is a mediator pattern implementation that receives commands and routes them to the correct command handler. It provides a single entry point for all write operations, enabling cross-cutting concerns like logging, validation, authorization, and retry logic to be applied uniformly via middleware/decorators. The command bus decouples the sender (e.g., API controller) from the handler, making the system more modular and testable. Libraries like MediatR (.NET), Axon Framework (Java), and similar exist in most language ecosystems."},{question:"What is the 'two-phase projection' strategy?",options:["Running projections on two different servers","Building the new read model in the background while the old one serves queries, then switching over atomically","Projecting events twice for redundancy","Using two databases for the same projection"],correctIndex:1,explanation:"Two-phase projection (also called blue-green projection) is a strategy for safely rebuilding or migrating read models without downtime. In phase one, you build the new projection in the background, processing historical events until it catches up. In phase two, you atomically switch the query endpoint from the old read model to the new one. This ensures users always get consistent query results and never hit a half-built read model. It's especially important in production systems where read model rebuilds can take hours for large event stores."},{question:"What is an eventual consistency window?",options:["A UI component showing data freshness","The time period between a write being committed and the read model reflecting that write","A database configuration parameter","The maximum number of allowed stale reads"],correctIndex:1,explanation:"The eventual consistency window is the time lag between when data is written on the command side and when it becomes visible in the read model. During this window, queries return stale data. The window's size depends on factors like event bus latency, projection processing speed, and system load. In well-tuned systems, this window is typically milliseconds to low seconds. Understanding and communicating this window is crucial for setting user expectations and designing UIs that handle transient staleness gracefully."},{question:"Which messaging pattern is commonly used to propagate events from the write side to the read side?",options:["Request-response HTTP calls","Publish-subscribe via a message broker","Shared database polling","Direct method invocation"],correctIndex:1,explanation:"Publish-subscribe (pub/sub) is the most common pattern for event propagation in CQRS. The write side publishes events to a message broker (Kafka, RabbitMQ, AWS SNS/SQS), and read-side projections subscribe to relevant event topics. This decouples the write and read sides  the write side doesn't know or care about its consumers. Multiple projections can independently subscribe and process events at their own pace. Pub/sub also enables adding new read models without modifying the write side, supporting the open-closed principle at the architectural level."},{question:"What is the 'outbox pattern' in CQRS?",options:["Storing emails in an outbox folder","Writing events to a local database table atomically with the state change, then asynchronously publishing them to the message broker","Buffering commands before processing","Caching read model updates"],correctIndex:1,explanation:"The outbox pattern solves the dual-write problem: when you need to both update a database and publish an event, doing them separately can lead to inconsistency if one fails. The outbox pattern writes the event to an 'outbox' table in the same transaction as the state change. A separate process (or CDC/change data capture tool like Debezium) then reads from the outbox table and publishes events to the message broker. This guarantees at-least-once delivery: the event is always published if the state change committed, and idempotent consumers handle potential duplicates."},{question:"What is the role of a query handler in CQRS?",options:["To process commands","To receive a query, fetch data from the read model, and return the result","To publish events","To validate business rules"],correctIndex:1,explanation:"A query handler is the read-side counterpart to a command handler. It receives a query object (e.g., GetOrderSummaryQuery), accesses the appropriate read model, and returns the data. Query handlers should be simple and focused  they don't enforce business rules or trigger side effects. They may apply filtering, pagination, or sorting on top of the pre-computed read model data. In some implementations, query handlers are so thin that they're just a direct database query, which is perfectly fine since the complexity lives in the projection that built the read model."},{question:"How does CQRS support multi-tenant systems?",options:["It doesn't  CQRS is incompatible with multi-tenancy","By partitioning event streams, projections, and read models per tenant, allowing independent scaling and isolation","By using a single shared read model for all tenants","By routing all tenants to the same command handler"],correctIndex:1,explanation:"CQRS naturally supports multi-tenancy through stream partitioning. Each tenant's events can be stored in separate streams or even separate event stores, providing data isolation. Projections can be tenant-specific, building separate read models per tenant. This enables per-tenant scaling  a high-volume tenant can have dedicated projection infrastructure while smaller tenants share resources. The command side can also enforce tenant-specific business rules. This isolation is much harder to achieve in a traditional CRUD architecture where all tenants share the same tables and queries."},{question:"What is temporal querying in event sourcing?",options:["Querying based on time zones","The ability to reconstruct the state of the system at any point in time by replaying events up to that moment","Running queries on a schedule","Caching queries for a specific duration"],correctIndex:1,explanation:"Temporal querying is the ability to answer 'what was the state at time T?' by replaying events up to that timestamp. Since event sourcing stores every state change as an immutable event, you can reconstruct any historical state by replaying the event stream up to the desired point. This is incredibly valuable for auditing, debugging, compliance, and analytics. For example, you could reconstruct a customer's account state at the exact moment a disputed transaction occurred. Traditional CRUD systems that only store current state cannot support this without complex auditing infrastructure."},{question:"What is the 'event replay' concept?",options:["Replaying video recordings of system events","Re-processing stored events through projections to rebuild read models or derive new insights","Undoing the last event","Broadcasting events to all subscribers simultaneously"],correctIndex:1,explanation:"Event replay is the process of re-reading events from the event store and processing them through projection logic. This is used to rebuild corrupted read models, create entirely new read models for new features, migrate to new storage technologies, or perform historical analysis. The ability to replay is a fundamental benefit of event sourcing  your event log is the source of truth, and any derived state can be rebuilt from it. Replay can be done at full speed (not real-time), so rebuilding a read model from millions of events typically takes minutes to hours depending on complexity."},{question:"How do you handle ordering guarantees when projecting events?",options:["Ordering doesn't matter in projections","By ensuring events for the same aggregate/stream are processed in order, typically using stream position or sequence numbers","By timestamp sorting only","By processing all events in parallel"],correctIndex:1,explanation:"Event ordering is critical for correct projections. Events within a single stream (typically per aggregate) must be processed in order  an OrderShipped event makes no sense before OrderPlaced. Most event stores guarantee ordering per stream via sequence numbers. The projection tracks its last processed position per stream and processes events sequentially. Across streams, strict global ordering is often unnecessary and would limit scalability. Kafka provides per-partition ordering, EventStoreDB provides per-stream ordering, and projections should be designed to handle events from different streams arriving in any interleaved order."},{question:"What is a 'live projection' vs a 'catch-up projection'?",options:["Live projections run in production; catch-up projections run in development","A live projection processes events in real-time as they arrive; a catch-up projection replays historical events to build or rebuild a read model","They are the same thing with different names","Live projections are faster than catch-up projections"],correctIndex:1,explanation:"A live projection subscribes to the event stream and processes new events as they are published, keeping the read model up-to-date in near real-time. A catch-up projection starts from a specific position (often the beginning) and processes historical events until it reaches the current position, at which point it becomes a live projection. When building a new read model or recovering from a crash, you first run catch-up to process missed events, then seamlessly transition to live processing. Most production projections operate in a hybrid mode: catch-up on restart, then live during normal operation."},{question:"What is the 'anti-corruption layer' concept in CQRS with bounded contexts?",options:["A firewall between services","A translation layer that maps between different bounded contexts' models, preventing one context's concepts from leaking into another","A data validation middleware","An encryption layer for events"],correctIndex:1,explanation:"An anti-corruption layer (ACL) is a DDD pattern used at the boundary between bounded contexts to translate between their different models and languages. In CQRS, when one context consumes integration events from another, the ACL translates external events into internal domain concepts. For example, the Shipping context might receive an 'OrderPlaced' integration event and translate it into a 'ShipmentRequested' internal event using its own domain language. This prevents external model changes from corrupting the internal domain model and maintains clean bounded context boundaries."},{question:"How does CQRS handle reporting and analytics use cases?",options:["CQRS cannot support reporting","By creating dedicated read models optimized for analytical queries, potentially using a data warehouse as the read store","By querying the write model directly","By exporting data to CSV files"],correctIndex:1,explanation:"CQRS excels at supporting diverse query patterns, including analytics. You can create dedicated read models that project events into star schemas, time-series databases, or data warehouses optimized for analytical queries. These analytical projections can aggregate, denormalize, and pre-compute metrics from the event stream. For example, a 'SalesAnalytics' projection might maintain daily revenue totals, top-selling products, and customer cohort data. Since projections are independent, adding analytics doesn't affect the operational read models or the write side."},{question:"What is the 'set-based validation' challenge in CQRS?",options:["Validating mathematical sets","Enforcing uniqueness constraints (like unique email) that require checking across multiple aggregates, which is hard when each aggregate is loaded independently","Validating command field types","Testing projection correctness"],correctIndex:1,explanation:"Set-based validation involves constraints that span multiple aggregates  for example, ensuring no two users have the same email address. In CQRS, aggregates are loaded and validated independently, so checking a cross-aggregate constraint isn't straightforward. Solutions include: using a lightweight read model (lookup table) to check uniqueness before processing the command, relying on database unique constraints as a safety net, using a domain service that coordinates the check, or accepting the rare duplicate and handling it asynchronously. This is one of the more nuanced challenges in CQRS design."},{question:"What is 'command validation' vs 'domain validation' in CQRS?",options:["They are the same thing","Command validation checks structural correctness (required fields, formats); domain validation enforces business rules using aggregate state","Command validation runs after domain validation","Domain validation is optional"],correctIndex:1,explanation:"Command validation and domain validation are two distinct layers. Command validation is stateless and checks that the command message is well-formed: required fields are present, email format is valid, amounts are positive, etc. This can be done in middleware before reaching the handler. Domain validation is stateful and happens inside the aggregate: does this customer have sufficient credit? Is this order in a state that allows cancellation? Is the inventory available? Separating these layers keeps the domain model focused on business rules while catching obvious errors early, providing better error messages and reducing unnecessary aggregate loading."},{question:"How can you test projections in a CQRS system?",options:["Only through manual end-to-end testing","By feeding a known sequence of events into the projection and asserting the expected read model state","Projections cannot be unit tested","By comparing the read model to the write model"],correctIndex:1,explanation:"Projections are highly testable because they are pure functions of events: given a sequence of events, the projection should produce a deterministic read model state. You can write unit tests that create a projection instance, feed it a series of events (OrderPlaced, ItemAdded, OrderShipped), and assert that the resulting read model contains the expected data. This is much simpler than testing through the full stack. You can also test edge cases like out-of-order events, duplicate events, and event schema versions. The deterministic nature of projections makes them one of the most testable components in a CQRS architecture."},{question:"What is the 'competing consumers' pattern in CQRS projection processing?",options:["Multiple projections competing for the same events","Multiple instances of the same projection consumer processing events in parallel for scalability, where each event is processed by exactly one instance","Different read models competing for database resources","Command handlers competing for lock acquisition"],correctIndex:1,explanation:"Competing consumers is a scalability pattern where multiple instances of the same projection processor share the workload. Events are distributed across instances (e.g., via Kafka consumer groups), and each event is processed by exactly one instance. This allows horizontal scaling of projection processing. However, you must ensure that events for the same aggregate/entity are always routed to the same instance (partition by aggregate ID) to maintain ordering guarantees. This pattern is essential for high-throughput systems where a single projection processor can't keep up with the event volume."},{question:"What is 'event-carried state transfer' in CQRS?",options:["Transferring the event store between environments","Including enough data in events so that consumers can update their local state without querying back to the source","Moving events from one stream to another","Migrating state between aggregates"],correctIndex:1,explanation:"Event-carried state transfer is a pattern where events contain all the data consumers need to update their local state, eliminating the need for callbacks or API queries. For example, instead of publishing CustomerAddressChanged { customerId: 123 } (requiring consumers to fetch the new address), you publish CustomerAddressChanged { customerId: 123, newAddress: {...} }. This reduces coupling and improves resilience  consumers don't depend on the source service being available. It's a form of 'fat events' and is especially valuable in CQRS where read models need to maintain denormalized copies of data from multiple services."},{question:"How does CQRS handle data deletion for GDPR compliance?",options:["GDPR doesn't apply to event-sourced systems","By using crypto-shredding (encrypting PII with per-user keys and destroying the key) or event rewriting strategies","By deleting events from the event store directly","By ignoring deletion requests since events are immutable"],correctIndex:1,explanation:"GDPR's 'right to be forgotten' is challenging with immutable event stores. The most common approach is crypto-shredding: personally identifiable information (PII) in events is encrypted with a per-user key stored separately. When a deletion request comes in, you destroy the key, rendering the PII in events unreadable. Another approach is storing PII in a separate mutable store referenced by events, allowing direct deletion. Some systems use event rewriting with tombstone events. The read models can be rebuilt without the deleted PII. This is a critical consideration when designing event-sourced CQRS systems for European markets."},{question:"What is a 'read model subscription checkpoint'?",options:["A bookmark in a document","A persisted position marker indicating which events a projection has already processed, enabling resumption after restarts","A health check endpoint for the read model","A database savepoint"],correctIndex:1,explanation:"A checkpoint (or bookmark) is a persisted record of the last event position successfully processed by a projection. When the projection process restarts (after a crash, deployment, or scaling event), it reads the checkpoint and resumes processing from that position rather than replaying all events from the beginning. Checkpoints should be updated atomically with the read model writes to prevent duplicate processing. If the checkpoint is updated but the read model write fails (or vice versa), you get inconsistency. Some systems store the checkpoint in the same transaction as the read model update to ensure atomicity."},{question:"What is the purpose of a 'command retry policy' in CQRS?",options:["To retry failed database queries","To automatically re-execute commands that fail due to transient errors like concurrency conflicts","To resend events to the message broker","To reconnect to the event store"],correctIndex:1,explanation:"In CQRS with event sourcing, optimistic concurrency conflicts are expected  two commands targeting the same aggregate may conflict if processed simultaneously. A retry policy automatically reloads the aggregate with the latest state and re-attempts the command when a concurrency exception occurs. This is safe because the command is re-validated against the updated state. Retry policies should have limits (max retries) and backoff strategies (exponential backoff) to avoid infinite loops. They should only retry transient errors  business rule violations should not be retried."},{question:"How does the Axon Framework support CQRS?",options:["It's a JavaScript UI framework","It provides building blocks for CQRS and event sourcing in Java, including command/event buses, aggregate support, sagas, and projections","It's a database migration tool","It's a message broker like RabbitMQ"],correctIndex:1,explanation:"Axon Framework is a Java/Kotlin framework specifically designed for building CQRS and event-sourcing applications. It provides: command buses for routing commands to handlers, event buses for publishing and subscribing to events, aggregate annotations for defining event-sourced aggregates, saga support for long-running processes, and query handling infrastructure. Axon Server (the companion product) provides a combined event store, command bus, and query bus. It's one of the most mature CQRS frameworks and is widely used in enterprise Java applications, reducing the boilerplate of implementing CQRS patterns from scratch."},{question:"What is EventStoreDB and how does it relate to CQRS?",options:["A general-purpose relational database","A purpose-built database for event sourcing that provides stream-based storage, subscriptions, and built-in projections for CQRS","A caching layer for event data","A monitoring tool for event-driven systems"],correctIndex:1,explanation:"EventStoreDB (formerly Event Store) is an open-source database specifically designed for event sourcing. Created by Greg Young (who coined CQRS), it stores events in streams with built-in optimistic concurrency, provides catch-up and persistent subscriptions for projections, and includes a projection engine that can create read models using JavaScript. It supports features like stream metadata, system events, and user-defined projections. Its native support for event sourcing patterns makes it a natural fit for CQRS architectures, though you can implement event sourcing on top of any database."},{question:"What is 'event versioning' and why is it important?",options:["Versioning the event store software","Maintaining version numbers for event schemas to handle backward/forward compatibility as the domain model evolves","Counting how many events exist","Tracking which events have been processed"],correctIndex:1,explanation:"Event versioning assigns version identifiers to event schemas (e.g., OrderPlacedV1, OrderPlacedV2) to manage schema evolution over time. As the domain model changes, events may gain new fields, lose old ones, or change their structure. Since events are immutable and stored forever, old versions must remain readable. Versioning strategies include: upcasting (transforming old versions to current when reading), weak schema (ignoring unknown fields, providing defaults for missing ones), or explicit version mapping. Without proper versioning, evolving the domain model becomes increasingly difficult as the event history grows."},{question:"What is the relationship between CQRS and microservices?",options:["CQRS can only be used with microservices","CQRS can be applied within a single microservice or across service boundaries; it's an architectural pattern that complements but doesn't require microservices","Microservices require CQRS","They are competing architectural styles"],correctIndex:1,explanation:"CQRS and microservices are orthogonal patterns that work well together but don't depend on each other. You can use CQRS within a monolith (separating read and write models in the same application) or within a single microservice. In a microservices architecture, CQRS naturally fits: each service owns its write model, and other services can build their own read models from integration events. However, CQRS also works perfectly in modular monoliths with separate bounded contexts. The key is applying CQRS where the complexity of read/write separation is justified, regardless of the deployment architecture."},{question:"How do you monitor the health of a CQRS system?",options:["Only check if the web server is running","Monitor projection lag, event processing rate, command failure rate, and the gap between write-side and read-side positions","Check disk space only","Monitor only the read model query latency"],correctIndex:1,explanation:"Monitoring a CQRS system requires observing both sides and the bridge between them. Key metrics include: projection lag (how far behind the read model is), event processing throughput, command acceptance/rejection rates, read model query latencies, event store growth rate, and consumer group lag (in Kafka-based systems). Alerting on projection lag is particularly important  if a projection falls too far behind, the read model becomes unacceptably stale. You should also monitor for projection errors, dead-letter queues, and concurrency conflict rates as indicators of system health."},{question:"What is a 'process manager' vs a 'saga' in CQRS terminology?",options:["They are always identical","A process manager is an orchestrator that sends commands based on events; a saga (in its original definition) is a sequence of transactions with compensating actions for rollback","Sagas are faster than process managers","Process managers handle queries; sagas handle commands"],correctIndex:1,explanation:"While often used interchangeably, these terms have distinct origins. A saga (originally from the 1987 Garcia-Molina/Salem paper) is a sequence of local transactions where each step has a compensating transaction for rollback. A process manager is a more general pattern: a stateful component that reacts to events and issues commands to coordinate a workflow. In practice, the CQRS community often uses 'saga' to mean what is technically a process manager. The key distinction is that sagas emphasize compensation for failure, while process managers emphasize stateful workflow coordination."},{question:"What is 'command deduplication' and how is it implemented?",options:["Removing duplicate command handlers","Detecting and discarding duplicate commands using unique IDs to ensure exactly-once processing semantics","Merging similar commands into one","Preventing users from submitting the same form twice via UI"],correctIndex:1,explanation:"Command deduplication ensures each command is executed at most once, even if delivered multiple times due to retries or network issues. Implementation typically involves: assigning each command a unique ID (UUID), storing processed command IDs in a deduplication table, and checking this table before processing. If the ID exists, the command is skipped and the previous result is returned. The deduplication table can be pruned after a retention period (e.g., 24 hours) since duplicates are unlikely after that window. In event-sourced systems, the event store's optimistic concurrency can sometimes serve as a natural deduplication mechanism."},{question:"What is the 'strangler fig' pattern in the context of migrating to CQRS?",options:["A database migration tool","Gradually replacing a legacy system by routing specific features to a new CQRS implementation while the old system continues to handle the rest","Removing unused code from the codebase","A load balancing algorithm"],correctIndex:1,explanation:"The strangler fig pattern (named after the strangler fig tree that gradually envelops its host) is an incremental migration strategy. Instead of rewriting the entire system to use CQRS at once (a risky big-bang approach), you identify specific bounded contexts or features that would benefit most from CQRS and migrate them individually. A routing layer directs traffic to either the old or new system based on the feature. Over time, more features move to the CQRS system until the legacy system is fully replaced. This reduces risk, allows learning, and delivers value incrementally."},{question:"What is the difference between 'inline projection' and 'async projection'?",options:["Inline projections are faster","Inline projections update the read model synchronously within the command transaction; async projections update it asynchronously after the transaction commits","Async projections use more memory","There is no practical difference"],correctIndex:1,explanation:"Inline (synchronous) projections update the read model as part of the same transaction that processes the command and stores events. This provides strong consistency between the write and read models  no eventual consistency lag  but it couples the command's performance to the projection's speed and means the command fails if the projection fails. Async projections update the read model in a separate process after events are committed, providing eventual consistency but better decoupling and performance. Most production CQRS systems use async projections, reserving inline projections for cases where strong read consistency is critical."},{question:"How should you handle 'event ordering across streams' in CQRS?",options:["Enforce strict global ordering for all events","Design projections to handle events from different streams arriving in any order, using timestamps or causation IDs for correlation when needed","Process only one stream at a time","Ignore cross-stream ordering entirely"],correctIndex:1,explanation:"Strict global ordering across all event streams is expensive and limits scalability. Instead, CQRS projections should be designed to tolerate out-of-order events from different streams. Techniques include: using correlation and causation IDs to link related events, designing projections to handle 'dangling references' (e.g., receiving a ShipmentCreated event before the referenced OrderPlaced event), and using reconciliation processes to fix temporary inconsistencies. If a projection absolutely needs cross-stream ordering, you can use a global position/sequence number, but this creates a scalability bottleneck."},{question:"What is 'event enrichment' in CQRS?",options:["Adding metadata to events after they're stored","Adding contextual data (user info, timestamps, correlation IDs) to events during creation so projections have all needed information","Compressing event payloads","Encrypting sensitive event data"],correctIndex:1,explanation:"Event enrichment is the practice of adding useful metadata and contextual information to events at creation time. This includes: correlation IDs (linking events to the originating command), causation IDs (which event caused this one), user/actor information, timestamps, and relevant denormalized data. Enriched events reduce the need for projections to make additional queries to build the read model. For example, including the customer name in an OrderPlaced event means the order list projection doesn't need to query the customer service. However, over-enrichment leads to large events and potential data duplication."},{question:"What role does Apache Kafka play in CQRS architectures?",options:["It serves as the primary write database","It acts as a durable, ordered event log for publishing and subscribing to events between the write and read sides","It replaces the need for a read model","It only handles command routing"],correctIndex:1,explanation:"Kafka is widely used in CQRS as the event backbone connecting the write side to read-side projections. Its partitioned, append-only log with configurable retention makes it suitable for event streaming. Kafka guarantees ordering within partitions (partition by aggregate ID for correct per-aggregate ordering), supports multiple consumer groups (each projection gets its own group), and provides durable storage. Kafka can also serve as a lightweight event store with log compaction, though purpose-built event stores offer better support for event sourcing patterns like stream-level reads and optimistic concurrency."},{question:"What is 'projection partitioning' in CQRS?",options:["Splitting the projection code into modules","Distributing projection workload across multiple workers by partitioning events (e.g., by aggregate ID or tenant) for parallel processing","Creating separate databases for each projection","Archiving old projection data"],correctIndex:1,explanation:"Projection partitioning divides the event processing workload across multiple projection instances. Events are partitioned by a key (typically aggregate ID, tenant ID, or entity type), and each partition is assigned to a specific worker. This enables horizontal scaling  you can add more workers to process events faster. The key requirement is that events for the same entity always go to the same partition to maintain ordering. Kafka consumer groups naturally support this pattern. Projection partitioning is essential for high-throughput CQRS systems where a single projection worker can't keep up with the event volume."},{question:"How do you implement 'exactly-once projection processing'?",options:["It's impossible in distributed systems","By storing the read model update and the projection checkpoint atomically in the same transaction","By processing events very slowly","By using TCP instead of UDP"],correctIndex:1,explanation:"Exactly-once processing semantics for projections are achieved through atomic checkpoint management. The projection's position checkpoint and the read model update are written in the same database transaction. If the transaction fails, neither the checkpoint nor the read model is updated, so the event will be reprocessed. If it succeeds, both are committed, and the event won't be reprocessed. When using different databases for the checkpoint and read model, you fall back to at-least-once processing with idempotent projections. Kafka's transactions API can also help by committing consumer offsets and producer writes atomically."},{question:"What is 'event-driven architecture' vs CQRS?",options:["They are the same pattern","Event-driven architecture is a broader pattern about communicating via events; CQRS specifically separates read and write models and often uses events as the synchronization mechanism","CQRS is a subset of event-driven architecture","Event-driven architecture requires CQRS"],correctIndex:1,explanation:"Event-driven architecture (EDA) is a broad paradigm where components communicate by producing and consuming events. CQRS is a specific pattern that separates command (write) and query (read) responsibilities into different models. While CQRS often uses events to synchronize the read and write sides (making it event-driven), you can implement CQRS with other synchronization mechanisms (like database triggers or polling). Conversely, you can have an event-driven architecture without CQRS. They complement each other well  CQRS with event sourcing naturally produces events that can drive an event-driven architecture."},{question:"What is the 'query side gateway' pattern in CQRS?",options:["A network firewall for database queries","An API layer that routes queries to the appropriate read model based on the query type","A caching proxy for the event store","A load balancer for write operations"],correctIndex:1,explanation:"The query side gateway is a routing layer that directs incoming queries to the appropriate read model or data store. Since CQRS may involve multiple specialized read models (e.g., Elasticsearch for search, Redis for real-time data, PostgreSQL for complex queries), the gateway determines which read model can best serve each query. It may also handle cross-cutting concerns like authentication, caching, and rate limiting for the read side. This pattern keeps the client simple  it sends queries to one endpoint, and the gateway handles the complexity of routing to the correct backend."},{question:"How does CQRS handle complex queries that span multiple aggregates?",options:["It can't  each query must target a single aggregate","By building denormalized read models that pre-join data from multiple aggregates, eliminating the need for runtime joins","By running distributed queries across aggregate boundaries","By using stored procedures on the write model"],correctIndex:1,explanation:"One of CQRS's key strengths is handling complex, cross-aggregate queries efficiently. Instead of joining data at query time, projections build pre-joined, denormalized read models. For example, an 'OrderWithCustomerDetails' read model might combine data from OrderPlaced events and CustomerUpdated events into a single denormalized table. Queries then read from this pre-computed view with zero joins. This shifts the computational cost from query time (where latency matters) to projection time (which is asynchronous and can be slower). The read model is designed backwards from the query needs."},{question:"What is the 'thin read layer' pattern in CQRS?",options:["A read model with minimal data","A lightweight query layer that directly reads from denormalized read models with minimal logic, often bypassing ORM/domain layers entirely","A compressed version of the read model","A read model stored in memory only"],correctIndex:1,explanation:"The thin read layer advocates keeping the query side as simple as possible  often just direct database queries returning DTOs without going through domain objects, repositories, or complex ORM mappings. Since the read model is already denormalized and shaped for the consumer, there's no need for domain logic on the query path. This can be as simple as raw SQL queries or lightweight data access returning flat objects directly to the API. This simplicity is a major benefit of CQRS  the read side can be trivially simple while all the complexity lives in the write side and projections."},{question:"What challenges arise when using CQRS in a system with complex authorization rules?",options:["CQRS doesn't support authorization","Authorization must be enforced on both the command side (can the user perform this action?) and the read side (can the user see this data?), potentially requiring filtered projections or row-level security","Authorization is only needed on the write side","Authorization is only needed on the read side"],correctIndex:1,explanation:"CQRS requires authorization on both sides. The command side checks if the user is allowed to perform the action (e.g., can this user cancel this order?). The read side must filter data based on user permissions (e.g., a manager sees all orders, a customer sees only their own). This can be implemented through: per-user/role read models, row-level security in the read database, query-time filtering, or separate projections per permission level. The challenge is that pre-computing per-user views can be expensive, while runtime filtering can be complex. The right approach depends on the authorization model's complexity."},{question:"What is an 'aggregate stream' in event sourcing?",options:["A river of data flowing between aggregates","A sequence of events belonging to a specific aggregate instance, identified by the aggregate type and ID","A backup of all aggregate states","A real-time dashboard of aggregate operations"],correctIndex:1,explanation:"An aggregate stream is the ordered sequence of all events that belong to a specific aggregate instance. It's typically identified by a stream name like 'Order-abc123' or 'Customer-456'. When loading an aggregate, the system reads all events from its stream and applies them sequentially to rebuild the current state. When processing a command, new events are appended to the stream with the expected version for optimistic concurrency. The stream is the fundamental unit of consistency in event sourcing  within a stream, events are strictly ordered and atomically appended."},{question:"How do you handle large read models that take hours to rebuild from events?",options:["Never rebuild them","Use snapshots/checkpoints during replay, parallel processing, and incremental rebuild strategies to reduce rebuild time","Only store the last 100 events","Skip events during rebuild for speed"],correctIndex:1,explanation:"Rebuilding large read models from millions of events can indeed take hours. Strategies to manage this include: parallel replay (processing events from different partitions simultaneously), snapshot-based replay (starting from a recent read model snapshot rather than event zero), incremental rebuild (rebuilding only the changed portions), and optimized batch writes (buffering read model updates and writing in large batches). The two-phase rebuild approach keeps the old read model serving queries while the new one catches up. For extremely large systems, consider maintaining read model snapshots specifically for fast rebuild."},{question:"What is the role of 'correlation ID' in CQRS?",options:["It identifies the database server","It links a command, its resulting events, and any downstream effects together for tracing and debugging across the entire workflow","It uniquely identifies an aggregate","It determines event ordering"],correctIndex:1,explanation:"A correlation ID is a unique identifier assigned when a workflow begins (typically when a command is received) and propagated through all resulting events, subsequent commands, and side effects. This enables end-to-end tracing: you can follow a single user action through command processing, event emission, projection updates, and saga coordination. In distributed CQRS systems, correlation IDs are essential for debugging  when something goes wrong, you can trace the entire causal chain. They're also valuable for monitoring, logging, and auditing the full lifecycle of a business operation."},{question:"What is 'event sourcing without CQRS'?",options:["Impossible  event sourcing requires CQRS","Using an event store as the persistence mechanism but reading by reconstructing aggregate state from events without a separate read model","Storing events but never reading them","Using CQRS without events"],correctIndex:1,explanation:"Event sourcing and CQRS are independent patterns that work well together but don't require each other. You can use event sourcing without CQRS by storing events as your persistence mechanism and reconstructing aggregate state by replaying events when you need to read. Queries would load the aggregate from its event stream rather than reading from a separate optimized read model. This approach gives you the audit trail and temporal query benefits of event sourcing but doesn't give you the read-side optimization benefits of CQRS. It works well for write-heavy systems with simple read patterns."},{question:"What is the 'event handler' vs 'event listener' distinction?",options:["They are always identical","An event handler modifies state (updating a projection); an event listener performs side effects (sending emails, triggering notifications) without modifying the read model","Event handlers are synchronous; listeners are asynchronous","Event listeners are deprecated in modern CQRS"],correctIndex:1,explanation:"While the terminology varies by framework, a useful distinction is: event handlers are responsible for updating read model state (projections) and should be idempotent and deterministic. Event listeners (or reactors/policies) trigger side effects like sending emails, calling external APIs, or publishing integration events. This distinction matters for replay: when rebuilding a read model by replaying events, you want event handlers to run but NOT event listeners (you don't want to re-send thousands of emails). Frameworks like Axon and Marten support this distinction explicitly."},{question:"How does CQRS handle search functionality?",options:["CQRS cannot support search","By projecting events into a search engine (like Elasticsearch) as a dedicated read model","By running full-text search on the event store","By using SQL LIKE queries on the write model"],correctIndex:1,explanation:"Search is a perfect use case for CQRS's polyglot persistence capability. A dedicated search projection consumes events and indexes them into a search engine like Elasticsearch, Solr, or Typesense. This search index becomes a specialized read model optimized for full-text search, faceting, and relevance scoring. The projection maps domain events to search documents, handling creates, updates, and deletes. This approach gives you powerful search without compromising the write model's data structure and without the limitations of database-native full-text search."},{question:"What is 'command sourcing' and how does it differ from event sourcing?",options:["They are the same thing","Command sourcing stores the commands (requests) rather than the resulting events (facts), which is generally less useful since commands don't capture the outcome","Command sourcing is faster than event sourcing","Command sourcing uses a different database type"],correctIndex:1,explanation:"Command sourcing stores every command sent to the system, while event sourcing stores every event produced by processing commands. The key difference is that commands are requests (which may be rejected), while events are facts (what actually happened). Replaying commands is problematic because the outcome might differ if the system state has changed. Replaying events always produces the same result since they represent what actually occurred. Command sourcing can be useful for auditing (what did users try to do?) but event sourcing is the standard for state reconstruction. In practice, many systems log commands for debugging while event-sourcing for state management."},{question:"How does 'backpressure' apply to CQRS projections?",options:["It's a plumbing concept irrelevant to software","When events are produced faster than projections can consume them, backpressure mechanisms slow down or buffer the event flow to prevent projection overload","Projections always process events instantly","Backpressure only applies to the write side"],correctIndex:1,explanation:"Backpressure occurs when the event production rate exceeds the projection consumption rate, causing the projection lag to grow continuously. Without backpressure handling, this can lead to unbounded queue growth, memory exhaustion, or unacceptable read model staleness. Solutions include: scaling projection consumers horizontally, buffering events in a durable message broker (Kafka handles this naturally), batching projection updates, optimizing projection processing speed, or in extreme cases, throttling the write side. Monitoring projection lag is the primary indicator of backpressure problems in a CQRS system."},{question:"What is the 'read model per screen' approach in CQRS?",options:["One database per UI screen","Designing each read model to exactly match the data needs of a specific UI view, eliminating over-fetching and under-fetching","Displaying the read model directly on screen","A front-end rendering technique"],correctIndex:1,explanation:"The 'read model per screen' (or 'read model per query') approach creates purpose-built read models that contain exactly the data needed for a specific UI view. Instead of a generic 'orders' table that serves multiple screens with different data needs, you might have 'order_list_items' (for the list view), 'order_details' (for the detail view), and 'order_analytics' (for the dashboard). Each projection shapes data for its specific consumer, eliminating joins, reducing payload size, and simplifying the query layer. This is where CQRS delivers its biggest performance and simplicity wins on the read side."},{question:"What is 'event stream merging' in CQRS projections?",options:["Combining two event stores into one","A projection that consumes events from multiple streams/aggregates to build a composite read model","Merging duplicate events","Joining two database tables"],correctIndex:1,explanation:"Event stream merging occurs when a projection needs data from multiple aggregate types to build its read model. For example, an 'OrderWithCustomer' projection consumes events from both Order and Customer streams. When an OrderPlaced event arrives, the projection creates a record with order details; when a CustomerNameChanged event arrives, it updates all orders for that customer. This cross-stream projection is one of the most powerful patterns in CQRS  it creates views that would require expensive joins in a traditional system. The challenge is handling events arriving in any order from different streams."},{question:"What is 'command routing' in a distributed CQRS system?",options:["DNS routing for API endpoints","Directing a command to the specific node or service instance that owns the target aggregate, often using consistent hashing on the aggregate ID","Routing commands to a message queue","Load balancing across all services equally"],correctIndex:1,explanation:"In a distributed CQRS system with multiple service instances, each command must reach the instance that can load and modify the target aggregate. Command routing uses the aggregate ID to deterministically select the handling instance  typically via consistent hashing or a routing table. This ensures commands for the same aggregate are serialized at the same instance, preventing concurrent modification conflicts. Frameworks like Axon Server and Microsoft Orleans provide built-in command routing. Without proper routing, you'd need distributed locking, which is much more expensive and fragile."},{question:"What is the significance of 'immutable events' in event sourcing?",options:["Events can be modified but shouldn't be","Events are stored as append-only, immutable facts  never updated or deleted  providing a trustworthy audit trail and enabling deterministic replay","Immutability is optional for performance","Only some events need to be immutable"],correctIndex:1,explanation:"Event immutability is a cornerstone principle of event sourcing. Once an event is stored, it is never modified or deleted. This provides a tamper-proof audit trail (critical for compliance), enables deterministic replay (you always get the same result from the same events), and simplifies the system (no update conflicts in the event store). To 'undo' something, you append a compensating event (e.g., OrderCancelled to reverse OrderPlaced) rather than deleting the original. This append-only nature also enables efficient storage (no random I/O for updates) and is what makes event stores fundamentally different from traditional databases."},{question:"How do you implement 'real-time projections' that update the UI immediately?",options:["Polling the read model every second","Using event-driven push notifications (WebSockets, SSE, or SignalR) triggered by projection updates to notify clients of changes","Refreshing the entire page","Having the client query the event store directly"],correctIndex:1,explanation:"Real-time projections combine async projection processing with push notifications to clients. When a projection processes an event and updates the read model, it also publishes a notification (via WebSocket, Server-Sent Events, or SignalR) to connected clients. The client receives the notification and either fetches the updated data or receives it inline. This creates a reactive UX where changes appear almost instantly. Libraries like SignalR (.NET) or Socket.io (Node.js) handle the connection management. This is particularly effective for collaborative applications where multiple users need to see each other's changes in real-time."},{question:"What is the 'aggregate design rule' of keeping aggregates small in CQRS?",options:["Aggregates should have as few fields as possible","Aggregates should be designed around consistency boundaries, not convenience  only include entities that must change together in a single transaction","Aggregates should never reference other aggregates","All entities in a domain should be in one aggregate"],correctIndex:1,explanation:"Vaughn Vernon's aggregate design rules emphasize keeping aggregates small by only including what must be transactionally consistent together. A large aggregate (e.g., an Order that contains all OrderItems, ShippingInfo, PaymentInfo, and CustomerInfo) creates contention  any modification locks the entire aggregate. Instead, design smaller aggregates (Order, Shipment, Payment) that reference each other by ID. In CQRS with event sourcing, smaller aggregates mean fewer events to replay when loading, less contention for concurrent commands, and clearer domain boundaries. The read model handles denormalization for query purposes."},{question:"What is 'command validation middleware' in CQRS?",options:["A network firewall","A pipeline component that validates command structure, permissions, and preconditions before the command reaches the handler","A testing framework for commands","A UI form validation library"],correctIndex:1,explanation:"Command validation middleware sits in the command processing pipeline before the handler. It performs cross-cutting validations that don't require domain knowledge: schema validation (required fields, correct types), authentication (is the user logged in?), authorization (does the user have permission?), rate limiting, and structural preconditions. This keeps command handlers focused on domain logic rather than boilerplate checks. The pipeline pattern (similar to HTTP middleware) allows stacking multiple validators, loggers, and other cross-cutting concerns. Libraries like MediatR (with pipeline behaviors) make this pattern easy to implement."},{question:"How does CQRS affect testing strategy?",options:["CQRS makes testing harder with no benefits","CQRS enables isolated testing: commands are tested with 'given events, when command, then events'; projections with 'given events, then read model state'","Only integration tests are possible with CQRS","Testing is the same as in any other architecture"],correctIndex:1,explanation:"CQRS significantly improves testability by creating clear, isolated components. Command handlers are tested using the 'given-when-then' pattern: given a set of prior events (aggregate state), when a command is processed, then specific new events should be emitted. Projections are tested by feeding events and asserting read model state. Read queries are tested by seeding the read model and verifying results. Each component can be unit tested independently without complex mocking. This separation also enables property-based testing (generate random event sequences and verify projection invariants) and makes it easy to test edge cases like concurrency conflicts and event schema evolution."},{question:"What is a 'subscription group' in the context of CQRS projections?",options:["A team of developers working on subscriptions","A set of projection consumers that share event processing workload, where each event is processed by exactly one consumer in the group","A group of event streams","A collection of similar events"],correctIndex:1,explanation:"A subscription group (similar to Kafka consumer groups) allows multiple instances of the same projection to share the workload. Events are distributed across group members, and each event is processed by exactly one member. This enables horizontal scaling of projection processing. When a member fails, its partitions are rebalanced to surviving members. EventStoreDB calls these 'persistent subscriptions' (with competing consumers), Kafka uses 'consumer groups', and other brokers have similar concepts. The key constraint is maintaining per-partition ordering to ensure events for the same entity are processed sequentially."},{question:"What is the 'domain event vs integration event' publishing strategy?",options:["Publish all events everywhere","Publish domain events internally within the bounded context for projections/sagas, and selectively publish curated integration events externally for other services","Only publish integration events","Domain events are not published at all"],correctIndex:1,explanation:"A well-designed CQRS system distinguishes between internal domain events and external integration events. Domain events contain internal details relevant to the bounded context (e.g., OrderItemPriceRecalculated with internal pricing logic details). Integration events are a curated, stable public API for external consumers (e.g., OrderTotalChanged with just the new total). The bounded context subscribes to its own domain events for projections and sagas, and a separate integration event publisher selectively transforms domain events into integration events. This prevents leaking internal domain knowledge and gives you freedom to refactor internally without breaking external consumers."},{question:"How do you handle 'eventual consistency' in the UI when using CQRS?",options:["Always show a loading spinner for 10 seconds after every action","Use optimistic UI updates, polling with version checks, or push notifications to give users immediate feedback while the read model catches up","Ignore the issue  users won't notice","Switch to strong consistency everywhere"],correctIndex:1,explanation:"Handling eventual consistency in the UI is critical for good UX. Strategies include: optimistic UI updates (immediately update the UI to reflect the expected result of the command, then reconcile when the read model catches up), version-aware polling (include the expected version in the response and poll until the read model reaches it), push notifications (the server pushes updates via WebSocket when the projection completes), and confirmation pages ('Your order has been placed and is being processed'). The key is setting user expectations  many real-world systems are eventually consistent (bank transactions, order processing) and users already understand slight delays."},{question:"What is 'event deduplication' in CQRS projections?",options:["Removing duplicate events from the event store","Ensuring that projections handle duplicate event deliveries gracefully by making projection handlers idempotent","Preventing users from creating duplicate records","Deduplicating query results"],correctIndex:1,explanation:"In distributed systems, events can be delivered more than once due to broker redelivery, consumer restarts, or network issues. Projections must handle this gracefully through idempotent processing. Strategies include: tracking the last processed event position/ID and skipping already-seen events, designing projection updates to be naturally idempotent (e.g., SET balance = 100 is idempotent, but SET balance = balance + 10 is not), or using upsert operations. Atomic checkpoint management (updating the checkpoint in the same transaction as the read model) is the most robust approach, ensuring an event is counted as processed only when its effect is committed."},{question:"What is the benefit of 'append-only storage' in an event store?",options:["It uses less disk space","Append-only writes are extremely fast (no random I/O), support high write throughput, and naturally prevent data loss through immutability","It makes reads faster","It simplifies indexing"],correctIndex:1,explanation:"Append-only storage is the foundation of event stores and provides several advantages. Write performance is excellent because appends are sequential I/O  the fastest operation for both SSDs and especially HDDs. There are no update-in-place operations, so no write amplification or fragmentation. Immutability means no data is ever lost or corrupted by overwrites. Concurrency is simplified since appends don't conflict with reads. The trade-off is that reading current state requires replaying events (mitigated by snapshots and read models), and storage grows indefinitely (mitigated by archiving or compaction strategies). These properties make event stores naturally suited to high-write-throughput systems."},{question:"What is the 'split-brain' problem relevant to CQRS?",options:["A cognitive issue for developers","When network partitions cause multiple instances to process commands for the same aggregate independently, leading to conflicting event streams","When the read model splits into two databases","When projections fall too far behind"],correctIndex:1,explanation:"Split-brain occurs when network partitions cause multiple nodes in a CQRS system to believe they are the authoritative handler for the same aggregate, leading to conflicting commands being processed independently. This can result in divergent event streams that are difficult to reconcile. Prevention strategies include: using the event store's optimistic concurrency (only one writer succeeds), cluster consensus protocols (Raft/Paxos for leader election), distributed locking, or accepting conflicts and implementing merge strategies. This is particularly relevant in multi-region deployments where network partitions between regions are common."},{question:"How does 'event store compaction' work?",options:["Compressing event data","Keeping only the latest event per key and discarding older events, similar to Kafka log compaction, to reduce storage while preserving final state","Merging multiple event stores","Removing empty events"],correctIndex:1,explanation:"Event store compaction (like Kafka's log compaction) retains the latest event for each key while removing older events. This is useful for scenarios where you care about the final state rather than the full history (e.g., user profile updates). After compaction, you lose the ability to replay the full event history but save significant storage. This is NOT appropriate for true event sourcing where the complete history is the source of truth  it's more suited for event-carried state transfer or CDC scenarios. True event-sourced systems typically use snapshots plus archiving instead of compaction to manage storage growth."},{question:"What is the 'command return value' debate in CQRS?",options:["Commands should always return the full entity","There is debate about whether commands should return void (pure CQS) or minimal data like the created entity's ID for practical API design","Commands should return events","Commands should return the read model state"],correctIndex:1,explanation:"Purists argue commands should return nothing (void), following strict CQS principles  you separate commands (do something) from queries (return something). However, pragmatists argue that returning at least the identifier of a created resource (e.g., the new order ID) is essential for practical API design  without it, the client doesn't know what was created. Some return a result object with the ID and success/failure status. Others return the full created entity for convenience. The consensus in the community is that returning a minimal result (ID + status) is a reasonable compromise that maintains the spirit of CQRS while being practical for API consumers."},{question:"How does GraphQL relate to the query side of CQRS?",options:["GraphQL replaces CQRS entirely","GraphQL is a natural fit for the CQRS query side, providing flexible querying of read models with client-specified field selection","GraphQL can only be used on the command side","GraphQL and CQRS are incompatible"],correctIndex:1,explanation:"GraphQL complements the query side of CQRS by allowing clients to request exactly the fields they need from read models. Each GraphQL query type can resolve from a different read model or data store, leveraging CQRS's polyglot persistence. GraphQL mutations map naturally to CQRS commands. The flexibility of GraphQL reduces the need for 'one read model per screen' since clients can compose their views from multiple resolvers. However, GraphQL doesn't replace the need for optimized read models  poorly designed resolvers can still cause N+1 query problems. The combination of GraphQL + CQRS can be very powerful when the read models are designed to support the GraphQL schema efficiently."},{question:"What is 'command authorization' in CQRS?",options:["Authorizing the event store to write events","Verifying that the user issuing a command has the required permissions to perform the requested action before processing it","Authorizing database connections","Giving commands priority over queries"],correctIndex:1,explanation:"Command authorization is the process of verifying that the user or system issuing a command has the necessary permissions. This typically happens in the command processing pipeline, either as middleware or within the handler. Authorization checks might include role-based access (is the user an admin?), resource-based access (does the user own this order?), or attribute-based access (is the user's department allowed to approve expenses above $1000?). In CQRS, authorization is especially important because commands change state  unauthorized mutations could have serious consequences. Some authorization checks require loading the aggregate (to check ownership), adding a dependency on the domain layer."},{question:"What is the role of 'metadata' on events in event sourcing?",options:["Metadata is optional and rarely used","Metadata carries contextual information (timestamp, user ID, correlation ID, causation ID) that aids in tracing, auditing, and projection processing","Metadata stores the event payload","Metadata is the event schema version"],correctIndex:1,explanation:"Event metadata is contextual information attached to events beyond the domain payload. Common metadata includes: timestamp (when the event was created), user/actor ID (who caused it), correlation ID (which workflow it belongs to), causation ID (which event or command caused this one), schema version, source service/aggregate, and IP address. Metadata is invaluable for debugging (trace an event back to its origin), auditing (who did what when), and operational concerns (filtering events by time range or source). Most event stores and frameworks support metadata as a first-class concept separate from the event body."},{question:"What is 'CQRS with a shared database'?",options:["Not possible  CQRS requires separate databases","Using the same database for both read and write models but with separate tables/schemas optimized for each concern","Sharing a database between microservices","Using a database shared between production and staging"],correctIndex:1,explanation:"CQRS with a shared database is the simplest implementation: the write model uses normalized tables with foreign keys and constraints, while the read model uses denormalized tables or materialized views in the same database. Synchronization can be done via database triggers, views, or application-level event publishing. This approach avoids the operational complexity of managing multiple databases while still gaining the benefits of optimized data models for each concern. It's a great starting point  you can later split to separate databases if scaling demands require it. The key insight is that CQRS is about separate models, not necessarily separate physical databases."},{question:"How do you handle 'projection failures' in production?",options:["Ignore them  the system will self-heal","Implement dead-letter queues for failed events, alerting, retry logic with exponential backoff, and the ability to replay from the last successful checkpoint","Restart the entire system","Delete the failed events"],correctIndex:1,explanation:"Projection failures are inevitable in production  events might reference data that doesn't exist yet, external services might be down, or bugs might cause exceptions. A robust approach includes: retry logic with exponential backoff for transient failures, dead-letter queues (DLQ) for events that fail repeatedly, alerting when the DLQ grows or projection lag increases, and the ability to fix the code and replay from the last checkpoint. Some systems support 'poison message' handling where a single bad event is skipped and logged rather than blocking the entire projection. Monitoring projection health (lag, error rate, throughput) is essential for operating CQRS in production."},{question:"What is the 'event-first' design approach?",options:["Writing events before commands","Designing the system by first identifying the domain events that occur, then working backwards to determine what commands trigger them and what read models consume them","Prioritizing event processing over command processing","Event-driven programming paradigm"],correctIndex:1,explanation:"Event-first design (also called 'event storming' when done as a workshop) starts by identifying what happens in the domain  the events. This is counterintuitive compared to traditional approaches that start with data models or API endpoints. By identifying events first (OrderPlaced, PaymentReceived, ItemShipped), you discover the natural domain boundaries, the commands that trigger events, the policies that react to events, and the read models that aggregate events. This approach naturally leads to CQRS/ES architecture because you've already identified the events that drive the system. It also improves domain understanding because events are expressed in business language."},{question:"What is the maximum recommended size of an event payload?",options:["Exactly 1 KB","There's no strict limit, but events should be small (1-10 KB typically), carrying only the data that changed, not entire entity snapshots","Events should be as large as possible for completeness","Events must be under 100 bytes"],correctIndex:1,explanation:"While there's no universal limit, best practice is keeping events small  typically 1-10 KB. An event should carry only the data relevant to what changed, not the entire entity state. Large events (carrying full entity snapshots) waste storage, slow down serialization/deserialization, and increase network bandwidth. If you need the full entity state, that's what snapshots are for. The exception is 'fat events' for event-carried state transfer, where including additional data reduces coupling. Kafka has configurable message size limits (default 1 MB), and EventStoreDB handles large events but performs best with smaller ones."},{question:"What is 'event sourcing with snapshots' and when should you take snapshots?",options:["Snapshot every single event","Periodically save the aggregate's current state (e.g., every N events) to speed up loading, only replaying events after the snapshot","Snapshots replace event sourcing entirely","Take snapshots only during system maintenance windows"],correctIndex:1,explanation:"Snapshots are periodic saves of an aggregate's computed state, stored alongside or separately from the event stream. When loading an aggregate, the system loads the latest snapshot and replays only the events after it. Common strategies for when to snapshot include: every N events (e.g., every 100), when loading time exceeds a threshold, or on a schedule. Snapshots are an optimization, not a replacement for events  the event stream remains the source of truth. Start without snapshots and add them only when aggregate loading becomes a measurable performance bottleneck. Premature snapshot optimization adds complexity for minimal benefit in systems with short-lived aggregates or few events per aggregate."},{question:"How does CQRS relate to Domain-Driven Design (DDD)?",options:["CQRS replaces DDD","CQRS and DDD complement each other: DDD provides the strategic (bounded contexts) and tactical (aggregates, entities) patterns that inform CQRS model design","DDD requires CQRS","They are competing methodologies"],correctIndex:1,explanation:"CQRS and DDD are complementary. DDD provides the strategic patterns (bounded contexts, ubiquitous language, context mapping) that help you identify where CQRS should be applied. DDD's tactical patterns (aggregates, entities, value objects, domain events) directly inform the write model design. Bounded contexts are the natural boundaries for CQRS implementations  each context can independently decide whether CQRS is appropriate. The combination of DDD + CQRS + Event Sourcing is often called the 'DDD triad' and is considered a powerful approach for complex domains. However, both patterns can be used independently."},{question:"What is the 'event store as single source of truth' principle?",options:["The read model is the source of truth","In event sourcing + CQRS, the event store contains the authoritative record of everything that happened; all other data stores are derived projections that can be rebuilt","Both the event store and read model are sources of truth","The database with the most data is the source of truth"],correctIndex:1,explanation:"When using event sourcing with CQRS, the event store is the single source of truth for the system. Every read model, cache, search index, and materialized view is a derived projection that can be rebuilt from the event store at any time. This has profound implications: data loss in a read model is recoverable (replay events), bugs in projections are fixable (fix code, rebuild), and new query patterns are supportable (create a new projection). The event store must therefore be treated with the highest level of care  it needs robust backups, replication, and durability guarantees. Losing the event store means losing the authoritative history of the system."},{question:"What is the 'closing the books' pattern in event sourcing?",options:["Financial auditing terminology only","Periodically creating a summary event that captures the current state, allowing old events to be archived while maintaining a starting point for future processing","Deleting old events monthly","Locking event streams from further writes"],correctIndex:1,explanation:"The 'closing the books' pattern (borrowed from accounting) addresses the challenge of ever-growing event streams. Periodically (e.g., end of month, end of quarter), the system creates a summary event that captures the aggregate's state at that point. Old events before the summary can then be archived to cold storage, reducing the active event store size. Future aggregate loading starts from the summary event rather than from the beginning of time. This is similar to snapshots but is modeled as a domain concept (like closing a financial period) rather than a technical optimization. It's particularly relevant for long-lived aggregates with many events."},{question:"How does CQRS handle cross-aggregate transactions?",options:["Use distributed two-phase commit across all aggregates","Avoid cross-aggregate transactions; instead use eventual consistency with sagas/process managers that coordinate through events and compensating actions","Lock all involved aggregates before processing","Cross-aggregate transactions are not possible"],correctIndex:1,explanation:"CQRS strongly discourages cross-aggregate transactions because they create tight coupling and reduce scalability. Instead, the pattern favors eventual consistency: a command modifies one aggregate and emits an event, a saga/process manager listens for that event and issues commands to other aggregates, and compensating commands handle failures. For example, placing an order might first create the Order aggregate, then a saga handles payment by commanding the Payment aggregate, and if payment fails, a compensating CancelOrder command is issued. This approach is more resilient and scalable than distributed transactions but requires careful design of compensation logic."},{question:"What is 'event-driven projection rebuild' vs 'state-based snapshot rebuild'?",options:["They produce different results","Event-driven rebuild replays all events through projection logic; state-based rebuild copies from a point-in-time snapshot of the read model as a starting point","Event-driven rebuilds are always faster","State-based rebuilds are more accurate"],correctIndex:1,explanation:"Event-driven rebuild processes every event from the event store through the projection logic to construct the read model from scratch. It's the most accurate approach and works even if the projection logic has changed. State-based rebuild starts from a snapshot of the read model taken at a specific point and only processes events that occurred after the snapshot. This is faster for large event stores but requires that the projection logic hasn't changed since the snapshot was taken. In practice, systems use a hybrid: maintain periodic read model snapshots for fast recovery, but support full event replay when the projection logic changes."},{question:"What is the significance of 'event ordering' in an event store?",options:["Events don't need ordering","Events within a stream must be strictly ordered to ensure deterministic aggregate state reconstruction and correct projection processing","Events should be randomly ordered for better distribution","Only global ordering matters"],correctIndex:1,explanation:"Event ordering is fundamental to correctness in event sourcing. Within a single stream (per aggregate), events must be strictly ordered  applying them in a different order would produce a different state (e.g., 'account opened' must come before 'funds deposited'). Event stores guarantee per-stream ordering through sequence numbers. Global ordering across all streams is sometimes useful for projections that need a consistent view of the entire system, but it's expensive and limits write throughput. Most projections only need per-stream ordering plus a global position for checkpointing. The ordering guarantees of your event store directly impact what your projections can reliably compute."},{question:"What is the 'Decider' pattern in functional CQRS?",options:["A design pattern for making decisions in the UI","A functional programming pattern that encapsulates command handling as a pure function: (State, Command)  Event[] and state evolution as (State, Event)  State","A consensus algorithm for distributed decisions","A pattern for deciding which read model to query"],correctIndex:1,explanation:"The Decider pattern, popularized by Jrmie Chassaing, models the command side as two pure functions: a 'decide' function that takes the current state and a command, returning zero or more events; and an 'evolve' function that takes the current state and an event, returning the new state. This functional approach makes the command side highly testable, composable, and free of side effects. The initial state, decide, and evolve functions together form a complete specification of the aggregate's behavior. This pattern has gained popularity in F#, TypeScript, and Kotlin CQRS implementations for its simplicity and mathematical elegance."},{question:"How do you handle 'late-arriving events' in CQRS projections?",options:["Reject them  late events are invalid","Design projections to handle events arriving out of expected order, using upsert logic, idempotent updates, and reconciliation processes","Queue them until the expected event arrives","Late events never occur in well-designed systems"],correctIndex:1,explanation:"In distributed systems, events can arrive at projections out of the expected business order  for example, a ShipmentCreated event might arrive before the OrderPlaced event it references. Projections must handle this gracefully. Strategies include: using upsert logic (create-if-not-exists, update-if-exists), storing 'pending' references that are resolved when the missing event arrives, periodic reconciliation processes that fix inconsistencies, and designing the read model schema to tolerate temporary gaps. The key insight is that projections in distributed CQRS are not traditional ETL pipelines  they must be resilient to real-world messaging disorder."},{question:"What is the 'event catalog' concept?",options:["An online store for event tickets","A documented registry of all event types in the system, including their schemas, versions, producers, and consumers  serving as the contract between services","A database table listing past events","A logging system for events"],correctIndex:1,explanation:"An event catalog is a living documentation of all event types in the system. For each event, it describes: the schema (field names, types, constraints), version history, which service produces it, which services consume it, and example payloads. It serves as the contract between producers and consumers, similar to an API specification. Tools like AsyncAPI, EventCatalog (eventcatalog.dev), and schema registries (Confluent Schema Registry) help maintain this catalog. In a CQRS system with many event types flowing between bounded contexts, the event catalog is essential for governance, onboarding new developers, and preventing breaking changes."}],Ah={"cap-theorem":df,"acid-base":uf,databases:hf,"design-url-shortener":pf,"event-driven":mf,"load-balancers":ff,microservices:gf,cqrs:yf},xa=[{id:"cap-theorem",title:"CAP Theorem",category:"fundamentals",description:"Consistency, Availability, Partition Tolerance  pick two.",content:`In a distributed system you can only guarantee two of three properties:

**Consistency (C):** Every read receives the most recent write or an error.

**Availability (A):** Every request gets a non-error response.

**Partition Tolerance (P):** The system works despite network partitions.

Since partitions are inevitable, the real choice is CP vs AP.

**CP**  reject requests during a partition to stay consistent (e.g., banking).
**AP**  serve requests with potentially stale data (e.g., social feeds).`,tips:[`Don't just recite CAP  apply it: "I choose AP here because"`,"Mention PACELC as a more nuanced model.","Real systems aren't purely CP or AP; they make per-operation trade-offs."],relatedIds:["consistency-patterns","databases"],quizQuestions:[{question:"Which CAP property is always required in a distributed system?",options:["Consistency","Availability","Partition Tolerance","Durability"],correctIndex:2,explanation:"Network partitions are inevitable, so Partition Tolerance is always needed."},{question:"A social media feed that shows slightly stale posts during a partition is an example of?",options:["CP system","AP system","CA system","ACID system"],correctIndex:1,explanation:"It favours Availability over Consistency  classic AP."}]},{id:"acid-base",title:"ACID vs BASE",category:"fundamentals",description:"Strict transactional integrity vs eventual consistency.",content:`**ACID** (SQL databases):
- **Atomicity**  all or nothing
- **Consistency**  valid state transitions
- **Isolation**  concurrent txns don't interfere
- **Durability**  committed data survives crashes

**BASE** (NoSQL):
- **Basically Available**  always responds
- **Soft State**  state may change without input
- **Eventually Consistent**  converges over time

Choose ACID when correctness matters (payments). Choose BASE when availability & scale matter (analytics, feeds).`,tips:["Isolation levels matter  know Read Committed vs Serializable.","Many modern databases (CockroachDB, Spanner) offer ACID at scale."],relatedIds:["cap-theorem","databases"],quizQuestions:[{question:"Which ACID property ensures a transaction is all-or-nothing?",options:["Consistency","Isolation","Atomicity","Durability"],correctIndex:2,explanation:"Atomicity means the entire transaction succeeds or rolls back."}]},{id:"networking-basics",title:"Networking Basics",category:"fundamentals",description:"TCP/IP, HTTP, DNS, WebSockets  the foundation.",content:`**TCP**  reliable, ordered byte stream. 3-way handshake.
**UDP**  unreliable, fast. Good for video/gaming.
**HTTP/1.1**  request/response, keep-alive.
**HTTP/2**  multiplexed streams, header compression.
**HTTP/3**  QUIC (UDP-based), faster handshake.
**WebSockets**  full-duplex persistent connection.
**DNS**  translates domain names to IPs. Hierarchical caching.`,tips:["Know when to use WebSockets vs polling vs SSE.","DNS can be used for load balancing (round-robin DNS)."],relatedIds:["load-balancers","cdn"],quizQuestions:[{question:"Which protocol is HTTP/3 built on?",options:["TCP","UDP/QUIC","SCTP","WebSocket"],correctIndex:1,explanation:"HTTP/3 uses QUIC, which runs over UDP."}]},{id:"scalability",title:"Scalability",category:"fundamentals",description:"Vertical vs horizontal scaling, stateless design.",content:`**Vertical Scaling**  bigger machine (limited ceiling).
**Horizontal Scaling**  more machines (preferred for web).

Key principles:
- **Stateless services**  any instance can handle any request
- **Shared-nothing architecture**  nodes don't share memory/disk
- **Data partitioning**  split data across nodes
- **Replication**  copies for read throughput & fault tolerance

Amdahl's Law: speedup limited by the serial fraction of work.`,tips:["Always start with the simplest approach; scale when needed.","Horizontal scaling requires solving distributed coordination."],relatedIds:["load-balancers","databases","consistent-hashing"],quizQuestions:[{question:"Which scaling approach adds more machines?",options:["Vertical","Horizontal","Diagonal","Elastic"],correctIndex:1,explanation:"Horizontal scaling = adding more machines."}]},{id:"latency-throughput",title:"Latency & Throughput",category:"fundamentals",description:"Numbers every engineer should know.",content:`**Key latencies:**
- L1 cache: ~1 ns
- RAM: ~100 ns
- SSD random read: ~16 s
- HDD seek: ~4 ms
- Same datacenter round-trip: ~0.5 ms
- Cross-continent: ~150 ms

**Throughput** = requests/second the system handles.
**Bandwidth** = max data transfer rate.

Little's Law: L =   W (avg items = arrival rate  avg time in system).`,tips:["Memorize the orders of magnitude, not exact numbers.","Use these to do back-of-envelope calculations in interviews."],relatedIds:["caching","cdn"],quizQuestions:[{question:"Roughly how long is a cross-continent network round-trip?",options:["1 ms","15 ms","150 ms","1500 ms"],correctIndex:2,explanation:"Cross-continent RTT is roughly 100150 ms."}]},{id:"load-balancers",title:"Load Balancers",category:"building-blocks",description:"Distribute traffic across servers.",content:`**Layer 4 (Transport)**  routes based on IP/port. Fast, no content inspection.
**Layer 7 (Application)**  routes based on HTTP headers, URL, cookies. More flexible.

**Algorithms:**
- Round Robin / Weighted Round Robin
- Least Connections
- IP Hash (sticky sessions)
- Consistent Hashing

**Health checks**  periodic probes remove unhealthy backends.

Examples: AWS ALB/NLB, Nginx, HAProxy, Envoy.`,tips:["L7 is slower but enables content-based routing, SSL termination, etc.","Mention health checks  interviewers love hearing about failure handling."],relatedIds:["scalability","networking-basics","cdn"],quizQuestions:[{question:"Which LB layer can route based on HTTP headers?",options:["Layer 3","Layer 4","Layer 7","Layer 2"],correctIndex:2,explanation:"Layer 7 (Application) LBs inspect HTTP content."}]},{id:"databases",title:"Databases",category:"building-blocks",description:"SQL vs NoSQL, replication, partitioning.",content:`**SQL**  relational, ACID, schemas, joins. Great for structured data.
**NoSQL types:**
- Key-Value (Redis, DynamoDB)
- Document (MongoDB)
- Wide-Column (Cassandra, HBase)
- Graph (Neo4j)

**Replication:** Leader-follower, multi-leader, leaderless.
**Partitioning:** Range, hash, composite.
**Indexes:** B-tree (reads), LSM-tree (writes).`,tips:[`Don't say "NoSQL is faster"  it depends on the access pattern.`,"Justify your DB choice with the read/write ratio and consistency needs."],relatedIds:["acid-base","consistent-hashing","caching"],quizQuestions:[{question:"Which index structure is optimized for write-heavy workloads?",options:["B-tree","LSM-tree","Hash index","Bitmap index"],correctIndex:1,explanation:"LSM-trees batch writes to memory then flush  great for writes."}]},{id:"caching",title:"Caching",category:"building-blocks",description:"Speed up reads with in-memory data stores.",content:`**Levels:** Browser  CDN  API Gateway  Application  Database query cache.

**Strategies:**
- **Cache-Aside**  app checks cache, falls back to DB, populates cache
- **Read-Through**  cache loads from DB automatically
- **Write-Through**  write to cache + DB simultaneously
- **Write-Behind**  write to cache, async flush to DB

**Eviction:** LRU, LFU, TTL.

**Tools:** Redis, Memcached.`,tips:["Always discuss cache invalidation  it's the hard part.","Mention thundering herd: use locks or request coalescing."],relatedIds:["latency-throughput","databases","cdn"],quizQuestions:[{question:"In Cache-Aside, who is responsible for populating the cache?",options:["The cache itself","The database","The application","The CDN"],correctIndex:2,explanation:"In cache-aside, the application checks the cache and populates it on a miss."}]},{id:"message-queues",title:"Message Queues",category:"building-blocks",description:"Async communication between services.",content:`**Queue**  point-to-point, one consumer gets each message (SQS, RabbitMQ).
**Pub/Sub**  one message, many subscribers (Kafka, SNS).
**Event Streaming**  ordered, durable log (Kafka, Pulsar).

**Benefits:**
- Decouples producers & consumers
- Handles traffic spikes (buffering)
- Enables retry & dead-letter queues

**Guarantees:** At-most-once, at-least-once, exactly-once (hard!).`,tips:["Kafka for high-throughput ordered events; SQS for simple task queues.","Idempotent consumers handle at-least-once delivery safely."],relatedIds:["event-driven","microservices"],quizQuestions:[{question:"Which delivery guarantee is hardest to achieve?",options:["At-most-once","At-least-once","Exactly-once","Best-effort"],correctIndex:2,explanation:"Exactly-once requires coordination between producer, broker, and consumer."}]},{id:"cdn",title:"CDN",category:"building-blocks",description:"Content Delivery Networks  serve content from the edge.",content:`**Push CDN**  you upload content proactively.
**Pull CDN**  CDN fetches on first request, caches.

**Benefits:** Lower latency, reduced origin load, DDoS mitigation.

Edge locations worldwide. Cache static assets (images, JS, CSS) and even dynamic content (edge compute).

Examples: CloudFront, Cloudflare, Akamai, Fastly.`,tips:["Use pull CDN for most web apps; push for large static catalogs.","Mention cache invalidation strategies (versioned URLs, purge APIs)."],relatedIds:["caching","latency-throughput","load-balancers"],quizQuestions:[{question:"Which CDN type fetches content on the first request?",options:["Push CDN","Pull CDN","Hybrid CDN","Edge CDN"],correctIndex:1,explanation:"Pull CDNs lazily fetch and cache content on the first request."}]},{id:"consistent-hashing",title:"Consistent Hashing",category:"building-blocks",description:"Distribute data with minimal redistribution when nodes change.",content:`**Problem:** Simple hash(key) % N breaks when N changes  massive redistribution.

**Solution:** Hash ring. Nodes and keys mapped to a circle. Key goes to the next node clockwise.

**Virtual nodes**  each physical node gets multiple positions on the ring for better balance.

When a node joins/leaves, only K/N keys move (K=total keys, N=nodes).

Used by: DynamoDB, Cassandra, Memcached, Akamai.`,tips:["Always mention virtual nodes  without them, distribution is uneven.",'Great answer for "how would you partition this data?"'],relatedIds:["databases","scalability","caching"],quizQuestions:[{question:"What problem do virtual nodes solve?",options:["Network latency","Uneven data distribution","Cache invalidation","Leader election"],correctIndex:1,explanation:"Virtual nodes spread each physical node across the ring for balanced distribution."}]},{id:"microservices",title:"Microservices",category:"patterns",description:"Decompose into independently deployable services.",content:`**Monolith  Microservices:**
- Each service owns its data and logic
- Communicates via APIs or events
- Independently deployable and scalable

**Challenges:**
- Distributed transactions (Saga pattern)
- Service discovery
- Network latency & partial failures
- Data consistency across services

**When to use:** Large teams, different scaling needs per component, polyglot tech stacks.`,tips:["Start with a monolith, extract services when complexity demands it.","Mention the Saga pattern for cross-service transactions."],relatedIds:["message-queues","event-driven","api-gateway"],quizQuestions:[{question:"What pattern handles distributed transactions across microservices?",options:["Two-Phase Commit","Saga","CQRS","Circuit Breaker"],correctIndex:1,explanation:"Saga uses a sequence of local transactions with compensating actions."}]},{id:"event-driven",title:"Event-Driven Architecture",category:"patterns",description:"React to events instead of direct calls.",content:`**Components:**
- **Event Producer**  emits events (user clicked, order placed)
- **Event Broker**  routes events (Kafka, EventBridge)
- **Event Consumer**  reacts to events

**Patterns:**
- Event Notification  lightweight signal
- Event-Carried State Transfer  event contains full data
- Event Sourcing  store all events as the source of truth
- CQRS  separate read and write models

**Benefits:** Loose coupling, scalability, audit trail.`,tips:["Event sourcing + CQRS is powerful but complex  justify the complexity.","Idempotency is crucial in event-driven systems."],relatedIds:["message-queues","microservices","cqrs"],quizQuestions:[{question:"In Event Sourcing, what is the source of truth?",options:["Current state in DB","The event log","A snapshot","The cache"],correctIndex:1,explanation:"Event sourcing stores all state changes as an immutable event log."}]},{id:"cqrs",title:"CQRS",category:"patterns",description:"Separate read and write models for different optimization.",content:`**Command Query Responsibility Segregation:**
- **Write side**  validates and processes commands, stores events
- **Read side**  optimized projections/views for queries

The read model is eventually consistent with the write model.

**When useful:**
- Read and write workloads have very different patterns
- Need different data models for reading vs writing
- High read-to-write ratio

Often paired with Event Sourcing.`,tips:["CQRS adds complexity  only use when read/write patterns truly differ.","The read model can use a completely different database (e.g., Elasticsearch)."],relatedIds:["event-driven","databases"],quizQuestions:[{question:"What is the main benefit of CQRS?",options:["Stronger consistency","Independent optimization of reads and writes","Simpler codebase","Reduced storage"],correctIndex:1,explanation:"CQRS lets you optimize the read and write paths independently."}]},{id:"api-gateway",title:"API Gateway",category:"patterns",description:"Single entry point for all client requests.",content:`**Responsibilities:**
- Request routing to backend services
- Authentication & authorization
- Rate limiting & throttling
- Response aggregation (BFF pattern)
- SSL termination
- Request/response transformation
- Caching

**Examples:** Kong, AWS API Gateway, Zuul, Envoy.

**BFF (Backend for Frontend):** Separate gateways per client type (web, mobile, IoT).`,tips:["API Gateway can become a single point of failure  discuss redundancy.","BFF pattern is great when mobile and web need different response shapes."],relatedIds:["microservices","load-balancers","rate-limiting"],quizQuestions:[{question:"What does BFF stand for in the API Gateway context?",options:["Best Friend Forever","Backend for Frontend","Binary Format Framework","Buffered File Fetcher"],correctIndex:1,explanation:"BFF = Backend for Frontend  a gateway tailored to each client type."}]},{id:"rate-limiting",title:"Rate Limiting",category:"patterns",description:"Protect services from being overwhelmed.",content:`**Algorithms:**
- **Token Bucket**  tokens added at fixed rate; request costs a token
- **Leaky Bucket**  requests queue and drain at fixed rate
- **Fixed Window**  count requests per time window
- **Sliding Window Log**  track timestamps of recent requests
- **Sliding Window Counter**  hybrid of fixed window + log

**Where:** API Gateway, per-service, per-user, per-IP.

**Response:** HTTP 429 Too Many Requests + Retry-After header.`,tips:["Token bucket is the most commonly used  know it well.","Distributed rate limiting needs a shared store (Redis)."],relatedIds:["api-gateway","caching"],quizQuestions:[{question:"Which algorithm adds tokens at a fixed rate?",options:["Leaky Bucket","Fixed Window","Token Bucket","Sliding Log"],correctIndex:2,explanation:"Token Bucket refills tokens at a steady rate."}]},{id:"consistency-patterns",title:"Consistency Patterns",category:"patterns",description:"Strong, eventual, and causal consistency models.",content:`**Strong Consistency**  reads always return the latest write. Requires coordination (slower).

**Eventual Consistency**  reads may return stale data, but the system converges. Faster, more available.

**Causal Consistency**  operations that are causally related are seen in order. A sweet middle ground.

**Read-your-writes**  a user always sees their own updates.

**Monotonic reads**  you never see older data after seeing newer data.

Choose based on user expectations and business requirements.`,tips:["Most web apps need read-your-writes, not full strong consistency.",'Eventual consistency is fine for many use cases  quantify "how eventual".'],relatedIds:["cap-theorem","databases","cqrs"],quizQuestions:[{question:"Which consistency model guarantees you always see your own writes?",options:["Strong","Eventual","Read-your-writes","Causal"],correctIndex:2,explanation:"Read-your-writes ensures a user sees their own updates immediately."}]},{id:"design-url-shortener",title:"Design URL Shortener",category:"problems",description:"TinyURL / Bit.ly  generate short links and redirect.",content:`**Requirements:**
- Shorten a URL  7-char code
- Redirect short URL  original
- Analytics (click count)
- High read throughput (100:1 read:write)

**Key Decisions:**
- **ID Generation:** Base62 encoding of auto-increment ID, or MD5/SHA hash truncated
- **Storage:** Key-value store (DynamoDB) or SQL with index on short_code
- **Caching:** Redis for hot URLs (80/20 rule)
- **Scaling:** Stateless service + DB sharding by hash

**Back-of-envelope:** 100M URLs/month = ~40 writes/sec, 4000 reads/sec.`,tips:["Discuss collision handling if using hash truncation.","Mention custom aliases and expiration as features."],relatedIds:["databases","caching","consistent-hashing"],quizQuestions:[{question:"What encoding gives the shortest URL codes?",options:["Base16","Base36","Base62","Base64"],correctIndex:2,explanation:"Base62 (a-z, A-Z, 0-9) maximizes info per character without special chars."}]},{id:"design-chat-system",title:"Design Chat System",category:"problems",description:"WhatsApp / Slack  real-time messaging at scale.",content:`**Requirements:**
- 1:1 and group messaging
- Online/offline status
- Message history & search
- Push notifications
- Read receipts

**Architecture:**
- **WebSocket servers** for real-time delivery
- **Message queue** (Kafka) for reliable delivery
- **Chat storage:** Cassandra (write-heavy, time-series)
- **User presence:** Redis with TTL heartbeats
- **Push:** FCM/APNs for offline users

**Key challenge:** Ordering in group chats (vector clocks or server-assigned timestamps).`,tips:["Start with 1:1, then extend to groups.","Discuss how to handle the user being on multiple devices."],relatedIds:["message-queues","databases","networking-basics"],quizQuestions:[{question:"Which protocol is best for real-time chat?",options:["HTTP polling","Long polling","WebSockets","SMTP"],correctIndex:2,explanation:"WebSockets provide full-duplex persistent connections  ideal for chat."}]},{id:"design-newsfeed",title:"Design News Feed",category:"problems",description:"Facebook / Twitter feed  fanout and ranking.",content:`**Two approaches:**

**Fan-out on Write (Push):**
- When user posts, push to all followers' feeds
- Fast reads, slow writes
- Problem: celebrities with millions of followers

**Fan-out on Read (Pull):**
- Build feed at read time by querying followed users
- Slow reads, fast writes

**Hybrid:** Push for normal users, pull for celebrities.

**Ranking:** ML model scores posts by relevance, recency, engagement.

**Storage:** Feed cache in Redis (sorted set by timestamp/score).`,tips:["The hybrid approach is the expected answer.","Discuss the celebrity problem explicitly  shows depth."],relatedIds:["caching","message-queues","databases"],quizQuestions:[{question:'Which fanout model has the "celebrity problem"?',options:["Fan-out on Read","Fan-out on Write","Both equally","Neither"],correctIndex:1,explanation:"Fan-out on Write must push to millions of followers for celebrity posts."}]},{id:"design-rate-limiter",title:"Design Rate Limiter",category:"problems",description:"Build a distributed rate limiter service.",content:`**Requirements:**
- Limit requests per user/IP/API key
- Distributed (multiple servers)
- Low latency overhead
- Configurable rules

**Architecture:**
- **Rules engine**  load rules from config DB
- **Counter store**  Redis (INCR + EXPIRE, or sorted sets for sliding window)
- **Middleware**  intercepts requests before hitting the service

**Algorithm choice:** Token Bucket (flexible, allows bursts) or Sliding Window Counter (smoother).

**Race conditions:** Use Redis Lua scripts for atomic check-and-increment.`,tips:["Discuss where to place the limiter (client, server, middleware, API gateway).","Mention race conditions in distributed counters  Lua scripts solve it."],relatedIds:["rate-limiting","caching","api-gateway"],quizQuestions:[{question:"How to avoid race conditions in distributed rate limiting with Redis?",options:["Optimistic locking","Lua scripts","Mutex locks","Two-phase commit"],correctIndex:1,explanation:"Redis Lua scripts execute atomically  perfect for check-and-increment."}]}];xa.forEach(v=>{Ah[v.id]&&(v.quizQuestions=Ah[v.id])});const bf=[{from:"cap-theorem",to:"databases"},{from:"cap-theorem",to:"consistency-patterns"},{from:"acid-base",to:"databases"},{from:"networking-basics",to:"load-balancers"},{from:"networking-basics",to:"cdn"},{from:"scalability",to:"load-balancers"},{from:"scalability",to:"consistent-hashing"},{from:"latency-throughput",to:"caching"},{from:"latency-throughput",to:"cdn"},{from:"load-balancers",to:"api-gateway"},{from:"databases",to:"cqrs"},{from:"databases",to:"consistency-patterns"},{from:"caching",to:"rate-limiting"},{from:"message-queues",to:"event-driven"},{from:"message-queues",to:"microservices"},{from:"microservices",to:"design-chat-system"},{from:"event-driven",to:"design-newsfeed"},{from:"cqrs",to:"design-newsfeed"},{from:"rate-limiting",to:"design-rate-limiter"},{from:"consistency-patterns",to:"design-url-shortener"},{from:"api-gateway",to:"design-rate-limiter"},{from:"databases",to:"caching"},{from:"consistent-hashing",to:"databases"},{from:"caching",to:"design-url-shortener"},{from:"databases",to:"design-chat-system"},{from:"message-queues",to:"design-chat-system"},{from:"caching",to:"design-newsfeed"}],pl="sd-prep-progress";function vf(){try{const v=localStorage.getItem(pl);return v?JSON.parse(v):{}}catch{return{}}}function wf(v){localStorage.setItem(pl,JSON.stringify(v))}function xf(){const[v,N]=be.useState(vf);be.useEffect(()=>{wf(v)},[v]);const M=be.useCallback(P=>v[P]??"not-started",[v]),h=be.useCallback((P,K)=>{N(C=>({...C,[P]:K}))},[]),L=be.useCallback(()=>{N({}),localStorage.removeItem(pl)},[]),E=be.useCallback(()=>{const P={locked:0,"not-started":0,"in-progress":0,completed:0},K={fundamentals:{total:0,completed:0},"building-blocks":{total:0,completed:0},patterns:{total:0,completed:0},problems:{total:0,completed:0}};for(const C of xa){const y=v[C.id]??"not-started";P[y]++,K[C.category].total++,y==="completed"&&K[C.category].completed++}return{total:xa.length,byStatus:P,byCategory:K}},[v]);return{progress:v,getStatus:M,setStatus:h,resetAll:L,getStats:E}}const kf={fundamentals:{border:"border-blue-500/30",glow:"hover:shadow-blue-500/10",dot:"bg-blue-500"},"building-blocks":{border:"border-purple-500/30",glow:"hover:shadow-purple-500/10",dot:"bg-purple-500"},patterns:{border:"border-emerald-500/30",glow:"hover:shadow-emerald-500/10",dot:"bg-emerald-500"},problems:{border:"border-orange-500/30",glow:"hover:shadow-orange-500/10",dot:"bg-orange-500"}},Tf={locked:"bg-slate-600","not-started":"bg-slate-500","in-progress":"bg-amber-400",completed:"bg-emerald-400"},Sf={locked:"bg-slate-800/40 opacity-50","not-started":"bg-slate-800","in-progress":"bg-slate-800",completed:"bg-slate-800"};function Af({node:v,status:N,onClick:M}){const h=kf[v.category],L=N==="locked";return S.jsxs("button",{onClick:L?void 0:M,disabled:L,"data-node-id":v.id,className:`
        relative group flex items-center gap-2.5 px-4 py-3
        rounded-xl border transition-all duration-200 ease-out
        ${Sf[N]} ${h.border}
        ${L?"cursor-not-allowed":`cursor-pointer hover:scale-[1.03] hover:shadow-lg ${h.glow} hover:border-opacity-60`}
        min-w-[140px] max-w-[200px]
      `,children:[S.jsx("span",{className:`w-2.5 h-2.5 rounded-full shrink-0 ${Tf[N]} ${N==="in-progress"?"animate-pulse":""}`}),S.jsx("span",{className:`text-sm font-medium text-left leading-tight ${L?"text-slate-600":"text-slate-200"}`,children:v.title}),N==="completed"&&S.jsx("svg",{className:"w-4 h-4 text-emerald-400 shrink-0 ml-auto",fill:"currentColor",viewBox:"0 0 20 20",children:S.jsx("path",{fillRule:"evenodd",d:"M16.707 5.293a1 1 0 010 1.414l-8 8a1 1 0 01-1.414 0l-4-4a1 1 0 011.414-1.414L8 12.586l7.293-7.293a1 1 0 011.414 0z",clipRule:"evenodd"})})]})}function qf({questions:v}){const[N,M]=be.useState(0),[h,L]=be.useState(null),[E,P]=be.useState(0),[K,C]=be.useState(!1);if(v.length===0)return null;const y=v[N],Q=h!==null,U=we=>{Q||(L(we),we===y.correctIndex&&P(re=>re+1))},ie=()=>{N+1>=v.length?C(!0):(M(we=>we+1),L(null))},Pe=()=>{M(0),L(null),P(0),C(!1)};return K?S.jsx("div",{className:"mt-4 p-4 bg-slate-800/60 rounded-xl border border-slate-700/50",children:S.jsxs("div",{className:"text-center",children:[S.jsxs("div",{className:"text-2xl font-bold text-emerald-400 mb-1",children:[E,"/",v.length]}),S.jsx("div",{className:"text-sm text-slate-400 mb-3",children:E===v.length?" Perfect!":E>=v.length/2?" Good job!":" Keep studying!"}),S.jsx("button",{onClick:Pe,className:"px-4 py-2 bg-slate-700 hover:bg-slate-600 text-sm text-slate-200 rounded-lg transition-colors",children:"Try Again"})]})}):S.jsxs("div",{className:"mt-4 p-4 bg-slate-800/60 rounded-xl border border-slate-700/50",children:[S.jsxs("div",{className:"flex items-center justify-between mb-3",children:[S.jsx("span",{className:"text-xs font-medium text-slate-500 uppercase tracking-wider",children:"Quiz"}),S.jsxs("span",{className:"text-xs text-slate-500",children:[N+1,"/",v.length]})]}),S.jsx("p",{className:"text-sm text-slate-200 font-medium mb-3",children:y.question}),S.jsx("div",{className:"space-y-2",children:y.options.map((we,re)=>{let De="bg-slate-700/50 border-slate-600/50 text-slate-300 hover:bg-slate-700";return Q&&(re===y.correctIndex?De="bg-emerald-500/20 border-emerald-500/50 text-emerald-300":re===h?De="bg-red-500/20 border-red-500/50 text-red-300":De="bg-slate-700/30 border-slate-700/30 text-slate-500"),S.jsx("button",{onClick:()=>U(re),className:`w-full text-left px-3 py-2 rounded-lg border text-sm transition-all ${De}`,children:we},re)})}),Q&&S.jsxs(S.Fragment,{children:[S.jsx("p",{className:"mt-3 text-xs text-slate-400 leading-relaxed",children:y.explanation}),S.jsx("button",{onClick:ie,className:"mt-3 px-4 py-2 bg-blue-600 hover:bg-blue-500 text-sm text-white rounded-lg transition-colors",children:N+1>=v.length?"See Results":"Next Question"})]})]})}const qh={fundamentals:{label:"Fundamentals",bg:"bg-blue-500/20 text-blue-400"},"building-blocks":{label:"Building Blocks",bg:"bg-purple-500/20 text-purple-400"},patterns:{label:"Patterns",bg:"bg-emerald-500/20 text-emerald-400"},problems:{label:"Problems",bg:"bg-orange-500/20 text-orange-400"}},Cf=[{value:"not-started",label:"Not Started",active:"bg-slate-600 text-slate-200"},{value:"in-progress",label:"In Progress",active:"bg-amber-500/20 text-amber-300 ring-1 ring-amber-500/40"},{value:"completed",label:"Completed",active:"bg-emerald-500/20 text-emerald-300 ring-1 ring-emerald-500/40"}];function Rf(v){return v.split(`
`).map((N,M)=>{if(!N.trim())return S.jsx("br",{},M);const h=N.split(/(\*\*[^*]+\*\*)/g).map((L,E)=>L.startsWith("**")&&L.endsWith("**")?S.jsx("strong",{className:"text-slate-200",children:L.slice(2,-2)},E):L);return N.startsWith("- ")?S.jsx("li",{className:"ml-4 text-sm text-slate-400 leading-relaxed",children:h.slice(0)},M):S.jsx("p",{className:"text-sm text-slate-400 leading-relaxed",children:h},M)})}function Lf({node:v,status:N,onStatusChange:M,onClose:h,onNavigate:L,allNodes:E}){const P=be.useRef(null),K=v!==null;be.useEffect(()=>{const y=Q=>{Q.key==="Escape"&&h()};return K&&document.addEventListener("keydown",y),()=>document.removeEventListener("keydown",y)},[K,h]);const C=y=>{P.current&&!P.current.contains(y.target)&&h()};return S.jsxs(S.Fragment,{children:[S.jsx("div",{className:`fixed inset-0 bg-black/50 backdrop-blur-sm z-40 transition-opacity duration-200 ${K?"opacity-100":"opacity-0 pointer-events-none"}`,onClick:C}),S.jsx("div",{ref:P,className:`
          fixed z-50 bg-slate-900 border-l border-slate-700/50 shadow-2xl
          transition-transform duration-300 ease-out overflow-y-auto
          top-0 right-0 h-full
          w-full sm:w-[480px] md:w-[520px]
          ${K?"translate-x-0":"translate-x-full"}
        `,children:v&&S.jsxs("div",{className:"p-6 space-y-5",children:[S.jsxs("div",{className:"flex items-start justify-between gap-3",children:[S.jsxs("div",{className:"space-y-2",children:[S.jsx("span",{className:`inline-block px-2.5 py-1 rounded-md text-xs font-medium ${qh[v.category].bg}`,children:qh[v.category].label}),S.jsx("h2",{className:"text-xl font-bold text-slate-100",children:v.title}),S.jsx("p",{className:"text-sm text-slate-500",children:v.description})]}),S.jsx("button",{onClick:h,className:"p-2 rounded-lg hover:bg-slate-800 text-slate-500 hover:text-slate-300 transition-colors shrink-0",children:S.jsx("svg",{className:"w-5 h-5",fill:"none",stroke:"currentColor",viewBox:"0 0 24 24",children:S.jsx("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M6 18L18 6M6 6l12 12"})})})]}),S.jsx("div",{className:"flex gap-2",children:Cf.map(y=>S.jsx("button",{onClick:()=>M(y.value),className:`px-3 py-1.5 rounded-lg text-xs font-medium transition-all ${N===y.value?y.active:"bg-slate-800 text-slate-500 hover:text-slate-400"}`,children:y.label},y.value))}),S.jsx("div",{className:"space-y-1.5",children:Rf(v.content)}),v.tips.length>0&&S.jsxs("div",{className:"space-y-2",children:[S.jsx("h3",{className:"text-xs font-semibold text-slate-500 uppercase tracking-wider",children:" Tips"}),v.tips.map((y,Q)=>S.jsxs("div",{className:"flex gap-2 p-3 bg-amber-500/5 border border-amber-500/10 rounded-lg",children:[S.jsx("span",{className:"text-amber-400 shrink-0 text-sm",children:""}),S.jsx("span",{className:"text-sm text-amber-200/80 leading-relaxed",children:y})]},Q))]}),v.relatedIds.length>0&&S.jsxs("div",{children:[S.jsx("h3",{className:"text-xs font-semibold text-slate-500 uppercase tracking-wider mb-2",children:"Related Topics"}),S.jsx("div",{className:"flex flex-wrap gap-2",children:v.relatedIds.map(y=>{const Q=E.find(U=>U.id===y);return Q?S.jsx("button",{onClick:()=>L(y),className:"px-3 py-1.5 bg-slate-800 hover:bg-slate-700 text-xs text-slate-300 rounded-lg border border-slate-700/50 transition-colors",children:Q.title},y):null})})]}),v.quizQuestions.length>0&&S.jsx(qf,{questions:v.quizQuestions})]})})]})}const If={fundamentals:{label:"Fundamentals",color:"text-blue-400",bg:"bg-blue-500"},"building-blocks":{label:"Building Blocks",color:"text-purple-400",bg:"bg-purple-500"},patterns:{label:"Patterns",color:"text-emerald-400",bg:"bg-emerald-500"},problems:{label:"Problems",color:"text-orange-400",bg:"bg-orange-500"}};function Df({stats:v}){const[N,M]=be.useState(!1),h=v.byStatus.completed,L=v.total>0?h/v.total*100:0;return S.jsxs("div",{className:"w-full",children:[S.jsxs("button",{onClick:()=>M(E=>!E),className:"w-full text-left group",children:[S.jsxs("div",{className:"flex items-center justify-between mb-1.5",children:[S.jsxs("span",{className:"text-sm font-medium text-slate-300",children:[h," of ",v.total," completed"]}),S.jsx("span",{className:"text-xs text-slate-500 group-hover:text-slate-400 transition-colors",children:N?"Hide":"Details"})]}),S.jsx("div",{className:"h-2.5 bg-slate-700 rounded-full overflow-hidden",children:S.jsx("div",{className:"h-full bg-gradient-to-r from-emerald-500 to-emerald-400 rounded-full transition-all duration-500 ease-out",style:{width:`${L}%`}})})]}),N&&S.jsx("div",{className:"mt-3 grid grid-cols-2 sm:grid-cols-4 gap-2",children:Object.entries(If).map(([E,P])=>{const K=v.byCategory[E],C=K.total>0?K.completed/K.total*100:0;return S.jsxs("div",{className:"bg-slate-800/60 rounded-lg p-2.5 border border-slate-700/50",children:[S.jsx("div",{className:`text-xs font-medium ${P.color} mb-1`,children:P.label}),S.jsxs("div",{className:"text-xs text-slate-400 mb-1.5",children:[K.completed,"/",K.total]}),S.jsx("div",{className:"h-1.5 bg-slate-700 rounded-full overflow-hidden",children:S.jsx("div",{className:`h-full ${P.bg} rounded-full transition-all duration-500`,style:{width:`${C}%`}})})]},E)})})]})}const Uf=[{value:"all",label:"All",color:"bg-slate-600"},{value:"fundamentals",label:"Fundamentals",color:"bg-blue-500"},{value:"building-blocks",label:"Building Blocks",color:"bg-purple-500"},{value:"patterns",label:"Patterns",color:"bg-emerald-500"},{value:"problems",label:"Problems",color:"bg-orange-500"}],zf=[{value:"all",label:"All"},{value:"not-started",label:"Not Started"},{value:"in-progress",label:"In Progress"},{value:"completed",label:"Completed"}];function Pf({search:v,onSearchChange:N,categoryFilter:M,onCategoryChange:h,statusFilter:L,onStatusChange:E}){return S.jsxs("div",{className:"space-y-3",children:[S.jsxs("div",{className:"relative",children:[S.jsx("svg",{className:"absolute left-3 top-1/2 -translate-y-1/2 w-4 h-4 text-slate-500",fill:"none",stroke:"currentColor",viewBox:"0 0 24 24",children:S.jsx("path",{strokeLinecap:"round",strokeLinejoin:"round",strokeWidth:2,d:"M21 21l-6-6m2-5a7 7 0 11-14 0 7 7 0 0114 0z"})}),S.jsx("input",{type:"text",value:v,onChange:P=>N(P.target.value),placeholder:"Search topics...",className:"w-full pl-10 pr-4 py-2.5 bg-slate-800 border border-slate-700 rounded-xl text-sm text-slate-200 placeholder-slate-500 focus:outline-none focus:ring-2 focus:ring-blue-500/40 focus:border-blue-500/40 transition-all"})]}),S.jsxs("div",{className:"flex flex-wrap gap-2",children:[S.jsx("div",{className:"flex flex-wrap gap-1.5",children:Uf.map(P=>S.jsx("button",{onClick:()=>h(P.value),className:`px-3 py-1.5 rounded-lg text-xs font-medium transition-all ${M===P.value?`${P.color} text-white shadow-lg`:"bg-slate-800 text-slate-400 hover:bg-slate-700 hover:text-slate-300"}`,children:P.label},P.value))}),S.jsx("div",{className:"w-px bg-slate-700 mx-1 hidden sm:block"}),S.jsx("div",{className:"flex flex-wrap gap-1.5",children:zf.map(P=>S.jsx("button",{onClick:()=>E(P.value),className:`px-3 py-1.5 rounded-lg text-xs font-medium transition-all ${L===P.value?"bg-slate-600 text-white":"bg-slate-800 text-slate-400 hover:bg-slate-700 hover:text-slate-300"}`,children:P.label},P.value))})]})]})}const Ef=[{category:"fundamentals",label:"Fundamentals",color:"text-blue-400",accent:"border-blue-500/30"},{category:"building-blocks",label:"Building Blocks",color:"text-purple-400",accent:"border-purple-500/30"},{category:"patterns",label:"Patterns",color:"text-emerald-400",accent:"border-emerald-500/30"},{category:"problems",label:"Problems",color:"text-orange-400",accent:"border-orange-500/30"}];function Wf(){const{getStatus:v,setStatus:N,getStats:M}=xf(),[h,L]=be.useState(null),[E,P]=be.useState(""),[K,C]=be.useState("all"),[y,Q]=be.useState("all"),U=be.useRef(null),[ie,Pe]=be.useState({}),we=M(),re=be.useMemo(()=>new Set(xa.filter(B=>!(E&&!B.title.toLowerCase().includes(E.toLowerCase())||K!=="all"&&B.category!==K||y!=="all"&&v(B.id)!==y)).map(B=>B.id)),[E,K,y,v]),De=be.useMemo(()=>{const B={fundamentals:[],"building-blocks":[],patterns:[],problems:[]};for(const pe of xa)B[pe.category].push(pe);return B},[]);be.useEffect(()=>{const B=()=>{if(!U.current)return;const le=U.current.getBoundingClientRect(),Ee={};for(const Y of xa){const We=U.current.querySelector(`[data-node-id="${Y.id}"]`);if(We){const _e=We.getBoundingClientRect();Ee[Y.id]={x:_e.left-le.left+_e.width/2,y:_e.top-le.top+_e.height/2}}}Pe(Ee)},pe=setTimeout(B,100);return window.addEventListener("resize",B),()=>{clearTimeout(pe),window.removeEventListener("resize",B)}},[re]);const je=be.useCallback(B=>{const pe=xa.find(le=>le.id===B);pe&&L(pe)},[]),At=be.useMemo(()=>bf.filter(B=>re.has(B.from)&&re.has(B.to)),[re]);return S.jsxs("div",{className:"min-h-screen bg-slate-900",children:[S.jsxs("div",{className:"max-w-7xl mx-auto px-4 sm:px-6 py-6 space-y-6",children:[S.jsxs("div",{className:"space-y-1",children:[S.jsx("h1",{className:"text-2xl sm:text-3xl font-bold text-slate-100",children:"System Design Roadmap"}),S.jsx("p",{className:"text-sm text-slate-500",children:"Master system design concepts, patterns, and problems"})]}),S.jsx(Df,{stats:we}),S.jsx(Pf,{search:E,onSearchChange:P,categoryFilter:K,onCategoryChange:C,statusFilter:y,onStatusChange:Q}),S.jsxs("div",{ref:U,className:"relative",children:[S.jsxs("svg",{className:"absolute inset-0 w-full h-full pointer-events-none z-0",style:{overflow:"visible"},children:[S.jsx("defs",{children:S.jsx("marker",{id:"arrowhead",markerWidth:"8",markerHeight:"6",refX:"8",refY:"3",orient:"auto",children:S.jsx("polygon",{points:"0 0, 8 3, 0 6",fill:"#475569"})})}),At.map(B=>{const pe=ie[B.from],le=ie[B.to];return!pe||!le?null:S.jsx("line",{x1:pe.x,y1:pe.y,x2:le.x,y2:le.y,stroke:"#334155",strokeWidth:1.5,strokeDasharray:"4 4",markerEnd:"url(#arrowhead)",opacity:.5},`${B.from}-${B.to}`)})]}),S.jsx("div",{className:"space-y-6 relative z-10",children:Ef.map(B=>{const pe=De[B.category].filter(le=>re.has(le.id));return pe.length===0?null:S.jsxs("div",{className:`rounded-2xl border ${B.accent} bg-slate-800/20 p-4 sm:p-5`,children:[S.jsxs("div",{className:"flex items-center gap-3 mb-4",children:[S.jsx("div",{className:`w-1 h-6 rounded-full ${B.color.replace("text-","bg-")}`}),S.jsx("h2",{className:`text-sm font-semibold uppercase tracking-wider ${B.color}`,children:B.label}),S.jsxs("span",{className:"text-xs text-slate-600",children:[De[B.category].filter(le=>v(le.id)==="completed").length,"/",De[B.category].length]})]}),S.jsx("div",{className:"flex flex-wrap gap-3",children:pe.map(le=>S.jsx(Af,{node:le,status:v(le.id),onClick:()=>L(le)},le.id))})]},B.category)})})]})]}),S.jsx(Lf,{node:h,status:h?v(h.id):"not-started",onStatusChange:B=>{h&&N(h.id,B)},onClose:()=>L(null),onNavigate:je,allNodes:xa})]})}function Bf(){return S.jsx("div",{className:"min-h-screen bg-slate-900 text-slate-100",children:S.jsx(Wf,{})})}cf.createRoot(document.getElementById("root")).render(S.jsx(ef.StrictMode,{children:S.jsx(Bf,{})}));
