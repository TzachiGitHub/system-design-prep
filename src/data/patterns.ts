export interface RealWorldExample {
  company: string;
  useCase: string;
}

export interface Tradeoffs {
  pros: string[];
  cons: string[];
}

export interface Pattern {
  id: string;
  name: string;
  emoji: string;
  category: string;
  summary: string;
  diagram: string;
  whenToUse: string[];
  whenNotToUse: string[];
  realWorldExamples: RealWorldExample[];
  tradeoffs: Tradeoffs;
  interviewBuzzwords: string[];
  keyInsight: string;
}

export const patterns: Pattern[] = [
  {
    id: 'monolithic',
    name: 'Monolithic Architecture',
    emoji: 'ğŸ›ï¸',
    category: 'Application Architecture',
    summary: 'A single deployable unit containing all application logic â€” UI, business logic, and data access â€” in one codebase and one process.',
    diagram: `
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         MONOLITH                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  UI   â”‚ â”‚ Biz   â”‚ â”‚ Data â”‚  â”‚
â”‚  â”‚ Layer â”‚ â”‚ Logic  â”‚ â”‚Accessâ”‚  â”‚
â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â””â”€â”€â”¬â”€â”€â”€â”˜  â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚         Single Process          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
        â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”
        â”‚   DB    â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    `,
    whenToUse: [
      'Early-stage startups with small teams (< 10 engineers)',
      'Simple domain with limited bounded contexts',
      'MVP / proof-of-concept where speed-to-market matters most',
      'When team lacks DevOps expertise for distributed systems',
      'Low traffic applications (< 1000 RPS)',
    ],
    whenNotToUse: [
      'Multiple teams need to deploy independently',
      'Different components have vastly different scaling needs',
      'You need polyglot technology stacks',
      'Application is large (> 500K LOC) and hard to reason about',
      'Fault isolation is critical â€” one bug crashes everything',
    ],
    realWorldExamples: [
      { company: 'Basecamp', useCase: 'Rails monolith serving their entire product, famously defended by DHH' },
      { company: 'Stack Overflow', useCase: 'Serves 1.3B page views/month from a monolithic .NET app on just a few servers' },
      { company: 'Shopify', useCase: 'One of the largest Rails monoliths, modularized with components but still single deploy' },
      { company: 'Etsy', useCase: 'Ran a PHP monolith for years, deploying 50+ times/day with good CI/CD' },
    ],
    tradeoffs: {
      pros: [
        'Simple to develop, test, and debug â€” single codebase, single debugger',
        'Easy deployment â€” one artifact, one process to manage',
        'Low latency between components â€” in-process function calls, no network hops',
        'ACID transactions are straightforward â€” single database, no distributed transactions',
        'Simple IDE experience â€” refactoring, find-references, go-to-definition all work',
        'Lower infrastructure cost â€” no service mesh, API gateway, or container orchestration needed',
      ],
      cons: [
        'Tight coupling â€” changes in one module can break others',
        'Scaling is all-or-nothing â€” can\'t scale hot components independently',
        'Long build/deploy times as codebase grows',
        'Technology lock-in â€” entire app must use same language/framework',
        'Single point of failure â€” one memory leak or crash kills everything',
        'Onboarding new developers becomes harder as complexity grows',
      ],
    },
    interviewBuzzwords: [
      'single deployable unit', 'shared memory', 'vertical scaling', 'modular monolith',
      'big ball of mud', 'deployment coupling', 'shared database',
    ],
    keyInsight: 'Don\'t dismiss monoliths! Many successful companies run monoliths at scale. The key insight for interviews: start with a monolith and extract services when you have a clear reason â€” premature microservices are a common anti-pattern.',
  },
  {
    id: 'microservices',
    name: 'Microservices',
    emoji: 'ğŸ”¬',
    category: 'Application Architecture',
    summary: 'Application decomposed into small, independently deployable services, each owning its own data and communicating via APIs or messaging.',
    diagram: `
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Service Aâ”‚  â”‚ Service Bâ”‚  â”‚ Service Câ”‚
â”‚  (Users) â”‚  â”‚ (Orders) â”‚  â”‚(Payments)â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
     â”‚             â”‚              â”‚
â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”
â”‚  DB A  â”‚   â”‚  DB B  â”‚    â”‚  DB C  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚             â”‚              â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            Message Bus / API Gateway
    `,
    whenToUse: [
      'Large teams (> 20 engineers) that need independent deployment',
      'Different components have different scaling requirements (e.g., search vs checkout)',
      'Need for polyglot tech â€” ML team uses Python, backend uses Go',
      'Organization follows Conway\'s Law with clear team boundaries',
      'High availability required â€” fault isolation between services',
      'Rapid iteration needed on specific features without full regression',
    ],
    whenNotToUse: [
      'Small team (< 5 engineers) â€” overhead will slow you down',
      'Unclear domain boundaries â€” you\'ll get distributed monolith',
      'Startup MVP â€” you don\'t know your domain well enough yet',
      'Strong consistency requirements across services (distributed transactions are hard)',
      'No DevOps maturity â€” need CI/CD, monitoring, service discovery',
    ],
    realWorldExamples: [
      { company: 'Netflix', useCase: '1000+ microservices handling 200M+ subscribers, each service independently deployable' },
      { company: 'Uber', useCase: '4000+ microservices, domain-oriented platform with service mesh (moved from monolith)' },
      { company: 'Amazon', useCase: 'Famously mandated service-oriented architecture in 2002 (Bezos API mandate)' },
      { company: 'Spotify', useCase: 'Squad-based microservices aligned with autonomous team structure' },
      { company: 'Twitter', useCase: 'Decomposed Ruby monolith into JVM microservices to handle scale' },
    ],
    tradeoffs: {
      pros: [
        'Independent deployment â€” ship features without coordinating with other teams',
        'Independent scaling â€” scale only what\'s hot (e.g., search service during Black Friday)',
        'Fault isolation â€” one service crashing doesn\'t take down the system',
        'Technology flexibility â€” use the best tool for each job',
        'Team autonomy â€” each team owns their service end-to-end',
        'Easier to understand each individual service (small codebase)',
      ],
      cons: [
        'Distributed system complexity â€” network failures, partial failures, eventual consistency',
        'Data consistency is hard â€” no ACID across services, need sagas or eventual consistency',
        'Operational overhead â€” monitoring, logging, tracing, deployment pipelines per service',
        'Network latency â€” inter-service calls add milliseconds vs in-process calls',
        'Testing complexity â€” integration tests require running multiple services',
        'Debugging is harder â€” distributed tracing needed (Jaeger, Zipkin)',
      ],
    },
    interviewBuzzwords: [
      'bounded context', 'service discovery', 'API gateway', 'circuit breaker',
      'saga pattern', 'distributed tracing', 'eventual consistency', 'database per service',
      'Conway\'s Law', 'independently deployable',
    ],
    keyInsight: 'In interviews, always mention: "Each microservice owns its data." If two services share a database, you have a distributed monolith, not microservices. Also: microservices are an organizational pattern as much as a technical one.',
  },
  {
    id: 'event-driven',
    name: 'Event-Driven / Pub-Sub',
    emoji: 'ğŸ“¡',
    category: 'Communication Pattern',
    summary: 'Components communicate by producing and consuming events through a message broker, enabling loose coupling and asynchronous processing.',
    diagram: `
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Producer â”‚â”€â”€â”€â”€â–¶â”‚   Message Broker  â”‚â”€â”€â”€â”€â–¶â”‚Consumer Aâ”‚
â”‚(Order Svc)â”‚    â”‚  (Kafka/RabbitMQ) â”‚     â”‚(Email Svc)â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚                   â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚  Topic: "orders"  â”‚     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚                   â”‚â”€â”€â”€â”€â–¶â”‚Consumer Bâ”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚(Analytics)â”‚
                                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    `,
    whenToUse: [
      'Decoupling producers from consumers â€” producer doesn\'t need to know who processes events',
      'Asynchronous workflows (order placed â†’ send email â†’ update inventory â†’ notify shipping)',
      'Real-time data streaming and analytics (clickstream, IoT sensor data)',
      'When you need to fan-out one event to multiple consumers',
      'Event replay capability needed (audit logs, rebuilding state)',
    ],
    whenNotToUse: [
      'Simple request-response needed with immediate result',
      'Strong consistency required â€” eventual consistency is inherent',
      'Simple CRUD app with no complex workflows',
      'Team unfamiliar with async debugging and eventual consistency',
      'Low throughput â€” overhead of broker isn\'t justified',
    ],
    realWorldExamples: [
      { company: 'LinkedIn', useCase: 'Created Apache Kafka; processes 7 trillion messages/day for activity feeds, metrics, and data pipelines' },
      { company: 'Netflix', useCase: 'Event-driven data pipeline for recommendations, A/B testing, and real-time analytics' },
      { company: 'Uber', useCase: 'Real-time trip events flow through Kafka for pricing, ETA, matching, and analytics' },
      { company: 'Walmart', useCase: 'Order processing pipeline: order placed â†’ payment â†’ inventory â†’ shipping are all events' },
    ],
    tradeoffs: {
      pros: [
        'Loose coupling â€” producers and consumers are independent, can evolve separately',
        'Scalability â€” consumers can be scaled independently based on load',
        'Resilience â€” if a consumer is down, messages queue up and are processed later',
        'Auditability â€” event log provides natural audit trail',
        'Extensibility â€” add new consumers without modifying producers',
        'Natural backpressure handling â€” consumers process at their own pace',
      ],
      cons: [
        'Eventual consistency â€” no immediate confirmation of processing',
        'Debugging complexity â€” hard to trace a request across async boundaries',
        'Message ordering challenges â€” especially across partitions',
        'Duplicate messages â€” need idempotent consumers (at-least-once delivery)',
        'Increased infrastructure complexity â€” need to manage broker (Kafka, RabbitMQ)',
        'Event schema evolution is tricky â€” backward/forward compatibility needed',
      ],
    },
    interviewBuzzwords: [
      'pub/sub', 'event bus', 'message broker', 'fan-out', 'at-least-once delivery',
      'exactly-once semantics', 'dead letter queue', 'backpressure', 'event replay',
      'idempotency', 'topic', 'partition',
    ],
    keyInsight: 'Always mention idempotency in interviews. Since messages can be delivered more than once, every consumer must handle duplicates safely. Use an idempotency key (e.g., order_id) to deduplicate.',
  },
  {
    id: 'cqrs',
    name: 'CQRS',
    emoji: 'âœ‚ï¸',
    category: 'Data Pattern',
    summary: 'Command Query Responsibility Segregation â€” separate models for reading and writing data, allowing each to be optimized independently.',
    diagram: `
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Client   â”‚
              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
     â”‚  Command   â”‚     â”‚  Query   â”‚
     â”‚  (Write)   â”‚     â”‚  (Read)  â”‚
     â”‚  Service   â”‚     â”‚  Service â”‚
     â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”˜
           â”‚                 â”‚
     â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
     â”‚ Write DB  â”‚â”€â”€â”€â”€â–¶â”‚ Read DB  â”‚
     â”‚(Normalized)â”‚syncâ”‚(Denorml.)â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    `,
    whenToUse: [
      'Read-heavy systems (read:write ratio > 10:1)',
      'Complex querying needs different from write model (e.g., search, reporting)',
      'Need to scale reads and writes independently',
      'Different consistency requirements for reads vs writes',
      'Used with Event Sourcing for event-driven read model projections',
    ],
    whenNotToUse: [
      'Simple CRUD where read and write models are identical',
      'Small applications â€” adds unnecessary complexity',
      'Strong consistency required on reads immediately after writes',
      'Team lacks experience with eventual consistency patterns',
    ],
    realWorldExamples: [
      { company: 'Microsoft', useCase: 'Azure services use CQRS internally; major proponents of the pattern' },
      { company: 'Stack Overflow', useCase: 'Read-optimized denormalized tables for fast page loads, separate write path' },
      { company: 'Walmart', useCase: 'Product catalog: writes go to normalized store, reads from denormalized search-optimized store' },
    ],
    tradeoffs: {
      pros: [
        'Independent scaling â€” scale read replicas without affecting write performance',
        'Optimized models â€” read model can be denormalized for fast queries',
        'Separation of concerns â€” write validation logic separate from read projections',
        'Better performance â€” each side tuned for its workload',
        'Flexibility â€” read model can use different storage (Elasticsearch for search, Redis for cache)',
      ],
      cons: [
        'Increased complexity â€” two models to maintain instead of one',
        'Eventual consistency â€” read model may lag behind writes',
        'Data synchronization overhead â€” need mechanism to keep read model updated',
        'More code to write and test â€” projections, sync logic, event handlers',
        'Debugging complexity â€” issues may be in write path, read path, or sync',
      ],
    },
    interviewBuzzwords: [
      'read model', 'write model', 'projection', 'materialized view', 'read replica',
      'denormalization', 'command handler', 'query handler', 'eventual consistency',
    ],
    keyInsight: 'CQRS shines when read and write workloads are very different. In an interview, a great example: Twitter timeline. Writes (tweets) go to a normalized store. Reads (timeline) are served from a pre-computed, denormalized fan-out cache.',
  },
  {
    id: 'event-sourcing',
    name: 'Event Sourcing',
    emoji: 'ğŸ“œ',
    category: 'Data Pattern',
    summary: 'Store all state changes as an immutable sequence of events rather than just the current state. The current state is derived by replaying events.',
    diagram: `
  Event Store (Append-Only Log)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ E1: AccountCreated(id=1, balance=0)      â”‚
  â”‚ E2: MoneyDeposited(id=1, amount=100)     â”‚
  â”‚ E3: MoneyWithdrawn(id=1, amount=30)      â”‚
  â”‚ E4: MoneyDeposited(id=1, amount=50)      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ Replay â”€â”€â–¶ Current State: balance = 120
           â”‚
           â–¼ Project â”€â”€â–¶ Read Models (CQRS)
    `,
    whenToUse: [
      'Audit trail is critical (finance, healthcare, legal)',
      'Need to reconstruct past states ("What was the account balance on March 5?")',
      'Complex business domains where state transitions matter (order lifecycle)',
      'Event replay for debugging â€” reproduce bugs by replaying events',
      'Combined with CQRS for powerful read projections',
    ],
    whenNotToUse: [
      'Simple CRUD apps â€” massive overkill',
      'When current state is all that matters',
      'High-frequency updates on same entity (event log grows fast)',
      'Team unfamiliar with event-driven patterns',
      'When you need simple ad-hoc queries (querying event streams is complex)',
    ],
    realWorldExamples: [
      { company: 'Banking/Finance', useCase: 'Bank ledgers are the original event source â€” every transaction is recorded, balance is derived' },
      { company: 'EventStore (Greg Young)', useCase: 'Purpose-built event sourcing database used by many financial institutions' },
      { company: 'Walmart', useCase: 'Order lifecycle events from placed â†’ paid â†’ shipped â†’ delivered for full traceability' },
    ],
    tradeoffs: {
      pros: [
        'Complete audit trail â€” every change is recorded and immutable',
        'Time travel â€” reconstruct state at any point in time',
        'Event replay â€” rebuild read models, fix bugs by replaying corrected logic',
        'Natural fit for event-driven architecture',
        'Debugging â€” replay events to reproduce issues exactly',
      ],
      cons: [
        'Complexity â€” fundamentally different from CRUD thinking',
        'Event schema evolution is hard â€” old events still need to be readable',
        'Storage growth â€” events accumulate (mitigated by snapshots)',
        'Eventual consistency â€” current state may lag behind latest events',
        'Querying â€” can\'t easily query current state without projections',
        'Learning curve â€” team needs to think in events, not state mutations',
      ],
    },
    interviewBuzzwords: [
      'append-only log', 'event replay', 'snapshot', 'projection', 'temporal query',
      'immutable events', 'event versioning', 'aggregate', 'domain events',
    ],
    keyInsight: 'Use the banking analogy in interviews: "A bank doesn\'t store your balance and overwrite it â€” it stores every transaction and computes the balance. That\'s event sourcing." Mention snapshots to address the performance concern of replaying millions of events.',
  },
  {
    id: 'serverless',
    name: 'Serverless / FaaS',
    emoji: 'â˜ï¸',
    category: 'Deployment Pattern',
    summary: 'Run code in ephemeral, event-triggered functions managed by a cloud provider. No server provisioning â€” you pay only for execution time.',
    diagram: `
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ API GW  â”‚â”€â”€â”€â–¶â”‚ Lambda / â”‚â”€â”€â”€â–¶â”‚  DynamoDBâ”‚
  â”‚ / Event â”‚    â”‚ Cloud Fn â”‚    â”‚  / S3    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â–²              â”‚
  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”    Auto-scales
  â”‚ Trigger â”‚    0 to N instances
  â”‚(HTTP/S3/â”‚
  â”‚ Queue)  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    `,
    whenToUse: [
      'Unpredictable or spiky traffic â€” auto-scales from 0 to thousands',
      'Event-driven processing (S3 upload â†’ thumbnail, SQS message â†’ process)',
      'APIs with variable load â€” pay-per-invocation is cheaper at low traffic',
      'Rapid prototyping â€” no infrastructure to manage',
      'Scheduled tasks / cron jobs',
    ],
    whenNotToUse: [
      'Long-running processes (> 15 min Lambda limit)',
      'Low-latency requirements â€” cold starts add 100ms-10s',
      'Stateful applications â€” functions are ephemeral',
      'High, consistent traffic â€” dedicated servers are cheaper',
      'Complex local development/debugging needs',
      'Vendor lock-in is a concern (tight cloud coupling)',
    ],
    realWorldExamples: [
      { company: 'Netflix', useCase: 'Media encoding pipeline triggered by S3 uploads, scales to thousands of concurrent functions' },
      { company: 'iRobot', useCase: 'Roomba IoT events processed by Lambda for telemetry and fleet management' },
      { company: 'Coca-Cola', useCase: 'Vending machine backends on serverless â€” handles spiky, unpredictable demand' },
      { company: 'Stripe', useCase: 'Webhook processing via serverless functions for event-driven payment workflows' },
    ],
    tradeoffs: {
      pros: [
        'Zero server management â€” no patching, no capacity planning',
        'Auto-scaling from 0 to infinity â€” true elasticity',
        'Pay-per-use â€” no cost when idle, great for variable workloads',
        'Fast time-to-market â€” focus on code, not infrastructure',
        'Built-in high availability and fault tolerance',
      ],
      cons: [
        'Cold starts â€” first invocation can be slow (100ms to 10s depending on runtime)',
        'Execution time limits (AWS Lambda: 15 min, Cloud Functions: 9 min)',
        'Vendor lock-in â€” tightly coupled to cloud provider APIs',
        'Limited local debugging and testing capabilities',
        'Stateless â€” no in-memory state between invocations',
        'Cost can spike unpredictably with high, sustained traffic',
      ],
    },
    interviewBuzzwords: [
      'FaaS', 'cold start', 'warm start', 'ephemeral compute', 'event-triggered',
      'pay-per-invocation', 'stateless', 'vendor lock-in', 'provisioned concurrency',
    ],
    keyInsight: 'In interviews, mention cold starts proactively and how to mitigate them: provisioned concurrency, keep-alive pings, or choosing lightweight runtimes. This shows you understand real-world tradeoffs, not just theory.',
  },
  {
    id: 'layered',
    name: 'Layered (N-Tier)',
    emoji: 'ğŸ°',
    category: 'Application Architecture',
    summary: 'Application organized into horizontal layers (presentation, business, data access), each depending only on the layer directly below it.',
    diagram: `
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Presentation Layer  â”‚  (UI, Controllers, API)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Business Layer     â”‚  (Domain logic, rules)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Data Access Layer   â”‚  (Repositories, ORM)
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Database Layer     â”‚  (SQL, NoSQL)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    Each layer only calls the layer below
    `,
    whenToUse: [
      'Traditional web applications with clear separation of concerns',
      'Enterprise applications where teams own different layers',
      'When you want a well-understood, conventional architecture',
      'Applications with standard CRUD operations',
    ],
    whenNotToUse: [
      'Highly interactive, real-time applications',
      'When layers become pass-through (adding latency without value)',
      'Microservices â€” each service is usually too small for layers',
      'When cross-cutting concerns dominate (logging, auth, caching)',
    ],
    realWorldExamples: [
      { company: 'Traditional Enterprise', useCase: 'Most Java/Spring or .NET enterprise apps follow N-tier (Controller â†’ Service â†’ Repository)' },
      { company: 'Django', useCase: 'MTV (Model-Template-View) framework naturally enforces layered architecture' },
      { company: 'Ruby on Rails', useCase: 'MVC pattern is a variant of layered architecture (Model â†’ Controller â†’ View)' },
    ],
    tradeoffs: {
      pros: [
        'Simple to understand â€” clear separation of concerns',
        'Well-known pattern â€” easy to onboard new developers',
        'Testable â€” can mock each layer for unit testing',
        'Layer independence â€” can swap implementations (e.g., change ORM)',
      ],
      cons: [
        'Can become a "big ball of mud" if layer boundaries aren\'t enforced',
        'Performance overhead â€” requests pass through all layers even when unnecessary',
        'Monolithic tendency â€” often deployed as a single unit',
        'Rigid â€” changes often cascade across multiple layers',
        'Pass-through layers add complexity without value',
      ],
    },
    interviewBuzzwords: [
      'separation of concerns', 'MVC', 'dependency inversion', 'layer isolation',
      'horizontal partitioning', 'data access layer',
    ],
    keyInsight: 'In interviews, mention that layered architecture is often the starting point, but it\'s important to enforce boundaries. Without discipline, layers leak and you get a "smart controller" anti-pattern where the presentation layer contains business logic.',
  },
  {
    id: 'hexagonal',
    name: 'Hexagonal (Ports & Adapters)',
    emoji: 'â¬¡',
    category: 'Application Architecture',
    summary: 'Core business logic is isolated at the center, interacting with the outside world through ports (interfaces) and adapters (implementations). Dependencies point inward.',
    diagram: `
            â”Œâ”€â”€â”€â”€ Adapters â”€â”€â”€â”€â”
            â”‚                  â”‚
  REST â”€â”€â”€â”€â”€â”¤   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”œâ”€â”€â”€â”€â”€ PostgreSQL
  API       â”‚   â”‚   Core   â”‚  â”‚      Adapter
            â”‚   â”‚ Business â”‚  â”‚
  GraphQL â”€â”€â”¤   â”‚  Logic   â”‚  â”œâ”€â”€â”€â”€â”€ Redis
  Adapter   â”‚   â”‚ (Ports)  â”‚  â”‚      Adapter
            â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
  CLI â”€â”€â”€â”€â”€â”€â”¤                  â”œâ”€â”€â”€â”€â”€ S3
  Adapter   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      Adapter

     Driving Adapters â”€â”€â”€â–¶ Core â—€â”€â”€â”€ Driven Adapters
    `,
    whenToUse: [
      'Domain logic is complex and needs protection from infrastructure changes',
      'Need to support multiple input channels (REST, GraphQL, CLI, events)',
      'Want to swap infrastructure (change DB, cache, or queue) without touching business logic',
      'Emphasis on testability â€” mock all external dependencies via ports',
    ],
    whenNotToUse: [
      'Simple CRUD â€” too much abstraction for basic operations',
      'Prototype/MVP where speed matters more than architecture',
      'Small services that don\'t have complex domain logic',
    ],
    realWorldExamples: [
      { company: 'Netflix', useCase: 'Hexagonal architecture in core platform services to swap implementations' },
      { company: 'Alistair Cockburn', useCase: 'Originally proposed the pattern; widely adopted in DDD communities' },
      { company: 'Enterprise Java', useCase: 'Spring applications using ports/adapters with dependency injection' },
    ],
    tradeoffs: {
      pros: [
        'Testability â€” core logic tested without any infrastructure',
        'Flexibility â€” swap databases, APIs, or messaging without changing business logic',
        'Domain focus â€” business logic is pure, not polluted by framework concerns',
        'Multiple entry points â€” same logic accessible via REST, CLI, events, tests',
      ],
      cons: [
        'More interfaces and abstractions â€” more code to write',
        'Learning curve â€” developers unfamiliar with DDD may struggle',
        'Over-engineering risk for simple applications',
        'Indirection â€” more hops to trace through code',
      ],
    },
    interviewBuzzwords: [
      'ports and adapters', 'dependency inversion', 'clean architecture', 'domain-driven design',
      'driving adapters', 'driven adapters', 'use case boundary',
    ],
    keyInsight: 'The key principle is "dependencies point inward" â€” the core domain never depends on infrastructure. This is the same principle behind Clean Architecture (Uncle Bob) and Onion Architecture. Mention this connection in interviews.',
  },
  {
    id: 'service-mesh',
    name: 'Service Mesh',
    emoji: 'ğŸ•¸ï¸',
    category: 'Infrastructure Pattern',
    summary: 'A dedicated infrastructure layer that handles service-to-service communication, providing features like load balancing, encryption, observability, and resilience â€” without changing application code.',
    diagram: `
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Service A           â”‚    â”‚ Service B           â”‚
  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
  â”‚ â”‚   App Code      â”‚ â”‚    â”‚ â”‚   App Code      â”‚ â”‚
  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â–²â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚    â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
  â”‚ â”‚  Sidecar Proxy  â”‚â—€â”¼â”€â”€â”€â”€â”¼â–¶â”‚  Sidecar Proxy  â”‚ â”‚
  â”‚ â”‚  (Envoy)        â”‚ â”‚    â”‚ â”‚  (Envoy)        â”‚ â”‚
  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚    â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                         â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
                  â”‚Control Planeâ”‚ (Istio, Linkerd)
                  â”‚ (config,   â”‚
                  â”‚  policies) â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    `,
    whenToUse: [
      'Large microservices deployment (50+ services) needing uniform communication policies',
      'Need mTLS encryption between all services without code changes',
      'Require observability (distributed tracing, metrics) across services',
      'Complex traffic management (canary deployments, A/B testing, traffic splitting)',
      'Polyglot services â€” mesh handles communication regardless of language',
    ],
    whenNotToUse: [
      'Small number of services (< 10) â€” overhead isn\'t justified',
      'Monolithic application â€” no service-to-service communication',
      'Team lacks Kubernetes expertise (most meshes require K8s)',
      'Latency-critical paths where sidecar overhead matters (~1ms per hop)',
    ],
    realWorldExamples: [
      { company: 'Google', useCase: 'Created Istio; runs internal equivalent for all Google services' },
      { company: 'Uber', useCase: 'Uses service mesh for traffic management across 4000+ microservices' },
      { company: 'Lyft', useCase: 'Created Envoy proxy, now the standard sidecar in most service meshes' },
    ],
    tradeoffs: {
      pros: [
        'Zero-code networking features â€” mTLS, retries, timeouts, circuit breaking',
        'Uniform observability â€” distributed tracing and metrics for all services',
        'Traffic management â€” canary deploys, traffic splitting, fault injection',
        'Security â€” mTLS between all services, RBAC policies',
        'Language agnostic â€” works with any language since it\'s at the network layer',
      ],
      cons: [
        'Operational complexity â€” another layer to manage, debug, and upgrade',
        'Latency overhead â€” sidecar proxy adds ~1-3ms per hop',
        'Resource consumption â€” each sidecar uses CPU and memory',
        'Steep learning curve â€” Istio configuration is notoriously complex',
        'Debugging â€” network issues now involve proxy configuration',
      ],
    },
    interviewBuzzwords: [
      'sidecar proxy', 'Envoy', 'Istio', 'Linkerd', 'control plane', 'data plane',
      'mTLS', 'traffic splitting', 'canary deployment', 'observability',
    ],
    keyInsight: 'A service mesh separates application logic from networking logic. Think of it as a "smart network" for microservices. In interviews, mention the data plane (sidecars) vs control plane (configuration) distinction.',
  },
  {
    id: 'saga',
    name: 'Saga Pattern',
    emoji: 'ğŸ“–',
    category: 'Data Pattern',
    summary: 'Manage distributed transactions across multiple services using a sequence of local transactions, each with a compensating action for rollback.',
    diagram: `
  Choreography Saga:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  event   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  event   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚Order   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚Payment â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚Shippingâ”‚
  â”‚Service â”‚         â”‚Service â”‚         â”‚Service â”‚
  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”˜
       â”‚ fail?            â”‚ fail?            â”‚ fail?
       â–¼                  â–¼                  â–¼
  compensate         compensate         compensate
  (cancel order)     (refund)           (cancel ship)

  Orchestration Saga:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Saga         â”‚â”€â”€â–¶ Step 1: Create Order
  â”‚ Orchestrator â”‚â”€â”€â–¶ Step 2: Process Payment
  â”‚              â”‚â”€â”€â–¶ Step 3: Ship Order
  â”‚              â”‚â”€â”€â–¶ On failure: run compensations in reverse
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    `,
    whenToUse: [
      'Distributed transactions spanning multiple microservices',
      'Long-running business processes (order â†’ payment â†’ shipping â†’ delivery)',
      'When 2PC (two-phase commit) is too slow or not available',
      'Need to maintain data consistency across services without shared DB',
    ],
    whenNotToUse: [
      'Single database â€” just use ACID transactions',
      'Simple operations that don\'t span services',
      'When strong consistency is absolutely required (sagas provide eventual consistency)',
      'Compensations are not possible (e.g., sending an email can\'t be "undone")',
    ],
    realWorldExamples: [
      { company: 'Amazon', useCase: 'Order processing saga: reserve inventory â†’ charge payment â†’ ship (compensate: restock â†’ refund â†’ cancel)' },
      { company: 'Uber', useCase: 'Ride booking saga: match driver â†’ start trip â†’ process payment (compensate on failure at each step)' },
      { company: 'Booking.com', useCase: 'Reservation saga: hold room â†’ charge card â†’ confirm (compensate: release room â†’ refund)' },
    ],
    tradeoffs: {
      pros: [
        'Enables distributed transactions without 2PC coordination',
        'Each service maintains ACID locally â€” simpler than distributed ACID',
        'Supports long-running processes that span minutes/hours',
        'Better availability than 2PC â€” no global lock',
        'Choreography version is fully decentralized',
      ],
      cons: [
        'Eventual consistency â€” intermediate states are visible to users',
        'Compensating transactions are hard to design and test',
        'Complex to debug â€” saga state spread across services',
        'Idempotency required â€” compensations may be retried',
        'Orchestrator can become a single point of failure (orchestration variant)',
        'No isolation â€” "dirty reads" of intermediate states possible',
      ],
    },
    interviewBuzzwords: [
      'compensating transaction', 'choreography vs orchestration', 'distributed transaction',
      'two-phase commit (2PC)', 'eventual consistency', 'idempotency', 'saga state machine',
    ],
    keyInsight: 'In interviews, always compare choreography (event-based, decentralized but hard to track) vs orchestration (central coordinator, easier to manage). Recommend orchestration for complex sagas with many steps, choreography for simple 2-3 step flows.',
  },
  {
    id: 'api-gateway',
    name: 'API Gateway',
    emoji: 'ğŸšª',
    category: 'Infrastructure Pattern',
    summary: 'Single entry point for all client requests, handling cross-cutting concerns like authentication, rate limiting, routing, and response aggregation.',
    diagram: `
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Mobile â”‚  â”‚  Web   â”‚  â”‚ 3rd    â”‚
  â”‚  App   â”‚  â”‚  App   â”‚  â”‚ Party  â”‚
  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
            â”‚API Gatewayâ”‚
            â”‚â€¢ Auth     â”‚
            â”‚â€¢ Rate Limitâ”‚
            â”‚â€¢ Routing  â”‚
            â”‚â€¢ Aggregateâ”‚
            â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”
  â”‚User Svcâ”‚ â”‚Order   â”‚ â”‚Product â”‚
  â”‚        â”‚ â”‚Svc     â”‚ â”‚Svc     â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    `,
    whenToUse: [
      'Microservices architecture â€” single entry point simplifies client code',
      'Need centralized auth, rate limiting, and logging',
      'Different clients need different API shapes (BFF pattern)',
      'Response aggregation â€” combine data from multiple services in one call',
      'API versioning and traffic management',
    ],
    whenNotToUse: [
      'Simple monolithic application â€” not needed',
      'When it becomes a bottleneck or single point of failure',
      'Overly complex routing logic that should be in services',
    ],
    realWorldExamples: [
      { company: 'Netflix', useCase: 'Zuul gateway handles billions of requests/day, A/B testing, canary routing' },
      { company: 'Amazon', useCase: 'API Gateway service used by AWS customers; internally uses similar patterns' },
      { company: 'Kong', useCase: 'Open-source API gateway used by many companies for plugin-based API management' },
    ],
    tradeoffs: {
      pros: [
        'Centralized cross-cutting concerns â€” auth, logging, rate limiting in one place',
        'Client simplification â€” clients talk to one endpoint, not many services',
        'Response aggregation â€” reduce number of client-server round trips',
        'Protocol translation â€” REST externally, gRPC internally',
        'Versioning â€” handle API versioning at the gateway level',
      ],
      cons: [
        'Single point of failure â€” must be highly available',
        'Latency â€” adds a network hop to every request',
        'Bottleneck risk â€” all traffic flows through it',
        'Complexity â€” gateway routing logic can become complex',
        'Coupling â€” changes in services may require gateway updates',
      ],
    },
    interviewBuzzwords: [
      'BFF (Backend for Frontend)', 'reverse proxy', 'rate limiting', 'request aggregation',
      'API composition', 'Zuul', 'Kong', 'protocol translation', 'edge service',
    ],
    keyInsight: 'Mention the BFF (Backend for Frontend) variant: different API gateways for mobile vs web clients, each tailored to the client\'s needs. This is a common Netflix/Spotify pattern and shows depth in interviews.',
  },
  {
    id: 'strangler-fig',
    name: 'Strangler Fig (Migration)',
    emoji: 'ğŸŒ¿',
    category: 'Migration Pattern',
    summary: 'Incrementally replace a legacy system by routing traffic between old and new implementations, gradually "strangling" the old system until it can be decommissioned.',
    diagram: `
  Phase 1:           Phase 2:           Phase 3:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Proxy/ â”‚         â”‚ Proxy/ â”‚         â”‚  New   â”‚
  â”‚ Router â”‚         â”‚ Router â”‚         â”‚ System â”‚
  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚                 / \\
  â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â–¼â”€â” â”Œâ–¼â”€â”€â”€â”€â”
  â”‚  Old   â”‚     â”‚ Old  â”‚ â”‚ New â”‚      Old system
  â”‚ System â”‚     â”‚(shrk)â”‚ â”‚(grow)â”‚     decommissioned
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”˜
  100% old       50/50            100% new
    `,
    whenToUse: [
      'Migrating from monolith to microservices incrementally',
      'Replacing a legacy system without big-bang rewrite risk',
      'Need to keep serving users during migration (zero downtime)',
      'Risk mitigation â€” can roll back individual components',
    ],
    whenNotToUse: [
      'Small system that can be rewritten quickly',
      'Legacy system is well-understood and rewrite is straightforward',
      'No proxy/routing layer available to split traffic',
    ],
    realWorldExamples: [
      { company: 'Amazon', useCase: 'Migrated from monolith to services over many years using strangler fig approach' },
      { company: 'Shopify', useCase: 'Gradually extracting services from their Rails monolith' },
      { company: 'BBC', useCase: 'Migrated iPlayer from legacy to new platform using strangler fig pattern' },
    ],
    tradeoffs: {
      pros: [
        'Low risk â€” incremental migration, can stop at any point',
        'Zero downtime â€” users are served throughout migration',
        'Rollback capability â€” route traffic back to old system if new fails',
        'Team learning â€” team builds confidence with each migrated piece',
      ],
      cons: [
        'Longer migration timeline â€” can take months/years',
        'Routing complexity â€” proxy must manage old vs new correctly',
        'Data synchronization â€” may need to keep old and new DBs in sync during transition',
        'Cost â€” running two systems in parallel is more expensive',
        'Risk of "permanent strangler" â€” migration never completes',
      ],
    },
    interviewBuzzwords: [
      'incremental migration', 'legacy modernization', 'proxy routing', 'feature toggle',
      'parallel run', 'big-bang rewrite (anti-pattern)',
    ],
    keyInsight: 'In interviews, contrast this with "big-bang rewrite" â€” which almost always fails (Netscape Navigator being the classic cautionary tale). Strangler fig is the safe path. Named after a fig tree that grows around a host tree until it replaces it.',
  },
  {
    id: 'circuit-breaker',
    name: 'Circuit Breaker',
    emoji: 'âš¡',
    category: 'Resilience Pattern',
    summary: 'Prevent cascading failures by wrapping calls to downstream services. When failures exceed a threshold, the circuit "opens" and fails fast instead of waiting for timeouts.',
    diagram: `
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚        Circuit Breaker          â”‚
          â”‚                                 â”‚
  Request â”‚  CLOSED â”€â”€â–¶ OPEN â”€â”€â–¶ HALF-OPEN â”‚
  â”€â”€â”€â”€â”€â”€â–¶ â”‚  (normal)  (reject) (test 1    â”‚
          â”‚    â”‚         â”‚       request)   â”‚
          â”‚    â”‚ fails   â”‚ timer  â”‚         â”‚
          â”‚    â”‚ > N     â”‚expires â”‚ success?â”‚
          â”‚    â–¼         â–¼       â”‚ â”€â”€â–¶CLOSEDâ”‚
          â”‚  count++   fail fast â”‚ fail?    â”‚
          â”‚             return   â”‚ â”€â”€â–¶OPEN  â”‚
          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    `,
    whenToUse: [
      'Calling unreliable external services or APIs',
      'Preventing cascade failures in microservices',
      'Downstream service has intermittent failures or high latency',
      'Need fast failure instead of slow timeout',
    ],
    whenNotToUse: [
      'Calling a local library or in-process component',
      'Operations where retry is better than failing fast',
      'Internal calls within a monolith',
    ],
    realWorldExamples: [
      { company: 'Netflix', useCase: 'Hystrix library (now deprecated in favor of Resilience4j) â€” pioneered circuit breaking in microservices' },
      { company: 'AWS', useCase: 'AWS SDK built-in circuit breakers for API calls to prevent hammering degraded services' },
      { company: 'Uber', useCase: 'Circuit breakers on payment service calls â€” fail fast and queue for retry vs blocking ride requests' },
    ],
    tradeoffs: {
      pros: [
        'Prevents cascading failures across services',
        'Fails fast â€” better UX than waiting for timeout',
        'Gives downstream service time to recover',
        'Provides fallback mechanism (cached data, default response)',
        'Reduces load on failing services',
      ],
      cons: [
        'Adds complexity to service calls',
        'Tuning thresholds is tricky â€” too sensitive = false opens, too lenient = slow detection',
        'Need monitoring to track circuit state',
        'Fallback logic must be designed for each endpoint',
        'Half-open state testing can cause request spikes',
      ],
    },
    interviewBuzzwords: [
      'Hystrix', 'Resilience4j', 'cascading failure', 'fail fast', 'fallback',
      'bulkhead', 'timeout', 'retry with exponential backoff', 'half-open state',
    ],
    keyInsight: 'Always pair circuit breaker with other resilience patterns in interviews: retries with exponential backoff, bulkhead (isolate pools), and timeouts. These together form the "resilience trinity" for microservices.',
  },
  {
    id: 'sidecar',
    name: 'Sidecar Pattern',
    emoji: 'ğŸï¸',
    category: 'Infrastructure Pattern',
    summary: 'Deploy a helper process alongside each service instance to handle cross-cutting concerns like logging, monitoring, networking, and security â€” without changing the service code.',
    diagram: `
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚          Pod / Host          â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
  â”‚  â”‚  Main    â”‚ â”‚  Sidecar  â”‚ â”‚
  â”‚  â”‚  Service â”‚ â”‚  (proxy/  â”‚ â”‚
  â”‚  â”‚  (app)   â”‚ â”‚  logging/ â”‚ â”‚
  â”‚  â”‚          â”‚ â”‚  monitor) â”‚ â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
  â”‚    localhost communication   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    `,
    whenToUse: [
      'Adding observability (logging, tracing) without code changes',
      'Service mesh proxies (Envoy as sidecar)',
      'Polyglot services â€” add same capability to services in different languages',
      'Security (mTLS termination) as infrastructure concern',
    ],
    whenNotToUse: [
      'Simple applications that don\'t need the overhead',
      'When sidecar latency or resource consumption is unacceptable',
      'Capabilities better implemented as libraries in the service itself',
    ],
    realWorldExamples: [
      { company: 'Kubernetes/Istio', useCase: 'Envoy sidecar proxy injected into every pod for service mesh' },
      { company: 'Datadog', useCase: 'Agent runs as sidecar to collect logs, metrics, and traces from services' },
      { company: 'AWS App Mesh', useCase: 'Envoy sidecar for traffic management in ECS/EKS' },
    ],
    tradeoffs: {
      pros: [
        'Language agnostic â€” same sidecar works with any tech stack',
        'Separation of concerns â€” networking/observability separate from business logic',
        'Independent deployment â€” update sidecar without changing service',
        'Consistent behavior across all services',
      ],
      cons: [
        'Resource overhead â€” each sidecar consumes CPU/memory',
        'Latency â€” adds network hop (localhost, but still)',
        'Complexity â€” more moving parts to manage and debug',
        'Lifecycle management â€” sidecar must start before service',
      ],
    },
    interviewBuzzwords: [
      'sidecar proxy', 'Envoy', 'Kubernetes pod', 'service mesh data plane',
      'cross-cutting concerns', 'polyglot', 'DaemonSet vs Sidecar',
    ],
    keyInsight: 'The sidecar is the building block of service meshes. Understanding sidecars shows you understand how Istio/Linkerd work under the hood. In Kubernetes, a sidecar is simply another container in the same pod sharing the network namespace.',
  },
  {
    id: 'data-mesh',
    name: 'Data Mesh',
    emoji: 'ğŸ—ºï¸',
    category: 'Data Architecture',
    summary: 'Decentralized data architecture where domain teams own their data as products, with self-serve infrastructure and federated governance. Applies microservices principles to data.',
    diagram: `
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚  Orders     â”‚  â”‚  Users      â”‚  â”‚  Payments   â”‚
  â”‚  Domain     â”‚  â”‚  Domain     â”‚  â”‚  Domain     â”‚
  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
  â”‚ â”‚  Data   â”‚ â”‚  â”‚ â”‚  Data   â”‚ â”‚  â”‚ â”‚  Data   â”‚ â”‚
  â”‚ â”‚ Product â”‚ â”‚  â”‚ â”‚ Product â”‚ â”‚  â”‚ â”‚ Product â”‚ â”‚
  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Self-Serve Data    â”‚
              â”‚  Infrastructure     â”‚
              â”‚  + Federated Gov.   â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    `,
    whenToUse: [
      'Large organizations with many data-producing domains',
      'Central data team is a bottleneck â€” domains should own their data',
      'Need for domain-specific data models and SLAs',
      'Data lake/warehouse has become a swamp â€” unclear ownership',
    ],
    whenNotToUse: [
      'Small organization with few domains',
      'Single team handles all data â€” no organizational need',
      'Limited data infrastructure maturity',
      'Domains don\'t have engineering capacity to manage data products',
    ],
    realWorldExamples: [
      { company: 'Zalando', useCase: 'Early adopter of data mesh, domains publish data products for analytics and ML' },
      { company: 'Netflix', useCase: 'Domain-oriented data pipelines where each team owns their data quality' },
      { company: 'Thoughtworks', useCase: 'Zhamak Dehghani coined data mesh; Thoughtworks promotes it as the future of data architecture' },
    ],
    tradeoffs: {
      pros: [
        'Domain ownership â€” teams closest to data manage it',
        'Scalable organization â€” no central data team bottleneck',
        'Data quality â€” domain experts define and maintain data contracts',
        'Agility â€” domains evolve data products independently',
      ],
      cons: [
        'Organizational change required â€” not just a tech pattern',
        'Duplication risk â€” multiple domains may store similar data',
        'Governance complexity â€” federated governance is harder than centralized',
        'Infrastructure cost â€” self-serve platform is expensive to build',
        'Cross-domain queries are harder without centralized warehouse',
      ],
    },
    interviewBuzzwords: [
      'data product', 'domain ownership', 'self-serve data infrastructure',
      'federated governance', 'data as a product', 'data contract', 'data catalog',
    ],
    keyInsight: 'Data mesh applies the four principles of microservices to data: domain ownership, data as a product, self-serve infrastructure, and federated governance. It\'s an organizational pattern more than a technical one â€” mention this nuance in interviews.',
  },
];
